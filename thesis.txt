   VIRTs: Volumetric Illustrative Rendering
Techniques to Improve X-Ray Vision Perception
    on Optical See Through Mixed Reality



               Thomas John Clarke
                      BSoftwEng(Hons)




        A dissertation presented for the degree of
             DOCTOR OF PHILOSOPHY




        Australian Research Centre for Interactive
                and Virtual Environments

                       March 2025



                      Supervisors:
           A. Prof. Ross Smith (Coordinating)
                A. Prof. Wolfgang Mayer
                    Dr. Joanne Zucco
                                Abstract
    Seeing through objects is a desirable trait not just for superheroes trying to
escape from a burning building or locate a villain but for any professional who
needs to consider what they cannot yet see. This can be related to a construction
worker who needs to know what is going on the floor above them or a surgeon who
needs to know where an object is within the human body.
    Augmented Reality (AR) X-ray vision can extend a user’s sight into concealed
spaces and through virtual objects to reveal hidden areas in the real world. However,
this functionality hasn’t been well explored using Optical See-Through (OST) Aug-
mented Reality (AR). OST AR devices allow users to perceive both the real world
and digital overlays, enhancing their value for context switching between tasks that
require virtual and real-world information. The ability to see the real world and the
digital overlay simultaneously makes occlusion difficult, which is further impacted
by the limited color gamut of current OST AR displays, impacting colors like darker
colors and blacks the worse. This occurs because OST AR displays use optical
combiners that direct light from a microdisplay into the user’s eyes while allowing
real-world light to pass through. Less vibrant colors on a OST AR devices emit
or reflect less light, making them appear dimmer on an optical see-through display.
The physical and computational constraints of current OST AR devices pose further
challenges for realizing X-ray vision for volumetric information. These challenges
raise significant questions about designing volumetric X-ray vision in OST AR while
minimizing depth misinterpretation and enhancing alignment accuracy.
    To address these challenges, this dissertation aims to develop an effective method
of X-ray vision on OST AR devices for displaying volumetric visualizations. It also
examines how different visualization styles influence human depth perception in sce-
narios demanding precise spatial understanding. This research’s findings establish a
foundation for designing future X-ray vision visualizations and systems aimed at as-
sisting humans in stressful and dangerous tasks where precise spatial comprehension
is essential.
    Discerning objects and understanding their shape is a key concern for X-ray vi-
sualizations, and the way objects are visualized can profoundly affect this ability.
X-ray visualizations investigated in prior research were extended to use an Optical
See Through (OST) Augmented Reality (AR) device. A study that examines the
accuracy of depth perception and object placement in a mobile AR X-ray vision sce-
nario between edge-based, saliency-based, and occlusion-based X-ray vision methods
was investigated. Demonstrating that occlusions provide significant cues for depth
perception but may hinder accuracy in placement tasks
    Inspiring the visualizations developed in this dissertation, I coined a new term
called Volumetric Illustrative Rendering Techniques (VIRTs), which captures a new
category of volumetric techniques for OST AR, which adapt illustrative techniques
that convey the shape of objects in 2D illustrations. The Hatching, Stippling, and
Halo VIRTs are the first x-ray visualizations specifically designed for OST AR de-
vices and used independently with direct volume rendering. This thesis demon-
strates that VIRTs can effectively enhance spatial understanding in X-ray vision on
OST AR devices and showcases the efficient algorithms applied in a system that
achieves these effects.
    This dissertation conducts three controlled user studies focusing on depth percep-
tion, object recognition, and spatial accuracy in OST AR environments to evaluate
the effectiveness of the visualizations. Through controlled user studies, it exam-
ines their effectiveness and impact on object placement, perception, and depth of
understanding of X-ray visualizations and VIRTs. The first study examines the
accuracy of depth perception and object placement in a mobile AR X-ray vision
scenario, demonstrating that occlusions provide significant cues for depth percep-
tion but may hinder accuracy in placement tasks. The perception study investigates
spatial understanding in complex hierarchical scenarios, revealing that outline-based
visualizations are the most effective for object identification among those examined
in this thesis. The depth perception study assesses the limits of depth perception
using visualizations, and the results indicate that users can discern differences in
the depth placement of irregular objects at sub-centimeter resolution.
    This dissertation lays the foundation for future innovations in OST AR based X-
ray visualizations. Providing groundwork could enhance future X-ray applications,
particularly in improving depth perception through adaptive rendering and advances
in AR hardware. These advancements will enhance applications in medical imaging,
remote inspections, and augmented training, making complex spatial tasks more
intuitive and precise.




                                          ii
Acknowledgments

This was a long journey filled with emotion, drama, and friendship. I wholeheartedly
regret taking on this PhD. At the same time, it did expose me to research, and I
loved the work. This was an unimaginably difficult time of my life, which forced
me to grow at an incredible rate and come face-to-face with challenges like abuse
and poverty. Still, my time with the Wearable Computer Lab was fun. Like-minded
people surrounded me, and you all made me grow as a person immensely. I already
miss the freedom being surrounded by so much technology afforded me. Despite
the stress and drama that plagued me all this time. Thanks to you, I have many
positive memories and stories to tell.
    First, I should thank my supervisors. I am sorry for all (well, most) of the
arguments and heated debates. Most people would have left me once they realized
that the research was not what they hoped for and that it was struggling to get out
there. This was a hard PhD, there is no doubt about it, but I did not help it much.
Still, I doubt that I should have done a PhD; I must have been quite a handful.
    Dr. Joanne Zucco, you always told me what I needed to hear, whether it was
a hard truth or providing silence and listening. You are an amazingly gifted per-
son, and I doubt I would have maintained my sanity without you. Thank you for
leaving your comfort zone to be the person I needed and staying on board the ship
throughout this turbulent journey.
    Dr. Wolfgang Mayer, you always looked at things from the other angle; you
found critics of my work that no one in my field, without you pushing me to do
more and move further, I would not be where I am today. I appreciate that you
seem to be the person I believe you are to this day. It was an honor and privilege
to have you as a supervisor.
   Dr. Ross T. Smith, you must have gone through a lot for me, things that I
probably never want to hear about. Thank you for being there when I got emotional
and for listening to me complain about my reviewers every time. Thank you for not
abandoning me when I lost my temper because my research hit a dead end. Thank
you for turning a blind eye when I was a broken human. Thank you for pouring so
much work into me, even while you did not benefit much. You are a great supervisor
and a talented manager. I sincerely wish you the best for the rest of your career,

                                         iii
and I hope things become more interesting for you.
    To the other people who helped me aid my research. Dr. Brandon Matthews,
Thank you for being the intermediary between Ross and me and for talking me
through some of the most technical conversations I may ever get to have. I would
also like to extend a thank you to Dr. Adam Drogemuller. You helped me grow
my understanding of data visualization experiences and helped me become a better
data analyst.
    To everyone with whom I was lucky enough to experience my Ph.D., thank you
for being there and for being good people (for the most part). Thank you for helping
me when I needed it, letting me stir you up, and not telling me off for distracting you
and having interesting problems. You all made my PHD worthwhile and eventful,
and the stories we have made I will keep telling to people for the rest of my life.
    I would also like to thank the staff of UNISA (now the University of Adelaide)
for making me a stronger person by constantly presenting me with challenges that I
had to overcome at some of the most difficult times. Regardless of my health, be it
from cancer, depression, or heart conditions, you maintained your dedicated focus
on this task. Your efforts took me from a good manager to an excellent one as I
learned how to fix the issues you manufactured in a subtle manner. After working
with you all, I do not think there is any problem I can not handle.
    I would also like to thank Dr. Ameliorate and Bradly Ross Richards for providing
me with your artwork for this thesis. My friends (you know who you are) for putting
up with me being broke, time-poor, and miserable for years on end (I am surprised
you hang around). I want to thank my employers over these years during this Ph.D
who allowed me to keep my house, Dr. Michael Ulpen from Sabit University, and
everyone One Forty One, especially my employer, Dr. Jan Rombouts, for allowing
me to keep a roof over my head and understanding when I was being put in a difficult
position. And a special thanks to the team at Siemens Healthineers for teaching me
the skills I needed to know for this Ph.D.
    Finally, thank you to my family. I want to thank my mum (Kylie Dunstan) for
housing me when I returned to the country during the COVID-19 pandemic and
could not find a suitable place to rent. Also, my sister (Paige Wijeratne), along
with my Mum, listened to me stress out and lose my hair over having no money and
being harassed on all sides of my life. It must have been hard having to experience
that. I also want to thank my partner (Nguyen Ha Chau) and her mother (Ha Thi
Duy Tra) for feeding me and caring for me while I was busy writing my thesis and
making sure that I left my computer at least once a month and not only ate food
but ate healthily. This thesis would have harmed me more if not for your efforts.
    This research was supported by an Australian Government Research Training
Program (RTP) Scholarship.


                                          iv
Thesis Declaration


I, Thomas Clarke, declare that:

Thesis presents work carried out by myself and does not incorporate without
acknowledgment of any material previously submitted for a degree or diploma in
any university; to the best of my knowledge, it does not contain any materials
previously published or written by another person except where due reference is
made in the text; and all substantive contributions by others to the work
presented, including jointly authored publications, is clearly acknowledged.

This thesis does not contain material that has been accepted for the award of any
other degree or diploma in my name, in any university or other tertiary institution.

No part of this work will, in the future, be used in a submission in my name, for
any other degree or diploma in any university or other tertiary institution without
the prior approval of The University of South Australia.

This thesis does not contain any material previously published or written by
another person, except where due reference has been made in the text.

The work(s) are not in any way a violation or infringement of any copyright,
trademark, patent, or other rights whatsoever of any person.

The research involving human data reported in this thesis was assessed and
approved by The University of South Australia Human Research Ethics
Committee. Approval #: 203273

Signature:




Date: 7/04/2025




                                         v
vi
Acronyms

2AFC Two-alternative forced choice 36, 72, 187, 188, 195, 210, Glossary: Two-
   alternative forced choice

AABB Axis Aligned Bounding Box 135, 141, 158, Glossary: Axis Aligned Bound-
   ing Box

AI Artificial Intelligence 1, 6, Glossary: Artificial Intelligence

AR Augmented Reality i, 3–5, 7–12, 14, 15, 21, 34, 37–45, 47, 50, 51, 54, 56, 59,
   61, 62, 65–67, 73–83, 85, 91, 93–95, 97, 118, 120, 122, 124, 127, 136, 137, 140,
   141, 149–151, 162, 165, 166, 180, 183, 185–188, 194, 207–214, 262, 263, 267,
   Glossary: Augmented Reality

BFS Breadth-First Search 138, Glossary: Breadth-First Search

CAVE Cave Automatic Virtual Environment 33, 37, 40, 73, Glossary: Cave Auto-
   matic Virtual Environment

CT Computed Tomography 1, 2, 6, 7, 10, 24, 28–31, 42, 43, 56, 80, 125–127, 130,
    132, 133, 135, 144, 153, 154, 211, 214, 261, 264, 271, Glossary: Computed
    Tomography

CTC Circulating Tumor Cells Glossary: Circulating Tumor Cells

DI Digital Imaging Glossary: Digital Imaging

DICOM Digital Imaging and Communications in Medicine 1, 40, 133, 138, 151,
   271, 272, Glossary: Digital Imaging and Communications in Medicine

DVR Direct Volume Rendering 5, 7–11, 14, 15, 26–33, 35, 36, 44, 79, 81, 125, 127–
   144, 149–151, 162, 166, 167, 185–188, 192, 194, 203, 205, 208–210, 214, 261,
   264, 267, 272, Glossary: Direct Volume Rendering

FPS Frames Per Second 91, 98, 135, Glossary: Frames Per Second

                                          vii
     Acronyms


GAN Generative Adversarial Nets 155, Glossary: Generative Adversarial Nets

GPR Ground Penetrating Radar 126, Glossary: Ground Penetrating Radar

GPU Graphics Processing Unit 27, 128, 131, 139, 143, 147, 265, Glossary: Graphics
   Processing Unit

GUI Graphical User Interface 5, Glossary: Graphical User Interface

HCI Human-Computer Interaction 13, 209, Glossary: Human-Computer Interac-
    tion

HMD Head Mounted Device 3–5, 7–11, 14, 15, 37, 38, 40–44, 49, 55, 56, 62, 65,
   66, 71–76, 78–80, 82, 91, 93, 94, 100, 124, 126, 127, 129, 136, 137, 139–141,
   165, 166, 185, 194, 208–210, 213, 263, 265

HSD Honestly Significant Difference 109, 172

HU Hounsfield Units 138–140, 143, Glossary: Hounsfield Units

IMU Inertial Measurement Unit 122, Glossary: Inertial Measurement Unit

JND Just Noticeable Difference 203, 204, Glossary: Just Noticeable Difference

LMM Linear Mixed Effects Models 107, 109, 111–115, 197, 199, 200, Glossary:
   Linear Mixed Effects Models

MR Mixed Reality 5, 9, 13, 15, 22, 27, 33, 34, 37, 38, 42, 71, 126, 129, 139, 144,
   164–166, 190, 265, Glossary: Mixed Reality

MRI Magnetic Resonance Imaging 1, 2, 6, 28–31, 42, 80, 125–127, 132, 133, 135,
   144, 153, 154, 211, 214, 261, 264, Glossary: Magnetic Resonance Imaging

OST Ocular See Through 1, 4, 8–11, 14, 40–44, 56, 65–67, 73–83, 85, 91, 93–95,
   118, 120, 122, 124, 127, 137, 138, 140, 141, 150, 151, 165, 166, 180, 183, 187,
   194, 207–214, 263, 267, Glossary: Ocular See Through

PC Personal Computer 272, Glossary: Personal Computer

PSE Point of Subject Equality 203, Glossary: Point of Subject Equality

R&D Research and Development Glossary: Research and Development

SAR Spatial Augmented Reality 37, 38, 40, 52, 53, 56, 57, 65, 67, 69, 106, 262,
   Glossary: Spatial Augmented Reality

                                       viii
                                                                          Acronyms


SDF Sign Distance Field 132, 155, 158, 194, Glossary: Sign Distance Field

SHS School of Health Sciences Glossary: School of Health Sciences

SLAM Simultaneous Localization and Mapping 85, Glossary: Simultaneous Local-
   ization and Mapping

SQL Structured Query Language Glossary: Structured Query Language

SUS System Usability Scale 104, Glossary: System Usability Scale

T Tesla 138, 139, Glossary: Tesla

UI User Interface 33, 67, 160, 162, 265, Glossary: User Interface

VIRT Volumetric Illustrative Rendering Techniques 11, 22, 125, 127, 144, 145,
    147, 148, 150, 151, 160, 162–170, 172–189, 192, 197–200, 202–205, 207–210,
    212–214, 265–267, 269, 295

VR Virtual Reality 3, 38, 40, 41, 43, 47, 74, 75, 78, 136, 161, 162, Glossary: Virtual
   Reality

VST Video See Through 3, 4, 10–12, 14, 34, 40, 42, 58, 59, 65, 66, 74–76, 78, 79,
   81–84, 93, 94, 97, 141, 208, Glossary: Video See Through




                                          ix
Acronyms




           x
Glossary

Artificial Intelligence A branch of computer science that focuses on creating in-
     telligent machines capable of performing tasks that typically require human
     intelligence. Artificial Intelligence (AI) involves developing algorithms and
     models that enable computers to process data, learn from it, and make deci-
     sions or predictions. AI finds applications in a wide range of fields, including
     natural language processing, machine learning, robotics, and data analysis,
     revolutionizing industries and enhancing automation and problem-solving ca-
     pabilities. 1

Augmented Reality A technology that overlays digital information and virtual
   objects onto the real world, enhancing the user’s perception and interaction
   with their environment through a smartphone or smart glasses device. By
   blending the physical and digital realms, AR provides immersive experiences
   and practical applications across various industries, ranging from entertain-
   ment and gaming to education and industrial design. i, vii, 3–5, 7–12, 14, 15,
   21, 34, 37–45, 47, 50, 51, 54, 56, 59, 61, 62, 65–67, 73–83, 85, 91, 93–95, 97,
   118, 120, 122, 124, 127, 136, 137, 140, 141, 149–151, 162, 165, 166, 180, 183,
   185–188, 194, 207–214

Axis Aligned Bounding Box A rectangular cuboid aligned with the coordinate
     axes, typically used in computer graphics and computational geometry to en-
     close a set of objects or to define the spatial extent of an entity. Axis Aligned
     Bounding Boxes (AABBs) are commonly employed in collision detection al-
     gorithms, rendering optimizations, and spatial partitioning techniques. They
     provide a simple and efficient way to approximate the geometry of complex
     shapes and facilitate various operations in virtual environments, including aug-
     mented reality applications. 135

Breadth-First Search An algorithm for traversing or searching tree or graph data
    structures. It starts at the root (or an arbitrary node) and explores all nodes
    at the present depth level before moving on to nodes at the next depth level.
    BFS is commonly used in shortest path problems in unweighted graphs, level-

                                          xi
     Glossary


     order traversal of a tree, and in scenarios where we need to explore all nodes
     at the same distance from the starting node. 138


Cave Automatic Virtual Environment Cave Automatic Virtual Environment,
    or CAVE, is an immersive virtual reality environment that utilizes multiple
    large displays and surround sound to create a fully immersive experience. Users
    typically stand within a room-sized cube, and the walls, floor, and ceiling are
    used as screens to display virtual content. A CAVE provides a highly immer-
    sive and interactive virtual reality experience, enabling users to navigate and
    interact with digital environments naturally and intuitively. It finds applica-
    tions in fields such as scientific visualization, architectural design, and medical
    research. 33

Classifier A classifier is an algorithm in machine learning and statistics used to
     assign categories or labels to data points based on input features. It operates
     by learning patterns from labeled training data and then predicting the cat-
     egories of new, unseen data. Classifiers are used in various applications such
     as image recognition, spam detection, medical diagnosis, and more. 24

Computed Tomography A medical imaging technique that uses X-ray technol-
   ogy to create detailed cross-sectional images of the body. Computed Tomogra-
   phy (CT) scans provide valuable diagnostic information by capturing multiple
   X-ray images from different angles and combining them to generate a three-
   dimensional representation. CT scans are commonly used to diagnose various
   conditions and guide medical procedures. 1, 24, 125, 211, 271,


Digital Imaging and Communications in Medicine A standard for managing,
     storing, and transmitting medical images and related data. DICOM (Digital
     Imaging and Communications in Medicine) enables interoperability between
     different medical imaging devices and systems, ensuring compatibility and con-
     sistent communication of medical information. It is widely used in healthcare
     settings to store and exchange medical images, such as X-rays, CT scans, and
     MRIs. 1, 40, 133, 271,

Direct Volume Rendering A technique in computer graphics and visualization
     that allows for the direct generation of images from volumetric data, such as
     medical scans or scientific simulations. DVR enables the exploration and visu-
     alization of complex three-dimensional structures and phenomena by applying
     various rendering algorithms and shading techniques. 5, 15, 81, 125, 166, 185,
     208, 272,

                                          xii
                                                                            Glossary


Frames Per Second A metric that measures the number of individual frames or
    images displayed in one second. In the context of digital displays, including
    augmented reality, a higher frame rate, measured in frames per second (FPS),
    contributes to smoother and more fluid visual experiences. Higher FPS is
    particularly important in applications such as gaming and video playback. 91,
    135

Gantry The circular or cylindrical structure of a CT scanner that houses the X-
    ray tube and detector array. The gantry rotates around the patient during
    the scanning process, capturing X-ray images from various angles. It plays a
    crucial role in producing cross-sectional images of the body, which are then
    used to reconstruct detailed 3D images. The design of the gantry allows for
    precise positioning and accurate imaging during CT scans. 7

Generative Adversarial Nets A class of artificial intelligence algorithms intro-
    duced by Ian Goodfellow and his colleagues in 2014. Generative Adversarial
    Nets (GANs) consist of two neural networks, a generator, and a discrimina-
    tor, which are trained simultaneously through adversarial training. GANs are
    widely used for generating realistic synthetic data, image-to-image translation,
    style transfer, and other tasks in the realm of artificial intelligence and machine
    learning. 155

Graphical User Interface A type of user interface that allows users to interact
    with a computer or software application through graphical elements such as
    icons, buttons, windows, and menus. Graphical User Interfaces make it easier
    for users to perform tasks and access functions by providing visual representa-
    tions of actions and options. GUIs are commonly used in operating systems,
    software applications, and websites, enhancing user accessibility and usability.
    5

Graphics Processing Unit A specialized electronic circuit designed to accelerate
    the processing of images and videos for display on a computer screen. Graph-
    ics Processing Units (GPUs) are essential components in rendering realistic
    graphics for applications such as gaming, video editing, and complex simula-
    tions. Their parallel processing capabilities make them well-suited for handling
    large-scale graphical computations. 27, 128

Ground Penetrating Radar A geophysical method that uses radar pulses to im-
    age the subsurface. Ground Penetrating Radar (GPR) is employed for de-
    tecting and mapping features underground, such as utilities, archaeological
    artifacts, and geological structures. It plays a crucial role in various fields,

                                         xiii
     Glossary


     including civil engineering, environmental assessment, and archaeological ex-
     ploration. 126

Head Mounted Device Technological devices that are worn on the head and typ-
    ically feature a display screen, enabling users to experience immersive visual
    and auditory content. Head Mounted Devices, such as virtual reality headsets
    or augmented reality glasses, provide a user-centric interface and are widely
    used for gaming, virtual simulations, training, and entertainment purposes. 3,
    15, 82, 126, 165, 185, 208

Hounsfield Units A unit of measurement used in computed tomography (CT)
    imaging to quantify the radiodensity of tissues. Hounsfield Units (HU) assign
    a numerical value to tissue attenuation, where higher values represent denser
    or more radio-opaque structures and lower values indicate less dense or more
    radio-lucent structures. HU values help in differentiating and characterizing
    various tissues and abnormalities in medical imaging. 138

Human-Computer Interaction A field of study focusing on the design and use
   of computer technology, particularly the interactions between humans (the
   users) and computers. HCI is concerned with the ways humans interact with
   computers and design technologies that let humans interact with computers
   in novel ways. It involves the study of how people use technology, the design
   of user-friendly interfaces, and the development of new interaction techniques
   to improve the user experience. 13, 209

Inertial Measurement Unit A sensor module that combines multiple inertial
     sensors, such as accelerometers, gyroscopes, and magnetometers, to measure
     and track an object’s motion and orientation in real time. Inertial Measure-
     ment Units (IMUs) are commonly used in applications that require motion
     sensing, such as robotics, virtual reality, and motion capture systems. IMUs
     provide essential data for estimating objects’ position, velocity, and orientation
     changes. 122

Just Noticeable Difference The smallest change in a stimulus that can be de-
     tected by an observer, typically defined as the threshold at which a difference
     becomes perceptible to a human sensory system. In the context of visual per-
     ception, the Just Noticeable Difference (JND) refers to the minimum change
     in brightness, color, or other visual attribute that can be perceived by the
     human eye. Understanding JND is crucial in various fields, including image
     processing, display technology, and user interface design, to ensure optimal
     user experiences and minimize perceptual errors. 203

                                         xiv
                                                                          Glossary


Linear Mixed Effects Models A statistical modeling approach that incorporates
     both fixed effects and random effects to analyze data with hierarchical or clus-
     tered structures. LMMs are used to account for within-group dependencies
     and capture individual variations, making them suitable for studying longi-
     tudinal or repeated measures data in various fields, including social sciences,
     biology, and psychology. 107, 197


Magnetic Resonance Imaging A medical imaging technique that uses strong
   magnetic fields and radio waves to create detailed images of the internal
   structures of the body. Magnetic Resonance Imaging (MRI) provides high-
   resolution images that help diagnose various conditions and assess the health
   of organs and tissues. MRI scans are widely used in medicine for their non-
   invasive nature and ability to visualize soft tissues with excellent contrast. 1,
   28, 125, 211

Microsaccades Microsaccades are tiny, involuntary eye movements that occur even
    when we try to maintain fixation on a point, serving important functions in
    visual perception and attention by preventing sensory adaptation and aiding
    in gathering additional visual information from the environment. 92

Mixed Reality A technology that merges elements of both the physical and vir-
    tual worlds, creating a seamless and interactive environment. Mixed Reality
    combines aspects of both Augmented Reality (AR) and Virtual Reality (VR),
    allowing users to interact with digital objects while maintaining awareness of
    the real world. This technology finds applications in fields such as gaming,
    architecture, training, and collaborative workspaces. 5, 15, 126, 164, 190


Non-Euclidean Space A type of geometric space that is not based on the postu-
    lates of Euclidean geometry. In non-Euclidean space, the parallel postulate of
    Euclidean geometry does not hold, leading to the development of hyperbolic
    and elliptic geometries. These spaces have unique properties and are used in
    various fields, including physics, computer science, and cosmology, to model
    complex structures and phenomena. 55


Ocular See Through A technology that provides a transparent display, allowing
    users to view the real world while overlaying digital information or virtual
    objects onto their field of vision. By enabling the simultaneous perception
    of the physical and digital environments, OST enhances user experiences and
    finds applications in various fields, including gaming, navigation, and indus-
    trial training. 1, 40, 81, 127, 165, 187, 207

                                         xv
     Glossary


Pathological Relating to the study of diseases or disorders in living organisms.
    Pathological conditions involve abnormal changes in structure or function that
    can lead to health issues. Pathological studies aim to understand diseases’
    causes, mechanisms, and effects, often involving laboratory analysis, imaging,
    and clinical observations. see 5

Pathologist A medical professional who specializes in the study and diagnosis of
    diseases. Pathologists analyze tissues, cells, and bodily fluids to understand
    the nature and causes of illnesses. Their expertise is crucial in providing
    accurate diagnoses, guiding treatment decisions, and contributing to medical
    research. 5, 6, 261

Personal Computer A type of computer designed for individual use, typically
     consisting of a central processing unit (CPU), memory, storage, and input/output
     devices. Personal computers (PCs) are widely used for various purposes, in-
     cluding work, communication, entertainment, and personal productivity. They
     offer a flexible and customizable computing platform for users to perform tasks,
     run software applications, and access the internet. 272,

Perspective-Corrected Projection A technique used in computer graphics to
    ensure that textures and objects are rendered accurately from the viewer’s
    perspective. This method corrects distortions that can occur in standard pro-
    jections, providing a more realistic and visually consistent representation of
    3D objects on a 2D screen. Perspective-corrected projection is essential for
    applications such as virtual reality, gaming, and simulations to enhance the
    immersive experience and maintain visual fidelity. 52

Point of Subject Equality In psychophysics, the Point of Subject Equality (PSE)
     refers to the stimulus intensity at which a subject perceives two stimuli as be-
     ing equal in some specific aspect, such as brightness, loudness, or size. It is a
     key concept used to study perceptual phenomena and understand the relation-
     ship between physical stimuli and subjective perception. The determination
     of PSE plays a crucial role in various fields, including sensory research, human
     factors engineering, and user experience design. 203

Regression Analysis A statistical method used to examine the relationship be-
    tween one dependent variable and one or more independent variables. The
    goal of regression analysis is to model the expected value of the dependent
    variable in terms of the independent variables, allowing for prediction and
    understanding of the underlying patterns. It is widely used in fields such as
    economics, biology, engineering, and social sciences. 24

                                         xvi
                                                                           Glossary


Sign Distance Field A mathematical representation used in computer graphics
     to define the distance from a point in space to the nearest surface of an object.
     Sign Distance Fields (SDFs) are commonly employed in rendering techniques
     such as ray marching and distance field rendering. They allow for efficient and
     accurate rendering of complex shapes and can be used in various applications,
     including augmented reality, computer-aided design, and video games. 132,
     194

Simultaneous Localization and Mapping Simultaneous Localization and Map-
    ping (SLAM) is a technique used in robotics and computer vision to construct
    a map of an unknown environment while simultaneously tracking the position
    of the observer within that environment. SLAM systems utilize sensor data,
    such as camera images or laser scans, to estimate the robot’s trajectory and
    create a map of its surroundings. This technology is essential for autonomous
    navigation in various applications, including self-driving cars, drones, and mo-
    bile robots. 85

Spatial Augmented Reality A technology that enhances the physical environ-
     ment by projecting digital content onto real-world surfaces, such as walls,
     floors, or objects. Spatial Augmented Reality (SAR) combines projection map-
     ping techniques with computer vision and spatial tracking to create interactive
     and immersive experiences. SAR finds applications in areas such as art instal-
     lations, architectural visualization, and interactive displays. 37, 106

System Usability Scale A widely used questionnaire-based method for assessing
     the usability of a system or product. The System Usability Scale (SUS) con-
     sists of a set of standardized statements and Likert scale responses that provide
     quantitative measures of perceived usability, effectiveness, and user satisfac-
     tion. SUS scores are commonly used in user experience research to evaluate
     and compare the usability of different systems or product designs. 104

Tesla A unit of measurement for the magnetic field strength used in Magnetic
     Resonance Imaging (MRI). The Tesla is the standard international unit, rep-
     resenting the strength of the magnetic field influencing the behavior of atomic
     nuclei during the imaging process. Higher Tesla values often contribute to im-
     proved image resolution and quality in MRI, impacting diagnostic capabilities
     in medical imaging. 138

Two-alternative forced choice Two-alternative forced choice (2AFC) is a psy-
    chophysical method used to measure the detectability or discriminability of a
    stimulus. In a 2AFC task, the observer is presented with two alternatives and

                                         xvii
     Glossary


     must choose which one matches the target stimulus or is different from a ref-
     erence stimulus. This method is commonly used in perceptual experiments to
     assess sensory thresholds and performance in various domains, such as vision,
     audition, and touch. 36, 71, 187, 210

Ultrasound A medical imaging technique that uses high-frequency sound waves to
     visualize internal structures of the body. Ultrasound imaging, also known as
     sonography, produces real-time images by emitting sound waves and capturing
     their reflections off tissues and organs. It is commonly used for non-invasive
     imaging of various body parts, such as the abdomen, heart, and developing
     fetus, and plays a crucial role in diagnosing medical conditions. 1

User Interface The point of interaction between a user and a digital or mechanical
    device, system, or software application. User interfaces encompass visual,
    auditory, and tactile elements that allow users to interact with and control
    devices or software, making it easier for humans to operate and communicate
    with machines. Effective user interfaces are crucial for ensuring a positive user
    experience and optimizing usability. 33, 160

Video See Through A technology that utilizes a video feed from cameras to over-
    lay digital information or virtual objects onto the real world, allowing users to
    see their surroundings through a display device. By merging live video footage
    with computer-generated graphics, VST enhances user perception and enables
    interactive experiences in fields such as gaming, medical imaging, and remote
    assistance. 3, 34, 81, 141, 208

Virtual Reality A technology that creates a simulated and immersive digital envi-
     ronment, typically experienced through a head-mounted display and motion-
     tracking devices. It transports users to a computer-generated world, allowing
     them to interact with realistic or fantastical scenarios, offering a range of ap-
     plications from entertainment and gaming to training and therapy. 3, 38, 136

Voxel A volumetric pixel, or voxel, represents a value on a regular grid in three-
    dimensional space. Voxels are commonly used in 3D graphics, medical imaging,
    and scientific simulations to model spatial data, enabling efficient rendering
    and analysis of complex structures. Unlike traditional 2D pixels, voxels contain
    depth information, making them essential for volumetric rendering and spatial
    partitioning. 26–28, 35, 127, 131, 133, 138–141, 143, 144, 157–159

X-ray A form of electromagnetic radiation commonly used in medical imaging to
    visualize the body’s internal structures. X-ray imaging involves passing X-
    ray photons through the body, with denser tissues absorbing more photons,

                                        xviii
                                                                           Glossary


     resulting in varying levels of brightness in the final image. X-rays are widely
     utilized for diagnosing bone fractures, detecting abnormalities, and examining
     structures like the chest, teeth, and bones. 1, 44

X-ray Vision A perceptual or technological capability, either fictional or simu-
    lated, that allows an observer to see through opaque objects and reveal their
    hidden interior structures or the environment beyond them. In practice, X-ray
    vision is achieved through imaging technologies or visual effects that mimic
    this ability, providing insights not accessible through direct surface observa-
    tion. 11–13, 15, 21, 22, 24, 28, 41, 44, 46–55, 57–68, 79–85, 87, 89, 96, 97, 102,
    110, 111, 113, 115, 116, 119–124, 126, 127, 138, 141, 142, 145, 146, 148–151,
    155, 161, 163, 165, 185, 187–189, 204, 205, 207–211, 213, 214, 261–265

X-ray Visualization A rendering or graphical technique used in computer graph-
    ics, medical imaging, and augmented reality to simulate the effect of looking
    through solid objects. Instead of physically penetrating materials as in real
    X-rays, this method selectively hides, fades, or highlights parts of a model
    or environment to reveal obscured internal structures or layers for analysis,
    learning, or interaction. 48, 50, 51, 53, 56–59, 61, 62, 64, 66, 79–85, 88, 91,
    93–98, 102–104, 106–117, 120, 121, 123–125, 142–144, 147, 150, 151, 162, 163,
    173–178, 183, 185, 207–209, 211, 213, 214, 262, 264–266, 269




                                         xix
Glossary




           xx
Contents

Acronyms                                                                              vii

Glossary                                                                              xi

1 Introduction                                                                         1
  1.1   Motivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    5
        1.1.1   Siemens Internship: Holographic Overlay System . . . . . . .           6
  1.2   Research Goal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    9
  1.3   Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
  1.4   Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
  1.5   List Of Publications . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
  1.6   Dissertation Structure . . . . . . . . . . . . . . . . . . . . . . . . . . 13

2 Background                                                                          15
  2.1   Human Perception and Depth Perception . . . . . . . . . . . . . . . . 15
        2.1.1   Depth Perception Fundamentals . . . . . . . . . . . . . . . . . 16
        2.1.2   Research into the Depth Perception of Color . . . . . . . . . . 21
  2.2   Illustrative Rendering Techniques . . . . . . . . . . . . . . . . . . . . 22
        2.2.1   Volumetric Illustrative Rendering Techniques . . . . . . . . . . 26
  2.3   Direct Volume Rendering (DVR) . . . . . . . . . . . . . . . . . . . . 28
        2.3.1   Visualizing Volumetric Data . . . . . . . . . . . . . . . . . . . 28
        2.3.2   Use Cases for Direct Volume Rendering . . . . . . . . . . . . . 30
        2.3.3   Human Computer Interaction (HCI) Experiments Using Di-
                rect Volume Rendering (DVR) . . . . . . . . . . . . . . . . . . 31
  2.4   An introduction to Augmented Reality (AR) and Virtual Reality (VR) 37
        2.4.1   Mixed Reality (MR) Hardware . . . . . . . . . . . . . . . . . . 37
        2.4.2   Designing Applications for Stereoscopic Displays . . . . . . . . 41
        2.4.3   Volume Rendering In Mixed Reality . . . . . . . . . . . . . . . 42
  2.5   Augmented Reality Enabled X-ray Vision . . . . . . . . . . . . . . . . 44
        2.5.1   Observations from Systematic Literature Review . . . . . . . . 46
        2.5.2   Research Exploring X-ray Vision . . . . . . . . . . . . . . . . 47

                                         xxi
    CONTENTS


        2.5.3   X-ray Vision Techniques . . . . . . . . . . . . . . . . . . . . . 48
        2.5.4   Applications of X-ray Vision . . . . . . . . . . . . . . . . . . . 63
        2.5.5   Hardware For X-ray Vision . . . . . . . . . . . . . . . . . . . . 64
  2.6   Perception and Depth Perception tasks in Augmented Reality (AR)
        and Virtual Reality (VR) Head Mounted Displays . . . . . . . . . . . 71
        2.6.1   Depth Perception User Studies on Ocular See Through (OST)
                Augmented Reality (AR) displays . . . . . . . . . . . . . . . . 73
  2.7   Research Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79

3 Spatial Estimation In Augmented Reality Aided X-Ray Vision                          81
  3.1   Updates Since Running This Study . . . . . . . . . . . . . . . . . . . 82
  3.2   Adopting X-ray Visualizations for Augmented Reality . . . . . . . . . 83
        3.2.1   Random Dot X-ray Visualization . . . . . . . . . . . . . . . . 85
        3.2.2   Tessellation X-ray Visualization . . . . . . . . . . . . . . . . . 85
        3.2.3   Edge-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
        3.2.4   Saliency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87
  3.3   Rendering Considerations . . . . . . . . . . . . . . . . . . . . . . . . 88
        3.3.1   Video Image Overlay Method . . . . . . . . . . . . . . . . . . 89
  3.4   Pilot Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
  3.5   User Study    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94
        3.5.1   Research Questions . . . . . . . . . . . . . . . . . . . . . . . . 95
        3.5.2   Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
        3.5.3   Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
        3.5.4   Study Design . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
        3.5.5   Study Variables . . . . . . . . . . . . . . . . . . . . . . . . . . 102
        3.5.6   Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
  3.6   Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
        3.6.1   Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . 107
        3.6.2   Time Required (H.5) . . . . . . . . . . . . . . . . . . . . . . . 111
        3.6.3   Subjective Results (H.8 & H.9) . . . . . . . . . . . . . . . . . 115
  3.7   Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
        3.7.1   Accuracy of Placement (H.1) . . . . . . . . . . . . . . . . . . . 118
        3.7.2   Placement Accuracy from User Viewpoint by Axis (H.2 & H.3) 118
        3.7.3   Time Required (H.5) . . . . . . . . . . . . . . . . . . . . . . . 119
        3.7.4   User Behaviour Results (H.6 & H.7)        . . . . . . . . . . . . . . 119
        3.7.5   Subjective Results (H.8 & H.9) . . . . . . . . . . . . . . . . . 120
        3.7.6   Summary of Discussion . . . . . . . . . . . . . . . . . . . . . . 121
  3.8   Future Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 121
  3.9   Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123

                                         xxii
                                                                        CONTENTS


4 Designing X-ray Visualizations with Volume Rendering                              125
  4.1   Fundamentals of Volume Rendering . . . . . . . . . . . . . . . . . . . 127
        4.1.1   Technical Description of Volume Rendering . . . . . . . . . . . 127
        4.1.2   Visualizing Volume Data . . . . . . . . . . . . . . . . . . . . . 128
        4.1.3   Direct Volume Rendering (DVR) . . . . . . . . . . . . . . . . 128
        4.1.4   Light Simulation Using Direct Volume Rendering . . . . . . . 129
        4.1.5   Cinematic Rendering . . . . . . . . . . . . . . . . . . . . . . . 129
        4.1.6   Real Time Direct Volume Rendering . . . . . . . . . . . . . . 130
  4.2   Display Hardware for Stereoscopic Direct Volume Rendering . . . . . 132
        4.2.1   Designing Direct Volume Rendering for Volumetric Displays . 132
        4.2.2   Autostereoscopic Displays . . . . . . . . . . . . . . . . . . . . 134
        4.2.3   Stereoscopic Head Mounted Displays . . . . . . . . . . . . . . 136
        4.2.4   Implementation Details . . . . . . . . . . . . . . . . . . . . . . 138
  4.3   X-ray Vision Techniques for Direct Volume Rendering on Ocular See
        Through Augmented Reality Devices . . . . . . . . . . . . . . . . . . 141
  4.4   VIRT: Volumetric Illustrative Rendering Techniques . . . . . . . . . . 144
        4.4.1   X-ray Vision of Empirical Volumes . . . . . . . . . . . . . . . 144
        4.4.2   Halo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
        4.4.3   Stippling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
        4.4.4   Hatching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
  4.5   Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150

5 Random Volume Generator: Generating Irregular Hierarchical Ob-
  jects for Controlled User Studies                             153
  5.1   Noisy Hierarchical Spheres . . . . . . . . . . . . . . . . . . . . . . . . 155
  5.2   Random Volume Generator’s System Design . . . . . . . . . . . . . . 156
        5.2.1   System Design     . . . . . . . . . . . . . . . . . . . . . . . . . . 157
        5.2.2   Voxel-Based Verification of Volume Requirements . . . . . . . 158
        5.2.3   Immersive User Interface Design . . . . . . . . . . . . . . . . . 160
  5.3   Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161

6 Perception of Volumetric Illustrative Effects Visualizations within
  OST AR                                                             163
  6.1   Volumetric Illustrative Rendering Techniques . . . . . . . . . . . . . . 164
  6.2   User Study    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
        6.2.1   Research Questions . . . . . . . . . . . . . . . . . . . . . . . . 165
        6.2.2   Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
        6.2.3   Hypotheses . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
        6.2.4   Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168
        6.2.5   Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168

                                         xxiii
    CONTENTS


        6.2.6   Study Environment . . . . . . . . . . . . . . . . . . . . . . . . 171
  6.3   Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
        6.3.1   Accuracy (H.1 & H.3) . . . . . . . . . . . . . . . . . . . . . . 172
        6.3.2   Time Required (H.2 & H.4) . . . . . . . . . . . . . . . . . . . 173
        6.3.3   User Behavioural Analysis (H.5) . . . . . . . . . . . . . . . . . 174
  6.4   Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
        6.4.1   Accuracy (H.1 & H.3) . . . . . . . . . . . . . . . . . . . . . . 180
        6.4.2   Time Required (H.2 & H.4) . . . . . . . . . . . . . . . . . . . 181
        6.4.3   Participant Behaviour (H.5) . . . . . . . . . . . . . . . . . . . 181
        6.4.4   Subjective Results (H.6) . . . . . . . . . . . . . . . . . . . . . 182
  6.5   Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183

7 The Limits of Depth Perception when Using Volumetric Illustrative
  Rendering Techniques                                             185
  7.1   Volumetric Illustrative Rendering Techniques Impact on Depth Per-
        ception on Ocluar See Though Devices . . . . . . . . . . . . . . . . . 187
  7.2   User Study    . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
        7.2.1   Research Questions . . . . . . . . . . . . . . . . . . . . . . . . 188
        7.2.2   Hypothesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
        7.2.3   Participants . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
        7.2.4   Study Design . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
        7.2.5   Study Environment . . . . . . . . . . . . . . . . . . . . . . . . 193
        7.2.6   Generation and Placement of Volumes . . . . . . . . . . . . . 194
  7.3   Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195
        7.3.1   Psychometric Analysis of Depth Perception        . . . . . . . . . . 195
  7.4   Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
        7.4.1   Psychometric Analysis (H.1 & H.2) . . . . . . . . . . . . . . . 202
        7.4.2   Time Required (H.3) . . . . . . . . . . . . . . . . . . . . . . . 203
        7.4.3   User Behavioral Analysis (H.4) . . . . . . . . . . . . . . . . . 204
        7.4.4   Subjective Results (H.5) . . . . . . . . . . . . . . . . . . . . . 204
  7.5   Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205

8 Conclusion                                                                        207
  8.1   X-ray Vision Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . 207
  8.2   X-ray Visualizations for Direct Volume Rendering . . . . . . . . . . . 208
  8.3   Evaluation of Volumetric Illustrative Rendering Techniques . . . . . . 209
        8.3.1   Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
        8.3.2   Depth Perception . . . . . . . . . . . . . . . . . . . . . . . . . 210
  8.4   Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
        8.4.1   Augmented Reality Enabled X-ray Vision . . . . . . . . . . . . 211

                                         xxiv
                                                                        CONTENTS


         8.4.2   Direct Volume Rendering Displayed Using Ocular See Through
                 Augmented Reality . . . . . . . . . . . . . . . . . . . . . . . . 211
         8.4.3   Further Evaluations . . . . . . . . . . . . . . . . . . . . . . . . 213
   8.5   Final Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214

Bibliography                                                                       215

List of Figures                                                                    261

List of Tables                                                                     269

A Holographic Overlay System Details                                               271

B X-ray Vision Literature Review Methodology                                       273

C Class Diagram for Random Generating Volumes                                      275

D Chapter 5: Counting Everything Post Hoc                                          277

E Chapter 3’s Participant Comments on X-Ray Vision                                 281
   E.1 Favorite Visualizations and Why . . . . . . . . . . . . . . . . . . . . . 281
   E.2 General Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282

F Feedback from Chapter 5 Examing the Effect of Perception on
  VIRTs                                                      285
   F.1 Things That Were Liked and Disliked About VIRTs . . . . . . . . . . 285
         F.1.1 Halo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
         F.1.2 Hatching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
         F.1.3 No VIRT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
         F.1.4 Stippling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292

G Feedback from Chapter 6 Examining the limits of Depth Percep-
  tion                                                         295
   G.1 Things That Were Liked and Disliked About VIRTs . . . . . . . . . . 295
         G.1.1 Halo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
         G.1.2 Hatching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
         G.1.3 No VIRT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
         G.1.4 Stippling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
   G.2 Validation for Answers Given in the Post-Study Questionaire . . . . . 305
         G.2.1 What Made The Visualization Easy to use? . . . . . . . . . . 305
         G.2.2 What Made The Visualization Difficult to use?         . . . . . . . . 307

                                          xxv
    CONTENTS


      G.2.3 Why Do You Think You Performed Better With These Visu-
             alization? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
      G.2.4 Why Do You Think You Performed Worse With These visu-
             alization? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
  G.3 General Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
  G.4 Validation for Answers Given in the Post-Study Questionaire . . . . . 313
      G.4.1 What Made The Visualization Easy to use? . . . . . . . . . . 313
      G.4.2 What Made These visualization Difficult to use? . . . . . . . . 314
      G.4.3 Why Do You Think You Performed Better With These visu-
             alization? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
      G.4.4 Why Do You Think You Performed Worse With These visu-
             alization? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
  G.5 General Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319

H Statements of Authorship                                                      321




                                       xxvi
Chapter 1

Introduction

This thesis investigates visualisations that aim to understand and improve percep-
tion when using Ocular See Through (OST) augmented reality. The topic area has
been motivated by medical visualisations due to the need for data to be presented in
a way that aligns intuitively with the user’s working area, ensuring that overlaid in-
formation is easy to interpret and supports natural understanding. Although other
application areas, such as manufacturing [1], microbiological [2], geological [3,4], and
meteorological tasks [5], may also benefit from the perceptual support of the AR
visualizations of this research. Since the late 1980s, medical visualizations have pro-
vided healthcare professionals with a detailed and intuitive means of understanding
complex anatomical structures, physiological processes, and pathological conditions
within the human body. By translating intricate medical data into visual formats,
medical visualizations aid in diagnosis, treatment planning, and patient education.
Medical visualizations can be derived from an extensive range of imaging techniques
which can produce 3D datasets such as Computed Tomography (CT) scans, Mag-
netic Resonance Imaging (MRI) scans, ultrasounds, and X-rays as Digital Imaging
and Communications in Medicine (DICOM) files. These visualizations enable med-
ical practitioners to assess and interpret the intricate spatial relationships between
organs, tissues, and abnormalities accurately, facilitating more informed medical
decision-making. Moreover, these visualizations contribute to medical research by
offering insights into disease progression, treatment effectiveness, and the develop-
ment of innovative medical interventions.
    Medical imaging technologies can utilize advanced photorealistic rendering de-
rived from CT and MRI scans, Artificial Intelligence (AI) methods for image under-
standing, and computational methods providing decision support to doctors. These
technologies support medical professionals and help guide the planning and execu-
tion of procedures tailored to a patient’s specific needs. MRI images are typically
used to view details with soft tissues but are also used to avoid the radiation caused
by CT scanners. AI-based medical imaging is a newer technology used to view

                                           1
       1. INTRODUCTION




Figure 1.1: Images of CT and MRI films being utilized in a medical setting. Anna
Shvets took the photos shown here, which were licensed under a Creative Commons
Attribuation licence 1


and provide ongoing analysis of tissue or indicate any immediate changes to the
environment [6, 7].
    Medical visualizations are commonly designed with 2D data in mind, as these
tend to be viewed on desktop displays or tablets, or by more traditional display
methods like placing films on light boxes and lit boards. 2D visualizations require
training in order to understand them, however experienced users can make more
confident decisions with them due to understanding the structure of the human
body [8]. Traditionally, MRI and CT scans are printed onto transparent films and
placed on adjustable light displays as shown in Figure 1.1. These adjustable light
displays allow different scan slices to be visible in the film. These films are not easy
to interpret for people without proper training.
    Display technology employed to view tomographic data in radiology and related
fields can significantly impact data interpretation. Most medical visualizations dis-
played on a desktop typically represent 2D slices through the body. Desktop and
tablet interfaces will normally display this data in three different axes—one each
for depth, width, or breadth—with an additional display presenting a 3D model
for spatial awareness [9, 10]. This limitation may lead to solutions that only con-
sider the primary axis. This would allow surgeons to perform surgeries with more
flexibility than they would currently be comfortable with [11]. 3D visualizations
in some situations have been observed to be easier to understand, like identifying
the distance between the operating area and high-risk areas or different methods
to access the infected area [11–13]. This indicates that surgical planning could be
further optimized and enhanced if 3D visualizations were displayed in a technically
  1
      https://www.pexels.com/cs-cz/foto/ruka-doktor-ukazovani-lekar-4226264/


                                           2
Figure 1.2: Two similar slices of different human heads: on the left, one a Mag-
netic Resonance Imaging (MRI) scan1 ; and on the right a Computed Tomography
(CT) scan2 . Both these images are licensed under a Creative Commons Attribution
license.

correct manner which is also contextually appropriate rendering and presentation.
on their given displays and in the representative display environment.
    On desktop displays, 2D visualizations can present shapes more clearly than
3D visualizations [14, 15]. However, stereoscopic 3D displays (Augmented Reality
(AR) and Virtual Reality (VR)) have been shown to represent 3D medical data as
effectively as, or even better than, 2D visualizations [16]. Surgeons can navigate data
from any angle, leading to a more intuitive presentation and greater precision [17–
19]. These findings suggest that stereoscopic immersive Head-Mounted Displays
(HMDs) see allow users to perceive 3D data naturally and intuitively [20–22]. Most
research on medical AR has focused on 2D visualizations [23], likely due to the
challenges that still need to be addressed in using immersive AR devices for medical
applications.
    The use cases for AR in medicine are inspiring, but there is a range of lim-
itations that need to be addressed before AR becomes common practice [24, 25].
Firstly, medical practitioners may not risk losing sight of the real world [25]. This
can happen by placing any camera-based or occlusive display over the user’s eyes.
This makes a Video See Through (VST) AR HMDs solution difficult because these
systems have the potential to partially blind the user if they are unplugged or discon-
nected [25]. Systems that are designed for robotic surgery that utilize a video-based
surgery tend to use a robotic arm with inbuilt measures for removing any apparatus
from the patient where a well-trained expert can aid the situation [26], preventing
  1
      https://www.flickr.com/photos/reighleblanc/3854685038
  2
      https://pixabay.com/photos/head-magnetic-resonance-imaging-mrt-254863/


                                          3
     1. INTRODUCTION


any issues that can occur from minor distortions of the input images. AR HMDs
currently show a distorted view of reality, which can negatively influence depth per-
ception and mislead users when performing precise interactions depending on the
real world environment [27,28]. OST AR avoids the limitation of superimposing the
real world with the virtual world. However, because this technology simply overlays
information over the screen and covers the user’s eyes, it still faces limitations in
how the virtual world is perceived in comparison to the real world.
     OST AR devices misrepresent users’ depth perception compared to real-world
vision, even under ideal conditions. These devices are not designed to fully align
the virtual world with the user’s perception, offering a view of the real world that
is ’good enough’ for most applications. Some AR displays, such as OST AR, and
sensors, like infrared cameras, struggle under bright lighting conditions. This causes
inconsistent sensor readings and may reduce display visibility due to the OST AR’s
shaded lens used to reflect light as a display [29, 30]. This shaded lens struggles to
render transparent objects and can mandate a limited color gamut [29,30]. This lack
of visibility and distortions caused by the display diminish users’ spatial awareness
of the AR graphics [31–34]. These issues are less pronounced with mobile AR and
VST AR displays [35, 36].
     OST AR devices all misrepresent their users’ real depth perception compared to
their real-world vision, even under ideal circumstances. AR devices are not designed
to fully align the virtual world with the user’s vision; they provide a good enough
world viewpoint into the real world. Some AR displays (like OST AR displays) and
sensors (such as infrared cameras) do not work under bright lighting conditions as
they dim the colors shown in the display and introduce inconsistent sensor readings.
Users’ spatial awareness is also lessened when viewing graphics using AR [31–34].
OST AR require a shaded lens to properly reflect light back at the users [29, 30].
These issues do not exist as much with mobile AR [35]. OST AR displays also have
difficulties when rendering transparency due to their limited color gamut and how
it is used to inform brightness [29, 30].
     This dissertation explores methods for visualizing spatial data to ensure AR-
rendered graphics seamlessly integrate with physical-world objects, such as the hu-
man body. A key challenge for any AR system is accurately presenting objects
located within or behind other objects [37–40]. To understand the data’s location,
humans typically need specific cues to perceive an object inside another, such as
occlusion or a real-world metaphor like a hole. Without these cues, the data often
appears to hover in front of the occluding object, giving the impression that it is
positioned incorrectly.
     This research utilizes volume data as a medium, a 3D representation of space
representing a set of samples from a given position relating to a given value in this


                                          4
area of the given voxel [41]. volumes data allows this dissertation to move beyond
traditional research by creating visualizations that utilize Direct Volume Rendering
(DVR), allowing dynamic adjustments based on spatial and occlusion cues. This will
enhance depth perception and improve integration with the physical environment.
DVR still allows for the same functionality as other graphical formats but provides
new stereoscopic challenges that need to be overcome. This dissertation also employs
an off-the-shelf optical see-through (OST) AR device, the HoloLens 2 2 , which uses a
lightly shaded lens and a waveform display to render images to ensure repeatability
and consistency in testing these visualization techniques.


1.1       Motivations
Augmenting the real world using wearable devices (esp. HMDs) is a promising field
that has received much attention recently. Augmenting can take many forms, but
seeing through or inside objects is particularly interesting in several domains. It is
common for immersive medical visualizations to present volume data as a 3D model
so that it can be viewed from any direction. HMDs allow for a natural way to view
3D data as users change their perspective by moving around the volume [42, 43].
HMDs can be integrated with current systems to extend current working practices
like cardiography [42] and pathology [44].
    Hanna et al. [44] looked into the potential of Mixed Reality (MR) within patho-
logical use cases, for example, enabling remote communication for autopsies for
guidance and instruction using a 3D overlay and sharing scanned copies of spec-
imens. Pathologists, like many medical professionals, are required to be able to
keep a sterile environment. AR HMDs, like the HoloLens, utilize gesture commands
that can keep an environment sterile and avoid with a keyboard and mouse which
may contaminate the space. This hands-free interaction is particularly beneficial in
medical situations where sterility is paramount.
    A wide range of medical visualizations fall outside the scope of volumetric data
but benefit from 3D medical visualizations by using Graphical User Interface (GUI)
in conjunction with AR. Endoscopies use a long flexible tube with a camera and light
at one end to examine the inside of the body. Immersive approaches have created
visualizations that hover above the body to provide medical practitioners with an
indication of where inside of the body they are looking [45]. Other visualizations
have focused on guiding the practitioner with a Graphical User Interface (GUI) so
they can either instruct people on what to expect in a given situation, like explaining
to a nurse what tools she might be expected to use next [46], or they could be used
to try to communicate the angle and direction an incision should be performed using
  2
      https://www.microsoft.com/en-au/hololens


                                          5
     1. INTRODUCTION




Figure 1.3: A image of Hanna et al.’s [44] sterile system for Pathologists. Used
with permission from the College of American Pathologists


a 2D GUI projected on the area around the patient [47]. These applications could
benefit from better in situ visualizations at a single point, providing a more natural
interface and a more intuitive sense of data.
    Research has explored overlaying a visualization for instructing a practitioner
on using a syringe [48, 49]. Overlaying a MRI or CT visualization has also been
attempted on several occasions where it was found to to ensure that the medical
practitioner can focus on what they are doing while observing the patient sponta-
neously and also the practitioner with annotations and instructions [43, 43, 50, 51].
Visualizations can also educate the internal anatomy to less experienced practition-
ers or novices [37].


1.1.1    Siemens Internship: Holographic Overlay System
Enabling medical practitioners to more intuitively understand their patients and
helping to ease the education of medical data are intuitive benefits to companies
specializing in creating machines to produce this data. The benefits of data overlay
formed the basis of an early internship I undertook at Siemens in Forchheim, Ger-
many, where I explored the challenges still present in using scan data overlaid on
patients’ bodies. At the commencement of this Ph.D. I was supported to travel to
Siemens Forchheim, Bavaria, Germany, as an intern in the School of Health Sciences
Digital Imaging CT Research and Development Circulating Tumor Cells AI depart-
ment I was asked to create a system capable of overlaying the volumetric data from
a CT scanner and calibrating it to the bed of a CT scanner so it could be overlaid
on the patient. This allows radiologists to view the patient data as it is overlaid


                                          6
     Figure 1.4: The Holographic Overlay system in use with a CT scanner.


onto the patient and communicate with the current status of the CT machine to
track the patient.
    I was tasked with bringing the Booij et al. [52] research, which focused on a 2D
interface that used a Kinect to collect a patient’s anatomical data to aid radiologists
in placing electrodes in the correct position on the patient’s body utilizing a 3D
reconstruction of the patient’s body. The correct placement of electrodes on a
patient’s body can be a difficult procedure to learn as it requires knowledge of
where certain organs exist. It is possible to determine where to place an electrode by
evaluating the shape of the patient’s body. An issue with the Booij et al. [52] system
is that a 2D representation of a human is not always an accurate guide for varying
body types since organs, muscles, and other body parts may be misplaced causing
significantly different results [53,54]. By using a 3D HMD, AR display (HoloLens1),
a 3D overlay could be made to instruct any operator of the CT machine on how to
use the system.
    After the CT scan had been completed, the system would replace the electrode
guidance functionality with a visualization slice on a Sagittal Plane of the radio-
logical data. This was replaced in the 3D system with an iso-surface or a DVR
visualization that was to be superimposed over the patient’s body to better guide
and inform the operator of the quality of the CT scan. This was aimed at lowering
the required learning for CT operators.
    The gantry bed would need to be tracked, and its position known relative to the
patient and the gantry to provide the radiologist with instructions on where to place


                                          7
     1. INTRODUCTION


electrodes [52]. This allowed the volume to be visualized wherever the patient was,
and it also provided flexibility for the operator to apply the electrodes. The final
results of this system can be seen in Figure 1.4. The technical details of this system
are described in more detail in Appendix A.
    Porting DVR to a HMD AR display revealed more challenges than anticipated,
primarily due to the high processing cost and the resulting visualization appearing
slightly distorted in comparison to the surrounding environment. This distortion
was likely caused by the limited depth cues provided by OST AR displays, which
made it difficult for users to determine the correct spatial position of virtual objects
when physical objects were already present in the same location [55].
    One issue that was noted with the system was that the visualization was not
dynamic and was only able to show a subset of a single range of the data before
requiring to reload the information. What was desired was the ability to view all of
the data in a dynamic manner such as DVR. For medical imaging, this capability
is critical because important diagnostic features may not be confined to a single
slice or range but distributed throughout the entire scan volume. Being able to
see the whole volume at once allows users to explore the data more intuitively and
avoid missing relevant anatomical details or pathologies. Particularly those with
less specialized radiology training.
    A prototype visualization of this can be seen in Figure 1.5 which presents a cube
rendered with DVR, showing the same inflated abdomen dataset as in Figure 1.4,
developed at Siemens Healthineers. This visualization has several issues when viewed
through an AR display (Microsoft HoloLens), the visualization initially appeared
to be in the correct spatial position. However, the stereoscopic properties of the
HMD made it evident that the visualization was rendered as a cube rather than a
volumetric structure. As a result, this method was ultimately excluded from latter
iterations.
    Figure 1.4 depicts a challenge with the Holographic overlay system. All of the
visualizations that were supposed to be inside of the patient appeared smaller and
outside of them. With enough time, users learned to orient themselves around this
issue. This presentation of the data via AR HMD in this format seemed to cause
more confusion about anatomy than simply displaying it on an external monitor or
sitting above the patient.
    Overall, my Internship at Siemens highlighted that the current AR technologies
were limited, including:

  1. When examining whether different X-ray visualization effects influence how
     accurately users can perceive the spatial alignment of a virtual object embed-
     ded within a real-world object, there is a visual mismatch that occurs, causing
     the virtual object to appear misaligned. This issue is due to the difficulty in

                                           8
Figure 1.5: This is a image of a cube rendered to show the same data of an inflated
abdomen shown in Figure 1.4 but using DVR which was developed at Siemens
Healthineers.


      judging depth due to the lack of depth cues [55].

  2. Sub-centimetre precision poses a general challenge for visualization and per-
     ception using MR devices. The effect of OST AR visualizations on the user’s
     depth perception requires further investigation.

  3. There currently exists no standard method to present frequently changing
     volumetric data inside an object.

  4. Medical information is often represented as volumetric data with continuously
     varying properties, rather than as discrete surface objects. Allowing for more
     direct contextual customization of the visualizations in real time paired with
     the ability to see the entire volume at once not just the given surfaces of a set
     range. Conventional polygonal rendering methods struggle to represent this
     type of data in a complete and integrated way.


1.2     Research Goal
This research seeks to investigate the use of DVR within physical objects when
using OST AR HMDs. The holographic overlay system displayed this data out of
place. Figure 1.6 shows that the visualization is positioned correctly compared to

                                         9
     1. INTRODUCTION




Figure 1.6: An example of where it is difficult to interpret depth due to the absence
of depth cues. The circular object is a virtual object displayed against the wall in
the next room. Left) shows the room with no X-ray vision; center) shows the same
room with X-ray vision enabled. However, the lack of depth cues makes it impossible
to determine where the circular object in the next room is located. Right) shows
the same rooms as on the left, displayed using an isometric perspective to illustrate
where the items are actually located.


the camera. However, because it is superimposed, it appears much larger and closer
to the viewer. This was identified as a challenge to be addressed before systems
become practical.
    X-ray vision has been shown to repair this issue using virtual holes [37]. However,
this solution is not perfect. X-ray vision focused on medical situations, and many
of the solutions were designed to accommodate VST display technology rather than
OST HMDs, which is not practical in a medical situation [56]. The solutions ap-
peared to restrict the users’ view of the data, making tasks like determining the CT
scan quality more challenging than needed. The visualizations also utilized static
geometry to generate the X-ray visualization while preventing the required level of
flexibility.
    The high-stress occupations found in medicine demand a level of predictability
from the tools that they use. The influence on X-ray vision’s ability to hinder
perception has previously been tested [57], but how it obscures spatial understanding
is still unknown. There are also many unknowns about the ecological effects of X-ray
vision, as most research in this area has mainly focused on perceptual tasks.
    DVR itself has not been utilized on OST AR devices, despite having many ap-
plications in the medical domain alone. This, in turn, indicates there are a plethora
of unknown consequences that may arise from its utility. While creating X-ray vi-
sualizations that are designed to suit the OST AR display, it is important to note
that these visualizations need to both suit the tasks they are required for and simul-
taneously function regardless of the use case. Ensuring these visualizations should
suit the purposes where viewing DVR information within the object itself is rele-
vant, regardless if they are referring to manufacturing [58], biological [59] or surgical
tasks [37]. These visualizations should be designed to allow users to view surfaces


                                           10
of any shape. X-ray visualizations should improve depth perception while allowing
users to gain insight into the volume quickly.
    The following factors motivate this research to focus on investigating:

  • How DVR techniques can be developed for OST AR, particularly for X-ray
    vision;

  • If visual cues may aid in understanding spatial arrangement and relative depth
    perception;

  • Observing the effects of visual cues and the accuracy that can be achieved on
    an OST AR HMD display.


1.3     Research Questions
To address the research goals listed in Section 1.2 focusing on creating X-ray Vision
methods for DVR. This thesis will explore the following questions:

 R.1 How do X-ray visualization effects influence how accurately users can perceive
     the spatial alignment of a virtual object embedded within a real-world object?
     (Chapter 3);

 R.2 Can Volumetric Illustrative Rendering Techniques (VIRT)s aid a person’s com-
     prehension of a volume when determining individual objects using direct vol-
     ume rendering? (Chapter 6);

 R.3 What is the minimum difference in depth that participants can reliably distin-
     guish between volumetric objects, independent of any given VIRT? (Chapter
     7);


1.4     Contributions
This thesis contributes the following:

 C.1 A systematic literature review of X-ray Vision describing the work to date.
     Presents an overview of the studies performed and an analysis of the use cases.
     (presented in Chapter 2, published in P.3);

 C.2 An algorithm that allows VST AR x-ray visualizations to be displayed on an
     OST AR device. (described in Chapter 3, published in P.4);

 C.3 Adapting X-ray techniques from VST AR to OST AR. (Presented in Chapter
     4, published in P.6);

                                         11
      1. INTRODUCTION


C.4 A study comparing X-ray Vision effects that had only previously been used
    on VST AR to be viewed on OST AR device. Showing that geometry-based
    X-ray Vision techniques were better suited to OST AR devices (presented in
    Chapter 3, published in P.4);

C.5 implementation of Volumetric X-ray Vision utilizing Volumetric Illustrative
    Rendering Techniques (VIRTs). (described in Chapter 4, published in P.6);

C.6 The Random Volume Generation System. A tool to randomly generate vol-
    umes that allow for controlled user studies in X-ray Vision. (presented in
    Chapter 5, published in P.5);

C.7 A perception-based counting study to determine how well users could see the
    three VIRTs created. Findings show that different VIRTs could negatively and
    positively impact a user’s ability to perceive accurately and determine what
    elements were in a volume. (presented in Chapter 6);

C.8 A study to determine the different perceivable depth thresholds that the Hatch-
    ing Stippling and Halo VIRTs can have between each other and a baseline con-
    dition by utilizing a 2FCA psychophysical study design. (presented in chapter
    7);


1.5     List Of Publications
P.1 Clarke, T. J. (2021). Depth Perception using X-Ray Visualizations. 2021
    IEEE International Symposium on Mixed and Augmented Reality Adjunct
    (ISMAR-Adjunct), 483–486. https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00114
    Contributed To:

       • Chapter 1;

P.2 Smith, R. T., Clarke, T. J., Mayer, W., Cunningham, A., Matthews, B., &
    Zucco, J. E. (2020). Mixed Reality Interaction and Presentation Techniques for
    Medical Visualisations. In P. M. Rea (Ed.), Biomedical Visualisation: Volume
    8 (pp. 123–139). Springer International Publishing. https://doi.org/10.1007/978−
    3 − 030 − 47483 − 67
    Contributed To:

       • Chapter 1;
       • Chapter 2;




                                       12
 P.3 Clarke, T. J., Gwilt, I., Zucco, J., Mayer, W., and Smith, R T. (2024)
     Superpowers in the Metaverse: Augmented Reality Enabled X-Ray Vision in
     Immersive Environments. In Geroimenko V., Augmented and Virtual Reality
     in the Metaverse.
     Contributed To:

        • Chapter 2;

 P.4 Clarke, T. J., Mayer, W., Zucco, J. E., Matthews, B. J., & Smith, R. T.
     (2022). Adapting VST AR X-Ray Vision Techniques to OST AR. Proceed-
     ings - 2022 IEEE International Symposium on Mixed and Augmented Reality
     Adjunct, ISMAR-Adjunct 2022, 495–500. https://doi.org/10.1109/ISMAR-
     Adjunct57072.2022.00104
     Contributed To:

        • Chapter 3;

 P.5 T. J. Clarke, W. Mayer, J. E. Zucco and R. T. Smith, "Generating Pseudo
     Random Volumes for Volumetric Research," 2023 IEEE International Sympo-
     sium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), Sydney,
     Australia, 2023, pp. 266-270, doi: 10.1109/ISMAR-Adjunct60411.2023.00061.
     Contributed To:

        • Chapter 4;

 P.6 T. J. Clarke, W. Mayer, J. E. Zucco, A. Drogemuller and R. T. Smith, "Volu-
     metric X-ray Vision Using Illustrative Visual Effects," 2023 IEEE International
     Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),
     Sydney, Australia, 2023, pp. 769-771, doi: 10.1109/ISMAR-Adjunct60411.2023.00168.
     Contributed To:

        • Chapter 5;


1.6      Dissertation Structure
Following this chapter is the literature review (Chapter 2), which will focus on all
the related research done on MR with a focus on medical applications, followed
by a look into the research done on Human-Computer Interaction (HCI) regarding
volumetric data and finished with a systematic literature review on X-ray Vision
covering all of the previous work done over the past 31 years in this field.
    The results from the literature review were used to inform the study in Chapter
3, which looks at a variety of popular X-ray Vision effects that are designed to utilize

                                          13
     1. INTRODUCTION


a range of different AR HMDs. To achieve this, a system was developed that allows
VST AR techniques to be used on OST AR devices. This system was then used
to perform a spatial estimation task, enabling an in-depth evaluation of how these
effects function across different augmented reality hardware configurations.
    Chapter 4 investigates the technical work involved in volume rendering and de-
tails the complexities involved in creating DVR techniques for X-ray vision, con-
cluding in the creation of the Volumetric Illustrative Rendering Techniques (VIRTs).
This was followed by a modular method to create random volumes and details, the
Random Volume Generation system. The Random Volume Generation system al-
lows controlled studies to be conducted with volumes to access the VIRTs (described
in Chapter 5). The combination of the Random Volume Generation system and the
VIRTs is then utilized in Chapter 6 to run a perception-based user study designed to
determine how well a user can count regions within a volume. The chapter (Chapter
7) then took the VIRTs and tested the degree of depth thresholds users could reli-
ably distinguish depth by utilizing a 2FCA psychophysical study. The final chapter
draws together the results and findings of this dissertation (Chapter 8).




                                        14
Chapter 2

Background

This section begins by exploring the history of human vision research, tracing its
development to our current understanding of human perception and its integration
with Mixed Reality (MR) hardware. We then look at the research behind how
depth perception and perception can be considered with MR Head-Mounted Displays
(HMDs) see. Followed by a systematic literature review of Augmented Reality (AR)
enabled X-ray Vision. The discussion then shifts to volume rendering, emphasizing
human-centered research using Direct Volume Rendering (DVR), and concludes with
an analysis of illustrative effects and their applications within DVR.


2.1      Human Perception and Depth Perception
Understanding the mechanisms of how perception works has been a goal for humans
long before computing or MR devices. Perception has been studied since Democritus
conceived that sight was formed from small indivisible particles (460-371 BC). Since
then, we have learned the anatomical structures of eyes and that sight is processed in
the mind rather than the eye (1011 - 1021). During the 18th century, we started to
gain a more modern view of how people see light (based on the reflection of light off of
other objects), and we began to learn about how we observe beauty, aesthetics, and
apparent deceptions. Newton’s particle theory in Optics (1704) proposed that light
is composed of small particles that travel in straight lines. These particles change
direction and speed when they hit a reflective surface, dispersing into different colors.
This started a revolutionary shift in our perception of light. Later findings disproved
the belief in pure white light despite Goethe’s defense of Aristotle’s theory (1810).
This understanding of vision enabled technologies like motion images (developed in
1932), leading to the first motion picture projector (the Phantoscope) in 1895.
    In 1889, Gustav Fechner [61] coined psycho-physical, and we developed a metric
for quantitatively determining changes in people’s perception. Their study involved
seeing at what point a user could no longer determine if more or fewer dots were in

                                           15
     2. BACKGROUND




Figure 2.1: This graph of depth cues and distance provides guidelines for depth
perception in relation to the distance and key perception parameters. Used with
permission from Cutting and Vishton [60].


a pair of images. All pairs of images had ten more or fewer dots than the other one.
This study showed that the more dots placed on a page, the harder it is for someone
to determine the difference, creating the foundation of psychophysics analytics [61].
From this point, qualitative experiments on perception began to run, and it became
possible to understand precisely how human perception functioned. Leading to our
current understanding of topics like depth perception.


2.1.1     Depth Perception Fundamentals
Throughout the late 19th century and the 20th century, psychologists began to study
depth perception and came up with many factors to describe it. Most of these find-
ings believed that depth perception was created by utilizing accommodation, con-
vergence, motion perspective, binocular disparities, height of the visual field, aerial
perspective, occlusion, and the relative size and density of objects [60]. In 1995,
Cutting and Vishton took over a century’s worth of research and concluded that
the human ability to determine what parameters of depth perception are required
to be effective. Notably, not all depth cues are equal, and many are situational.
Figure 2.1 shows a breakdown of how these depth cues can be contrasted based on
how far away they are from each other. The obvious difference between the depth
of two objects is shown using the virtual axis, while the horizontal axis shows the


                                          16
Figure 2.2: Depth cues and placements: Several images showing various cases of
monoscopic forms of depth perception a) Shows an example of Occlusion trees that
are occluded behind the main tree; b) An example of relative size where the larger
trees a tree is the further in the foreground it seems to be; c) The field’s height
where the trees are located higher up in the image will seem farther away from the
user.


distance or the depth away from the viewer’s vision they are effective.
    Different forms of depth perception can be described by their utility in other
spaces (Illustrated in Figure 2.1). Personal space refers to anywhere within 2m of
the viewer, giving the viewer the required depth perception to interact with objects.
The action space relates to any distance between 2 and 25m where depth perception
functions by using the optical relationship between different objects. The vista space
works by utilizing the changes of color in the horizon [60].
    Occlusion is considered to have the most influence over any depth perception
technique [60] and is effective as long as both the occluder and the occluded are in
sight. The user can tell what object is closer to them [60]. However, occlusion itself
only reveals the order of these functions [60]. Occlusion power comes from it being
such an obvious depth cue [62], requiring only contrast, the opacity of objects, and
the assumption that the object is not changing its shape without the viewer knowing
about it.
    Relative size and density are based on the user’s current knowledge of the world
and only noticed a deduction in accuracy at distances past 5km away from the
viewer [37]. Relative size refers to someone’s ability to determine depth based on
how small it looks to them. Relative density refers to how densely these objects
seem to be clustered together. Figure 2.3 illustrates how together, these techniques
discern the depth of field away from an object. If the users are familiar with an
object, they can determine how far away it is. However, this is much more powerful
when there is more than one object in the distance. This depth cue, unlike occlusion,


                                         17
       2. BACKGROUND




Figure 2.3: Aerial Perspective, Relative Size, and Relative Density: An Image of
a mountain view in Bavaria, Germany. Several circles whose depths indicate how
forests are viewed at various depths are shown. A line showing the gradual effects of
the aerial perspective is also shown, indicating how it can be utilized to determine
depth. The background image is licensed under a Creative Commons Attribution
Universal 1.0 International license. 1


can be used to gain a more granular idea of depth.
    When looking off into the distance, objects may appear visibly lower the further
away they are from the viewer. This is the Height in the Visual Field shown in
Figure 2.3. This cue relies on the objects touching the ground, so things like airplanes
are no good. This cue tends to work very well when an object is within several meters
of a user (depending on its scale) and still works well up to about 1,000m away.
    Aerial Perspective Refers to our ability to look through transparent objects.
Generally, this refers to one’s ability to look through the water in the air, making
mountains look blue, and is also applicable underwater and when viewing gas-like
elements [60]. Transparent objects can be pretty rare, so most of these elements will
naturally be seen from a distance in Figure 2.3. This, however, may not be the need
to be the case when using computer graphics [60].
    Motion parallax is considered the depth cue relating to how we perceive motion.
  1
      https://pxhere.com/en/photo/965104



                                           18
Figure 2.4: Depth Cues and Motion: Schematic illustration of motion compo-
nents arising from observer translation and scene-relative object motion [63]. (a)
An observer fixates on a traffic light while moving to the right, as a car indepen-
dently moves left. (b–d) Depiction of the car’s image motion components related to
self-motion and object motion. (d) accounts for image inversion by the eye’s lens.
(b) If the car is stationary, it shows a leftward image motion due to the observer’s
movement. (c) If the car moves left while the observer moves right, the car’s image
motion also includes an object motion component. (d) The net image motion of
the car, vret, is to the right. This figure is licensed under a Creative Commons
Attribution license and was produced by French and Deangelis [63].


This seems foundational to how we perceive depth [64]. As seen in Figure 2.4, the
motion perspective normally relies on the user viewer focusing on an object that
is moving concerning them [63]. This could be looking at a ball they are about
to catch or a house in the distance while a user passes it in a car or some other
form of transport [63]. Motion is an effective depth cue within 15 meters, but as
Figure 2.1 illustrates, the effectiveness lessens when the objects are further away
from the viewer and declines when an object is within 2m of a viewer as they can’t
fully perceive the object [65].
    People determine the depth perception of objects based on how their eyes are fo-
cused, which can also be a useful depth cue [66]. Figure 2.5 illustrates how changing
the accommodation and convergence allows us to focus on a given object. Humans
can change the disposition between their eyes [67]. Accommodation relates to the
ability to change the shape for focus. Of the eye to see an object clearly and at dif-
ferent distances. Whereas convergence relates to the ability to turn the eyes to focus
on nearby objects inwardly. These eye movement behaviours have a limited range,
Figure 2.1 shows at closer distances to us, this depth cue works the most effectively
together, but it also shows as the eyes move farther apart, objects positioned further
in front of or behind the focal point become increasingly blurred [37].


                                         19
     2. BACKGROUND




Figure 2.5: Convergence and Accommodation: A depiction of how convergence
and accommodation work in the real world using Mixed Reality and OST AR. It
consists of 4 diagrams showing how accommodation and convergence work together
to better perception. The blured ducks indicate a point where a duck would not
be in focus to the viewer based on the position sitting in this position due to the
effect of convergence and accommodation. The frames below show examples of the
resulting perceived images of the objects in each diagram. Each of the four diagrams
showcases a human eye (the circular object) viewing some ducks. The point where
the eyes meet is their vergence or the point of convergence (depicted by the line).
The cone represents the accommodation, which focuses on the physical distance the
viewer is from the display. This image was inspired by work by Rosedaler. This is
licensed under a Creative Commons Attribution licence 2 .


   Since both eyes have a different view of the real world, the image each eyes
perceive is inherently different. This enables humans with several different abilities
that are processed in the brain:

  • Stereopsis : Refers to our human ability to assemble a 3D image from two
    2D images from each eye. Giving us the ability to see the world in 3D.

  • Diplopia : Also called double vision. This is what occurs with binocular
    disparity, which can’t be completely fixed by Stereopsis, leaving the viewer
    with two images that are not correctly aligned.

These cues give viewers a clear indication of how the world is around them and are
referred to in combination as binocular disparities. Binocular disparities are more
effective when viewed closer to the viewer as the further they are away from each
eye, the similar position they are in each eye [60].

                                         20
Figure 2.6: Three images of the Stanford bunny sitting behind a wall, each using
a different X-ray Vision effect. To the left, a simple depth cue by placing the bunny
behind a column. In the center, a virtual grid is placed over the physical wall to
explain to the viewer that the bunny is behind the wall. On the right is highlighting
the edge of the bricks with AR using edge detection to indicate that the bunny is
behind the wall.


    Other cues can give a viewer a better sense of depth. For example, it is possible
to directly tell the viewer how far things are by presenting them with a neat grid
texture. Explaining to a user exactly how big a world is can obviously present a
high level of depth perception. Objects with a high amount of contrast can also be
clearer to see; however, this can be seen as improving the relief size and density.
Finally, humans may process depth perception in ways we have yet to understand
fully; it is quite possible there is an aspect to living on earth, like gravity, that may
even have an effect on our sense of depth perception [60, 68].


2.1.2      Research into the Depth Perception of Color
This dissertation utilizes several colorful objects in its evaluations and creates two
studies that directly utilize someone’s ability to discern color from a specified area.
This particular section highlights the papers of note that researched the impact of
colors on depth perception. By understanding this research, the changes in depth
perception that various colors can provide were mitigated across this dissertation.
    Ping et al. [69] highlight the impact that colors can have on depth perception.
When talking about medical visualization in general, it is very common to have
several transparent layers in a single visualization. Some research has found that
people tend to perceive different colors as being closer or further away than oth-
ers [69, 70]. This section of the thesis discusses the research that has been factored
into considerations regarding how color influences depth perception.
    Aio and Li [70] wanted to test how the luminance and Contrast affected the
depth perception of transparent plans when viewed on a computer monitor. The
goal was to determine what methods could be utilized to make it more obvious
  2
      https://commons.wikimedia.org/wiki/User:Rosedaler


                                           21
      2. BACKGROUND


which object was behind the other. They had two conditions to achieve this. They
utilized luminance contact, testing the difference between changing the gradient
between light and dark to dark to light. They utilized four planes of different sizes to
distinguish depth perception. This study was conducted using an autoscopic display,
which allowed for the presence and absence of motion parallax and no binocular
parallax, giving the participants extra depth cues. Throughout this study, Aoi
and Li [70] noticed participants underestimated depth perception but it could be
improved by using either (or both) binocular parallax and motion parallax. They
also noted that occluding darker or lighter colors did not make much difference. Aio
and Li [70] next utilized the information from their prior study to test this data on
medical data and less difference between the choices of different colors.


2.2      Illustrative Rendering Techniques
Cutting and Vishton [60] claim there are several real-world elements to create depth
perception, which then need to be adapted for virtual displays [71]. However, artists
have been able to establish depth perception even when illustrating non-realistic
environments by using "Just Enough Reality" [72] to determine depth accurately.
    The latter part of this thesis looks at using Volumetric Illustrative Rendering
Techniques (VIRT)s as a method of X-ray Vision. Building on these perceptual
foundations, the challenge in computer graphics—and particularly in advanced vi-
sualization domains such as X-ray or mixed reality displays—is not just to replicate
the cues found in natural vision, but to enhance and adapt them for clarity and in-
sight. While perceptual mechanisms like stereopsis and texture gradients provide a
basis for spatial understanding, there are scenarios where simply mimicking the real
world is insufficient. Here, artistic illustrative effects become invaluable: by deliber-
ately emphasizing, abstracting, or revealing underlying structures, these techniques
enable viewers to "see" information that might otherwise remain obscured. In this
way, the migration from perception-driven rendering toward illustrative approaches
is not only a technical evolution, but also a creative one, leveraging artistic conven-
tions to extend the capabilities of visual communication in graphics.
    This section is going to look at how these effects have previously been used,
what their impact has been on computer science, and what their utility has been
when using MR devices. While this thesis only looks into a subsection of illustra-
tive techniques, limiting itself to either Hatching, Stippling, or Halos, the actual
definition of this is broader [73]. Illustrative techniques can also include the scope
of non-photorealistic rendering, like using cell-shading and the deformation of video
footage to look like it was produced by a pencil or paintbrush [73]. This section will
also examine some user studies that have investigated this effectiveness [73].

                                           22
    The largest examples of illustrative effects being used can be seen in scientific
textbooks like Gray’s anatomy [74]. This textbook utilized hatching’s ability to
communicate texture and depth using a black-and-white image. These images were
collected over years of diagramming the human body by directing unclaimed bodies
from workhouses and mortuaries. Due to their clarity and accuracy, they are still
widely used today.
    Early work in the field by Interrante et al. [75–77] looked at how illustrative
effects can aid the perception of transparent objects. Transparent objects make it
difficult to understand the exact surface of the shape that a transparent object is
formed as. This was done by pre-computing textures that used transparent and
opaque regions. The first work was done by creating several different textures,
including multiple methods that depict valleys and ridges, grids, and curvature
information [75].
    To improve the on their prior work [75], Interrante [77] created a visualization
that utilized valleys, ridges, and curvature to explain the objects’ flow. This texture
was calculated by drawing lines around the parts of the mesh with the highest
curvature and having them move toward the ridges and valleys of the shapes. This
dissertation extends this research by creating a texture that could be viewed from
all sides, requiring less computation when the object is viewed from different angles.
    The textures mentioned in Interrante et al.’s [76] work were later tested with a
tipping and a grid-based pattern on each. A user study was done to determine if
the direction of the lines or opacity affected users’ ability to determine the shape of
the surface. At the same time, the participant viewed the graphics on a stereoscopic
display. This study tested whether participants could accurately determine the
closest surface of one noisy sphere to another inside of it. The analysis did show that
texturing the object improved depth perception compared to a base line condition
of having no texturing effect, but there was no significant difference if participants
could determine which shell was the closer to themselves.

Hatching

The hatching which was developed in Interrante et al. [76] was later extended by
Hertzmann and Zorin [78]. Hertzmann and Zorin [78] developed an algorithm that
could translate hatching over to smooth surfaces by using a piecewise smooth sub-
division to reconstruct a smooth surface from the mesh to compute the necessary
qualities. This allowed for a surface-based rendering technique that worked much
like a shadow but also thinned the lines to the point of being invisible when they
were in the direct view of the camera. They then used a combination of noise gener-
ation and denoising functions to create human errors that would be seen in a work
of art.

                                          23
     2. BACKGROUND


    The system that Hertzmann and Zorin [78] created was not designed for real-
time interactions. This means that even simple actions like rotating around the
model are not possible. Praun et al. [79] pre-generated a tonal art map based on
different levels and then used these tonal art maps to determine the direction of the
hashes before drawing them on the objects themselves. This system allowed for a
wide variety of different configurations.
    This method of hatching was then furthered by Pelt et al. [80] and applied to
an iso-surface representing Computed Tomography (CT) data. Their algorithm
was modified to consist of just a geometry shader rather than a fragment shader,
removing the need for preprocessing the hatching. Rather, this system is able to
compute the curvature of the iso-surface and an appropriate direction for the hashing
in real time while providing a relatively fast frame rate, which would then create a
textured striped pattern over a 3D object.
    Another system was created by Lawonn et al. [81], which could run at even
faster rates than Pelt et al.’s [80] work as long as it receives extensive pre-processing.
It first identifies key regions: contours, defined by surface normal and view vector
perpendicularity, and feature regions, identified by maxima and minima in the mean
curvature field. Then, the direction of the lines is calculated directly from the
direction of the curvature.
    Hatching effects are also closely tied to brush-like effects, as they require the
system to understand brush direction and stroke size. Gerl and Isenberg [82] then
furthered the possible interactions of hatching and painterly effects. This technique
preprocessed a Classifier to segment areas of the 3D mesh, then it used a Regression
Analysis to choose the most appropriate direction of the stroke directions. To aid
the AI methods, users were also given several interactions that allowed them to
reconfigure the angle and direction of the effect [82].
    Lawonn et al. [83] furthered this technique and paired it with a visualization
of a cylinder, making the illustrative effects inside of it more apparent than the
effects outside. The cylinder gave a similar impression to X-ray Vision, where the
illustrative effects in the cylinder were clear and easy to see, while the effects outside
the cylinder were duller. The hatching was modified to work on a set of vessels, and
the caps of all the vessels and each location where the vessels split were identified
so the vessels could be rendered differently.
    Lawonn et al. [83] then ran a study comparing their version of hatching to the
same cylinder from their previous study [83] with pseudo-chroma depth rendering
and Phong shading. Users were asked to define the model’s depth tips of two
vessels. pseudo-chroma depth rendering used the chroma colors to indicate depth,
with more saturated colors being closer to the viewer and less saturated colors
being further away whereas Phong shading used light and dark to indicate depth.


                                           24
Figure 2.7: Examples detailing the stippling algorithm created by Pastor and
Strotthote [85]. The top of this image shows how the stippling subdivision is imple-
mented using a graph function. The bottom image shows an example of how this
stippling appears when it is applied to the target object (The bones representing a
human hand). A small amount of stipple can be seen on the left of the image where
the stipple was evenly and sparsely placed, but there is more depth perception in
the center and right images where stippling is more frequent and varied. Used with
permission from IEEE © 2004.


This study showed that while participants performed faster with the pseudo-chroma
depth performed, they were more accurate at assessing the distances and felt more
confident in their answers using the hatching condition. This shows that hatching
may allow for better depth perception.

Stippling

Lu et al. [84] furthered the stippling techniques shown by Interrante et al. [76]. By
looking at the curvature of the model, finding localized curvature of 3D models, and
spacing out the dots between the various pixels on the screen. This effect created a
realistic stippling effect for 2D images.
    An issue with drawing the dots for stippling was that Lu et al.’s [84,86] found that
the distance between the dots requires to be randomly placed using a noise-based
function rather than just at random. It was important space stippling randomly but


                                          25
     2. BACKGROUND


evenly distributed. One solution for this was created by Pastor and Strothotte [85].
Their version of stippling created a 3D Voronoi pattern over the 3D model, then a
graph would be created linking the starting point of all of the dots which shared
a boundary. This allowed for a seamless decline in the number of Voxels shown as
they would be separated into groups based on the parent-child relationship seen in
the upper part of Figure 2.7. This enabled the even stippling thresholds seen in the
lower part of Figure 2.7 creating a sense of depth and shape of the objects it was
applied to.
    Another way of creating even stippling while allowing for different angles is to
utilize a geometry shader to further subdivide the mesh. This technique was initially
proposed by Meruvia and Pastor [87]. By doing this, you can subdivide each polygon
to give each polygon a set number of dots within it and evenly distribute the dots
inside of each polygon. This system operates under the assumption that areas with
more polygons will require a higher density of dots, whereas flat areas will not [87].
    This method of stippling was later extended by Ma et al. [88] to allow for the
stippling to be applied to a 3D model in real time that utilized pre-computation to
speed up the process rather than a more complex shader. The dot was placed using
blue noise inside the Voronoi, adjusted in size to varying levels, and adjusted in
tones based on where they appeared in parallel on the GPU. This type of stippling
allowed for the effect to be placed realistically onto 3D models even as they were
changing their shapes.

Halo

Outlines [89], Boundary Enhancements [90], feature lines [91, 92], and Halos [93] go
by many other names, but they all relate to outlining either individual objects or
highlighting areas of very high curvature from the perspective of the viewer. With
traditional rendering, this tends to be done by viewing the distance between various
pixels on the depth map [83, 93] or by calculating the local curvature of the sur-
rounding fragments [89, 90]. However, this can be very different when working with
DVR in part to the lack of a defined surface making any surface based calculations
challenging.


2.2.1     Volumetric Illustrative Rendering Techniques
In more recent years, many papers have been striving to take volumetric data and
present it as illustrative images, with the belief that these images will be easier to
communicate and understand in a 2D format.
   Initially Interrante et al.’s [76] proposed two methods to convert their illustrative
techniques to DVR:


                                          26
   • Scan-Conversion Method: Converts texture slabs into a grayscale volume.
     This method is efficient for generating multiple views but compromises stroke
     crispness due to volume data resolution limitations [76].

   • Geometric Definition Method: Directly applies geometrical definitions of
     strokes during ray casting. This method maintains fine detail but is computa-
     tionally expensive, requiring repeated intersection tests for each view [76].

Since this thesis is focused on real-time volume rendering for immersive MR devices,
a new volume was required to be generated each frame. This section will have more
of a focus on papers that utilize the Geometric Definition Method. Which has since
been heaver extended by other to now utilize techniques that are available on more
modern Graphics Processing Units (GPUs) like fragment shaders.
    Rheingans et al. [94] developed a method that was able to separately compute
the expected color and the transparency. This allowed the lighting to react to
certain elements and allow different textures within the volume to have a different
appearance even if the Hounsfield unit was similar at that voxel. Rheingans et
al. [94] DVR algorithm was able to present an accurate texture representation of
the various surfaces of the volume and allowed for objects to be presented without
regard to density or realism.
    Lu et al. [86] created a system that was able to apply stippling to a complete
volume. This system used algorithms like ray marching similar to DVR to perform
this calculation. Visualization focused on rendering the various surfaces to make
their curvatures clear but also took into account the amount of density it would
have required to reach a given surface. Surfaces that were facing the camera were
faded out, and the lighting algorithm mentioned in Lu et al. [86] allowed for shading
to become an option to be utilized by the final visualization.
    Work done by Bruckner et al. [95, 96] proposed that one method to get around
some unwanted details that are an issue with DVR would be to utilize non-photo-
realistic rendering. This style of rendering utilized on the surfaces of different objects
and rendered them based on their curvature [96]. From this, they developed a simple
cartoon-like shading algorithm [96], Stippling [96], and Halos [95].
    When volume rendering is utilized in microscopy, it can be difficult to tell the
difference between the different boundaries and the elements being visualized. Guo
et al. [59] used a halo visualization to separate the different molecules that can be
viewed, as well as two new techniques for promoting contrast in regions of the vol-
ume called Phase Contrast Volume Rendering (PCVR) and Difference Interference
Contrast Volume Rendering (DICVR). PCVR enhances the contrast of almost vis-
ible parts of the volume, allowing for a higher contrast. DICVR tries to extend
PCVR further by using interference contrast based on microscopy principles. They

                                           27
      2. BACKGROUND


simplified the equation required to create these effects each time by performing mul-
tiple rendering passes. This work by Bruckner et al. [95, 96] aimed at providing a
more simplistic method for creating DVR and extending its functionality in biology.


2.3     Direct Volume Rendering (DVR)
The other focus of this thesis is volume rendering. Volume rendering relates to the
visualization of a volume of data, which is typically created using a CT or Magnetic
Resonance Imaging (MRI) machine. However, it can also be used for other scientific
visualizations, such as fluid simulations, meteorological data, and geological data.
Preprocessed Volume Rendering will generally present surfaces of the volumes using
polygonal structures utilizing iso-surfaces [97], but DVR does not require explicit
surfaces to be precisely stated. DVR can present the data within the natural format
as a 3D image [98]. This allows the system to dynamically represent the volumes
from within the system. However, it does highlight the need for techniques like CT
and MRI to be directly aligned with the physical data source. An X-ray Vision
technique needs to be developed to meet this criterion. This section highlights the
prior work in this area that influenced the research in this dissertation.


2.3.1    Visualizing Volumetric Data
Generally, medical volumetric data is viewed using 2D slices of a human body. After
years of training, medical practitioners can be very precise when using these slices,
but they are not intuitive to use or to learn how to use [13]. Like other forms of 3D
data visualizations, converting this data into a 3D version makes it more intuitive
to read and interact with [8]. This process involves reconstructing 3D models from
volumetric data, enabling more intuitive visualization and interaction compared to
traditional 2D slice-based approaches.
     Early methods to visualize volumes would focus on calculating the surface con-
tours and calculating the exterior surface to match [99], which was problematic as
it created ambiguity when there were irregularities on the slice data such as those
caused by noise [100], requiring user intervention to overcome [101]. Herman et
al. [102] tried creating a more automated surface by creating Cuberilles, which func-
tioned similarly to Minecraft blocks Figure 2.8. This was useful as it allowed for
varying resolution [103]. The continuation of this work was marching cubes [97].
Unlike the rough surface afforded by utilizing Cuberilles, this algorithm made a
smooth surface. Marching Cubes utilized the fact that any six Voxels neighboring
Voxels could be paired into 14 different symmetrical orientations if they were either
inside or outside of the threshold. This made it possible to create a 3D model of a


                                         28
Figure 2.8: A example of Cuberilles. The left side shows the armadillo in its
original form. On the right, the same model is rendered using Cuberilles.


CT or MRI scan with a manageable polygon count that looked similar to the real
volume.
    Wünsche [104] introduced a visualization toolkit designed for the exploration of
complex biomedical data, with a particular focus on curvilinear finite element data
sets. Unlike conventional volume visualization approaches that assume regularly
gridded data, curvilinear finite element models define geometry in material space,
where grid lines are curved when mapped into world coordinates. The system de-
rives iso-surfaces in material space and then renders them in world space, enabling
accurate visualization of organs modelled using FE techniques, such as the left ven-
tricle of the heart. The toolkit provides several novel features: a modular design for
comparing multiple models simultaneously, a generalized field structure allowing the
creation and manipulation of scalar, vector, and tensor fields, and boolean filters for
segmentation and icon placement. Additional innovations include global color map
controls for consistent interpretation across models, and flexible element, plane, and
point selection mechanisms. This framework allowed researchers to integrate and
explore biomedical data ranging from scalar tissue properties to tensor fields derived
from MRI, supporting both quantitative analysis and interactive visualization.
    Liu et al. [105] extended this line of work by introducing a novel interface for DVR
aimed at making transfer function design more intuitive and accessible, particularly
for non-expert users. Traditional DVR requires carefully crafted transfer functions
to map volume data values to color and opacity, a task that can be challenging
without specialized visualization knowledge. To address this, Liu and colleagues
proposed a spreadsheet-style, constructive visual component-based interface that
follows a “programming-by-example” paradigm. Their system automatically ana-
lyzes the Douglas–Peucker algorithm [106] histograms of the volume data using to

                                          29
     2. BACKGROUND


detect meaningful structures, from which it generates “unit transfer functions” rep-
resenting simple, recognizable features. Users can then combine, refine, and merge
these units interactively to build more complex transfer functions. Preliminary
evaluations demonstrated that even novice users were able to produce meaningful
visualizations significantly faster and with less guidance than when using traditional
transfer function editors, highlighting the potential of example-based interfaces for
democratizing DVR in biomedical applications.
    Iso-surfaces are still used widely today as they provide the most efficient means
of displaying a shell; however, tasks like diagnostic exploration and interactive tasks
are better enabled by DVR [107]. Farrell [108] found a method of using ray casting
to create a surface method showcasing one of the first attempts at DVR. However,
many of the concepts for this would later be formed by DVR, which was initially
developed in 1988 by Drebin et al. [98], who designed this type of visualization
as a fix for the all-or-nothing approach that is possible when using an iso-surface.
Drebin et al.’s [98] approach allowed for a realistic, transparent representation of
the volume collected from a MRI or glsct scanner [109, 110]. DVR functionality was
further refined by Engel et al. [111] to work with modern equipment. Allowing all
of the models to be viewed with minimal issues. DVR rendering was initially only
designed for CT and MRI data [109, 110], but it was later utilized in other fields.


2.3.2     Use Cases for Direct Volume Rendering
As with many fields, DVR can be utilized for education. MacDougall et al. [112]
provide an example using a large 3D display wall of molecules for chemistry research
and education. These models used the traditional ball on a stick model and DVR,
which better represented depth and provided a more realistic or cloudy model of the
quantum world. MacDougall et al. [112] found that elements of this system could
help create new drugs for the future and proposed use cases that would encourage
students to become more hands-on.
    Hibbard [5] talked about how the data from 2D plots is easier to view in 3D when
using volume rendering. Hibbard [5] then also conferred this data could be used to
allow this data to be manipulated by using a time-variant, allowing phenomena like
the wind to be simple to examine. These techniques were then extended by Riley
et al. [113] to allow for realistic visualizations of cloud maps. This was impossible
using iso-surfaces, which tended to require a form of lighting that clouds did not
utilize [113]. DVR allows climate scientists to explore the internal patterns of the
effect of time and space on weather phenomena [114].
    DVR is also used while testing the quality of materials to visulize them. These
materials can constis of metals alloys [115], minerals [115] concrete [115], resins [1],


                                          30
and combinations of different materials used to create a single one [115, 116]. Ma-
terial science requires understanding materials’ internal structures formed under
different circumstances [116]. This could be done in the way of running CT scans
of being dented or folded, allowing for structural analysis of how different condi-
tions can affect different materials [116]. This can also be applied to the creation
of different materials, like sponge-like materials that need to move in certain ways
or quantum materials that will arrange atoms, creating some highly precise and
gas-like materials [115, 117]. Volume rendering can also be used to show how con-
ducive liquids like resins are moving through non-conductive ones [1], allowing for
real-time testing of how to develop products using these materials and presenting
communication methods with end users [1].
    Molecular Sciences also use volume rendering to visualize the output gathered
from electronic microscopes [2, 118]. This allows the user to view the contents of
a sample collected in 3D based on the different densities, much like CT and MRI
data. Nguyen et al.’s [118] DiffTEM system adds to this by allowing denoising that
utilizes many images of the data collected from different orientations before the data
is rendered.
    Geologists can utilize DVR to represent the values of radar data [119]. They tend
to do this by using ground-penetrating radar [119]. Unlike the previous examples of
use cases for DVR, radar has many blank areas; Zehner [120] states that DVR can
be used to both present a more full view of the area but to also better represent the
level of uncertainty that can be viewed from this viewpoint.


2.3.3    Human Computer Interaction (HCI) Experiments Us-
         ing Direct Volume Rendering (DVR)
Understanding how people visualize or interact with DVR is important to this re-
search. While being able to visualize various data using DVR is one thing, the
user experience is more important than the act of being able to display the content
because the content rendered by the DVR needs to be a more pleasant experience
than the alternative situations for it to have utility. The following section looks at
how various studies over time have evaluated systems using DVR.
   One of the first instances of human interaction being a concept using DVR is the
work by Hui et al. [121] on a cursor for these interactions. This cursor is displayed
in Figure 2.9 and works similarly to how a mouse works on a 2D plane, but it had
the ability to be rotated on another plane using another 1D input, like the scroll
wheel on a mouse, to rotate the plane the mouse cursor was sitting on. To inform
the user of the depth of the plane was highlighted on the outside of the volume, and
a variation blind effect would be used to prevent the cursor from moving too much


                                         31
     2. BACKGROUND




Figure 2.9: Examples of Hui et al.’s [121] cursors on 3D planes. Right) Nail on
plane, where the cursor has the ability to rotate around the volume; Left) Venetain
blind, a non-flat plane, which allows the user to navigate easier on all dimensions
using the cursor. Used with permission from IEEE © 1993.


while still being able to get everywhere [121].
    Kersten et al. [122] performed one of the first studies ever to be done using DVR
using a 3D display. This study was focused on how transparency affected depth
perception when using DVR on a stereoscopic display. The type of transparency
used was commonly associated with direct volume rendering. The study design
would slowly rotate a cylinder filled with Perlin noise [123] and then rotate it slowly
in one direction. To tell what direction a cylinder was rotating, users would have
to understand the approximate position of elements in the noise. Participants of
this study had to do this when using a mono and stereoscopic display and between
various amounts levels of opacity. Kersten et al.’s [122] findings showed that DVR
is much more effective on stereoscopic displays. This shows that the real use case
for these techniques may be within the use of stereoscopic devices.
    Several years later, Kersten-Oertel et al. [124] looked at methods to tell depth
within a sparse volume rather than an occluded one. This was in the form of a set
of Cerebral vascular volumes. Five different depth cues and a baseline were utilized
throughout this study: Edge Detection (Halos), Pseudo-Chromadepth, Fog, Kinetic
depth, and Stereo Vision. Kersten-Oertel et al. [124] tested a combination of ex-
perts and novices separately on two different studies. All of the studies utilized the
same procedure, where two vessels were highlighted, and the participants guessed
which one was closer to the participant. One of these studies utilized each depth
cue individually, while the other focused on their different combinations. Individ-
ual Chromadepth, Fog, and Stereo were shown to be much more beneficial than

                                          32
the other cues when shown individually. While chroma depth and stereo seem to
have showcased the most substantial values for the combination, Kersten-Oertel et
al. [124] study again shows why Stereo-vision of DVR is such a strong depth cue.
    Another user study looking into the transparency created by using DVR was done
by Corcoran and Dingliana [125]. This system used two layers of volume rendering:
an outer transparent layer and an inner occluded layer. By having occluded surfaces,
Corcoran and Dingliana [125] could provide lighting to parts of the volume that were
occluded by rendering the image throughout multiple passes.
    To evaluate the effectiveness of placing shadows in DVR Corcoran and Dingliana [125]
ran a series of studies using a computer monitor running at approximately 20fps,
each consisting of less than 20 participants. The first one was designed to test the
users’ preferences. This was done by having the participants view the same object
side by side. Participants were asked to say whether the shadowed-enabled or non-
shadowed versions told them more about the volume. The next study looked at
shape perception, which had participants arrange two images of similar body parts
from two different datasets. This shape perception study showed that raycasted
shadows were not essential for shape perception, but instead, the User Interface
(UI) was because participants were able to orientate their depth perception by with
the ability to rotate the object. The next study looked at relative depth perception.
One user chose a point on the screen that was closest to themselves, and they found
that showing shadows significantly increased depth perception. They note here that
it was uncommon for participants to answer this incorrectly. The final experiment
had users determine how far into a volume the point was by having them estimate
the location of an artifact inside of the volume. This task found that depth per-
ception was not affected by the distance or the presence of shadows inside of the
volume; rather, it may hinder the absolute depth perception. Overall, Corcoran and
Dingliana [125] show that shadows can help detail information in a Volume, but
they don’t necessarily improve perception if there are other depth cues present like
motion.
    Three studies to use DVR with MR were done by Laha et al. [126–129] who looked
at studies which were focused on determining the immersion of different MR devices
displaying volumetric information. To best determine the amount of immersion
when using MR devices, they enabled and disabled head tracking and limited the
area that was displayed to the user to 360◦ , 270◦ , 180◦ , 90◦ . These conditions allowed
them to determine the required or appropriate level of immersion that was preferable
for each task. All of their studies utilized a selection of open-source real-world data
to do their studies and utilized a range of different tasks suited for each dataset on
each one. The first study was focused on testing if there was any noticeable benefit to
MR when needing to utilize a Cave Automatic Virtual Environment (CAVE) using


                                         33
     2. BACKGROUND


the restricted motion tracking [127]. This study which was focused on restricted
head motion caused them to notice that any combination of the two conditions
was able to grant better results. Laha et al.’s [126] next study utilized the NVisor
SX111 a (Video See Through (VST) AR display) under similar circumstances. This
study found that the performance of their participants was improved by providing
the most immersion possible [126]. The final study ran by Laha et al. [129] looked
at what happened when iso-surfaces were used as the data set, requiring them to
change the tasks required and also had them reintroduce stereo vision back to the
conditions. This study observed that the combination of all of the results was the
most suitable for the tasks required to interact with volumetric data visualizations.
    Afterward, Laha et al. [130] created a taxonomy of the different types of tasks
that are possible when using MR, with the goal of remove the domain dependency
that exists with empirical studies. This was done by consulting 167 people using a
questionnaire regarding how this taxonomy should be shaped. This was done with
the aim to allow basic types of interactions to be considered similar to other types
of interactions for different fields. These different types were classified as:

  • Searching: Searching for the presence or absence of an object, or counting
    the amount of a given object.

  • Pattern Recognition: Looking for trends like what side of the data set are
    there more blood vessels or repetition and asking the participant how many
    times certain items appear.

  • Spatial Understanding: This section is for tasks that require the participant
    to understand the orientation or position of a feature in the dataset.

  • Quantitative Estimation: These focus on tasks that have the users estimate
    the properties of a feature of the dataset.

  • Shape Description: Requires the user to describe a shape they are viewing.

They also note two possible viewing styles: Egocentric and exocentric. They also
note how the different dimensionality of the data should be considered. Overall, this
study paper has informed the design process of the later studies.
     Sketch-based interactions allow for an easy interaction with volumes. Shen et
al.’s [131] split the various use cases this technique could be used into seven different
categories: selection, cutting, segmentation, matching, coloring, augmentation, and
illustration.
     An early version of volumetric hatching used in this dissertation was inspired by
research from Feng et al. [132], which helped establish fundamental approaches to
illustrative rendering. Feng et al. [132] created a system that projected a grid over

                                           34
Figure 2.10: The 6 datasets used in Grosset et al.’s [133] research: (a) aneurysm,
(b) backpack, (c) bonsai, (d) flame, (e) Richtmyer-Meshkov instability & (f) thorax.
Used with permission from IEEE © 2013.


the area of the volumetric area. This was chosen to help highlight objects and clearly
present the depth to different areas when faced with users who do not understand
the depth of objects within the volume. This system worked by projecting a 2D grid
on the first available voxel in the ray marching algorithm when it was in the range
of the grid texture. They note in their findings that chroma color could be utilized
to highlight depth perception.
     Almost all interactions with DVR require good depth perception, even when they
are using a 2D display. Grosset et al. [133] created a user study that aimed to test
if the depth of field could be utilized with DVR. This study looked at replicating a
person’s ability to focus on an element based on how deep it was in the volume that
they were focused on. This was tested using a Dynamic and a Static experiment.
The static experiment asked participants to judged which of two circled features
in still images was closer in depth, testing whether Depth of Field and projection
type affected speed and accuracy. The Dynamic Experiment required participants
to watch short videos where the focal plane swept through the scene and then
judged which circled feature was closer, testing whether dynamic focusing improved
depth accuracy. The Dynamic Experiment allowed the user to control the depth
of field by tracking their eyes. The other experiment by Grosset et al. [133] set
the focal points at the most optimal place. While there was not much difference
between the different conditions, both studies provided similar answers. Whether
this technique was static or dynamic didn’t make much difference between various
conditions. However, they did notice a difference between the different sets of data
shown in Figure 2.10. Each different dataset which was visualized had a different
amount of depth perception, with the bonsai and thorax datasets being the easiest to
determine depth, while the flame and Richtmyer-Meshkov instability datasets were
the hardest to determine depth. This was likely because each dataset they utilized
presented a vastly different type of visualization, which utilized different depth cues
to various levels [133].
     Roberts et al. [134] investigated the impact of different types of reconstruction
filters for DVR graphics using a questionnaire. These reconstruction filters included

                                          35
       2. BACKGROUND




Figure 2.11: Conditions used in Englund et al.’s [135, 136] studies. (a) Direct
Volume Rendering (DVR), (b) Depth of Field, (c) Depth Darkening, (d) Volumetric
Halos, (e) Volume Illustration, and (f) Volumetric Line Drawings. Used with per-
mission from John Wiley and Sons © 2016


B-spline, Trilinear, CatMull-Rom, Int B-spline, and Welch filters. Participants were
asked to view several open-source datasets and rate their depth quality, layout,
sharpness, and jaggedness. This paper did not have many major findings. B-spline
was preferable for the tasks they suggested but not to a significant amount. What
was interesting about this research was that the datasets seemed to have more of an
impact than the conditions themselves did, much like Grosset et al.’s [133] study.
    To get around the issue of not being able to create a sated by Roberts et al. [134]
and Grosset et al.’s [133], Englund et al. [135, 136] created 15 volumetric images in
the shape of a cube and took photos of each of the sides these volumes, which allowed
them to take 90 images of different volumes, which they then utilized in a question-
naire. They then took these images with six different DVR visualization techniques
seen in Figure 2.11 3 : Depth Darkening (Depth Darkening), Depth of Field (Depth
of Field), Subtractive Color Blending (Volumetric Halos), Additive Color Blend-
ing (DVR), White Outlines(Volumetric Lines), Toon Shading (Depth Darkening),
Black Outlines(Volume Illustration). This questionnaire had participants perform
Two-alternative forced choice (2AFC) questionnaire for multiple depth perception
studies if participants could visually orient the images in the same positions and how
attractive the different conditions they presented were [135]. This study found that
depth darkening and Halo’s gave the best sense of depth perception out of all the
conditions, and the occlusion cue-in seems to be the most effective way to determine
depth perception based on it. Orienting the shapes was easiest to determine using
the toon shading, as that method highlighted the shapes of the foreground objects
the best. Most users preferred depth darkening [135].
    The Englund et al.’s [136] study design and conditions were later consulted on
by three experts. This clearly explains both the Subtractive Color Blending and
Black Outlines. One main issue an expert noted was that as these sources of data
only represented a single image, they didn’t accurately represent the interaction that
   3
    The descriptions used in Englund et al.’s [135,136] may be misleading when compared to other
research. So the names they have used are in parenthesis.



                                              36
people would have with volume data, as this was expected to be utilized to properly
view the data, like being able to rotate the volume. The same expert noted that the
Toon shading hid aspects of the volume due to its high opacity. One expert noted
that the outlines would be better suited if they could react to the surfaces’s outline.
Some of the most important expert findings from Englund et al.’s research [136] was
that by highlighting the edges of the objects, depth perception was most improved
by communicating to the user where the front and back of the objects were. These
factors likely were likely why the black outlines performed as well as they did.


2.4          An introduction to Augmented Reality (AR)
             and Virtual Reality (VR)
Augmented Reality or AR refers to the ability to bring virtual objects into the
real world [137]. AR technologies are part of Milgram et al.’s [137] Mixed Reality
Continuum (seen in Figure 2.12), which represents the spectrum of technology that
can augment the visual world. The more of the world that is the viewer’s perception
is moved towards only being able to sense the virtual world the more further along
the scale it becomes until a user is fully immersed in a virtual world. MR talks about
the spectrum between the real and virtual worlds, not including them. Resulting
in MR to include many different types of displays, given the types of interactions
ranging from HMDs, CAVEs, Handheld Devices, Spatial Augmented Reality (SAR)
displays, and even some large monitors can be used as mixed reality devices.


2.4.1        Mixed Reality (MR) Hardware
This section covers the relevant hardware for this dissertation, focusing on devices
that provide an MR experience to gain an understanding of the experiences that are
relevant and that combine real-world interactions with computer graphics. These
devices have been designed for different applications and, as such, have different
strengths and weaknesses across Milgram ’s [137] Mixed Reality Continuum. This


                                 Mixed Reality (MR)




    Real 
             Augmented
                   Augmented
               Virtual 


Environment            Reality (AR)                Virtuality (AV)        Environment




 Figure 2.12: A simple view of Milgram et al.’s Mixed Reality Continuum [137].


                                          37
       2. BACKGROUND


section focuses on the advantages and disadvantages of the hardware used in the
relevant research for this dissertation.
    The first AR system (seen in Figure 2.13) was created by Ivan Sutherland in
1968 "The Sword of Damocles" [138]. This modified a person’s sight by adding
computer graphics to it. By tracking where the user moved their head every 10ms,
the system would update the images that the system presented to them within 3D
space. The position of the user’s eyes in each lens would determine the direction of
the 3D object, which was tracked by a collection of exterior sensors placed around
the space users could walk around [138].
    Sutherland’s [138] Sword of Damocles became the progenitor of all modern AR
and Virtual Reality (VR) HMDs. In the following decades, researchers developed
methods to interact with computers using hand gestures [140] to interact with 3D
desktop displays and found cheaper and more ergonomic ways to present this infor-
mation [141]. Research was done to shrink the size of these displays into a more
portable size [142].
    AR can be found in many different industries [143]. Entertainment and gaming
use AR to create an experience that is based on navigation like Ingress or Poke-
mon Go 4 , also first-person shooters, puzzle games, and board games [143, 144].
Industrial maintenance uses AR to instruct people on the maintenance of complex
machinery [145], Tour Guides use AR to guide users around cities and buildings
and to explain the contextual information to the end user [145]. AR is also be-
ing used to extend existing telecommunications technologies, allowing volumetric
communications to be possible between different groups of people [145].
    Modern smartphones and tablets provide a suitable platform for presenting video
see-through mixed reality experiences that use the smartphone’s camera to capture
the physical world, which is augmented with digital content. The wide availability
of smartphones has also made them a popular choice for MR experiences. One
advantage of this approach is the display’s brightness, which often outperforms
head-worn displays that can appear washed out compared to head-worn displays.
Mobile devices don’t provide an immersive experience; smartphones need to be held
by the user, and they can allow for an AR experience or require a stand during the
operation. A use case that has been gaining popularity is for training or education
applications where 3D content is presented as an adjunct to a paper-based medium.
    Projector-based mixed reality systems, also known as SAR, employ projectors to
alter the appearance of physical objects that can be highly organic in shape instead of
a typical projector screen. This form of augmented reality has received less attention
compared to head-worn and hand-held solutions. However, projected information
provides some unique features that have the potential for several applications. An
  4
      https://pokemongolive.com/


                                          38
Figure 2.13: Ivan Sutherland’s [138] "The Sword of Damocles" one of the first head-
mounted AR systems. This image is a reprint published by [139] licensed under a
Creative Commons Attribution licence. Permission of the original image is granted
to make digital or hard copies of all or part of this work for personal or classroom use
is granted without fee, provided that copies are not made or distributed for profit
or commercial advantage.




                                          39
     2. BACKGROUND


advantage of projected information overlaid on physical objects is that multiple
observers can be perceived as the same projection simultaneously and from multiple
points of view. For example, visualizations of internal anatomical details directly
projected onto the human body [146].
    CAVEs environments are another instance of SAR. Caves are three-dimensional
cubicles that use several projected images to simulate an immersive 3D environment
without users needing to wear a headset. As such, caves can provide a collabora-
tive AR experience for one or more medical professionals who wish to explore vast
amounts of information collaboratively. For example, medical data can be visualized
in the form of three-dimensional models that can be viewed collaboratively in a cave
environment for diagnosis [147]. Digital Imaging and Communications in Medicine
(DICOM) files were processed into 3D models using an algorithm that converts vox-
els to polygons that are refined into visualizations to be viewed using 3D glasses in
a collaborative environment.
    HMDs rely on placing the display of the headset as close to the user’s line of sight
as possible. They allow users to move around the virtual content and interact with it
naturally through head movement. This offers a means of presenting mixed or virtual
reality content suitable for medical visualizations, training, step-by-step guidance,
and many other applications [143,148]. Since Ivan Sutherland’s HMD in 1968 [138],
it has been iterated on since, especially in the last several years since VR headsets
have drastically lowered in price and become a consumer commodity [148, 149].
    VR HMDs are available in many different forms. They are accompanied by sup-
porting technologies such as position tracking, front-facing cameras, eye tracking,
EEG sensors, and hand controllers to deliver compelling virtual experiences [150].
Desktop systems such as the Oculus Rift S and HTC Vive Pro offer high-quality
display systems that, coupled with a high-end PC, present immersive 3D environ-
ments in real time. They also incorporate tracking systems that enable hand-held
controllers to support interactions in virtual worlds. Standalone hardware options
such as the Oculus Quest and Vive Focus have also become available, removing the
need to be tethered to a PC and allowing for wide-area virtual environments to
be configured. One of the advantages of head-worn displays is that they leave the
operators’ hands free to perform other tasks, unlike hand-held technologies, such as
smartphones, which require the user to hold the device during operation.
    AR HMDs tend to come in two distinct types: VST, and Ocular See Through
(OST) HMDs. VST HMDs use exterior sensors (usually a camera) and an occlusive
headset (typically a VR headset) [29,30,151]. While OST AR use a range of different
transparent displays.
    There have been ongoing improvements in these devices’ quality, ergonomic de-
sign, resolution, and capability. Early consumer-level displays, such as the Sony


                                          40
Glasstron (1996), provided an augmented reality display solution connecting to a
desktop computer. This has since been taken to utilizing a Mobile platform with
the Microsoft HoloLens (2016). The HoloLens allowed the user to move around
untethered and interact with the virtual environment. The system was able to react
to the real-world environment, and it allowed for precise ( 1.5 cm) spatial tracking.
    OST AR HMDs like the Microsoft HoloLens [152] can also be used to display
AR content within the user’s field of sight. Objects should be locked into place in
the real world, allowing users to move around the 3D virtual content while being
able to view the real-world content. The HoloLens uses environmental information
to locate the real world and brings virtual objects into the user’s field of view.


2.4.2    Designing Applications for Stereoscopic Displays
AR devices require different types of interfaces based on the required context and
functionality [153, 154]. This section presents design considerations that are impor-
tant to understand when creating a display that accommodates X-ray Vision in a
medical setting, which is a key use case for this thesis.
    In their review by Akpan and Shanker [19], they investigated 3D data visual-
izations displayed on VR headsets compared to 2D displays. This review presented
studies that tested 3D visualizations identifying key performance elements:

  • Data can be easier to analyze with visualizations and promotes more experi-
    mentation with data sets in the correct situations. This can be seen in Cordil
    et al.’s [155] work where users are encouraged to move around the different
    axes of graphs to create new ones, encouraging data analysis on a different
    level. Allowing for a rapid visual analysis with clear results;

  • Aiding users in understanding the data at a faster rate. Tanagho et al. [156]
    has found that when guiding people using 2D and 3D interfaces for performing
    a Laparoscopic surgery, participants were faster and more accurate when using
    a 3D interface compared to a 2D one;

  • Verifying data using a 3D format can help users to more rapidly verify models.
    Majber et al. [157] found this to be the presenting simulation data to stake-
    holders for factory designs and layouts. The Interface and data visualized by
    the VR device were understandable even to people who were not knowledgeable
    about the products or factories;

  • The overall presentation of 3D data tends to be able interpreted more intu-
    itively than 2D models such as graphs or slices. Qu et al. [158] found this when
    they were simulating the growth of an eggplant, where they found the simula-
    tions were made much easier to understand when the user was presented with

                                         41
     2. BACKGROUND


     a 3D graphic of an approximation of the predicted growth of the plant rather
     than 2D graphs.

Similar findings can also be found in McIntire et al.’s [16] literature review on
the utility of stereoscopic displays compared to conventional monoscopic displays.
McIntire et al.’s [16] found that stereoscopic display either improved or made no
difference in spatial awareness, spatial understanding, classifying objects, spatial
manipulation, navigation, and educational activities. There is evidence that 3D
information made sense when viewed via a stereoscopic display.
     Guo et al. [159] noted that even if a VST AR HMD could be perfectly calibrated
to the real world, it still made surgeons nervous to really on an external device
connected to the HMD via a wired or wireless connection, because it could lose its
calibration making the system inaccurate, or it could turn off and occlude the sur-
geon’s vision altogether. Andrews et al. [160] note in their literature review that due
to the HoloLens’s gesture controls allow for operations within sterile environments.
Making the headset functional in a clinical or surgical setting. When Rodrigues et
al. [161] asked two surgeons how best to create an MR sweet of tools. Rodrigues et
al. [161] spent several months talking with two surgeons to choose the best hardware.
Both opted to use the HoloLens over all other hardware choices. Pratt et al. [43]
utilized the HoloLens to perform several surgeries where the patients CT and MRI
data was laid over the top of the patients. It is for these reasons that the research
in this dissertation chose to use an OST AR HMD (a HoloLens2) for this research.


2.4.3     Volume Rendering In Mixed Reality
Graphical capabilities on MR HMDs require more simultaneous processes than on
traditional displays as they as they require the rendering of multiple displays which
need to be redrawn from a slightly different angle each frame, eliminating current
hardware acceleration approaches [29]. This is made much harder as many modern
OST AR headsets utilize mobile hardware even to produce the most simple 3D
visualization [162], resulting in only a few papers on this topic. Authors have,
however, found ways of getting this technique to function on MR HMDs by utilizing
external devices and optimizing rendering through specialized GPU pipeline [163,
164]. This has made possible research that spans gamification, medical assistance,
collaboration, and high-fidelity graphics interaction, resulting in some interesting
features.
    Gamification is often used to increase student retention in education. One system
created by Cecotti et al. [22] allowed students to learn how to read medical images
using 3D spaces as well as 2D ones. The scene shown in Figure 2.14 shows the
information presented to the students and how it allows them to map individual

                                          42
Figure 2.14: The virtual scene created by Cecotti et al. [22] (a) the 3D data,
(b) the sagittal (y-z), (c) coronal (x-y), (d): transaxial (x-z) planes. Used with
permission from IEEE © 2021.


planes to a 3D object, allowing them to learn how various organs are seen using 2D
planes. This is currently integrated into their institution’s anatomy curriculum.
     Pelanis et al. [165] asked a series of surgeons to compare how using a 3D rendering
of CT data compared to a more traditional 2D representation of the same CT data
when using an OST AR HMD. The CT data consisted of images of 150 different
legions. The medical practitioners correctly accessed 89 of the 150 legions using
both formats, and there was no real difference in the accuracy of the visualizations.
There was a difference between the time required, where MRI scans required an
average time of 23.5 sections, while the OST AR HMD only took practitioners an
average of 6 sections to diagnose. Pelanis et al. [165] highlights that there is still an
advantage to using 3D graphics even when a person is highly trained.
     Cinematic Rendering is generally a costly method of viewing Volumetric data.
It cannot be run in real time and requires a very powerful computer. Stadlinger et
al. [166,167] utilized a workaround for this method where they took many images of
the volume and rendered them on 2D planes for the user to see. This allowed this
technique to be viewed using a lowered-powered machine. This made the processing
so efficient that Steffen et al. [168] could display cinematic rendering on the VR
headset.
     Steffen et al.’s [168] research had ten trained medical investigators look at ten
patient cases using both a desktop display and VR HMD. When these users were
asked to assess the volume details, they were more accurate when using a desktop.
They also noted that desktop displays offered better depth perception when viewing
cinematic rendering.
     Heinrich et al. [169] performed a series of tests looking at the difference be-
tween depth perception between AR and VR when looking at volumetric vascular
structures rendered as an iso-surface. They then utilized three different depth cues:
Phong Shading, pseudo-chroma depth shading, and surrounding the volume with a
Fog. Users would be presented with a set of spheres located at the ends of all of
the blood vessels and asked if they could sort them from nearest to farthest. Using


                                           43
      2. BACKGROUND


Pseudo-Chromadepth shading or Fog techniques clearly improved desktops, as these
showed the z-axis. However, VR users rarely make any errors. This was because
they could freely use an array of other depth perception cues, such as Convergence,
Binocular Disparities, Motion Perspective, and even, in the case of Fog and Aerial
Perspective. In contrast, the desktop was limited to Occlusion, Real Size, and Den-
sity, which any display could also use. However, this does show that just using a
AR device will improve the depth perception of a task using volume data, but it
also indicated that the need limits required for testing medical data needed to be-
come much more precise than before to enable the type of accuracy required by the
medical field.


2.5     Augmented Reality Enabled X-ray Vision
One important aspect of AR enabled X-ray vision relates to the field of research
looking at aligning the depth mismatch when virtual objects are placed inside of
physical objects in AR. As is shown in Figure 2.6 since AR superimposes the virtual
objects over the real world it is difficult to make them look like they are behind
an occlusive surface. It is to this end that many X-ray Vision effects have been
developed for different devices to solve these issues.
    The goal of this research is to determine methods where X-ray Vision effects can
work with DVR on OST AR HMDs. To ensure that this research in this dissertation
has a solid basis for the lessons learned by the previous works, a systematic litera-
ture review was conducted. This literature review was inspired by the PRIMA 2020
SRC (Page et al. 2021) protocols checklist. It utilized five databases: ACM Digital
Library 5 , IEEE Xplore 6 , Pub Med 7 , Web of Science 8 , and Scopus 9 using the
search terms: “("X-ray vision" OR "X-ray visualization" OR ("occlusion" AND "per-
ception") OR (visualization AND x-ray) OR "see-through vision" OR "ghosted views"
OR "augmented reality x-ray") AND ("AR" OR "Augmented Reality" OR "Mixed Re-
ality")” These terms were filtered by the paper’s Abstract, Title, and Keywords. A
backwards and forwards snowball was conducted where each papers citations and
references where each checked for overlaps on to the existing papers selected for the
study which then produced either caused the final search to be adjusted (to the
one mentioned earlier). Each time the search criteria was changed the search would
begin again. If the papers were not located in the databases listed (grey literature)
would be added if they passed all criteria and where papers cited by more than
  5
    https://dl.acm.org/
  6
    https://ieeexplore.ieee.org/Xplore/home.jsp
  7
    https://pubmed.ncbi.nlm.nih.gov/
  8
    https://www.webofscience.com/wos/
  9
    https://www.scopus.com/


                                         44
                         Duplicates 
                    Abstracts and
                                                          Titles Read
                Full Text Articles                     Gray Literature
                       Removed = 333                    Rejected = 500                 Rejected = 157                          Search = 2

    IEEE Xplore  

 Results Found = 127




                                                                                            Accessed for Eligibility = 248
                            Total Results Found = 994



                                                             Records Screened = 748
       ACM  

 Results Found = 39




                                                                                                                                  Final Result = 81
    Pub Med  

Results Found = 138                                                                                                                                          83

 Web Of Science  
                                                                                                                                    Papers Accepted
Results Found = 362

      Scopus 

Results Found = 425



Figure 2.15: The protocol utilized for this literature review with the matching
results.


five papers were also considered for entry into this database (2 papers found and
accepted). For a paper to be accepted, it needed to meet the eligibility criteria
outlined below as determined by the single reviewer. The process and results of this
review as depicted in Figure 2.15

Eligibility Criteria

All papers in this review needed to meet the following criteria:

   • X-ray vision (XRV) needed to be explored in the research, placing virtual
     objects behind real-world objects.

   • Mixed Reality Technologies other than VR must have been used.

   • Papers that simply superimposed data over the real world and papers that
     did not focus on making the content appear on the other side of or within the
     real-world object were not included.

   • Studies included in multiple papers were only recorded once all papers that
     highlighted the same research had been included in this review, as the content
     between both articles provides different amounts of information.

   • If the focus on XRV was light compared to other study elements and two or
     more reviewers agreed on it, then the work would not be included in favor of
     more focused research.

    These criteria ensured that the X-ray visualization was more than superimposing
a medical image onto a human subject. Any paper that did not utilize AR was
removed. This meant that some documents that utilized Virtual Reality technologies
to test AR in a completely virtual environment were not included.



                                                                                                                             45
     2. BACKGROUND




Figure 2.16: A bar graph representing the number of papers that were published
between 1990 and 2023 focusing on X-ray Vision, each categorized by the type of
research undertaken.


2.5.1    Observations from Systematic Literature Review
This section presents the complete findings of all the X-ray Vision papers that
were found as of October 30, 2023. It discusses all the devices on which X-ray
Vision techniques have been used, along with the techniques for X-ray Vision, their
functional mechanism, and the research focus. Following this, I looked at how each
user study was conducted and the parameters utilized for the various experiments.
    Of all the selected papers that were longer than two pages, 27 Technical papers
were found (3 of which also included a case study), and another 29 papers were
found that presented a user study. Three demonstrations have been showcased
at conferences. Ten short papers have highlighted several methods of research,
including a proposal, many of which detail pilot studies for some of the 29 user
studies. Every task that required its own set of results was considered a study, and
if no results were published for a given study, it was not included in our analysis.
    In recent years, peer-reviewed research proposals have become increasingly com-
mon compared to technical papers. Prior to 2010, the literature was predominantly
technical in nature. This shift may reflect an evolving trend in the field, where
research is expected to address a broader range of topics or where purely technical
contributions are less frequently regarded as sufficient on their own. Importantly,
research proposals continue to play a valuable role by introducing new ideas and
approaches, guiding future investigations, and helping to shape the direction of the
field. Over the past decade, our understanding of X-ray Vision has advanced through
a combination of user studies and research proposals that explore and refine emerg-
ing techniques. As shown in Figure 2.16, the field has steadily matured, with a
growing emphasis on evaluating the impact of these techniques and improving their
effectiveness.

                                        46
2.5.2     Research Exploring X-ray Vision
Creating an effective X-ray Vision solution for a task requires careful design, con-
struction, and system validation to discover an appropriate balance among the pos-
sible solutions to address the challenges related to AR and X-ray Vision technolo-
gies. The literature contains research studies investigating depth perception, spatial
awareness, interaction techniques, and usability of X-ray Vision systems. Figure 2.17
shows the prevalence of the different topics in the surveyed literature.
    The effectiveness of visualizations and interactions for a given task may depend
on the distance of the physical and virtual object(s) from the user. The human
eye’s ability to perceive depth and fine detail may vary as a function of distance,
and a human’s reach to manipulate physical (and sometimes virtual) objects is
constrained by human anatomy. Perception and interaction have been studied in
several settings, ranging from within arms’ reach distance to scenarios where distant
objects are interrogated and manipulated. Depth perception AR and VR tend to
look at methods of improving the user’s sense of space in an environment. Research
into the depth perception of X-ray Vision focuses on how to influence the impact on
depth perception when looking through an object.
    Depth perception and perception, more broadly, have received considerable at-
tention in the research community since accurate depth perception is vital for X-ray
Vision user experience. A key focus of research has explored depth perception in
X-ray Vision as related to fixing the depth/distance offset caused by trying to place
a virtual object on the other side of a real-world object in reference to the user’s
view point [170–172]. This makes the work in this field done with depth perception
different from other forms of research on depth perception. Some works are looking
into methods to enhance the depth perception of visualization [36, 173]. The major
finding that seems prevalent in this field is that partial occlusion is an effective tool
to improve depth perception.
    Spatial awareness concerns a user’s understanding of and ability to operate ef-
fectively in a physical environment augmented through X-ray Vision. Findings in
this field generally look at the impacts of using these visualizations on aspects like
the general placement of an object or attempt to navigate around a foreign envi-
ronment [169, 174]. X-ray Vision can orient people using exterior landmarks across
large buildings [175].
    Effective mechanisms to interact with and control an X-ray Vision visualization
are essential to the effectiveness and usability of an X-ray Vision system. Making
usability the second largest topic of research in X-ray Vision. Creating interac-
tion mechanisms that cater to the combined characteristics of physical and virtual
elements in X-ray Vision can be challenging. Interaction mechanisms, including ges-


                                           47
     2. BACKGROUND




Figure 2.17: A plot showing the studies that have been run on X-ray Vision by
device type.


tures, button and menu commands, and eye tracking, have been proposed by Wang
et al. [176, 177] and Liao et al. [178].


2.5.3    X-ray Vision Techniques
Partial occlusion is a very depth perception cue, while other techniques rely on other
depth cues to allow the user to look through real-world objects such as stereopsis.
There are many variations of these effects; very similar effects have been categorized
together in this section and will be discussed as a collection of work.
    There are two main categories for X-ray Vision effects: ones that typically uti-
lize visualization to present occlusion techniques and others that utilize other cues.
There has been much more research within the visualization category, so this has
been split into three different sections: Hole in the World-Based X-ray Visualiza-
tions, Real-World Overlays, and Computer Vision Techniques. Splitting the visual-
izations into these groups allows for a more explicit discussion of how they function
in their current form.
    Graphs in this section will note an "other" X-ray Vision category. These effects
are used as baselines for various experiments but do provide an understanding that
an object is supposed to be rendered underground. These effects include things
like presenting a written definition of the location of an object displayed like a


                                         48
Figure 2.18: Examples of X-ray Vision techniques that don’t function using tra-
ditional techniques but rather label or indicate where objects should be located. a
& b ) Cote and Mercier’s [179] underground schematics visualized on the ground; c
) Eren et al.’s [180] labels designed to tell the user if a objects is above or below the
ground; d ) Muthalif et al.’s [181] showcases the labels indicating how far below the
ground an object is. Permission was granted for images a © 2018, b © 2018, and c
© 2022 from IEEE. Figure (d) is licensed under a Creative Commons Attribution
licence.


sign [180]. Other options are a line between the user and the object [182] or a line
between the ground and the object [181]. Options that were not included but can
give a similar form of information could be considered overlaying the ground with
the schematics [179]. These solutions were not included as even though they gave a
clear indication of where the user was, they were all complicated for the user, adding
steps beyond looking at a 3D object and determining its location. These solutions
make great base conditions as they are flawed and complicated, but they are robust.

X-ray Vision Effects

X-ray Vision can also be achieved by utilizing other depth cues, such as accommo-
dation convergence, relative scale, and density.

Auxiliary Augmentation utilizes the relationship between two objects, employ-
ing the depth cues of relative size and density and the effect of motion. Using another
occlusive X-ray Vision technique on one object provides enough information to in-
form the user of the position of the other object [183]. Auxiliary Augmentation
effect should be possible on a 2D display as it uses relive scale and density as its
primary depth cues. Still, it is also made more potent on a stereoscopic display like
a HMD.

Vergence X-ray Vision is designed to have people change and move their focus
away from the foreground to focus on the background image. This method simulates
binocular distortion. By tracking a user’s eyes, it is possible to tell when they are
trying to look past an object. This can advise what should and should not be kept
in view whilst creating and placing objects approximately where the users expect
them to be. By taking advantage of that, it is possible to blur virtual objects when
a user is not looking at them to present a viewing experience similar to real life


                                           49
     2. BACKGROUND


using Accommodation and Convergence [184, 185].
    This effect has not seen widespread use only being utilized by Kitajima et al. [184]
who manually created a depth based AR system where the plans were moved so a
user could only focus on one to determine if they could regain tell if what was in front
or behind. This study did not find any significant difference in the depth perception
of the objects, but they did note that the users found it easier to focus on one object
at a time. One possible reason is that Accommodation and Convergence are difficult
to track, and off-the-shelf AR solutions do not provide this capability. It makes it a
difficult version of X-ray Vision to use fully [184].

Transparency Based X-ray Visualizations

One form of X-ray Vision used is the ability to adjust the transparency of a part
of an image (as seen in Figure 2.27). By configuring the transparency according
to the current view, a user can be given the impression that the object is either in
the foreground or that the transparency is low enough to feel as if they are looking
inside the object. This form of X-ray Vision is called Alpha Blending.
    Three papers [186–188] have explored Alpha Blending and its possible applica-
tions when adapting it to security cameras, allowing for one line of sight to combine
the information of several security cameras at once. Rather than having each secu-
rity camera link to a single screen, their X-ray Vision method makes the foreground
object more transparent, presenting the content behind them. Early observations
of this technique presented better information and insight when looking through
buildings. Tsuda et al. [188] found that this technique works best when paired with
another visualization technique. The inverse of this has also been used as a visu-
alization technique by making the virtual object less visible when compared to the
real world [189].
    Making the virtual world more transparent can help the user focus on the real
world using the AR system as a guide. This method is commonly used in medicine
where users want to use X-ray Visualization as a guide while still being able to
concentrate on their work since they are not distinct [190–192]. However, when
used from a stereoscopic viewpoint, these transparent effects don’t seem to address
the depth perception mismatch as they don’t provide a clear depth cue to the user.

“Hole in the World” X-ray Visualizations

Hole-based X-ray Visualizations focus on creating a sense of partial occlusion by
allowing a user to look through an object. Figure 2.19 depicts the three ways this
can has been achieved. These methods may use partial occlusion by simply having
a user look through a 2D window (Figure 2.19 virtual window), while others use


                                          50
Figure 2.19: Armadillos sitting behind a corkboard in AR with the same orienta-
tion using hole-in-the-worlds effects to function. On the left are virtual holes, and
on the right are virtual windows.


tunnelling [38] or cutaways [190, 193]. The most popular of these techniques is
creating an open box for people to look into, allowing a sense of how far away the
virtual object is placed in a real-life object. This provides the impression that the
virtual object is inside a real-life object. This can be seen in Figure 2.19 where the
armadillos seem to be sitting inside boxes inside the wall rather than superimposed
in front of the wall.
    Although hole-in-the-world visualizations work quite effectively utilizing any AR
display [37, 38, 181, 190, 193, 194], they have a few limitations. Firstly, the visual-
ization needs to be in a static position. It cannot rotate or change size relative to
the user’s motions, so the hole should not react to the user’s position. The user
could choose to make the hole larger or smaller without breaking the effect. There
is a limited field of view into the hole. Although the limited view creates the X-ray
Vision effect, it restricts the user’s view of the data. One advantage of this X-ray
Vision method is that it can be viewed using any display.

Virtual Box/Hole: The Virtual hole X-ray Visualization was the first type of
X-ray Vision to be implemented and was designed by Bajura et al. [37] (seen in
Figure 2.20). Who aimed to visualize a baby fetus inside of its mother’s womb. They
accomplished this by presenting a box for the baby to be seen in, which would allow
for some level of occlusion. It is still one of the most used X-ray Visualizations and is
still being researched in modern studies due to their effectiveness and versatility [51,
190, 195]. Virtual holes have been found to provide a relationship between virtual
objects and the real world.
     Research has also observed that introducing some particle occlusion alone may
aid depth perception [183]. It has been observed by Kytö et al. [183] that users
could better determine distance if they had a partially occluded object in the scene
for users to use as a reference or even using other virtual objects as a reference.
This was demonstrated in a study that partially occluded the edges of other objects

                                           51
     2. BACKGROUND




Figure 2.20: A image of Bajura et al.’s [37] X-ray Vision technique. Used with
permission from ACM © 1992. Permission to make digital or hard copies of all or
part of this work for personal or classroom use is granted without fee, provided that
copies are not made or distributed for profit or commercial advantage


and found that as long as some of the X-ray Vision effects were covering part of the
virtual scene, then it was possible to make smaller objects behind it appear to be a
given depth behind as there was another object sitting in the front [183].
    SAR X-ray Vision techniques papers utilize virtual holes as they work well with
Perspective-Corrected Projection [196–198]. The initial research in this space looks
at what is required for a SAR based X-ray Vision system. This was done by Avveduto
et al. [196], who asked participants to perform a mock biopsy like the one shown
in Figure 2.21. Participants were then asked to attempt this procedure using either
an image of a leathery surface or an image of buttons, which provided two different
levels of contrast as backdrops for the virtual hole. A virtual hole to a virtual
hole with an occlusion mask against a baseline. The authors found that using the
occlusion mask and the virtual hole provided accurate results, with participants only
being off by approximately 2.5cm (sd = 0.5), whereas in tasks where the participants
used no X-ray Vision 3cm (sd = 0.5).
    Heinrich et al. [197] later used virtual holes with SAR using Perspective-Corrected
Projection tested methods to illustrate depth perception with and without stereo
vision. They compared using the conditions shown in Figure 2.22 (Phong Shading,
Virtual Mirror, Depth-Encoding Silhouettes, Pseudo-Chromadepth, and Support-
ing Lines). This task asked participants to correctly identify the depth order of the
cubes that can be seen in Figure 2.22. This research found that the stereoscopic rep-

                                          52
Figure 2.21: This image contains the X-ray Vision study environment from Avve-
duto et al. [196]. a) shows a study setup that was utilized throughout this study,
with the virtual objects present. b) shows the no X-ray Vision condition used for
this experiment from the participant’s view. c) shows the X-ray Vision conditions
used for this experiment from the participant’s view. Used with Permission from
Avveduto et al. [196].


resentation of SAR improved all conditions regarding time required and perceived
difficulty. Pseudo-Chromadepth and Support Lines were the most effective X-ray
Visualizations, whereas Phong and the Virtual Mirror conditions were the most
challenging.

Cutaways and Tunnelling: Cutaways and Tunnelling present a hole through
an object, revealing an object on the other side of the object [38, 193, 199]. Both
these techniques can be seen in Figure 2.23. They appear as a box with no back.
The point of these visualizations is to give the sense of going through a real-world
object. These visualizations focus more on indicating where the data is in the real
world rather than giving the user a better sense of depth when looking through a




Figure 2.22: The conditions used in Heinrich et al.’s [197] investigation in to SAR
based virtual holes. a) Phong Shading. b) Virtual Mirror. c) Depth-Encoding
Silhouettes. d) Pseudo-Chromadepth. e) Supporting Lines. Used with permission
from IEEE © 2019.


                                        53
     2. BACKGROUND




Figure 2.23: A image of the tunneling x-ray visualization used to look through
multiple walls. The first arrow looks into a storeroom, whereas the second arrow
looks out to the outside. White arrows showcase the depth of the tunnel.


particular piece of data [38].
     Avery et al. [38] observed a slight offset to the perception of depth when tunneling
is used on its own [38]. This can be improved by combining this technique with either
a real-world overlay [200] or any Computer Vision-Enabled Technique [38] on either
the front or back to help ground the visualization. This allowed Avery et al. [38]
and Lerotic et al. [200] to give their users the perspective of looking through a wall,
whereas [201] used a smaller form of this to look through an individual wall. This
smaller attempt would be considered a cutaway [193].
     Cutaways don’t appear much in AR literature however some notable works ex-
ist [202]. The smaller size of the effect means they may require another visualization
to repair the depth offset. However, no research has been done to test whether this
could be an issue. Research from Erat et al. [201] did find that cutaways make for
a very natural way to look through a wall to interact with something like a drone
on the other side when compared to looking at the drone from a first-person’s view-
point, but little else is understood about using cutaways in AR for X-ray Vision.
When testing this use case, they found that technological limitations still held users
back from utilizing X-ray Vision to render objects underneath the ground [201].

Virtual Windows: Virtual Windows shows the user a perspective of a 2D hole
in the wall. These are sometimes presented as a crack in the wall [193] or will have a
slight frame, but what separates this visualization from cutaways is that they ignore
the content that is not known and start the virtual space immediately after the


                                           54
real-world surface [?]. This can be seen in Figure 2.19 , merging with the surface of
a given material [27, 203].
    Guo et al. [203] utilized virtual holes to enable video games to take place in a Non-
Euclidean Space. The authors aimed to look at methods where X-ray Vision could
be made to be more interactive. Enabling people to play a game together regardless
of their spatial requirements. Their experiment had them tag virtual objects that
were to tag objects behind the physical walls, which showed to be almost as useful
as giving the participants a virtual mini-map with the targets located on it.
    Another form of virual windows has been titled "Contextual Anatomic Mime-
sis" by Bichlmeier et al. [27]. This is one of these windows designed to work au-
tonomously with medical applications. The technique that blends virtual anatomical
structures with the appearance of real tissue, gradually transitioning from realistic
skin to virtual content. It gradually moves from having the appearance of skin to
looking more like the virtual content while still presenting a slight overlay based on
the roughness of the skin [27]. The effects of Contextual Anatomic Mimesis were
furthered by Martin-Gomez et al. [36], who have shown a significant improvement
when paired with a another second effect to help represent the depth of an object.
Noting that using more than 1 depth cue is beneficial. These effects ranged from
hatching to applying shading to show shadows to the effect where it was found that
hatching outperformed the other conditions (hole, ghosted mask, constant, shaded,
black, chromatic, and bright) [36]. These results have led us to the understanding
that illustrative effects may have a slight advantage when providing an understand-
ing of the positioning of data for medical diagrams as they can not only provide a
sense of depth perception but are also easy to understand for a general user.

Real-World Overlays

Another way of providing X-ray Vision effects is to represent the physical environ-
ment using a virtual pattern such as a wireframe or a grid. This is typically done
in two ways. One way is to place a pattern on the ground, allowing users to retain
some knowledge of depth by using a constant geometric cue [182]. The other method
utilizes overlaying the real-world object with a pattern of some sort. The second
method is used when viewing virtual objects in stereoscopic displays, as it requires
a level of geometric saliency to be perceptible. Binocular disparities are required for
these types of visualizations to function [204].
    Overlaying a real-world overlay as a guide over the ground does not require any
specific device since as long as the overlay is able to interact with the real world
environment. It will perform better, however, when it is used with a binocular
HMD as this can better represent the surface [205]. While they do provide a good
occlusive cue, the main role they serve is to help the user locate the position of an

                                           55
     2. BACKGROUND




Figure 2.24: This is a demonstration of real world overlay’s and how they are
utilized as forms of X-ray vision. the X-ray Visualizations Wireframe, Random Dot,
and Grid and are placed over colorful patterned box with a virtual cube, sphere
icosahedron, and a Stanford bunny rendered inside. The image was taken using the
HoloLens’ screen view.


object with reference to a real world object. The additional cues provided from that
point moving forward should take note of what is required from that point moving
forward.
    Figure 2.24 illustrates forms real-world overlays can take, for example overlaying
grids [197, 198, 206, 207], Wireframes [182, 188, 208] and Random Dots [171, 204, 205,
209]. All of these effects are designed to utilize partial occlusion to highlight the
shape of the object and to help the user understand the orientation of the wall. This
gives the user a better comprehension of the distance between themselves and the
objects they are looking inside of.

Grids: Grids allow for a persistent barrier between a given surface and provide a
guide on where a surface is in an augmented world [198, 207]. Johnson et al. [207]
developed a surgical system that utilized a grid-based visualization to allow for CT
data to be viewed at the correct position within the patient when using a wearing an
AR HMD. A grid was utilized to represent different types of flesh and a 3D object.
They had surgical students utilize this system with laparoscopic video to evaluate
it, where it was received with a positive reception from the subjective study which
was conducted [207].
    Grids also tend to be utilized to explain to the user where the background of the
X-ray Visualization can be seen. Figure 2.25 shows how Heinrich et al. [198] utilized
a grid overlaying a virtual hole and tested how accurate a biopsy would be using this
as visualizations between an OST AR and SAR. This required testing the accuracy of
the initial insertion angle and the depth to which the user placed the device. Heinrich
et al. [198] utilized two conditions without X-ray Visualization. One was a target
that showed and would change in color when the needle approached the target, and
the other was a virtual needle for the users to match. This method demonstrated

                                          56
Figure 2.25: The conditions utilized for Heinrich et al.’s [198] research. The first
row shows how they were visualized using the HoloLens, while the second line shows
the output from the SAR display. The columns represent the three visualizations
they used in their study, which were then split into three, showing the difference
between their visualizations for guiding the angle and depth required for the needle.
Used with permission from IEEE © 2022.


that the glyph visualization was more effective than the X-ray Visualization, as it
provided participants with a more intuitive X-ray Vision experiance. However, both
of them performed better than showing the visualization of the desired entry of the
needle [198].

Random Dot: Random dot is another way of expressing more occlusion than
other geometric pattern effects typically employed [171, 204]. This effect provides
a partial sense of occlusion by providing a semi-occlusive layer over real-world ob-
jects [204]. This effect is one of the least computationally expensive but requires
an immersive environment with flat surfaces. Highlighting various pixels on the
surface of objects provides a stereoscopic effect of occlusion for the user and should
technically allow for a better sense of depth.
    Ghasemi et al. [171] conducted two studies to evaluate Random Dot visualiza-
tions. A psycho-physical depth perception experiment, which consisted of a series
of thin circles, was run to determine the effectiveness of this X-ray Visualization.
Participants were given six different distances that ranged from 5mm in front and
behind the visualization/wall. The results study found that this effect is most ef-
fective when around 50% of the space is filled using a random dot. The amount of
dots and their size could be altered, but it was most effective when more small dots
were used up close and fewer larger ones up close and more remote. This distance
could be increased if objects are further away from the wall, allowing for more depth
perception.




                                         57
     2. BACKGROUND




Figure 2.26: Ozgur et al.’s [93] X-ray Vision system using Halos. The silhouettes
are generated through orthographic projection onto the front and back planes, which
are oriented based on the sight line passing through the tumor’s center of mass.
These planes are positioned at the points where the organ’s surface intersects the
sight line. Used with permission from IEEE © 2017 and Ozgur et al. [93]


Halo and the Silhouette: This effect goes by many names but is most commonly
called Halo or Silhouette as it creates an outline of the virtual objects. It will be
referred to as the Halo visualization in this dissertation for simplicity. The Halo
X-ray Visualizations as stated in Ozgur et al.’s [93] paper presents the effect as a
bright light around the edges of the object of interest and use alpha blending to
communicate depth [93]. This effect is very similar to what is referred to as the
"silhouette" effect in medical visualization [94]. However, terminology varies across
fields, and in this thesis, I will consistently use the term "Halo visualization" to
describe this effect for clarity. Figure 2.26 show how the the halo is set to leverage
the depth perception cue of relative size. This X-ray Vision effect is unique due to its
lack of occlusion, making it effective for surgical situations [93]. It has been tested
using a monoscopic display in a study designed for minimally invasive surgeries. It
can also exploit a color-based effect, applying visual language to communicate depth
to the user [93].

Computer Vision-Enabled Techniques

In computer vision, several methods are used to highlight salient features in an
image. This is normally done by highlighting various salient factors within an image.
These techniques are commonly used on monoscopic VST devices as they provide
a good sense of where an object is positioned relative to another in 2D. This is
separate from the Halo technique as these do not generally utilize the image of the
real world while the halo technique utilizes the shape of the object that has been
inserted. To this date, the results lack the performance of Computer Vision-Enabled
Techniques in stereo, with most of the publications found in this space either being
demos or research proposals [195, 210].


                                          58
Edge-Based: Edge-based X-ray Visualizations (depicted in Figure 2.27) identify
areas in the user’s view that contrast highly between neighboring pixels and highlight
them. This high contrast effect tends to be regarded by most humans as salient,
making it a reliable method for determining salient artifacts [38]. Created initially by
Kalkofen et al. [211] to allow for better image-based X-ray Vision. This visualization
creates a predictable method for highlighting areas on an image (given that the
surface has a high level of contrast) and makes it a popular way of showcasing X-ray
Vision on a monoscopic AR device [38, 172, 211, 212]. By highlighting the visible
edges of an object, it is possible to create the appearance that an object is behind
the object either through partial occlusion [38, 211] or by cutting out elements of
the X-rayed object [175, 213].

Saliency: Saliency highlights areas of the real world that are likely to be of in-
terest and makes the areas that are unlikely to be less apparent. Figure 2.27 shows
one method of using saliency as an X-ray Visualization. The visualization technique
can vary between implementations because it can be done algorithmically [172] or
by training an AI model to detect the features a human is more or less likely to look
at [214]. A user could naturally peer through objects using the saliency method,
given that the camera could find some salient and non-salient areas of the im-
age [172]. As of the beginning of this thesis, Saliency has only been utilized with
VST AR.

Ghosting: Ghosting utilizes the effects of visual saliency and focuses on allowing
artistic approaches to partial occlusion [215,216]. Adaptive Ghosting is an extension
that combines the salient effect of ghosting and edge-based X-ray Vision effects.
    Zollman et al. [217] tested the effectiveness of Edge-Based, Ghosting, and Ran-
domly occluding underground pipes. Participants were asked to identify the depth
of the pipes, which were located underground, based on a series of Images they re-
ceived. This study found that ghosting was better equipped than edge-based X-ray
Vision at identifying depth underground and noted that it was easy to use, but it
did observe the details of the shape.




Figure 2.27: X-ray Vision showing alpha blending, Saliency, and edge-based Vi-
sualizations (Shown left to right) looking through a building.


                                          59
     2. BACKGROUND




Figure 2.28: A description of the X-ray Vision system used by Kalkofen et al. [39].
This image shows the various components of the graphics pipeline required to create
the adaptive ghosting effect. (a) The occluder and occluded elements are combined
into a ghosted view using state-of-the-art transparency mapping based on the im-
portance map of the occluder. However, this initial ghosted view lacks is difficult to
integrate into the real world seamlessly as it is either used to cover a area or it is
not. To address this, a new importance map is generated for the resulting ghosted
view which looks at area’s of high contrast and places the visualisation on to them
as well as over the rest of any are’s close to virtual elements that need to be seen
within the physical objects. (b) This new importance map is then compared to the
original importance map of the occluder. Features that were marked as important
in the occluder but are no longer prominent in the ghosted view are identified for
enhancement. (c) By applying these enhancements to the adaptive ghosted view,
the previously obscured important structures become clearly visible again. Used
with permission from IEEE © 2013.


     Adaptive Ghosting technique to the current environment makes it less varied
when viewing it from different directions [39]. Figure 2.28 presents Kalkofen et
al.’s [39] system for Adaptive Ghosting, which normalizes the contrast of the input
image to allow this effect to maintain a similar effect even in different lighting.
This system combats one major issue with computer vision-enabled X-ray Vision
techniques: their inconsistency depending on changes to the scene and lighting.
Adaptive Ghosting can cause unexpected artifacts and misleading visualizations,
but it is still possible to notice this effect even when using Adaptive Ghosting.

Comparisons Of X-ray Vision Effects

Given all these methods of X-ray Vision, it is quite common for people to try to
compare them to determine the positive and negative aspects. Studies in this area
have investigated:



                                         60
                                        Visualization Pairings in Literature

                            Binocular                          2                      5


   Computer Vision Enabled Techniques                          4          15    53         Literary Frequency
                                                                                                    60

                 Real−World Overlays                          22          65    15                  40

                                                                                                    20
                                       Hole                   46          22    4     2             0


             Transparency Adjustment



                                                                e
                                                 t




                                                                                      ys




                                                                                       r
                                                                                      es
                                                 en




                                                                                     la
                                                             ol




                                                                                   qu
                                                                        rla




                                                                                 cu
                                                         H
                                              stm




                                                                                ni
                                                                     ve




                                                                              no
                                                                             ch
                                             u




                                                                    O




                                                                           Bi
                                          dj




                                                                          Te
                                                                ld
                                        A




                                                              or


                                                                          d
                                     cy




                                                                          le
                                                             W
                                  en




                                                                        ab
                                                        l−
                                   r




                                                                    En
                                pa




                                                         a
                                                      Re
                           ns




                                                               on
                            a




                                                           isi
                         Tr




                                                         rV
                                                        te
                                                      pu
                                                   m
                                                 Co




Figure 2.29: A heat map illustrating how often different X-ray Vision effects are
compared to each other in the literature. The diagonal cells (from lower left to
upper right) show the total number of studies for each effect, while the center row
indicates studies that examined each effect individually without comparison.

  • The amount of occlusion density required for visualizations to create the X-
    ray Vision visualisations and effective limits to the practicality of these ef-
    fects [218];

  • How different displays react to X-ray Visualizations on Mobile Devices [175,
    213].

  • What is the impact of X-ray Vision between different visualizations [36, 172,
    182, 217].

These comparisons are detailed in Figure 2.29.
   Partial occlusion is important to most X-ray Visualizations. It shows how much
occlusion is needed for a given technique and how much occlusion makes interactions
impossible. This was the question that Santos et al. [218] addressed for mobile AR
X-ray Vision when using edge-based or saliency X-ray Vision effects. This study
had participants look at a box wrapped in either wrapping paper or crumpled foil.
This study utilized two tasks: One looking at what thresholds were required to allow
participants to see 2D objects using X-ray Vision effects, and the other looking at
the maximum allowable alpha values these effects could have [218].
   Santos et al.’s [218] first tested how the user could see four objects placed inside

                                                                61
     2. BACKGROUND


the box and asked them to reveal the maximum decision cutoff (threshold). The
next task was to lower the alpha value of the visualization until participants could
clearly see the object hidden behind it. Inside the box would be visualized an image
of an object they were asked to see, which could either be a small or a large object
about the box. Each second, the transparency of the visualization would decrease,
asking them to press a button on the display to stop the visualization when they
could identify the item. The combination of Santos et al.’s [218] results indicates
the clarity of the object was dependent on the size, color, and texture of the box
rather than the visualization used. The study noted Saliency was slightly harder to
see through but not to a significant level. This dissertation treats these results as a
guideline and has based its use of transparency and contrast with X-ray Vision on
these results.
    Sandor et al. [172] Compared Edge-based visualizations to their newer Saliency
visualization by having participants try to find a target in four scenes. This study
found little difference between saliency and edge-based visualizations, but partici-
pants preferred the edge overlay. After this, both these conditions were tested using
an online survey where participants would look at images presenting the information
where saliency was seen to be better for providing X-ray Vision for images.
    Dey et al. [175, 213] compared X-ray Vision on different-sized mobile devices to
determine the display size could impact the visualizations required for Mobile de-
vices. Firstly, they developed an X-ray Visualization called Melt, which showed a
part of the foreground and a part of the background together. Their first exper-
iment tested the Edge-based technique against the Melt X-ray technique from far
distances using a 7-inch display. This study found that Melt was a more accurate
in-depth estimation but took the participants longer to judge, which was likely used
to overcome the drawbacks of the visualization [175, 213].
    Dey et al. [213] ran a user study comparing Edge-based and Saliency using a
Mobile AR display. They created three levels of each X-ray Visualization, where
they were more or less sensitive to environmental concerns. These results indicated
that users struggled to use X-ray Visualization when the edges were too thick for
the edge-based variable, and saliency struggled in bright environments.
    All of the comparisons explained in this section utilized similar X-ray Vision
visualizations and most focused on how X-ray Visualizations worked in different
areas. All these examples utilize mobile devices, so how these would affect HMDs
in general is a question that is yet to be answered. Also, most of these comparisons
have utilized various forms of computer vision techniques (except for one use of the
Melt visualization). Currently, there are many unknowns that exist in this space.




                                          62
Figure 2.30: Eren et al.’s [180] different visualizations of underground pipe net-
works using X-ray Vision techniques. a) baseline, b) edge-based, c) virtual hole, and
d) cross-sectional techniques. Used with permission from Springer Nature © 2018.


2.5.4     Applications of X-ray Vision
X-ray vision has been applied across a wide range of domains where visualizing hid-
den or internal structures is valuable. Medical research has been a central focus,
where overlays of anatomical structures can support diagnosis and guide surgical pro-
cedures by allowing clinicians to perceive organs, tissues, and bones in situ [27,37,51].
While demonstrations have shown promise, challenges remain around depth percep-
tion and real-time performance, as highlighted in comparative evaluations of iso-
surfaces, direct volume rendering, and alternative rendering effects [27, 28]. Outside
of medicine, construction and maintenance tasks also benefit, with researchers ex-
ploring techniques for seeing into walls or underground pipe networks. Early work
by Feiner et al. [219] focused on gradually revealing occluded structures, and more
recent studies (e.g., Eren et al. [180, 220], shown in Figure 2.30) have evaluated
different visualization methods for accurately perceiving underground utilities.
    Beyond these domains, X-ray vision concepts have been investigated in secu-
rity, navigation, and education. In security, transparency effects and perspective-
corrected projections have been used to augment situational awareness, though eval-
uations suggest togglable, more occlusive effects may be most effective [186–188].
Navigation and tourism applications demonstrate how seeing through structures or
into the past can improve wayfinding and enrich cultural experiences, with evalu-
ations showing reduced cognitive load compared to traditional maps [221–223]. In
education, while the effects of X-ray vision itself remain underexplored, augmented
reality has been shown to increase engagement and may support long-term retention
in domains like anatomy [19, 57].
    Entertainment provides a final but growing space for X-ray vision. While formal


                                           63
     2. BACKGROUND




Figure 2.31: A promotional Image from Dr Grordborts Invaders made by Weta
Workshop for the Magic Leap. Used with permission from Weta Workshop.


research is limited [224], several commercial AR games such as Microsoft’s RoboRaid
and Dr. Grordborts Invaders (shown in Figure 2.31) have experimented with the
concept, using it to merge real and virtual environments in playful, interactive ways.
These applications highlight both the versatility of X-ray vision across domains and
the ongoing challenges of designing visualization techniques that are perceptually
effective, intuitive, and computationally feasible.


2.5.5     Hardware For X-ray Vision
X-ray Visualizations can be split into two key components to enable X-ray Vision:
A display to view the visualization and sensors to collect the data for X-ray Vision.
While many studies have utilized collected information, allowing them to require
only devices to collect pre-rendered devices, others have utilized a display.
    X-ray Visualizations rely on two essential aspects: the availability of data repre-
senting the internal or hidden structures, and the means to visualize this data effec-
tively. While many studies utilize pre-existing or independently collected datasets,
others focus on the development and evaluation of display technologies for present-
ing X-ray Vision. It is important to note that data acquisition is typically a separate
process from visualization, and in most cases, data is gathered without a specific
visualization method in mind. However, the choice of data acquisition can limit the
types of visualizations that can be effectively employed. For instance, real-time data
acquisition methods may restrict the complexity of visualizations due to processing
constraints, while pre-collected datasets allow for more intricate and computation-

                                          64
Figure 2.32: A bar graph of the type of display used to visualize the various X-ray
Vision effects.


ally intensive visualization techniques.

Displays

AR displays come in different variations, each with its own benefits. These different
types of displays can be generalized as OST AR, VST AR, and SAR. OST AR works
by projecting the image on a transparent reflective display, creating a transparent
display which is generally used for HMDs. VST AR displays utilize one or more
cameras and display them with graphics overlaid on them. SAR uses projectors
to place computer graphics to overlaid onto reality. Figure 2.32 shows there are
preferences between different types of X-ray Vision devices to an extent due to the
individual benefits and drawbacks caused by these displays which have been laid
out in Table 2.1. Figure 2.33, shows real-world overlays are much more common
than computer vision techniques, but real-world visual techniques are less common
on monoscopic displays.
    X-ray Vision systems can be supported by display devices, including mobile
phones, head-mounted displays, computer screens, and projectors [145]. Devices
range from traditional computer screens and mobile personal devices, head-mounted
displays to projectors. Most earlier devices only accommodate monocular vision,
while recent head-mounted displays can support stereoscopic vision. Figure 2.34
shows that the choice of technology is often a result of technological advances and
emerging products. It can be observed that mobile screen-based displays were most
popular following the release of powerful mobile phones in 2007, underpinned by
emerging computer vision techniques for tracking and the generation of visualiza-
tions. Similarly, stereoscopic head-mounted displays have gained popularity be-
cause of increased capabilities, technology maturity, and general availability in re-
cent years.
    Recently, research utilizing Microsoft HoloLens 1 and 2 has been widely used in
X-ray Vision research because of their advanced display tech and gesture recognition,
making them a useful device for security and medical operations [36, 161, 225, 226].


                                           65
     2. BACKGROUND




Figure 2.33: A bar graph displaying The types of AR that was used to visualize
the different types of X-ray Vision effects.


HMDs shine in medical settings, offering a sterile work environment. Medical appli-
cations use screens for remote interactions, like controlling robotic arms in minimally
invasive surgery [227]. In contrast, the construction and maintenance industries em-
ploy diverse devices due to their varied environments [180, 181, 206, 220].

Comparison of Displays

Comparing the effectiveness of different displays seems to be a relatively new area
of study for AR enabled X-ray Vision. Prior to starting this dissertation in 2019,
our review found no studies that compared X-ray Visualizations between different
papers. Since then, several studies have published their results on comparing VST
AR devices against OST AR devices.
    Gruenefeld et al. [182] performed depth perception studies using an OST headset
utilizing X-ray Vision, including Grid, Cut-out effects. The grid effect, placed on
the ground within the X-ray able space and a perpendicular wall facing the viewer,
could be used to approximate a relationship between the wall and the object. In
contrast, the cut-out visualization provided a hole in the wall, which the user could
look through to see where the object was on the other side, against a baseline that
pointed participants to the object with a red arrow and displayed a perpendicular line
going from the far point of the arrow to the bottom of the visualization. Gruenefeld
et al.’s [182] found that the weakest of the depth cues were the cut-out. This was
because the cues did not convey a clear depth indications to the user [182], because
the grid effect provided a clear indication of depth, allowing the users to count the
amount of grid squares between the object and the wall they had a better sense of
depth.
    Another set of studies published by Martin-Gomez et al. [228] studied the dif-
ference between four X-ray Visualizations (None (Superimposition), virtual hole,
ghosting, and Random Dot) on both VST AR and OST AR devices in the near
field. They found that users better utilized X-ray Vision on a VST AR headset
than on an OST AR device. This prompted another study investigating different

                                          66
                               Devices Examined for X−ray Vision Over the Years
                     10


                     8
Literary Frequency




                     6


                     4


                     2


                     0
                           19 2
                           19 3
                           19 4
                           19 5
                           19 6
                           19 7
                           19 8
                           20 9
                           20 0
                           20 1
                           20 2
                           20 3
                           20 4
                           20 5
                           20 6
                           20 7
                           20 8
                           20 9
                           20 0
                           20 1
                           20 2
                           20 3
                           20 4
                           20 5
                           20 6
                           20 7
                           20 8
                           20 9
                           20 0
                           20 1
                           20 2
                             23
                             9
                             9
                             9
                             9
                             9
                             9
                             9
                             9
                             0
                             0
                             0
                             0
                             0
                             0
                             0
                             0
                             0
                             0
                             1
                             1
                             1
                             1
                             1
                             1
                             1
                             1
                             1
                             1
                             2
                             2
                             2
                          19




                                                         Year
                                                   Paper Classification
                               SAR Projector(s)   Mono Mobile     Stereo Screen   Stereo HMD
                               Mono Images        Mono Screen     Mono HMD


Figure 2.34: Devices examined for X-ray vision across the literature over time.
Each point represents a publication, categorized by device type (vertical axis) and
year of appearance (horizontal axis). The distribution highlights how research inter-
est has shifted across different device categories, with increased attention to head-
mounted displays and mobile devices in more recent years.


rendering techniques for X-ray Vision (shading, hatching, ghosting) and brightness
levels, finding that bright, clear objects work best in OST AR.
    Heinrich et al.’s [198] research comparing OST AR displays to SAR displays
(previously mentioned in Section 2.5.3) presented findings that both OST AR and
SAR displays functioned in a similar fashion. However, they found that X-ray Vision
works better with OST AR displays, while UI elements (see Figure 2.25, Glyph Vis)
tend to function better when using projectors. These different conditions can be
seen and compared in Figure 2.25.

Sensors Utilized

The information used in X-ray Vision applications may possess different character-
istics, such as the nature of the data, its temporal characteristics, and its realism.
Three-dimensional models, point clouds, video feeds and photos, medical data, and
depth maps are examples of the diversity of data visualized in X-ray Vision. More-
over, one can distinguish static information, which remains unchanged for a task,
from dynamic information, which may change during a task. Finally, the degree of
realism of the data can vary.
    Figure 2.35 illustrates that static information prevails as the most common vir-
tual element in X-ray Vision, constituting 65% of the 54 papers examined. Examples
include 3D models, medical images, and building schematics, predominantly found in
medical, construction, and maintenance domains. Virtual objects range from simple

                                                       67
     2. BACKGROUND


geometric shapes to intricate representations of real objects [180, 181, 206, 217, 220].
    Dynamic data, as shown in Figure 2.35, typically consists of video feeds from
static or mobile cameras. By providing multiple perspectives in real time, these
feeds allow users to perceive distant or hidden details while still observing the actual
environment, a capability that has proven valuable in domains such as security and
robotic system control [195, 201]. In contrast, non-dynamic data is often employed
in construction, where pre-recorded or modelled information is used to represent
underground pipes and other hidden infrastructure [180, 181, 206, 217, 220]. Within
this context, X-ray Vision studies in construction have relied primarily on simulated
data to approximate real-world conditions and to address the challenge of clearly
communicating the location of underground structures to end users.
    Most of the research that utilizes live recordings in this review employed cam-
eras or medical equipment (shown in Figure 2.35). No studies were found that
visualized radar and other sensor data in a 3D manner to create an X-ray Vision
effect—limiting the types of visualizations that people were viewing. Techniques like
photogrammetry were not found either which would be able to recreate images from
several views into a 3D scene [229, 230]. This suggests that, so far, X-ray Vision
has primarily focused on presenting users with visualizations of the known world.
In contrast, little work has explored systems that could reveal entirely unknown or
hidden environments in real time, effectively giving users access to true X-ray vision.




                                          68
Figure 2.35: A plot showing nine devices found in the literature. Head-mounted
displays used as display devices are grouped. Portable screens like phones and
tablets, larger screens, and magic mirrors fall under the ’Screens’ category. Addi-
tionally, three user studies utilized Images and SAR.




                                        69
  2. BACKGROUND



Table 2.1: Advantages and Disadvantages of Devices Used for X-ray Vision

      OST AR Head-Mounted Displays (Optical See-Through)
Advantages                    Disadvantages

– Direct view of real world with overlays    – Limited brightness and contrast
– Maintains natural depth cues and pe-       – Smaller field of view vs. VST
  ripheral vision                            – Alignment/calibration issues
– Good for tasks needing situational         – Optical distortion of virtual content
  awareness (e.g., surgery)                  – Lower graphical fidelity
– Hands-free operation

       VST AR Head-Mounted Displays (Video See-Through)
Advantages                    Disadvantages

– High graphical fidelity and brightness     – Camera-mediated view
– Easier integration of computer vision           ∗ Latency
– Wider field of view possible                    ∗ Distortion
– Flexible virtual content manipulation      – Lower situational awareness
                                             – Reduced situational awareness
                                             – Can cause motion sickness
                                             – Heavier, bulkier hardware

                      Mobile Devices (Phones/Tablets)
Advantages                             Disadvantages

– Widely available, easy to use              – Less immersive
– Bright displays                            – Must be handheld
– Portable for quick tasks                   – Smaller screens limit detail
– Stable, high-quality graphics              – Hardware fragmentation
                                             – Limited spatial interaction

           Spatial Augmented Reality (SAR) / Projectors
Advantages                        Disadvantages

– Projects info directly on objects          – Needs precise calibration/tracking
– Multiple users at once                     – Works best in controlled environments
– No wearables needed                        – Sensitive to lighting/surface
– Limited multi-user suitability             – Limited multi-user suitability

                             Desktop/Fixed Screens
Advantages                              Disadvantages

– High computational power                       – Not immersive
– Stable, high-quality graphics                  – Stationary, low spatial interaction
– Large display for detail                       – Requires looking away from real world
– Comfortable for long use                       – Limited field of view
– Good for collaboration                         – Not mobile-friendly
– Easy integration with systems



                                            70
2.6     Perception and Depth Perception tasks in Aug-
        mented Reality (AR) and Virtual Reality (VR)
        Head Mounted Displays
Depth perception on MR HMDs has been a goal for over 25 years [71]. Several
common methods of testing depth perception in the real world. These include

  • Blind Walking or Blind reaching: Asking a participant to place their
    hands or to walk to a location where a virtual artifact was previously [31,231];

  • Verbal Reporting: Requesting the participant tell you how far away the
    virtual object is from them [31, 231];

  • Matching Protocols: Placing a virtual object relive to where the virtual
    object is (or was) [31, 231];

  • Two-alternative forced choice: Giving the participant where there is one
    correct and wrong answer on a set of conditions that will get closer to being
    equal to determine at what point can participants no longer determine proper
    depth perception [209, 232].

Early work in this field focused on testing how seeing graphics rendered using virtual
reality headsets may have changed, and how real-world factors impact the depth per-
ception of the environment [233]. Ellis et al. [233] Evaluated the difference between
monocular vision, binocular vision, and a stereoscopic display utilizing a rotating
display where participants were asked to judge the depths they saw different virtual
objects. To do this, they had participants place and verbally estimate positions in
which real or virtual objects were away from them. Overall, Ellis et al.’s [233] study
found a high amount of work stating that both binocular and stereoscopic displays
provided excellent depth perception when compared to monocular displays. They
also noted that when virtual content was displayed behind a real-world object, it
created the same mismatch, about 6cm closer to the viewer/participant than it was
displayed. Most depth estimations were within 2cm of the actual position [233].
    McCandless et al. [234] followed up this study by adding both motion and a time
delay on HMDs. People moving their heads causes motion parallax, which allows
for a sense of depth between objects. McCandless et al. [234] control study found
that when the virtual object was moved over a meter away, movements caused a
noticeable drop in depth perception, which was shown to be understated compared
to their motion parallax study. This demonstrated that the worse the experience
was, the more users moved around and noted an increase in the large time delay
between their head movements and the interaction time.

                                         71
     2. BACKGROUND


    Rolland et al. [235] looked into the perception of different shapes (cubes, oc-
tahedrons, and cylinders) when viewed from a HMD. They did this by presenting
these shapes in several different sizes and testing if participants could determine
what shapes were closer to them using a 2AFC study design. This study design
only found a little change between different designed shapes.
    Later on, Mather and Smith. [236] investigated a method to determine if using
multiple depth cues could improve depth perception. The depth cues investigated
in this experiment were Contrast, Blur, and Interpolation. All possible different
combinations of these conditions were used. This experiment was done using a
computer monitor. Participants saw many different textures displayed on four planes
partially overlapping each other, each with a different depth cue that could be used
as an aid. Participants would click on all the different textures from nearest to
furthest to determine where an item should be placed. This experiment showed
that participants could most easily tell where objects were with all three cues, but
they struggled with the other cues, especially interpolation.
    While a lot of work happening at this time was focused on the far-plane, Wither
et al. [237] looked at methods that make virtual objects appear further away rather
than just smaller. A Sony Glassatron was used which was a common monoscopic
HMD of the time, but they lacked basic depth cues. Their study utilized flat planes
as showdowns in the virtual world, giving users an on-screen map to view items with
and coloring the markers so they appeared in the correct positions. They would have
users view objects that were 38, 55, and 65 meters away, and users would have to
guess their location. This was done using a group of objects and using individual
objects. This study showed that shadows and size were the best indicators while
changing the color was the worst depth cue Wither et al. [237] tested.
    Armbrüster et al. [34] worked on determining what elements the virtual reality
headset displayed that affected the participants’ depth perception. This included:

  • The virtual environment would change between three different environments:
    one had no graphics shown as part of the world, one showed the world as a
    meadow, and the final one was a large but enclosed gray room.

  • participants were asked to guess a total of ten different distances. Four of
    these were in the near-field space, and six of these were in the action space.

  • They would also toggle a ruler on the ground that would showcase the distance
    from themselves in meters.

They also had two tasks, one where participants would either need to be able to
see spheres located at all of the different distances, or they could only see a single
sphere at a time. This research did not provide any clear indication of whether any

                                         72
Figure 2.36: The OST AR display was utilized for Swan et al.’s [239] study. These
images show how the OST AR HMD was mounted for people to view. Used with
permission from IEEE ©2007.


of these conditions were able to be observed, but they did note that participants
underestimated the distances of the objects they were viewing and that users had a
better sense of depth when objects were closer to them.
    Another study that examined the virtual environment’s effect on participants
was performed by Murgia and Sharkey [238]. This study looked solely into how
people perceive depth within the virtual space. To do this, they created a life-
sized virtual environment using a CAVE, designed to work with stereoscopic glasses
and react to users moving within the virtual environment. They tested a range of
conditions, including two levels of graphical quality, one where the environment was
bland and one where 1-meter objects would appear. They introduced real-world
reference objects to help the participants understand the correct depth at which
their virtual counterparts would be located. This study found that the clearer the
virtual environment was at displaying depth, the easier the user could determine
depth within it.


2.6.1    Depth Perception User Studies on Ocular See Through
         (OST) Augmented Reality (AR) displays
OST AR displays allow users to view the real world while graphics are overlaid on
the real world, which makes them useful for a range of stress-inducing professions.
This creates a different dynamic for depth perception as the whole environment is
the real world. This can lead to increased stress levels when performing precise
tasks, making depth perception a critical element of any augmented reality system
using OST AR displays.
   Swan et al. [239] performed the first depth perception study using an OST AR
devices. Participants were asked to either verbally report or blindly walk to a point


                                         73
     2. BACKGROUND


displayed using the headset while viewing real-world objects with and without a real-
world headset. The world was rendered virtually, or only the marker was rendered.
The headset was mounted on an apparatus shown in Figure 2.36 that was not able
to move. Forcing users to move to the position of the stimuli that they saw. The
users were asked to turn as the examiners removed the real-world object. This study
found that the AR conditions were more accurate than the VR conditions. They
also found that blind walking is performed more accurately than verbally guessing
where an object is positioned.
    Later on from this, Jones et al. [240, 241] ran a series of studies that looked into
several different methods of depth perception. Their first study ran a similar process
to Swan et al.’s [239] compare VST AR to OST AR. However, this time, the HMD
was mounted to their head instead of being fixed, allowing for depth cues involving
stereopsis [240, 241]. These results showed that the OST AR conditions did better
than the prior ones.
    One issue that was possible with the previous three experiments was that users
may have been estimating the location based on aspects in the environment [240,
241]. To get around this, Jones et al. [240] ran another study where they did not
just occlude the area behind the OST AR HMD, but they occluded the participant’s
complete vision. This indicated that users were using the real-world environment to
assist their depth perception. These results were further confirmed when users only
had their vision partially occluded.
    The prior work by Swan et al. [239] and Jones et al. [240, 241] explained that
when in the action space AR depth perception is more limited when compared to VR.
However, two key questions remained before OST AR devices could see widespread
use: first, how accurate are these devices; and second, how tasks performed in the
near field influence the effectiveness of OST AR techniques. The latter was inves-
tigated by Singh et al. [242] who created a study design based on blindly reaching
where they found that off-the-shelf hardware could present depth accurate to 2cm
to 4cm to the point [242, 243].
    Previous studies by Jones et al. [240, 241] Swan et al. [239] utilized a Sony
Glassatron. This was changed for the nVisor ST60 OST AR HMD seen in Fig-
ure 2.37 [244]. This research consisted of three separate studies: one of these was
done having participants move another virtual object to the exact location as a
physical one by moving a slider underneath the desk (shown in Figure 2.37 (a)); the
other two had participants placing a virtual object in the same position as the one
they were looking at (shown in Figure 2.37 (b)). One of these placement studies
was used to determine whether corrective feedback from graphical elements could
aid participants. All of these studies showed that there was approximately a 1cm
difference between placing virtual objects using an OST AR HMD than in real life


                                          74
Figure 2.37: Swan et al.’s [244] study design, detailing the apparatus used for
the two different types of experiments they ran utilizing reaching tasks. Used with
permission from IEEE © 2015.


when in this task environment. These experiments with the newer headsets showed
that the results were accurate to within 1–2 cm. This demonstrates that the type
of display technology used in different headsets can significantly impact the accu-
racy achievable with OST AR HMDs. The bais seen in this study’s graphs shows
was higher than this as it was common for people move the object too far away
from them rather than closer to them when trying to match the position of physical
objects [244].
    Many years after Swan et al.’s [239] initial research, Medeiros et al. [245] ran a
slightly different experiment that also tried to learn what the impact was between
depth perception between VST and an OST AR HMDs. The infrastructure to create
VST and OST AR HMDs was, at this point, quite different, and they began to have
different pros and cons. The Oculus VR HMD 10 (now owned by Meta (previously
Facebook)) had recently been publicly released, which used a Fresnel lens [246].
Meanwhile, the Meta lens (made by the company meta which is now insolvent
insolvent) OST AR HMD 11 utilized a distorted reflection of a screen’s projection.
Medeiros et al.’s [245] study design asked users to draw a line between two spheres
in the virtual space. The findings from Medeiros et al. [245] study showed that the
OST AR headset was less accurate in drawing lines between two points. Participant
comments indicated this was due to the difference in view windows between the
Oculus and the Meta HMDs. Participants using the Meta headset struggled to keep
both objects in view at once, while when they were wearing the Oculus, participants
seemed to have no issues. This study highlighted the importance of a field of view
when comparing activities between two headsets, as a smaller field of view could
 10
      https://en.wikipedia.org/wiki/Reality_Labs
 11
      https://en.wikipedia.org/wiki/Meta_%28augmented_reality_company%29


                                          75
     2. BACKGROUND




Figure 2.38: OST AR HMD which was utilized in Singh et al. [247] experiments.
(a) Details the utility of each lens and how they distort the user’s view. (b) Details
how these participants see through this lens, allowing for the HMD to function. (c)
The real-life implementation of the custom HMD. This image has been modified and
is used with permission from IEEE © 2018.


impact the ease with which a given participant can react to certain stimuli.
     The work that was started by Swan et al. [239] was later finalized by Singh et
al. [247] who developed the OST AR HMD detailed in Figure 2.38 which was able to
react to five different focal cues allowing for highly accurate visualizations. Due to
its large weight, this headset was placed on a desk whose height could be adjusted
to match the participants’. The near field depth perception of this headset was
tested by a series of matching the graphical cue-like tasks similar to what was done
prior in Swan et al.’s [244]. The first study tested the same conditions as Swan et
al.’s previous study [244]. This type of OST AR device was shown to be able to
improve depth perception to about 1 cm accuracy in the near field, regardless of the
user’s age. The second study evaluated how people of different ages performed the
task, which showed that people’s depth perception of AR seemed to function fine
regardless of age. The final study changed the brightness of the graphical content,
which showed no difference [247].
     Hua and Swan [248] utilized the same apparatus and ran a study that asked
users to tell the depth of an occluded object. This was done by allowing for the
option of a temporary occlusion barrier. The authors found this headset was able
to reduce the errors from a previous 4cm [249] on VST AR HMDs.
     Whitlock et al. [250] later looked at the difference between using controller ges-
tures and voice commands to aid the placement of objects using a HoloLens. Partici-


                                          76
Figure 2.39: Several images from Al-Kalbani et al.’s [33] study set up. (a) displays
grasp measurements required for Grasp Aperture (GAp) and grasp middle point
(gmp) (b - m) images of participants completing the task, (b - g) without shadows,
(h-m) with shadows. (n) the interaction space in reference to the Kinect. Used with
permission from IEEE © 2019


pants were asked to complete a series of precision tasks, including selecting, rotating,
and translating virtual objects placed at different distances from each type of control
interface. Whitlock et al. [250] found that participants perceived embodied freehand
gestures as the most efficient and usable interaction compared to device-mediated
interactions. Voice interactions demonstrated robustness across distances, while em-
bodied gestures and handheld remotes became slower and less accurate when the
distance was increased. These findings emphasize the importance of selecting appro-
priate interaction modalities based on distance when designing the studies in this
thesis.
    Many previous papers showcased how the depth perception of computer graphics
can be realized when they are placed in the real world. However, very few of these
focused on what happens if these graphical objects react to the real world, such as
having them leave a shadow over the ground. Using OST AR devices is difficult

                                          77
       2. BACKGROUND


as they cannot display darker shades of colors. Al-Kalbani et al. [33] ran a study
that looked at the accuracy of participants trying to grab hold of virtual 3D shapes
using a HoloLens2 as shown in Figure 2.39 (b - m). To do this the placement of
their hands was situated around the virtual cube as shown in Figure 2.39 (a) using a
remote sensor (a Kinect 2 12 ) placed just over 2 meters away as shown in Figure 2.39
(n). The results from this study showed that participants tended to overestimate
the area they had to grasp. Drop shadows were appreciated when they were visible
to the participants and increased the amount of time required to grasp the object,
but they did not improve the participant’s accuracy. Users also underestimated the
depth required by 2cm.
    It is common in AR experiments to display objects as if they are hovering in
reality; however, since this itself is not a natural appearance of the virtual object,
it may be misleading the end user. This is even more relevant when using OST
AR HMDs as they show virtual objects superimposed over the participants’ vision.
Rosales et al. [32] set out to test this hypothesis by testing if a participant’s depth
perception was more accurate by having participants blindly walk to a position where
they believed they saw a virtual cube that was either hovering or on the ground.
Whether an object was on or off the ground, participants tended to underestimate
the depth they needed to move towards, but targets off the ground they judged to
be closer to them.
    Adams et al. [251] also ran a study looking at how depth perception could be
influenced by having the graphics respond to the real world by investigating depth
perception with virtual objects placed on and off the ground. This study design
utilized a 3 x 2 x 2 x 2 design looking at three distances in the action space (3m,
4.5m, and 6m), the presence of shadows, causing the virtual object to hover off the
ground, and if it was being viewed by a VST or an OST AR display. The results from
Adams et al. [251] showed an underestimation of the values of over 17%. There was
a small increase in accuracy regarding the presence of shadows, showing a significant
improvement of 2%.
    Similar to Swan et al. [239], Medeiros et al. [245], and Adams et al. [251], Ping
et al. [252] aimed to determine what differences in depth perception can be observed
between AR and VR, and whether different depth cues have varying impacts. This
study used three different depth cues (points, lines, and boxes) as conditions and four
different illumination ranges which to change the texture of the surface (Ambient,
Half-Lamber, Blinn-Phong, Cook-Torrane). Participants were asked to use these
depth cues to align differently sized objects while they were between 1.75m and 7.35
m away from the shapes they were being asked to align. Participants would control
one of the shapes using a keyboard input, allowing them to move the virtual objects
 12
      https://en.wikipedia.org/wiki/Kinect


                                             78
forward or backward. Ping et al.’s [252] results showed that there was little difference
in the depth cues that improved depth perception between the OST and VST AR
HMDs, but they did note that illumination of the objects made a larger effect on
a participants depth perception. This effect was likely caused due to participants
perceiving the objects were the same shape regardless of their size, they would all
look alike unless and having more details on the shape may have given them more
attributes they could match.
    This finding caused Ping et al. [69] to investigate how different shaders could
influence depth perception. This study had a similar physical setup as their previous
study but only used a HoloLens [252]. The conditions were split into a 3 x 3 study
setup using three different shaders: Half-Lambert, Blinn-Phong, and Cook Torrance,
each colored as blue, yellow, or green, and displayed at either 25cm, 30cm, 35cm,
or 40cm. This study showed that participants seemed to be more accurate with the
brighter-colored virtual objects (yellow or green). Participants struggled to place
larger objects, and both the shaders that had more pronounced specular highlights
were found to aid depth perception the most.


2.7      Research Gap
This thesis looks at methods that can be used to allow a user to view DVR visual-
izations within the real-life objects they have been created from when using an OST
Device. This first required a better understanding of how X-ray Vision worked on
OST AR HMDs. Then, it required taking the learnings from that and placing and
implementing them in a way that could be utilized with DVR. We would aim to test
our findings to determine if these X-ray Visualizations impact the user’s experience.
    At the time of starting the research (2020) for this thesis, there was little research
on how X-ray Vision within the near field functions when using OST AR HMDs.
Some work was done using early versions of the technology within the action field
prior [51,201,210], but no research had been done in the near field. Around the same
time as our own research, several other papers came out trying to solve this issue,
highlighting the need to fill this gap [36, 182]. These papers looked at methods and
presented scenarios using static graphics overlaid onto the real world while having
users test out various methods while constraining the users’ actions by having them
stand in one place and controlling how they interacted with the task [36, 182]. The
research in this dissertation differs from these because these visualizations were in
an environment that allowed for a more ecologically relevant scenario with few con-
straints on the user’s freedom, presenting less biased results in favor of more realistic
conditions. The difference between Geometrical Saliency and Visual Saliency was
also compared to determine what makes the most appropriate X-ray Visualization

                                           79
     2. BACKGROUND


for an OST AR HMD. This was done by creating an algorithm that allowed for the
calibration of a camera image over the view of the use of an OST AR HMD.
    Then, taking the lessons learned from our research on X-ray Vision of OST
AR devices, a range of X-ray Visualizations was created and analyzed utilizing
illustrative effects. These were then tested using two separate studies; one of these
was aimed at testing the user’s ability to analyze the data, and the other tested how
accurate the user’s sense of depth was when looking at these objects. As previously
stated by Grosset et al. [133], the large range of distinct differences in publicly
available data sets can cause wildly different results across methods that utilize the
same condition. So, in a bid to allow our datasets to be utilized in HCI studies,
this type of visualization allowed us to perform tasks that could be repeated in a
numerous (over a septillion) different ways and modified for many different studies.
    To clarify the main research gap addressed by this thesis, the key points are
summarized as follows:

  • Lack of research on near-field X-ray vision in OST AR HMDs: Prior work
    (e.g., Blum [51], Rompapas [210], Erat [201]) focused on the action field or
    static overlays, but little was known about how X-ray vision functions in the
    near field.

  • Limitations of prior studies: Contemporary work (Gruenefeld [182], Martin-
    Gomez [36]) constrained user freedom (static standing positions, restricted
    interactions), limiting ecological validity.

  • Need for ecologically relevant scenarios: Few studies tested X-ray vision in
    realistic, less constrained conditions, which may produce more representative
    user experiences.

  • Unclear best method for X-ray visualization: No prior comparative analysis
    between geometrical saliency and visual saliency for OST AR HMDs.

  • Lack of integration with Direct Volume Rendering (DVR): Previous AR vi-
    sualization studies used static or simplified graphics; the use of DVR-based
    X-ray visualization in OST AR remains largely unexplored.

  • Dataset challenge: Public datasets vary widely, making reproducibility diffi-
    cult; this research proposes repeatable tasks using DVR-based X-ray visual-
    izations adaptable across many conditions.

   The end product of this thesis is several new X-ray Visualizations. That can be
used to overlay over the original object, person, or animal that had been scanned
by a CT or MRI scanner to provide the illusion of looking into the object.


                                         80
Chapter 3

Spatial Estimation In Augmented
Reality Aided X-Ray Vision

This chapter explores the parts of X-ray Vision that are functional or essential to
providing the illusion of looking through an object with the aim of understanding
how the visualization can support improved spatial estimation accuracy. While
limiting it’s self of OSTAR devices, and utilizing techniques from prior research on
other devices this research is able to draw knowledge from some of the most cited X-
ray Visualizations from Section 2.5 and looks at how users experience placing virtual
objects behind or inside a physical world object. The study presented in this chapter
aims to understand the depth and spatial perception challenges and their spatial
relationships when using Ocular See Through (OST) Augmented Reality (AR) X-
ray Vision by utilizing a method to migrate video footage over and recalibrate it
to their real world vision. Using the findings from this research informs the X-ray
Visualizations that should be made for Direct Volume Rendering (DVR) using OST
AR presented in Chapter 4.
    Overlaying three-dimensional virtual content on the physical world can be prob-
lematic in terms of aligning the perceived depth of the virtual and physical infor-
mation in AR. This is due to AR devices’ limitations to present data to people
intuitively, and misalignment between the physical and virtual world will present
this information as being further away [253]. This is made worse when virtual ob-
jects are placed beyond the physical object’s bounds [37] as the user’s sense of depth
is influenced by their ability to recognize visual relationships. When visual cues in
AR are not carefully designed, the user may perceive virtual objects to be smaller
instead of further away [254]. X-ray vision is one approach explored to address some
of the challenges of correctly perceiving where virtual objects are positioned in the
physical world.
    X-ray Visualizations have utilized a form of visual saliency [39,172,211,216] that
was possible by using a Video See Through (VST) AR display that would render

                                         81
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

the world again from the perspective of a camera. OST devices make this difficult
since the user can dynamically see the world through the display. The system does
not have a strong picture of the visual saliency the users are looking at. This is
challenging as there are many different shapes of human heads with eyes placed
at different positions; the position that visual objects appear in using an OST AR
Head Mounted Device (HMD) is it little better than a guess. Little focus is on
research merging physical and virtual objects with OST AR, considering depth has
been explored.
    To make a system more accessible, a system was designed that took OST AR
sensor calibration pipeline that can translate camera footage to a person’s sight.
OST AR X-ray Visualizations benefit from better adapting to changes in the real
world and adjusting the visualization to suit what the user is perceiving and is helpful
outside of X-ray Vision as it allows creation of visualizations or to make virtual
objects react to actual world virtual stimuli that any computer vision technology
can create. OST AR is necessary for several tasks that require the user’s vision
to be unobstructed, like driving [255], Surgery [256], and Security operations [225]
where VST AR HMDs are not suited. VST technologies suffer latency issues and
are vulnerable to hardware failure, which could be catastrophic for mission-critical
tasks like surgery. OST does not suffer from these limitations and may be better
suited to the applied tasks above, motivating to investigate further the use of X-ray
Vision techniques on OST technologies. To adapt VST AR X-ray Visualizations, this
research had to overcome several challenges, such as representing the visualization
within the correct depth of field and in the appropriate shape the user perceives it
to be, which will be explained in depth in this Chapter.


3.1      Updates Since Running This Study
Since running this study, several similar studies have published their results as are
detailed in Section 2.5.5. This research maintained its novelty by evaluating the
effects of various VST AR effects, such as Saliency and Edge-Based techniques.
These techniques have not been compared against Random Dot and wire-frame
X-ray Visualizations. Moreover, previous studies commonly restricted the user’s
movement while investigating the different approaches to depth cues. Rather, This
work focuses on a more natural interaction with these effects, looking at a less
restrictive solution to test the uses of various X-ray Visualizations to understand
better users’ natural processes for dealing with different types of occlusion.




                                          82
3.2     Adopting X-ray Visualizations for Augmented
        Reality
This research chose to focus on auxiliary effects (using groups of objects) and X-ray
Visualizations. many medical applications will likely cause some auxiliary effects in
the data, because most medical applications will require the user to look through
a large amount of data to find a small part of interest. Auxiliary effects work by
introducing additional reference objects or visual cues into the scene, helping users
to better judge spatial relationships and depth. In the context of volume rendering,
auxiliary effects can include the placement of markers, or anatomical landmarks
within or around the volume. These added elements provide extra context, making
it easier for users to locate, interpret, and interact with regions of interest within
complex volumetric data. By enriching the visualization with supplementary in-
formation, auxiliary effects help users navigate dense or ambiguous data, reduce
cognitive load, and enhance overall spatial understanding.
    The X-ray Visualizationss chosen for this study were:

  1. Edge and Saliency techniques occluded large amounts of the data behind the
     objects, while the other two drew lines over the object you were trying to look
     through. Creating a High Occlusion Model and a Low Occlusion Model for
     dynamic effects.

  2. The other consideration was using geometric saliency vs. visual saliency (or
     Real-world Overlays vs. Computer Vision Enabled X-ray Vision techniques).
     Since there is no record of using Computer Vision Enabled X-ray Vision tech-
     niques on stereoscopic AR devices.

Hole-in-the-world visualizations were ruled out as limiting the view of the visualiza-
tion, requiring more volume data to be produced. Four visualizations were chosen
due to the differences in their designs. Instead, a back face to this was applied to
the cube, which has been shown to improve depth perception [200].
    Four types of X-ray Visualization techniques are utilized for use with OST
AR displays: Random Dot, Tessellation (similar to wireframe), Edge-Based, and
Saliency. These visualizations were chosen because they were all only utilized up
to this point for VST AR X-ray Vision Papers. Edge and Saliency used Com-
puter Vision-Enabled Techniques, while the other two utilized a Real-world overlay
(Random-Dot and Wireframe). This could then further be split as one from each
of these groups blocked out a large portion of the user’s vision (Random Dot and
Saliency), while the other used thin lines to show contrast to the world via the
display (Edge and Wireframe).


                                         83
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION




Figure 3.1: a) Otuski et al.’s [204] version Random Dot b) A image of the Random
Dot visualization used in this chapter. Taken using a HoloLens2 under study con-
ditions. a) Was produced by By Otuski et al. [204] and is licensed under a Creative
Commons Attribution licence


    Any virtual object that is partially occluded by another virtual object is in front
of a given object [37,60]. However, to make this a seamless experience, it is required
to experience this. Research on all these techniques has shown the importance of
partial occlusion, but no work has been done comparing their ego-centric constraints
to this point [172,175,188,213,217]. While other studies have researched the impact
of the amount of occlusion, these forms of X-ray Vision can bring [218], which have
been used and considered when designing the parameters used when adapting these
visualizations. However, all these studies were much more controlled than ours and
were run using a range of VST devices.
    Regardless, some of the technical considerations for each of these visualizations
have been adapted around the same environmental considerations to ensure the
visualizations are as fair as possible. Firstly every visualization utilized a back face
to the cube, because it has been shown to improve depth perception [200]. The back
face was a contrasting pink color that contrasted with the virtual objects and the
large Voronoi cube (shown in Figure 3.11). Placing a plane on any face of the cube
that is facing away from the user that is visible in AR. Once a virtual object goes
past the back face, it will begin to clip behind this object until it is no longer visible
to the user.
    The X-rayable area was the 0.6m x 0.6m x 0.6m area within the cube between
the backface of the effect and all the X-ray Visualizations except for the baseline No
X-ray Visualization (referred to as None). None which provided no occlusion over
any part of the virtual objects within the x-rayable space. Simply, it superimposes


                                           84
Figure 3.2: a) Livingston et al.’s [259] initial version of the wireframe X-ray Vision
effect b) A image of the Tessellation visualization used in this chapter. Taken using
a HoloLens2 under study conditions. a) Was used with permission from IEEE ©
2003


the visualization of the objects. To the user None would likely appear to the user
to seem as if the back plate of the X-rayable area was in front of the x-rayable area
even though it is technically in the same area for each condition.


3.2.1    Random Dot X-ray Visualization
Random Dot appears as a grid of square dots that can be turned on or off. Gener-
ally, these are colored a slightly translucent shade of black Figure 3.1. The instance
of Random Dot used in this study uses the translucent shade of white Figure 3.1
b, since OST AR headsets can not render black. Random Dot was a real world
overlay X-ray Vision technique was used because it relied on Simultaneous Local-
ization and Mapping (SLAM) [257, 258] for positioning and obscured some of the
user’s vision into the X-rayable space and geometrical saliency, akin to Saliency.
The implementation of this visualization followed the published description of the
visualization by Otsuki et al. [205, 209], with a scale for different-sized dots and
density, a low-resolution texture (50 x 75 px), and randomly made half of the pixels
either transparent white or clear.


3.2.2    Tessellation X-ray Visualization
Wireframes have long been used as X-ray Visualizations, giving the user some iden-
tification of where the real world is compared to the virtual world and being able to
provide some identification of minor occlusion [188,260]. The Tessellation visualiza-


                                         85
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

tion shown in Figure 3.2 shows a similar but different take on the same properties as
a wireframe visualization but is using a geometry shader and calculating a uniform
dimension for each of the tiles to become between each edge of the shape. This al-
lowed for flexibility [261] by allowing geometrically salient patterns without needing
to add any extra polygons.
    Tessellation subdivides a wireframe into more triangles, covering more area.
Generally, this is done to increase the quality of a virtual object by creating increas-
ing the polygon count without adding extra geometry [261], but in this case, it was
used to create a wireframe-like effect that could be adjusted to cover more or less
of the cube. This visualization subdivided an existing wireframe further and would
normally be designed to allow for a dynamic range of quality to be applied to a
virtual object [261].
    A uniform Tessellation algorithm was used to ensure the triangles were placed
in an even and logical manner. The uniform triangular tessellation using fractional-
odd partitioning on triangle patches where each original triangle gets subdivided
into smaller triangles. This was set to produce five regular and evenly distributed
across the entire surface [262]. Allowing complete flexibility to manually manipulate
the size of the effect to choose the amount of the cube covered by the effect. For
this application, a uniform Tessellation was utilized to keep the quality increased
amount and aimed to create five new lines coming out of each edge [262]. This
allowed a visualization seen in Figure 3.2 like a Wireframe and retained geographical
sense while maintaining realistic and continuous while also manipulating the total
number of lines, allowing for more partial occlusion.


3.2.3     Edge-based
The Edge-Based visualization places a white line over all areas that show contrast
between sets of neighbouring pixels. This visualization was selected, since as seen
in Figure 3.3, the Edge-Based visualization occludes very little of the user’s vision
and uses salient points of interest in the user’s own point of view to provide the
visualization. Designed initially by Kalkofen et al. [211], this visualization provides
just enough occlusion to show virtual objects as opposed to the visually salient parts
of real-world objects, effectively guiding users to discern the presence and location
of virtual entities. The Sobel algorithm was utilized for this implementation with
a delta X and delta Y of 5 (performing the algorithm over a 5 by 5 grid). The
Sobel algorithm was chosen over other edge-based detection algorithms due to how
well it performed in parallel, using only a single step, unlike other edge detection
algorithms, such as the canny edge detection.




                                          86
Figure 3.3: a)Kalkofen et al.’s [211] version of X-ray Vision b)A image of the
Edge-Based Visualization used in this study. Taken using a HoloLens2 under study
conditions. a) Was produced by Kalkofen et al.’s [211] and was used with permission
from IEEE © 2007


3.2.4     Saliency
Visual saliency describes how much a given object or region in a scene may stand
out or attract attention to the viewer. As mentioned in previous chapters, Sandor
et al. [172] found that by using by showing attention-grabbing objects in the fore-
ground, you could create an X-ray Vision visualization that communicates to users
with a good level of depth perception where an object exists. Saliency was chosen
for this study because it partially occludes some of the user’s vision, like Random
Dot, and uses the visually salient regions in the user’s own point of view, similar to
the Edge-based visualization. Tian et al.’s [263] Saliency algorithm was adapted to
run in parallel on a GPU. This first required the HSL (Hue Saturation and Intensity)
value [264] to be extracted from the RGB color using a similar method to Saravanan
et al. [265]. The values of the individual Hue, Saturation, and Intensity Contrast
were then calculated along with the Dominance of both the warm colors and the
Saturation and Intensity. These methods were then normalized and weighted by
utilizing a static set of values that were calculated prior based on the lighting of the
study environment. The study environment provided consistent lighting and colors
to ensure these values remained consistent.
    Unlike the other visualizations, the Saliency algorithm would utilize salient fea-
tures from the real world to occlude the virtual objects [172]. VST AR devices
normally do this by rendering the real world partially on top of the virtual world.
In VST AR, depth is reconstructed from a stereoscopic camera feed, but in OST AR
the user sees the real world directly through transparent optics. Making it difficult
to know what the user is looking at and how to occlude the virtual objects over users
vision of the real world. To get around this issue, the visualization used in Figure 3.4
was used. Instead of producing saliency over the top of real-world constraints, this


                                          87
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION




Figure 3.4: Two images that use image saliency detection to find area’s of human
interest within the images to occlude virtual objects. a) Sandor et al.’s [172] version
of Saliency which changes the opacity of objects in the foreground to reveal another
video image in the background. b) An image of the Saliency Visualization used
in this study which increases the occlusion of the real world objects by creating a
transparent mask over the top of the objects from drawing back in the screen a
color that the HoloLens is unable to display at varying angles. b) Was taken using a
HoloLens2 under study conditions. In this setup, the large cube was decorated with
a Voronoi pattern of brightly coloured tiles. These coloured tiles acted as salient
features: the irregular edges, strong contrasts, and varied hues provided multiple
points of visual interest that the saliency algorithm could reliably identify. This
ensured consistent saliency detection across the cube’s surface, while also preventing
participants from relying on simple geometric cues such as uniform grids. a) Was
produced by Sandor et al.’s [172] and was used with permission from IEEE © 2010


version of saliency would make more salient areas of real-world objects more opaque
and others less opaque by rendering a black area over the x-rayable area. Rendering
the salient areas as black enabled the system to interpolate between fully occluding
the object and being completely transparent without the user being able to view
these abilities.
    Edge-Based and Saliency X-ray Visualizations required major additions to their
calibration to make them work with OST AR devices are described in detail in the
next section (Section 3.3).


3.3      Rendering Considerations
To take advantage of X-ray Visualizations that were traditionally designed for VST
AR systems, A system that could transmit camera data over to an OST AR display
was required. The following system was designed to display the output of a video
feed over a user’s vision for x-ray vision, allowing the system to highlight areas of
interest in the user’s vision. However, its potential uses are not limited to this.


                                          88
3.3.1      Video Image Overlay Method
Calibrating video see-through techniques, such as Saliency or Edge-Based visualiza-
tions for see-through video headsets and mobile devices [38, 172], presents the need
for a change in calibration as a 2D image will need to be able to respond to the
user’s depth cues to present the correct affect [266]. This system would need to
accommodate the difference between the distortion of human sight [267], reacting to
their movements as fast as possible [177,266] while understanding the depth of field.
To date, no algorithms for Saliency or Edge-Based visualizations were found that
made use of bifocal vision [205, 209], with each eye requiring its display that would
not overlay onto a video feed but the participants’ view of the real world. The
closest method previously used was Hamadouch’s [268] method, which displayed
Edge-Based X-ray Vision using projectors in collaboration with the HoloLens2. Our
technique extended this concept by applying X-ray Vision visualization to a virtual
projector, creating the illusion of depth within the augmented world and aligning
the virtual objects with the view of the physical world through the headset.
    As illustrated by Figure 3.6, each time a new image from the video feed was
received, the image was filtered to represent either a Saliency or Edge-Based map
and then distorted to align with the users’ vision. This image was then used as
input into a virtual projector from the user’s perspective but only interacting with
the X-rayable object. Outputting either a color-based saliency map for the Saliency
condition or a Sobel edge detection for the Edge-Based visualization, This was then
distorted for each of the user’s eyes. For Saliency, a shadow occludes the graphics
rendered within the cube, while a white light was projected for an Edge-Based
visualization.
    The HoloLens2 was used as the OST AR device, and a ZED Mini stereo camera 1
was used to capture the video feed. The ZED Mini was chosen due to its small size,
stereoscopic video feed however this this use the inbuilt funciationlity of the was
not required as the hovermap was able to provide this and the distortion algorithm
is incorrect for this use case. The ZED Mini was mounted on the front of the
HoloLens2, as close to the user’s eyes as possible, as shown in Figure 3.5 using a
custom 3D printed mount described in Section 3.5.4.
    On an OST AR display, it is impossible to mount a camera that will see the
world from the same perspective as the user, so these cameras need to be placed
away from the user’s view. First, the offset needs to be applied to the virtual
world to overlay the image from the camera. Radial distortion is required to match
the virtual representation of the real world so that the virtual content the camera
produced is accurately aligned with the user’s vision [269–271]. By applying this to
  1
      https://www.stereolabs.com/en-au/store/products/zed-mini



                                         89
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION




Figure 3.5: This image illustrates how different fields of view (FOV) were managed
to work with each other. The yellow line indicates the user’s entire bifocal field of
view (130◦ vertically). The red lines show the Field of View (FOV) of the HoloLens2
from the user’s point of view (29◦ vertically). The blue lines show the FOV of the
ZED Mini from the position it was mounted (54◦ vertically).


the visualization frame, the edges of the display were matched to the user’s vision
of the real world. Since the eye movements of the user required some input from
their bifocal cues, the distance-based distortion algorithm 3.3.1) was applied to the
visualization from the headset. This required the distance from the user and the
visualization to be tracked, as the difference in perspective between the camera and
the headset would need to be compensated for. Influencing the value along the
input camera image’s vertical axis (height position). Linear interpolation was used
to estimate any unrecorded values between the known ones. This gave the user the
impression that the projection was aligned appropriately on the box.
    The used parameters used to correct the distortion and the perspective offset
calculated from the participants subjective feedback during a short test after the
pilot study. Each of the participants used in the pilot study was asked to view the
visualizations overlaid on the box from 3 different distances (0.5m, 1m, and 1.5m)
at three different orientations on the box with the request provide feedback on the
alignment of the visualization. This feedback was used to adjust the distortion values
for the given depth away from the camera was positioned repairing the offset between
the camera and the HoloLens display as well as translating the image distortion to
from the camera to match the user’s vision.
    In order to make the visualization appear as if it were being projected from the
user’s perspective, the system needed to understand where the user was looking and
how far away they were from the box however since the pipeline took approximately
50ms to reach the display it was important that the system could predict the closest

                                         90
Algorithm 3.3.1 Algorithm for the radial distortion used by the camera. This is
a standard GPU variant of the Radial Distortion Algorithm [272–274]. All variable
values are Vector2s, except for D, which is the distortion value and indicates the level
of distortion required at a given pixel. To correctly distort the image to resemble
human sight, outlet areas of the viewing area, depending on the viewpoint of the
user, will need to use barrel distortion, while other inner areas will use pincushion
distortion.
 1: uv ← (uv − 0.5) × (Dz + 0.5)
 2: ruv ← DScalexy × (uv − 0.5 − DCenterxy )
 3: ru ← length(ruv)
 4: if P incushion_Distortion then
 5:     uv ← uv + ruv × ((tan(ru × Dx ) × (1.0/(ru × Dy ))) − 1.0)
 6: else
 7:     uv ← uv + ruv × (((1.0/ru) × Dx × arctan(ru × Dy ))) − 1.0)
 8: end if
 9: return ← uv


point the user would have been looking at when the image was taken. to accomplish
this the camera output is then rendered based on the location where the image
was taken, invisible to the user for the first frame until it is projected on the next
frame in the same place the image was taken. This allows the user to see the image
from the camera as if it were being projected from the user’s perspective. Figure 3.5
illustrates that the Camera had a wider field of view than the HoloLens2, so the user
could move around within the FOV of the camera without losing the visualization.
One issue to this methodology is that the field of view of the X-ray Visualization is
limited to interactions that are approximately within 10 cm from the box past this
point the visualization will be cut off by the edge of the camera’s field of view.
    The OST AR HMD used for this experiment (a HoloLens2) required a tethered
connection to receive a video feed from the camera as initial testing from the inbuilt
camera showed several issues: The battery life of the devices decreased dramatically.
The performance of the system dropped to a maximum frame rate of 30Frames Per
Second (FPS); the image had a low quality, and it had been extensively processed.
A tethered connection would inevitably cause a delay in when the system would
need to be known.
    The delay between the camera to the computational server and the display was
tested using a system similar to Gruen et al.’s [275] system and found the camera
(the ZED mini) produced a video lag of approximately 50ms from when the image
was captured to being displayed by the OST AR Headset (HoloLens2). The lag is
noticeable when viewing within an OST AR headset. To overcome this, a 5 Gbit/s
USB connection between the OST AR Headset and the PC was used to render the
visualization, enabling a rapid and stable visualization between the system and the
OST AR Headset. Most of the lag occurred between when the picture was taken

                                          91
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION




Figure 3.6: This diagram explains the frame-by-frame process that the system used
to process images; each frame away from receiving the first images is labeled up the
top of the diagram. Each system task is listed in a square box. ZED Mini describes
the process where the ZED Mini provides the system with the image. Filter explains
whether the Sobel edge-based filters are applied. The render state is when the final
scene is distorted and rendered to be overlaid onto the user’s vision.


and when the system could process it.
    To compensate for this lag, the asynchronous re-projection algorithm was imple-
mented, which recorded the position the user was looking at each frame using the
manner seen in Figure 3.6, and used the position recorded three frames before the
picture was acquired in the systems calculations [276, 277]. This approach enabled
the system to place the image at the approximate position it was taken since the
re-projection accounted for 48ms (3 frames) of the delay.
    This system also introduced noise created from a) the difference between the
estimated and actual positions of where the picture was taken and b) the user’s
involuntary head movements (e.g., Microsaccades [278]). To repair the damage
done by the difference between the estimated and actual positions, the position and
orientation where each image frame should have been were rendered, and the image
was distorted to match that position. The system would then render the frame it
received at the position where the user was looking four frames ago. This would
give the system a frame to apply the Sobel or saliency filter and apply the distortion
mentioned earlier.
    To help with the microsaccades, the position of the output image was first sent
into a one euro filter [279]. The filter provided the flexibility to allow large sweeping
head movements to appear as if they had no latency but filtered values when the
user stopped moving. This kept the image where the participant expected it without
restricting their movement.
    A preliminary study of 5 participants was used to determine the correct parame-
ters for the filter. Each participant used the system for 5 to 10 minutes and was able

                                           92
to set the parameters. This resulted in the following parameters being selected: One
euro filter’s frequency was set to 60, with a min cut-off set to 1, the beta was set to
200, and the D cut-off was set to 1. These settings would turn off the filter when the
user was making large motions like turning but would keep the image stable when
they were standing relatively still.
    The system was sent a new image every frame that corresponded to the position
the user was in approximately 50ms prior. The Edge-Based and Saliency visual-
izations had an approximately 50ms slower start-up time than the wireframe and
Random Dot visualizations. The system ran on the HoloLens with a constant frame
rate of 60 frames per second. Our system could achieve this consistently across all
visualizations, which was consistent with the camera, allowing for a close match to
the user’s perceived real-world view and the input video feed.


3.4      Pilot Study
A pilot study was conducted prior to the main study to test the viability of the
X-ray Visualizations running on an OST AR HMD on a cohort of 5 participants to
test depth perception. This was done by comparing the participants’ performance of
a VST AR HMD against their performance using an OST AR HMD to determine if
there were any distinct differences between the implementations on a given headset.
Similar to the design of studies done by Otsuki et al. [209] and Martin-Gomez et
al. [228] using the large Voronoi cube (shown in Figure 3.11) where the participants
were asked to guess what geometric shapes where inside or outside of the box if they
could determine it.
     The participant sat in a chair 1.5 meters away from the cube and was required
to say if any of the three provided objects (a sphere, a cube, and a star) were inside
it. No object would appear to be partially inside of the cube The answers they could
give were to say if they were certain it was inside the cube, they were certain it was
outside the cube, or they were unsure if it was inside or outside the cube. These
objects were randomly placed either inside or outside of the object.
     Both the VST and OST systems used a ZED mini 2 to provide the system with
visual information. The VST AR HMD was built from a Vive pro 3 using a ZED
Mini to provide AR support. The OST AR HMD was a HoloLens2 4 using a ZED
Mini as the system’s visual input.
     The pilot study was tested using a Friedman test with a post-hoc Wilcoxon
signed-rank test with a Bonferroni correction to determine if there were any signifi-
  2
    https://store.stereolabs.com/en-au/products/zed-mini
  3
    https://www.vive.com/au/
  4
    https://www.microsoft.com/en-au/HoloLens



                                          93
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

cant differences between the different visualizations and devices. The results of the
pilot study are to remain confidential due to the ethics agreement; however we did
find that Saliency and Random Dot were the most effective visualizations for depth
perception. Regarding the differences between the displays this pilot had similar
results to Martin-Gomez et al.’s [228] test regarding both devices, where both de-
vices produced a significantly different effect in regard to depth-based accuracy, with
VST AR being more accurate but having many more uncertain answers. Saliency
proved the most effective X-ray Visualization for depth perception, with None be-
ing the worst. A System Usability Scale (SUS) showed that people preferred to use
the Computer Vision-Enabled Techniques (Section 2.5.3). Possibly due to the small
amount of participants these results showed very few significant differences(p-value
> 0.05).


3.5      User Study
A placement task assessed how well participants could place virtual objects within a
large physical cube decorated with a colorful Voronoi pattern (shown in Figure 3.11
and Figure 3.7). Users could walk freely within a prescribed area in the study envi-
ronment detailed in Figure 3.12. This allowed for a more natural interaction with
the virtual objects and the environment and provided a more realistic scenario for
how these visualizations would be used in practice than testing depth based align-
ment tasks. Allowing movement was essential because depth judgments in AR rely
heavily on motion parallax and viewpoint changes, which cannot be experienced
in a seated, fixed-position setup. Furthermore, in practical AR applications users
naturally move around objects to inspect them from multiple perspectives, so incor-
porating movement provided both stronger perceptual cues and a more ecologically
valid evaluation of the visualization techniques than a purely seated, static study.
In order to ensure that participant specific skills like hand-eye coordination, spatial
reasoning, and motor skills did not affect the results of the study, a within-subjects
design was used where each participant experienced all conditions of the study, along
with a random effects style analysis to control for individual participant bias.
    The pilot study described in Section 3.4 which was used informed the design of
this study and to provide a comparison between the two types of AR devices. Since
for this Dissertation there was a requirement to use an OST AR device to enable
these visualizations, it was decided that the main study would only use an OST
AR device so the all that was required was to compare the different X-ray Visu-
alization techniques. While alignment tasks or comparisons to VST HMDs could
provide baseline performance measures, they would not isolate the contribution of
visualization techniques. The study therefore focused on comparing alternative vi-

                                          94
              Figure 3.7: A image of the Baseline (None) condition


sualizations within OST AR, as this directly addressed the research question of how
X-ray visualizations perform in see-through displays.
    Participants were either assisted with one of the four X-ray Visualization tech-
niques Random Dot (Figure 3.1), Tessellation, Edge-Based (Figure 3.3), and Saliency
(Figure 3.4) as described in the Section 3.2, or they saw no augmented visualiza-
tion(Figure 3.7). Moreover, additional virtual reference objects were placed in the
cube to assess their impact on placement accuracy. Additional measures included
the participant’s cognitive load, usability indicators, and rate of movement.


3.5.1     Research Questions
Our research questions were as follows:

RQ.1 Is there a difference in accuracy when placing a virtual object when different
     X-ray Visualization effects are used?

   RQ.1.1 Do different X-ray Visualizations affect the various axes (vertical, hori-
          zontal, and depth) differently?

RQ.2 What effect does adding more reference objects within an object have on this
     task?

RQ.3 Do participants move differently when they are presented with different X-ray
     Visualizations?

RQ.4 What are the participants’ perceived differences of the X-ray Visualizations
     on an OST headset?

                                          95
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

3.5.2    Hypotheses
The hypotheses of the study were as follows:

 H.1 Participants will place the virtual icosahedron closer to the correct target
     position when they are using the Saliency visualization (R.1). Rationale:
     Both Sandor et al. [172] and Dey et al. [213] found that Saliency was the
     most effective X-ray Visualization for depth perception in their study, and it
     performed the best in our preliminary tests.

 H.2 Participants’ vertical and horizontal placement of the virtual icosahedron will
     be further from the correct target position when using the Saliency visualiza-
     tion (R.1.1). Rationale: Saliency has the most potential occlusion, and I
     hypothesize that users may struggle to place the object accurately on the X and
     Y axis.

 H.3 Participants’ depth axis of the virtual icosahedron closer to the correct target
     position when they are using the Saliency visualization (R.1.1). Rationale:
     Saliency provides a powerful depth cue that is able to assist with more accurate
     placement of objects [172, 213].

 H.4 Participants will place the virtual icosahedron closer to the correct target when
     reference objects and any X-ray Visualization are present (R.2).

 H.5 Participants will take less time when the reference objects are added to the
     virtual scene (R.3). Rationale: The introduction of the virtual objects should
     result in an overall improvement in efficiency from the users as they have a
     better set of references to view the position of the study environment [60].

 H.6 Participants will move less when the reference objects are present (R.3). Ra-
     tionale: Embodied cognition suggests that participants should more less when
     they have more information to complete a task [280].

 H.7 Participants will stand farther back from the Voronoi cube when more refer-
     ence objects are present (R.3). Rationale: Participants will likely want to
     concentrate on more than one detail at a time. They will need to view the box
     more as a whole.

 H.8 Participants will find Saliency subjectively difficult to use and require a higher
     cognitive load than other X-ray Vision effects (R.4). Rationale: Although
     Saliency may provide a powerful depth cue that is able to assist with more
     accurate placement of objects, I suspect it requires much more attention, which
     may result in greater cognitive effort [57, 172, 217].


                                          96
 H.9 Participants will prefer X-ray Vision effects which occlude less of their vision
     (R.4). Rationale: It is likely that since participants are required to be in close
     proximity to the large physical cube with a Voronoi pattern, they will prefer to
     either use Tessellation or possibly Edge Based conditions.

Placement of the Object

I hypothesize that users can place the object better using Saliency similar to San-
dor et al.’s [172] results compared to the Edge-Based visualization on VST AR, and
because it performed the best in the preliminary tests that focused on depth per-
ception. I expect to get similar results from the position of objects using Saliency
when placing objects (H.1). Due to Saliency having the most potential occlusion,
I hypothesize that users may struggle to place the object accurately on the X and
Y axis (H.2) but will be able to place the object accurately along the depth (z)
axis (H.3). Saliency and Random Dot may yield similar results since they have
very similar opacity levels. I expect an improvement in depth perception due to
the impact the reference objects would have on relative size and density because
of findings shown by Cutting et al. [60] showing the benefits regarding relative size
and density to depth perception and Kyto et al. [281] finding showing that this was
enough to create a X-ray Visualization using only the depth cues of relative size and
density (H.4). The benefits of X-ray Vision have been seen to be highly effective
at any distance [172, 213, 228]. Overall, all of the X-ray Vision visualizations should
perform better than the baseline visualization (None).

User Movement and Task Completion

Introducing the virtual objects should result in an overall improvement in efficiency
from the users as they have a better set of references to view the position of the study
environment [60]. Therefore, participants will likely have improved depth perception
when they have fewer objects to look at (H.6). Moving less should enable them to
complete tasks faster (H.5). However, I hypothesize that this will also cause the
users to want to stand further away from the object (H.7) as they try to concentrate
on more than one detail at a time. They will need to view the box more as a whole.

Subjective Analysis

Although Saliency may provide a powerful depth cue that is able to assist with
more accurate placement of objects, I suspect it requires much more attention,
which may result in greater cognitive effort (H.8) [57, 172, 217]. Results from other
devices show Saliency utilizes depth cues in a powerful manner that should lead to
more accurate placements [39, 57, 172, 217]. Still, the higher occlusion it produces

                                          97
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

will likely require in the near field will require much more attention to use as it
relies on the user’s understanding of Saliency will need to match the output from a
given algorithm [282, 283]. It is likely that since participants are required to be in
close proximity to the large physical cube with a Voronoi pattern, they will prefer
to either use Tessellation or possibly Edge Based conditions (H.9).


3.5.3      Participants
The study recruited 22 participants between the ages of 22-44 (mean = 29.35, σ =
6.43964). Two of the participants were female, and 20 were male. All participants
were required to have a normal or correct vision regarding depth perception. This
was determined by user self-reporting before the study. This was verified by vi-
sually examining their final placements against all other participants to look for
outliers where the task was not performed correctly. Two male participants were
removed from the study as their data showed they either did not understand the
study procedure or their sight was impaired.


3.5.4      Study Design
The Zed mini allowed the system to observe the local environment to produce the X-
ray Visualizations (Edge-Based and Saliency). At the same time, a Vive controller
was used to give users better control over the placement of the virtual icosahedron
inside the large colorful box. All the additional sensors were attached using 3D-
printed mounts (described in Section 3.5.4) to ensure repeatability and reliability.
    Rather than creating a new AR device to work, this study aimed to use off-
the-shelf components best. This meant that this study required the use of several
consumer-level components that needed to work together. The HoloLens2 was used
as the AR device in this study because it allowed additional sensors positioned
relative to the display (to allow for more precise controls and a faster image input),
the refresh rate on the device when running Saliency and Edge-Based algorithms
was adequate (approximately 30 FPS), it was possible to offload GPU processing to
a tethered machine (increasing the frame rate to 60 FPS). This slower performance
is due to the processing the HoloLens requires for each frame before the image can be
displayed on the HoloLens2. Instead, a Zed Mini was used, which could be processed
concurrently on a remote device in less time.
    The choice of off-the-shelf devices for this study meant that it was not possible
to take full advantage of some possible depth cues, specifically accommodation and
convergence, since very few of the available headsets provide adequate accommoda-
tion. I did consider the Magic Leap 5 as an alternative because it utilizes two focal
  5
      https://www.magicleap.com


                                         98
Figure 3.8: A image of the hardware used for this study on a glass mannequin
head from three different angles.


planes, which would allow for some use of the accommodation depth cue. However,
the two focal planes would have greatly complicated the setup this system would
have required. While the two depth planes of the Magic Leap were potentially useful,
this technology would have complicated the camera pass-through to the user’s vision
and reduced the available frame rate, potentially causing issues for the study [177].

Vive Controller Set Up

The HoloLens 2 has an intuitive interaction design but has limited accuracy, es-
pecially at a distance. MRTK utilizes Interpolation to create a smoother sense of
motion within AR, making objects feel like they are drifting through the air. This
works well for placing items in an approximate location but does not allow the
participant to make precise controls, which is challenging. Generally, when using
HoloLens2 to do this, it is recommended that you use long-distance motions for
precise controls, but you would rather grab objects and place them. This was not
an option, as participants could not touch the physical objects. So, a new system
was built to view the participants’ interactions.
    Rather than use the MRTK controls for this study, a Vive controller was utilized
that controlled a ray out of the end of the controller, allowing participants to interact
with the inside of the cube. Using controllers enabled a more precise and predictable
interaction than the default controls from MRTK. A separate VR system would be
used to track the position of the Vive puck and controller. It would then correct
this position and transform it to be relative to where the puck should be relative
to the HoloLens2, and send it to the main system as the position and rotation of
the main controller. The system would then portray a ray coming out of the end
of this controller. To lower the sensitivity of the Vive sensors and the HoloLens2
connection, a one-euro filter [279] was used to remove any noise caused by the Vive’s
hardware that may be transmitted to the HoloLens2.




                                           99
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION




Figure 3.9: A image of the prototype models developed to prototype the final ZED
MINI camera mount


3D printed mounts

To attach the sensors to the HoloLens2, 3D-printed mounts were required to fit
comfortably onto the HoloLens2 with minimal movement due to head motions. This
allowed the system to know where the camera was for each frame and where the
controllers were located. Compared to the headset. Allowing for the controllers to
be tracked in relation to the HMD.
    The Zed mini was required to capture as much of the participants’ view as
possible to locate it as close as possible to the users’ eyes. This meant the zed
camera needed to be placed on top of the front of the HoloLens2, where two slots
allow for the small items to be mounted. To make this work, a keyhole-fit mechanism
was utilized to hold these items in the same place without any chance of movement.
This required precise knowledge of the physical parameters of the HoloLens2. To
be able to detect both the holes of the headset to establish that rapid prototyping
was enacted, the development of a series of 3D printed models is required to try
to determine the parameters of the HoloLens2 headset, which can be seen in figure
Figure 3.10. The final version of this mount can is available open source 6,7
    The 3D-printed back Vive puck mount sat on the back of the HoloLens2 in
Figure 3.8 was used to keep the Vive Puck visible and in the same place relative to
the user. This model was made open source to allow others to utilize this system 8 .
This design was created using photogrammetry. This version did not require such
a complex design, and it acquired as many points of reference as possible to ensure
the design’s accuracy. This design utilized a two-piece system that could pressure
clamp the mount if necessary to allow for more flexibility. The areas of the design
this would work with where could be found with the screw that could connect to a
Vive puck as well as the top and bottom.
  6
    https://www.thingiverse.com/thing:4561113
  7
    https://github.com/tomishninja/HoloLens2-Sensor-Mount-Repository
  8
    https://www.thingiverse.com/thing:4657299



                                        100
Figure 3.10: A diagram of the base dimensions for the HoloLens2 calculated utiliz-
ing 3D prototyping. The top of the HoloLens2 Mount can be seen shaded in yellow.


Environment and Apparatus

The physical study environment consisted of two cubes placed in an area where
the participant could freely walk. The area where the participant could walk was
constrained so that they were at most 1m from either of the physical scenes. The
study area arrangement is depicted in Figure 3.12.
    The large Voronoi cube and reference scene, shown in Figure 3.11, were very
distinct in appearance, measuring 60cm along each axis. The reference scene on
the right-hand side of Figure 3.11 was the reference scene with a felt base. Smaller
objects held up by stilts were placed in the cube that the participant would use as
a reference. The other large Voronoi cube was a brightly colored cube decorated in
a Voronoi pattern designed to support Edge-Based and Saliency effects. This cube
held the virtual objects displayed by the HoloLens2 that the user could interact
with.
    The large physical cube with a Voronoi pattern was designed to be a 1:1 physical
representation of the virtual world displayed in the brightly colored Voronoi cube.
These cubes were designed to hold three geometric objects: a cube, a sphere, and an
icosahedron. The 3D-printed objects within the reference scene were ivory-colored
to ensure that the shadows on the objects would be similar to the virtual ones
displayed by the HoloLens2 and presented in the same orientation as the virtual
objects. During the preliminary tests, no participant noted that the bright colors
on the Voronoi cube were distracting. Each stilt featured a square base measuring
100cm2 , which ensured that the center of each object was positioned at least 5cm
away from any other object. This enforced a minimum distance of 4 cm from any
other object (except cube boundaries), ensuring that some spatial estimation was
required to place the object.
    The geometric objects were identical in the physical reference scene and the


                                        101
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION




Figure 3.11: The study environment used for the spatial estimation experiment.
Left is the large physical cube with a Voronoi pattern (for tracking) where X-ray
Vision techniques are performed. Right is a reference scene with movable 3D-printed
objects that need replicating during the task.


virtual space. Each object had similar dimensions. The sphere (113.3cm3 ) and the
icosahedron (109.9cm3 ) were made to be the largest size possible that would fit
into the dimensions of the cube-shaped object (216cm3 ). The placement of each
reference object was decided randomly along all axes while accounting for collisions
and ensuring that each object was wholly within the cube.


3.5.5    Study Variables
This section details the variables considered in this study. The independent variables
are used to create each condition that will be tested throughout this study. The
dependent variables will state the metrics that were required to track to answer the
prior research questions.

Independent Variables

This study utilized a 2 by 5 design, with two separate baselines. The first baseline
related to the presence of reference objects: participants either had no reference
objects or two reference objects (a cube and a sphere) available in the scene (see
Figure 3.11). The second baseline related to visualization: participants either viewed
the scene with one of four X-ray Visualization techniques (Random Dot, Tessellation,

                                         102
Saliency, Edge-Based) or with no visualization at all (the baseline condition for this
factor, None).
    Thus, the design contained two orthogonal baselines:

  • Reference baseline: no cube or sphere present.

  • Visualization baseline: no X-ray Visualization applied.

   The conditions tested in this design are as follows:

  • (2) The presence of the reference objects: two reference objects (a cube and a
    sphere) and no reference objects (Shown in Figure 3.11).

  • (5) X-ray visualization: Random Dot, Tessellation, Saliency, Edge-Based and
    None. These were all previously mentioned and described in Section 3.2

H.1 to H.7 utilize both of these conditions, while H.8 & H.9 only uses the X-ray
Visualizations.

Dependent Variables

The dependent variables state the different tracked measures utilized to confirm this
research’s hypotheses.

Quantitative Variables

  • Accuracy: Accuracy was measured as the distance between the center of the
    actual placement position and the center of the target position (H.1, H.2,
    H.3 & H.4).

  • Perspective Accuracy by axis: Perspective accuracy was measured as the
    distance between the actual placement position (along the horizontal, vertical,
    and depth axes) and the target position from the participant’s perspective at
    the time of placement (H.2, H.3 & H.4).

  • Time: Overall time taken between the user starting the condition and the
    time the user chose to end the condition. The time the user spent holding the
    object for each condition was recorded (H.5).

  • Distance Moved: Throughout each task, data was gathered from the headset
    regarding the distance the headset was moved every frame. The total sum of
    this was used to determine how much various users felt the need to move
    around to get an understanding of various scenes (H.6 & H.7).



                                         103
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

  • Distance Icosahedron Was Moved: For each task, users would be given as
    many opportunities to move the icosahedron as they felt necessary. Whenever
    the user moves the icosahedron, the distance it moves will be tracked similarly
    to its movement. Allowing for an assessment of the movement required to
    place the shape in every instance.

Subjective Variables

  • PAAS Questionnaire: To gain an understanding of the possible cognitive
    loads undertaken via the various X-ray Visualizations, the PAAS mental effort
    scale [284] was utilized (H.8).

  • System Usability Scale (SUS): A SUS questionnaire was utilized to deter-
    mine how easy the various X-ray Visualizations where to use (H.8).

  • Favorite X-ray Visualization: At the end of each study, users were asked
    to state what X-ray Visualization was their favorite (H.9).

  • Comments and Feedback: At the end of each study, users were asked to
    give feedback in the form of written text after completing the study (H.9).


3.5.6    Task
Participants were asked to perform a demographics survey at the start of each
session. Then, they were asked to wear a HoloLens2 with an HTC Vive puck and
a Zed mini mounted on it and given an HTC Vive controller for hand interactions
with the study. Allowing a pointer to come out of the controller to interact with
the box’s interior. Participants were given a tutorial on how to use the controller
for the task. This explanation included:

  • When the pointer interacts with the icosahedron, it changes its color and
    surrounds the object with a transparent bubble.
  • Using the touchpad on the controller, the ray cast could be grown (up to 1
    meter in length) and shrunk as necessary.
  • Participants could start and end each iteration by pressing the menu button
    on the Vive controller.

    Participants were instructed to place the virtual icosahedron in the same position
in the Voronoi cube as the physical icosahedron was positioned in the physical
reference scene. The other reference objects and their physical counterparts in the
inverse cube were also pointed out to them. In each iteration of the study, one of
four different X-ray Visualizations or no visualization would be randomly chosen and

                                         104
Figure 3.12: A top down view of the study setup for the spatial estimation exper-
iment color coded in to the different areas of note: a) Colourful Voronoi Cube; b)
Black Inverse Reference Cube; c) Area the participant could traverse; d) Examiner
area; e) Main Processing Server; f) Projector setup; g) Vive sensors




                                       105
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

shown on the outside of the cube. The initial position of the virtual icosahedron
was at 5cm along all axes measured from the bottom front corner of the target
Voronoi cube. If virtual reference objects were present for the iteration, they would
be located at the correct relative position indicated by their physical counterparts.
All positions of the geometrical objects were generated before the study using a
pseudo-random selection.
    A randomized order of conditions was used instead of a Latin balanced square.
This choice was made to maintain flexibility in participant scheduling and accom-
modate uncertainty in recruitment numbers. Balanced Latin squares require fixed
multiples of participants to achieve complete counterbalancing, which can become
problematic when participant availability is unpredictable or when demographic rep-
resentation is uneven. Randomization ensured that order effects were distributed
across the sample while allowing the study to adapt to potential imbalances in par-
ticipant demographics caused by pandemic-related constraints.
    A Spatial Augmented Reality (SAR) calibration guidance tool was used to posi-
tion the physical objects in 3D space by the examiner within the reference scene [285].
The desired height position of each object was indicated using a wireframe repre-
sentation of these models to ensure that they were in the correct position and facing
the correct way.
    During each task, the participants could freely move around the area shown
in Figure 3.12 but were not allowed to step outside the area. Participants could
reposition the icosahedron as often and as far as they wanted and had no time limit
to complete the task. This was aimed to reduce any pressure on the participants to
complete the task quickly and allow them to focus on accuracy while also allowing
them to move around the cube as much as they felt necessary focusing on how well
they could perform the task with no restrictions, creating a more realistic experience.
    Participants were given three practice iterations of each task before data collec-
tion began. Participants were given instructions and guidance during these practice
iterations. Following the practice iterations, participants were presented with 10
iterations for each X-ray Visualization effect. This would then be split into two
randomly interleaved groups: one with two reference objects and one without any
reference objects. After completing the ten iterations, the participant answered a
visualization technique questionnaire, including a System Usability Survey [286], the
PAAS subjective rating scale [284], and several other custom questions. This proce-
dure was repeated randomly for each X-ray Visualization and the baseline condition.
In total, each participant spent an average of approximately 96 minutes (±61.52)
doing in this study, with the fastest participant spending about 31 minutes in the
study and the slowest taking almost 4 hours and 21 minutes to complete this task.



                                         106
                                                  Impact of X−ray Visualization on Placement Accuracy
                                                                                    ●                                          ●


                                                                                                                               ●
                                              ●
Placement Accuracy(cm)


                                                            *




                                                                                              Placement Accuracy(cm)
                                                                                                                                              ●
                                   ●                                                                                           ●
                                                                         ●
                                                                                                                               ●              ●
                                                            ●            ●




                                                                               *                                       15             ***
                         15        ●

                                   ●
                                                            ●
                                                            ●
                                                                         ●

                                                                         ●
                                                                                                                               ●

                                                                                                                               ●
                                                                                                                               ●
                                                                                                                               ●
                                                                                                                               ●
                                                                                                                               ●
                                                                                                                               ●
                                                                                                                                              ●
                                                                                                                                              ●

                                                                                                                                              ●
                                                                         ●                                                                    ●
                                                                         ●                                                                    ●
                                                            ●            ●
                                              ●                          ●
                                                                                                                                              ●
                                              ●             ●                                                                                 ●
                                                                                                                                              ●

                                   ●                                                                                                          ●
                                                                                    ●                                                         ●
                                   ●
                                   ●
                                              ●                                     ●                                                         ●

                                              ●                                                                                               ●
                                              ●




                         10                                                                                            10


                                                                                                                        5
                          5

                                                                                                                        0
                          0                                                                                                  FALS
                                                                                                                                  E
                                                                                                                                            TRU
                                                                                                                                                E
                              Random Dot Tesselation      Edge     Saliency        None                                     Reference Objects Were
                                                  X−ray Visualizations                                                        Present or Absent


Figure 3.13: A graph representing the accuracy of participants’ placement based
on the distance the object was placed from the goal position within the large physical
cube with a Voronoi pattern using the X-ray Visualizations. The error bars indicate
each condition’s confidence levels (CL = 95%). Significance differences are displayed
as the lines on the top of the graph stars indicate significance (* = p < 0.05; ** =
p < 0.01; *** = p < 0.001).


3.6                               Results
The analyses followed a within-group design, with a single group of participants
completing all conditions (presence of reference objects and X-ray Visualizations)
to answer hypotheses H.1 to H.7. Linear Mixed Effects Models (LMM) was used
to analyze the data, with participants as a random effect and the presence of ref-
erence objects and X-ray Visualizations as fixed effects while allowing for repeated
measures. This allowed for the differences between each participant to be accounted
for while still being able to analyze the effects of the independent variables
    All results with a p-value < 0.1 are reported in this section. Any p-value < 0.05
is considered significant. All results from the post hoc analysis in this section can
be found in the appendix of this thesis.


3.6.1                                  Quantitative Results
A LMM was used to examine the differences across all variables measured within this
section. LMMs are now standard for analyzing repeated-measures data in behavioral
and cognitive experiments, because they allow for both fixed effects of experimental
manipulations and random effects (e.g. subject-level variability). This approach has
several advantages over traditional methods such as ANOVA, including the ability to
handle unbalanced data, account for individual differences, and model complex hier-
archical structures [287–289]. Unlike traditional repeated-measures ANOVA, LMMs
are flexible to unbalanced data and missing observations, which was advantageous
in this study context [290].
    LMMs uses a similar method to linear regression (y = Xβ + ε) that is given

                                                                             107
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

                                        Impact of X−ray Visualization on Placement Accuracy on the X axis
                         20                                ●
                                                                                             20                                                          ●




Placement Accuracy(cm)




                                                                                                 Placement Accuracy(cm)
                                                                                                                                               ***
                                                                                                                                     ●

                                                                         ●
                                                                                                                                                         ●
                                                                                                                                                         ●




                         10        ●
                                   ●
                                              ●


                                              ●
                                                                         ●
                                                                         ●


                                                                                    ●
                                                                                                                          10         ●
                                                                                                                                     ●
                                                                                                                                     ●
                                                                                                                                     ●                   ●



                                                                                    ●


                                                                                    ●




                                                                                                                           0
                          0
                                                                                                                                                         ●
                                                                                                                                                         ●
                                                                                                                                                         ●
                                                                                                                                                         ●




                                                                                    ●
                                                                                                                 −10                 ●
                                                                                                                                     ●
                                                                                                                                     ●
                                                                                                                                     ●
                                                                                                                                     ●
                                                                                                                                                         ●




                                                                                    ●                                                                    ●
                                                                                                                                     ●
                                                                                                                                     ●



                −10                ●
                                   ●

                                              ●
                                                           ●
                                                           ●


                                                           ●
                                                                         ●
                                                                                    ●
                                                                                                                                     ●




                                                                                                                                                         ●

                                                                         ●
                                   ●
                                                                         ●




                                                                                                                                              nt                   nt
                                   ●




                                                                                                                                         Abse                  rese
                                   ●

                                                                                                                                   ere                 ere P
                                                                                                                               RO W                RO W
                              Random Dot Tesselation     Edge      Saliency        None                                         Reference Objects Were
                                                  X−ray Visualizations                                                            Present or Absent


Figure 3.14: A Graphs representing the accuracy of participants’ placement based
on the distance they are from the goal position on the X-axis within the large,
physical cube with a Voronoi pattern using the presence of the reference objects.
Positions were obtained from the final placement of the object. The error bars
indicate each condition’s confidence levels (CL = 95%).


fixed and random effects. Fixed Effects may not vary per condition; and random
effects are estimated using β ∼ N (µ, σ 2 ), allowing the system to account for different
variances between results of the random effect (mixed effects) [289, 291]. Overall,
the model predicts the outcome variable (y) using a matrix of predictor variables
(X) against a single column of fixed effects regression coefficients (β); The effect
from the random effects (Z) is then multiplied to the random effect of a given fixed
effect (γ), this value is then added against a vector column of the residuals values
(ε) for each fixed effect.
                                  y = Xβ + Zγ + ε

This approach accounts for individual differences between participants in repeated
measures on both normally distributed and non-normally distributed data and en-
ables the examination of the effect of the number of objects [251, 292, 293]. The

Table 3.1: Table of X-ray Visualizations used in this study. Showing the mean
(µ), standard deviation (σ), and the median (M) of the distance from the correct
target position of all the X-ray Visualizations from the user’s sight (x, y, and z) and
the distance to the goal (*). All measurements are in cm. The second column from
the left indicates the presence of the reference objects (T indicates the presence of
reference objects, while F indicates the absence of all reference objects in the scene).
                                      Random Dot       Tessellation     Edge-Based             Saliency                                     None
                                     µ    σ     M     µ     σ      M   µ    σ     M         µ      σ    M                           µ        σ     M
                             F     5.82 2.84 5.09 5.47 2.38 5.25 6.19 2.99 5.68           6.09 2.40 5.75                          5.78      2.57 5.65
                          *)
                             T     4.24 1.97 4.09 4.58 2.18 4.30 4.46 2.15 4.17           4.86 2.76 4.41                          4.27      2.02 3.90
                             F     -0.69 3.19 -0.88 -0.08 3.34 -0.24 -0.04 3.64 0.02      -0.21 3.49 -0.68                        -0.26     3.40 -0.37
                          x)
                             T     -0.19 2.86 -0.16 0.07 3.35 -0.31 0.00 2.85 -0.15       0.05 3.22 -0.04                         0.39      2.54 0.36
                             F     -0.98 2.90 -0.98 0.03 3.05 0.09 -0.93 2.66 -1.13       -0.89 3.01 -0.83                        -0.34     2.56 -0.55
                          y)
                             T     -0.46 2.22 -0.21 -0.49 2.25 -0.14 -0.26 2.36 -0.03     -0.79 2.36 -0.56                        -0.07     2.56 -0.12
                             F     0.15 2.55 0.25 -0.33 2.84 -0.65 -0.76 3.21 -0.84       0.00 3.18 0.24                          -0.48     2.84 -0.89
                          z)
                             T     0.02 2.32 0.27 0.17 2.13 0.15 -0.59 2.35 -1.15         -0.08 2.96 -0.61                        -0.00     2.37 -0.04




                                                                             108
                                        Impact of X−ray Visualization on Placement Accuracy on the Y axis
                                                                                                                                                            ●
                                                                         ●

                                                                                                                                             ●
                                                           ●




                                                                                                        Placement Accuracy(cm)
Placement Accuracy(cm)


                                                                                                                                             ●
                                                                                                                                                            ●
                                                           ●                                                                                 ●




                         10        ●
                                              ●            ●
                                                                         ●
                                                                         ●
                                                                         ●


                                                                                      ●
                                                                                                                                 10          ●
                                                                                                                                             ●
                                                                                                                                             ●
                                                                                                                                                            ●




                                                                                                                                                            ●
                                                           ●
                                                                                      ●                                                                     ●
                                              ●                                                                                                             ●
                                   ●
                                                                                                                                                            ●
                                                                                      ●                                                                     ●
                                   ●




                                                                                                                                  0
                          0
                                                                                                                                                            ●
                                                                                                                                                            ●


                                                                                                                                                            ●
                                                                                                                                                            ●
                                                                                                                                                            ●




                                                           ●
                                                                                                                        −10                  ●
                                                                                                                                             ●
                                                                                                                                                            ●
                                                                                                                                                            ●


                                                                                      ●
                                              ●
                                   ●                       ●
                                   ●




                −10
                                                           ●
                                   ●                       ●
                                                                                                                                             ●
                                                           ●
                                                                                                                                                            ●
                                                                         ●
                                                           ●
                                                                         ●                                                                   ●




                                                                                                                                                 bsen
                                                                                                                                                      t            nt
                                                                                                                                                              rese
                                              ●




                                                                                                                                          ere A         ere P
                                                                         ●




                                                                                                                                      RO W         RO W
                                                                         ●




                              Random Dot Tesselation     Edge      Saliency        None                                                 Reference Objects Were
                                                  X−ray Visualizations                                                                     Present or Absent


Figure 3.15: Graphs representing the accuracy of participants’ placement based on
the distance they are from the goal position on the Y-axis within the large physical
cube with a Voronoi pattern using either different (Right) X-ray Visualizations or
(Left) the presence of the reference objects. Positions were obtained from the final
placement of the object. The error bars indicate each condition’s confidence levels
(CL = 95%).


model was specified with the factors of the presence of virtual reference objects and
X-ray Visualization effect techniques, the dependent variable of the accuracy, view-
point accuracy, distance moved, distance away, or time. The model was specified
with fixed effects of x-ray visualization, the presence of reference objects, and an
interaction effect between them, with a random effect of participants on the inter-
cept. Significance values were extracted using Type II Wald chi-square tests using
the following algorithm:
                                                                         h                i−1
                                                          W = g(θ̂)⊺ GV (θ̂)G⊺                  g(θ̂)

Here, G denotes the Jacobian matrix of g(θ) with respect to θ. Each element
of G represents the partial derivative of one component of g(θ) with respect to
a corresponding parameter in θ, that is,

                                                                              ∂gi (θ)
                                                                  Gij =               .
                                                                               ∂θj

    pairwise post-hoc comparisons were conducted using Tukey’s Honestly Signifi-
cant Difference (HSD) for multiple comparisons to further validate p-values which
is shown below:
                                      X̄i − X̄j
                                  q= q
                                                                               M Se
                                                                                nt

q is the test statistic, X̄i − X̄j are the two groups being compared, M Se is the mean
square error from the groups calculated from the LMM, nt is the total number of
observations.


                                                                             109
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

                                       Impact of X−ray Visualization on Placement Accuracy on the Z axis
                                                                                                                                            ●



                                                                                                                  20
                                                         ●




                         20




                                                                                         Placement Accuracy(cm)
Placement Accuracy(cm)
                                                                                                                              ●


                                                   **                         ●




                                                                                                                                     *
                                              **                                                                              ●             ●

                                                                                                                                            ●




                                                                                                                  10
                                                         ●         ●
                                                                                                                              ●
                                                                   ●                                                          ●




                         10
                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●
                                   ●
                                                                              ●

                                   ●                               ●

                                                         ●




                                                                                                                   0
                          0
                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●




                                                                                                         −10                  ●

                                                                                                                              ●
                                                                                                                              ●
                                                                                                                              ●
                                                                                                                              ●
                                                                                                                                            ●




                                   ●




                                                                                                                                      t            nt
                                   ●



                −10                                                                                                               bsen         rese
                                   ●                     ●
                                                         ●




                                                                                                                            ere A        ere P
                                   ●
                                                         ●




                                                                                                                       RO W
                                                         ●




                                                                                                                                     RO W
                                                         ●




                              Random DotTesselation     Edge    Saliency     None                                        Reference Objects Were
                                               X−ray Visualizations                                                        Present or Absent


Figure 3.16: Graphs representing the accuracy of participants’ placement based
on the distance the object was placed from the goal position on the Z-axis within
the large physical cube with a Voronoi pattern using either different (Right) X-ray
Visualizations or (Left) the presence of the reference objects. The error bars indicate
each X-ray visualization’s confidence levels (CL = 95%). Significance differences are
displayed as the lines on the right side of the graph stars indicate significance (* =
p < 0.05; ** = p < 0.01; *** = p < 0.001).


Accuracy of Placement (H.1)

Analysis of the accuracy of the user’s placement of the virtual icosahedron object
as compared to the target shape’s position showed a significant fixed effect of the
X-ray Visualization effects (χ2(4, N= 20) = 13.897, p = 0.007) and the presence
of the additional reference objects (χ2(1, N= 20) = 77.641, p < 0.0001), with no
significant interaction effect.
    Post-hoc pairwise comparisons showed significantly improved accuracy when ad-
ditional reference objects were present (p < 0.0001, df = 989, t = 8.771), which can
be seen on the right side of Figure 3.13. Saliency showed significantly lower accuracy
than None (p = 0.0358, df = 989, t = -2.853), and Saliency was also significantly
less accurate than Tessellation (p = 0.0414, df = 989, t = 2.8). The differences in
placement accuracy using of different X-ray Vision effects can be viewed in the left
side of Figure 3.13

Placement Accuracy from User Viewpoint by Axis (H.2 & H.3): The
placement accuracy of the virtual icosahedron object was also analyzed in relation
to the participant’s viewpoint at the time of placement. The initial placement
provides insight into how accurately an object can be positioned within a given space,
allowing for measuring distance along each axis. This enables the determination of
how precisely actions were perceived at the final moments of placement and helps
isolate the areas most affected by different visualizations.
    Positions were transformed into the relative space of the headset using its position
and orientation matrix just after the user placed the object to compute the actual

                                                                       110
and correct final placement. The distance was then calculated along the three axes of
the user’s headset: the x-axis (horizontal) and y-axis (vertical) are the horizontal and
vertical axis of the participant’s view, respectively, and the z-axis was the direction
the participant was looking.
    The viewpoint accuracy on the x-axis, the LMM showed users were significantly
more accurate when the reference objects were present (χ2(1, N= 20) = 5.6295, p
= 0.0184), while the X-ray Vision effect (χ2(4, N= 20) = 5.8363, p = 0.2117) and
the interaction effect (χ2(4, N= 20) = 0.4646, p = 0.9768) was not found to be
significant. The post-hoc comparison for the presence of reference objects showed a
significant improvement in accuracy with reference objects present (p = 0.0184, df
= 989, t = -2.362). All these results can be seen in Figure 3.14.
    Regarding the viewpoint accuracy on the y-axis, the LMM showed a signifi-
cant difference between different X-ray Vision effects (χ2(4, N= 20) = 13.8972, p
= 0.0076) and no significance for the presence of reference objects(χ2(1, N= 20)
= 77.6411, p < 0.0001) and the interaction effect (χ2(1, N= 20) = 5.1383, p =
0.27340). The post-hoc comparisons between all of the X-ray Vision effects showed
no significant values. There was some variability between None and Saliency (p
= 0.0551, df = 989, t = 2.697) and Saliency and Tessellation (p = 0.0985, df =
989, t = -2.469). The left-hand side of Figure 3.15 shows that where Saliency was
slightly less accurate, but it was not significantly different. Whereas, the right side
of Figure 3.15 does present some benefits to having reference objects in a scene.
    The viewpoint accuracy on the z-axis model showed a significant fixed effect of
the X-ray Visualization effects (χ2(4, N= 20) = 11.9119, p = 0.0066). The presence
of the reference objects showed no significant effect (χ2(1, N= 20) = 0.0002, p =
0.9878). The interaction effect (χ2(4, N= 20) = 2.5402, p = 0.6374) showed not
significantly different. The post-hoc effect showed that the Edge-Based visualization
was significantly less accurate at presenting depth than None (p = 0.0499, df = 989,
t = -2.734) and Saliency(p = 0.0048, df = 989, t = -3.476). Some variation could
also be found when comparing Edge-Based to Random Dot (p = 0.0661, df = 989, t
= -2.628) and Tessellation (p = 0.0782, df = 989, t = -2.563). All these effects can
be seen in Figure 3.16.


3.6.2     Time Required (H.5)
This section focuses on answering H.5 in detail by looking at how long this task
took between the different conditions and if this changed the quantity of time they
spent moving the icosahedron around.




                                          111
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

                                          Effect of the Time Required for Participants to Move the Icosahedron




                                                                                                Time Required (seconds)
                                                              ●                                                                                   ●




Time Required (seconds)
                          200        ●
                                                 ●
                                                 ●
                                                                    *                                                     200      ●
                                                                                                                                                  ●
                                                                                                                                                  ●
                                                                                                                                                  ●

                                                 ●

                                                                                    **                                             ●

                                                                             ●
                                                                                                                                          ***     ●
                                                                                                                                                  ●
                                                                                                                                                  ●
                                                 ●            ●



                                                              ●
                                                                            ***
                                                                             ●


                                                                             ●
                                                                                                                                   ●
                                                                                                                                   ●

                                                                                                                                   ●
                                                                                                                                                  ●
                                                                                                                                                  ●
                                                                                                                                                  ●
                                                                                                                                                  ●
                                                                                                                                                  ●
                                                 ●            ●                                                                                   ●
                                                                                                                                                  ●
                                     ●                                                                                                            ●
                                                 ●                           ●                                                                    ●
                                     ●                                       ●                                                     ●              ●
                                     ●                                                                                             ●
                                                                                                                                   ●
                                                                                                                                   ●              ●



                                                                                                                          100
                                     ●                        ●
                                     ●
                                     ●                                                    ●
                                                                                          ●                                                       ●
                                                                                                                                                  ●
                                                                                                                                   ●
                                                                                                                                   ●
                                                                                                                                   ●              ●
                                                                                          ●                                                       ●
                                                 ●            ●              ●                                                     ●
                                                                                                                                   ●
                                                 ●                                                                                 ●
                                                                                                                                   ●              ●
                                                                                                                                                  ●
                                                              ●              ●                                                     ●              ●



                          100
                                                              ●                                                                    ●              ●
                                                                                                                                                  ●
                                                              ●                                                                    ●
                                                                                                                                   ●              ●
                                     ●           ●                           ●                                                     ●
                                                              ●              ●
                                                 ●            ●
                                     ●                                       ●
                                     ●           ●
                                                 ●                           ●
                                     ●           ●                                        ●
                                     ●           ●
                                                 ●
                                     ●                                                    ●
                                                                                          ●
                                                                                          ●




                                                                                                                            0
                            0                                                                                                    FALS
                                                                                                                                      E
                                                                                                                                                TRU
                                                                                                                                                    E
                                Random Dot Tesselation      Edge        Saliency         None                                   Reference Objects Were
                                                     X−ray Visualizations                                                         Present or Absent


Figure 3.17: Two box plots analyzing the time it took to complete the task com-
pared between different (Right) X-ray Visualizations and (Left) with and without
the reference objects. The error bars indicate each condition’s confidence levels (CL
= 95%). Significance differences are displayed as the lines on the right side of the
graph stars indicate significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).


Time Required for task completion (H.5):               The completion time for each
iteration was measured from the time the participant pressed the start button until
the time they pressed the end button. The model of completion time showed signif-
icant fixed effects of both the X-ray Visualization effects (χ2(4, N= 20) = 24.441,
p < 0.0001) and the presence of the reference objects (χ2(1, N= 20) = 14.2427,
p = 0.0002). No significant interaction effect between X-ray Visualization and the
presence of the reference objects was found (χ2(4, N= 20) = 2.4244, p = 0.6582).
    Post-hoc pairwise comparisons of the X-ray Visualizations showed None was sig-
nificantly faster than Edge (p < 0.0001, df = 983, t = -4.693), Saliency (p = 0.0039
df = 983, t = -3.536) and Tessellation (p = 0.0396 df = 983, t = -2.817). Com-
parisons of the presence of reference objects showed participants were significantly
slower with reference objects present (p = 0.0002, df = 989, t = -3.757). These
results and Figure 3.17 indicate that these X-ray Visualizations may slow down the
user slightly to allow them to achieve a similar level of accuracy.

Time Participants Spent Moving the Object (H.5) All of the times when
a user would press the button to pick up an object and put the same object down
were recorded and summarised across each task to record the time spent on each
task. This was calculated as a percentage of the time the participants moved the
icosahedron each time. This enabled the identification of which aspects of the task
posed greater challenges for participants: placing the icosahedron or viewing and
confirming its correctness. The LMM showed a significant fixed effect on the X-ray
Visualization condition (χ2(4, N= 20) = 36.4188, p < 0.0001) and the presence of
the reference objects (χ2(1, N= 20) = 7.7862, p = 0.005). No significant interaction
effect was found. (χ2(4, N= 20) = 4.9161, p = 0.296).
    Post-hoc pairwise comparison of the X-ray Visualizations showed participants


                                                                                  112
                                       Effect of Reference Objects on Participant Icosahedron Movement Time
                                                         ●                                                                                  ●




                         20                                                                                           20       ●
Placement Accuracy(cm)


                                                                                ●




                                                                                             Placement Accuracy(cm)
                                                                                                                                     *
                                                                                                                               ●            ●
                                                         ●          ●
                                                                                                                                            ●
                                                                    ●




                         10        ●


                                   ●                                ●
                                                                                ●                                     10       ●
                                                                                                                               ●

                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●
                                                         ●




                         0                                                                                             0

                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●
                                                                                                                                            ●
                                   ●




                −10
                                   ●

                                   ●


                                   ●
                                                         ●
                                                         ●
                                                                                                             −10               ●

                                                                                                                               ●
                                                                                                                               ●
                                                                                                                               ●
                                                                                                                                            ●




                                                                                                                               ●
                                                         ●
                                                         ●
                                                         ●




                                                                                                                            Absent       Present
                              Random DotTesselation   Edge      Saliency      None                                         Reference Objects Were
                                             X−ray Visualizations                                                            Present or Absent


Figure 3.18: Graphs representing the percentage of time participants spent moving
the icosahedron for each task for each X-ray Vision effect and (Right) the percentage
of time required to move the icosahedron that participants moved the icosahedron
using either different (Right) X-ray Visualizations or (Left) the presence of the
reference objects. The error bars indicate each X-ray visualization’s confidence levels
(CL = 95%). Significance differences are displayed as the lines on the right side of
the graph stars indicate significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).


held the object for significantly less time for Random Dot than Edge (p = 0.0130,
df = 989, t = -3.183), Tessellation (p = 0.0364, df = 989, t = 2.847), and Saliency
(p < 0.0001, df = 989, t = 5.475), Edge than for Saliency (p = 0.0012, df =
983, t = 3.847), and None than for Saliency (p = 0.0012, df = 983, t = 3.847).
Figure 3.18 shows that the more occlusion a visualization had caused, the longer
the participants were holding the icosahedron. Post-hoc comparisons of the presence
of reference objects showed participants were significantly slower without reference
objects present (p = 0.0056, df = 989, t = 2.778).

User Behaviour Results (H.6 & H.7)

This section is focused on how the different conditions affected the participant’s
ability to move around the environment and where they felt comfortable standing.

Distance Moved (H.6): The LMM for the distance moved throughout each it-
eration by the participants showed a significant fixed effect between the X-ray Vi-
sualization effects (χ2(4, N= 20) = 52.8212, p = 0.0043) and a significant difference
between the presence of the virtual reference objects (χ2(1, N= 20) = 7.8110, p =
0.0052). Still, no significant interaction effect between the X-ray Visualization effect
and the presence of the reference objects was found χ2(4, N= 20) = 0.4301, p =
0.9799. Post-hoc pairwise comparisons found that users moved significantly more
when the reference objects were present (p = 0.0055, df = 989, t = -2.782). Compar-
isons of the combined X-ray Visualization effects and the presence of the reference


                                                                        113
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

                                                         Effect of Reference Objects on Participant Icosahedron Movement Time

Percentage of Time Icosahedron was Moved
                                                                                                                 100




                                                                                                                                   Icosahedron was Moved (cm/s)
                                                                                                                                                                                    ●
                                                                  ●

                                                     ●




                                                                                                                                                                                               ●



                                                                                                                                                                                        *
                                           30                                                                                                                                                  ●
                                                                                                                                                                                               ●
                                                                                                                                                                                               ●



                                                                                                                                                                    75              ●          ●




                                                                                                                                           The Velocity
                                                                                                                                                                                    ●


                                                                  ●                                      ●                                                                                     ●

                                                                                                                                                                                    ●
                                                                                                                                                                                    ●
                                                                                                                                                                                    ●          ●
                                                                                                                                                                                               ●
                                                                                                                                                                                    ●
                                                                                                                                                                                    ●
                                                                                                                                                                                    ●
                                                                                                                                                                                    ●          ●
                                                                                                                                                                                               ●
                                                                                                                                                                                               ●
                                                                  ●
                                                                                                                                                                                               ●
                                                                                                                                                                                               ●
                                                                                                                                                                                               ●




                                           20                                           ●

                                                                                        ●
                                                                                                         ●


                                                                                                                                                                    50
                                                                  ●
                                                                             **         ●
                                                                                                         ●
                                                                                                         ●

                                                                                                         ●
                                                                                                         ●         ***
                                                                  ●
                                                                  ●                                      ●
                                                                                                         ●
                                                                                                         ●
                                                     ●                                  ●
                                                                                                         ●
                                                     ●
                                                                                        ●
                                                                                        ●



                                                                                                                                                                    25
                                                                  ●
                                                                                        ●                ●
                                                                  ●                                      ●
                                                     ●                                                                     ●




                                           10
                                                     ●
                                                     ●
                                                     ●                                                                     ●
                                                                                                         ●
                                                                                                         ●                 ●
                                                                                                         ●                 ●
                                                                                                                           ●
                                                     ●                                  ●                                  ●
                                                                  ●                                                        ●
                                                                                        ●
                                                                  ●                     ●
                                                                                        ●                                  ●
                                                                                                                           ●
                                                     ●            ●                                                        ●
                                                     ●            ●                                                        ●
                                                                  ●
                                                     ●
                                                     ●
                                                                                                                           ●
                                                                                                                           ●
                                                                                                                           ●




                                                                                                                                                                                    bsen
                                                                                                                                                                                         t            nt
                                                                                                                                                                                                 rese
                                            0                                                                                                                               W ere A        Were P
                                                                                                                                                                         RO           RO
                                                Random Dot Tesselation                 Edge       Saliency                None                                             Reference Objects Were
                                                                      X−ray Visualizations                                                                                   Present or Absent


Figure 3.19: The distance participants walked for each when (Left) different X-
ray Visualizations were displayed to them or (Right) the differences when reference
objects were or were not available. The error bars indicate each X-ray visualization’s
confidence levels (CL = 95%). Significance differences are displayed as the lines on
the right side of the graph stars indicate significance (* = p < 0.05; ** = p < 0.01;
*** = p < 0.001).

                                                    Impact of Reference Objects on Participant Distance from Colorful Voronoi Box
                                                                                                                 175



                                                                                                                                   Distance Between Participants
                                                            **




                                                                                                                                   and the Center of the Box(cm)
                                                                                                                                                                                ●




                                                                                  **
Distance Between Participants
and the Center of the Box(cm)




                                                                                                                                                                                               ●




                                            200
                                                                                                                                                                                               ●
                                                                                                                                                                                ●
                                                                                                                                                                                ●              ●
                                                                                                                                                                                ●              ●


                                                                                                ***                                                                             ●
                                                                                                                                                                                ●
                                                                                                                                                                                ●              ●



                                                                                                                                                                   150
                                                                                                                                                                                ●              ●
                                                                                                                                                                                ●              ●




                                                                                                                    ***
                                                                                                             *                 ●




                                                          ●
                                                          ●
                                                          ●
                                                                       ●
                                                                       ●
                                                                       ●
                                                                       ●
                                                                                            ●


                                                                                            ●                                  ●
                                                                                                                                                                   125
                                            150
                                                                       ●
                                                                       ●
                                                          ●                                                                    ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●            ●                                                       ●
                                                                                                                               ●
                                                          ●
                                                          ●            ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●
                                                          ●




                                                                                                                                                                   100

                                                                                                                                                                    75
                                                                                                                                                                                ●
                                                                                                                                                                                ●



                                            100                                                                                                                                 ●



                                                                                                                                                                                ●
                                                                                                                                                                                ●
                                                                                                                                                                                ●
                                                                                                                                                                                               ●



                                                                                                                                                                                               ●




                                                                                                                                                                                               ●

                                                                                                                               ●                                                               ●
                                                          ●
                                                          ●
                                                          ●                                                                    ●
                                                          ●



                                                                                                                                                                                                      nt
                                                                                            ●


                                                                                                                                                                                         t
                                                                                                                                                                                    bsen          rese
                                                                       ●




                                                                                                                                                                              ere A
                                                          ●




                                                                                                                                                                                            ere P
                                                                                                                               ●

                                                          ●
                                                                                                             ●




                                                                                                                                                                         RO W          R O W
                                                  Random Dot Tesselation                Edge          Saliency            None                                             Reference Objects Were
                                                                           X−ray Visualizations                                                                              Present or Absent


Figure 3.20: The distance participants walked for each iteration was based on
the differences between (Right) each X-ray Visualization they expressed or (Left)
when reference objects were unavailable. The error bars indicate each condition’s
confidence levels (CL = 95%). Significance differences are displayed as the lines on
the right side of the graph stars indicate significance (* = p < 0.05; ** = p < 0.01;
*** = p < 0.001).


objects showed significance between the Edge-Based and Saliency Visualizations (p
= 0.003, df = 989, t = 3.610) as well as some variance between Edge-Based and
Saliency (p < 0.0806, df = 989, t = 2.550), and Random Dot and Saliency (p <
0.0957, df = 989, t < 2.481) but no significance. Plots illustrating these effects can
be seen in Figure 3.19.

Distance from Box (H.7) The LMM of the distance the participants stood away
from the large physical cube throughout the task on average showed a significant
fixed effect of the X-ray Visualization effects (χ2(1, N= 20) = 14.1305, p = 0.0067).

                                                                                                                 114
Both the presence of reference objects (χ2(4, N= 20) = 0.5301, p = 0.4666) and the
interaction effects these (χ2(4, N= 20) = 0.5013, p = 0.9733) showed no significance.
Post-hoc pairwise comparisons of the X-ray Visualization effects showed a significant
result between Random Dot and saliency (p = 0.0217, df = 989, t = 3.021). Some
variation was found between Edge-Based and Random Dot (p = 0.0875, df = 989,
t = -2.716) and None and Saliency (p = 0.0652, df = 989, t = 2.633) but no
significance. Comparisons between X-ray Vision effects can be seen on the right
side of Figure 3.20.

Speed Object Was Moved (H.6) The speed at which the users moved the item
was also measured by the sum of the times the user moved the object divided by
the amount of distance the icosahedron would have been moved. Understanding
this provides insights into when participants felt confident in selecting a location for
the icosahedron or whether the visualization enhanced their spatial awareness. The
LMM showed a significant fixed effect between the X-ray Vision effects (χ2(1, N=
20) = 18.2724, p = 0.0015) and a significant fixed effect was shown regarding the
reference objects (χ2(4, N= 20) = 8.9925, p = 0.0097). No significant interaction
effect was found. (χ2(4, N= 20) = 1.8465, p = 0.764).
    Post-hoc pairwise comparisons found users movement was both significantly dif-
ferent and faster when reference objects were present (p = 0.0102, df = 989, t =
2.574), which can be seen in Figure 3.21. The post-hoc comparisons for the X-ray
Vision effects show that Random Dot was significantly faster both than Saliency (p
= 0.0017, df = 989, t = -3.754) and Tessellation (p = 0.0088, df = 989, t = -3.303),
which can be seen in Figure 3.21.


3.6.3     Subjective Results (H.8 & H.9)
This section focuses on answering H.8 and H.9 in detail by looking at the sub-
jective data collected throughout the study. All subjective data was found to be
non-normally distributed using a Shapiro-Wilk test (p < 0.05). Therefore, a non-
parametric Friedman test was used to determine if there were any significant dif-
ferences between the various conditions. Mixed Effect Models were not used since
the data was represents the participant’s subjective opinion of each condition rather
than a measurable quantity as such treatment of the participant as a random effect
was not necessary.
    The PAAS results (shown in the upper left plot of Figure 3.22) showed a signif-
icant difference between the X-ray Visualizations using a Friedman rank sum test
(χ2(4, N= 20) = 41.185, p < 0.0001). Post-hoc analysis with pairwise Wilcoxon
signed-rank tests was conducted with a Bonferroni correction applied. Comparisons


                                          115
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

                                          Impact of Reference Objects on the Velocity Participants Moved the Icosahedron
                               100                               ●
                                                                                                                                     100        ●




                                               **




                                                                                                      Icosahedron was Moved (cm/s)
Icosahedron was Moved (cm/s)
                                                                             ●                                                                             ●




                                                           **                                                                                        *
                                                                                                                                                           ●
                                                                                                                                                           ●
                                                                             ●
                                                                             ●
                                                                                                                                                           ●




                                                                                                                                     75
                                                      ●




                                75                    ●          ●
                                                                             ●
                                                                                                                                                ●
                                                                                                                                                ●
                                                                                                                                                           ●




                                                                                                              The Velocity
        The Velocity


                                                                                                                                                           ●
                                                      ●
                                                                                                                                                ●
                                                                                                                                                ●
                                                      ●          ●
                                                                                                                                                ●          ●
                                                                                                                                                           ●
                                                                                                                                                ●
                                                      ●                      ●           ●                                                      ●
                                                                                         ●
                                                      ●                                                                                         ●
                                                                                                                                                ●          ●
                                                                                                                                                           ●
                                                                             ●                                                                             ●
                                                                                                                                                           ●
                                                                                                                                                           ●
                                          ●                                                                                                                ●
                                          ●
                                          ●
                                          ●




                                50                                                                                                   50


                                25                                                                                                   25


                                                                                                                                             Absent      Present
                                     Random DotTesselation      Edge     Saliency      None                                                Reference Objects Were
                                                     X−ray Visualizations                                                                    Present or Absent


Figure 3.21: Box plots representing the speed/velocity in which the object was
moved based on the icosahedron: (Left) between each X-ray Visualization they
were experiencing or (Right) whether reference objects were present. The error bars
indicate each condition’s confidence levels (CL = 95%). Significance differences are
displayed as the lines on the right side of the graph stars indicate significance (* =
p < 0.05; ** = p < 0.01; *** = p < 0.001).


showed significantly increased cognitive load between Saliency and Edge-Based (p =
0.0208), Saliency and None (p < 0.0001), Saliency and Random Dot (p = 0.0003),
Saliency and Tessellation (p < 0.0001).
    The System Usability Scale results (shown in the lower left of Figure 3.22) showed
a significant difference between the X-ray Visualizations using a Friedman rank sum
test χ2(4, N= 20) = 45.234, p < 0.0001. Post-hoc analysis with pairwise Wilcoxon
signed-rank tests was conducted with a Bonferroni correction applied, resulting in a
significantly lower usability score for Saliency than Edge-Based (p = 0.0012), None
(p = 0.0002), Random Dot (p = 0.0002), and Tessellation (p = 0.0002).
    At the end of each study, users were asked if they could state their favorite
visualization and explain why. The results are tallied and displayed the results in
the right-hand side plot in Figure 3.22. No significant difference was found between
the X-ray Vision effects when using a Chi-Square Test χ2(4, N= 20) = 2.5, p =
0.6446 when participants where asked why this visualization seems to have been
their favorite. The results can be seen in Chapter E.
    Participants would seem to have picked Tessellation and None more often as
they were less occlusive and did not interfere with the task. Tessleation seems to
have provided users with enough visual cues to be able to navigate with it easily,
making it the second most preferred method of visualizing the internal structures
of an element. Users also reported liking Random Dot since its design matched the
shape of the box the most. User comments found in Chapter E show that Saliency
was criticized for blocking out too much of the object underneath. At the same
time, several participants liked how unobtrusive the edge-based visualization was,


                                                                                 116
                      Random Dot
                       Tesselation                                                                                       7.5




                                                                                                         ***
                                                                                                           ***
                            Edge                                                             ●




                                                                                                 **
X−ray Visualization



                             X−ray Visualization




                                                                                                                 Votes
                         Saliency                                                                                        5.0




                                                                                                   ***
                            None                                             ●




                                                   0.0    2.5          5.0             7.5
                                                                PAAS Score                                               2.5
                      Random Dot
                       Tesselation                                                                                       0.0




                                                                                                         ***




                                                                                                                               None

                                                                                                                                      Saliency

                                                                                                                                                 Edge

                                                                                                                                                        Tesselation

                                                                                                                                                                      Random Dot
                                                                                                           ***
                            Edge




                                                                                             ***
                         Saliency                                                      ●




                                                                                                   ***
                            None
                                                    20   40       60         80            100
                                                                SUS Score                                                      X−ray Visualization

Figure 3.22: Three plots displaying the subjective results collected throughout
the course of this study. (Upper Left) PAAS cognitive load results were acquired
after each participant finished using an X-ray Visualization in the study. Higher
scores indicate a higher cognitive load required. (Lower Left) The System Usabil-
ity Scale (SUS) results were acquired after each participant finished using an X-ray
Visualization in the study. Higher scores indicate better usability. (Right) A tally
of participants’ favorite visualizations was totaled by having them choose their fa-
vorite ones. Error bars show standard error values. Error bars indicate ± standard
deviation. Significance differences are displayed as the lines on the right side of the
graph stars indicate significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).


but some participants noticed a slight delay while using both it and Saliency.




                                                                                 117
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

3.7      Discussion
The findings from Section 3.6 present a broad spectrum of findings, which this section
discusses in detail. Notably, it was found that while Saliency was beneficial for depth
perception, it hindered the general placement of objects and was generally found to
be the hardest to use when evaluating its usability and participants’ cognitive loads.


3.7.1     Accuracy of Placement (H.1)
The results shown in Section 3.6.1 from this study did not support hypothesis one
but partially supported hypothesis three. The right-hand side of Figure 3.16 shows
Saliency positively affected depth perception compared to the edge-based visual-
ization. Table 3.1 shows that while there seems to be a lot of difference between
Saliency’s median and mean values being over 3 cm different, whereas other condi-
tions throughout this study were relatively consistent. This shows that users were
very inconsistent with saliency, with their results sometimes being very accurate and
sometimes being very inaccurate. This is likely because the occlusion created by the
saliency covered larger areas of the visualization, showing that more occlusion can
lead to better depth perception.


3.7.2     Placement Accuracy from User Viewpoint by Axis
          (H.2 & H.3)
The right-hand side of Figure 3.14 shows that Saliency was the least accurate visu-
alization on the horizontal axis. This can be explained better in-depth when looking
at insignificant trends on the vertical axis (shown in Figure 3.15) and the values for
x and y in Table 3.1 where both random dot and saliency show a worse mean and
standard deviation values. This is likely due to the high occlusion in this X-ray
visualization. The effects that occluded the least gave the user a better understand-
ing of space, where the shapes were inside the large physical cube with a Voronoi
pattern. Still, the results from Figure 3.14 and Figure 3.15 lacked the significance
to support hypothesis two.
    The results in Section 3.6.1 and Table 3.1 indicate that, on an OST AR device,
the edge-based visualization produced the poorest depth perception, particularly
when no reference objects were present. This could be due to two reasons: as
mentioned previously, prior work has shown that more occlusion allows for better
depth perception [209], provided the visualization does not occlude too much [218];
the other reason may be due to the edge-based visualization in this instance doesn’t
represent the shape of the cube well due to the mosaic nature of the pattern [38].
For this research, a Voronoi effect was chosen to be used on the large physical cube,

                                         118
as I hypothesized to aid both the Saliency and Edge-Based X-ray Vision effects by
creating more areas for them to highlight; however, due to the non-uniform pattern
on the box, the fact participants were looking down at it and within the box and
rather all may show a weakness in this format when compared to a brick wall [38,172].
Previous studies indicate Edge-Based visualization can tell a user if an object is in
front of or behind another object quite well [38, 211, 217]. Still, the results from this
study indicate the impact on depth perception can be slightly misleading. These
findings may also be true for the Saliency visualization; however, the improvement
which can be seen in Figure 3.16 and lowest mean accuracy showed in Table 3.1
illustrates that the depth perception Saliency presents may make it more viable for
certain situations.
    Section 3.6.1 shows that users did perform significantly better with reference
objects, supporting the fourth hypothesis. Overall, it seems that introducing the
reference objects may have allowed participants to place the icosahedron about 1 cm
closer to the target position. Interestingly, though, it seems to have much more to do
with the placement of all three axes, especially the user’s horizontal axis, which can
be seen in Figure 3.15 where a significant difference is less accurate than None and
Tessellation. Still, it does seem clear that users can more accurately place objects
when they have more than one reference in the static state.


3.7.3     Time Required (H.5)
Section 3.6.2 showed that all the fifth, sixth, and seventh hypotheses were not sup-
ported as Figure 3.17 shows that users seemed to take slightly longer (about 3
seconds) to complete each task when reference objects were present. They seemed
to have moved somewhat less and were closer to the large physical cube with a
Voronoi pattern. Due to the relatively similar results, they seem to indicate that
participants were not utilizing the depth cues of relative size and density because
they were either ineffective or not being used. Participants primarily used the vi-
sualizations to improve their spatial awareness of the object. In some cases, this
seemed to involve treating the visualizations as implicit measuring aids rather than
relying on them directly as depth cues. This observation, while not systematically
verified, suggests that users may appropriate visualization features in ways that were
not originally intended.


3.7.4     User Behaviour Results (H.6 & H.7)
There was no significant effect on the differences that participants moved between
when using different X-ray Vision effects. Still, no effect was found, which indicated
that when a more occlusive X-ray Vision effect was used (Saliency and Random

                                          119
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

Dot), participants did get slightly closer to the large physical cube with a Voronoi
pattern. This was likely because it allowed them to look into the box easier, not
because it encouraged them to stand closer to the box.
    Other behavioral findings which were observed in Section 3.6.2 were that users
were significantly faster when using no X-ray Vision effect (shown in Figure 3.17). It
seems that viewing the X-ray Vision effect slightly delays users’ movement as they
comprehend the layout of the various visualizations, making more straightforward
visualizations like that have a predictable pattern like Random Dot and Tessellation
easier to use. This could also be seen with the speed they moved the icosahedron
faster while using Random Dot (shown in Figure 3.21). Since Figure 3.18 showed
participants held the virtual objects for the least amount of time with no X-ray
Visualization effect and Tessellation, it seems that Random Dot effect may have
been as a guideline to determine where to place the object much like the reference
objects.


3.7.5     Subjective Results (H.8 & H.9)
The results from the qualitative data (reported in Section 3.6.2) seem to show that
participants would find Saliency hard to use (H.8). The upper left plot of Fig-
ure 3.22 illustrated how the PAAS scores varied between all conditions which was
also reflected in the SUS score shown in the lower left plot in Figure 3.22.
    All conditions showed a significant difference between them and Saliency, in-
dicating they tended to dislike the Saliency visualization on the OST AR device
when they were close to the visualization tracking smaller objects. Feedback from
this visualization noted that the occlusive nature of this visualization led them to
lose track of various virtual objects during the study and possibly caused some frus-
tration (H.9). It should be noted that this observation was not universal, as one
participant stated, "It was easier to position the objects accurately as opposed to the
others" about Saliency. Our participants reported that Random Dot and Saliency
blocked their vision too much, which may have been the reason for its lack of popu-
larity. Random Dot may be improved if the number of dots was increased and their
size decreased depending on the participant’s distance from the object.
    The questionnaires and user comments showed the None or baseline condition
was viewed as the most preferred visualization next to Tessellation. This further
indicates that X-ray Visualizations that occlude less would be preferred. However,
the visualizations occluded the most improved depth perception the most. It would
seem that moving forward to research these visualizations that it would make sense
not just to consider depth perception but also other tasks that may be incorporated
into these studies.


                                         120
3.7.6     Summary of Discussion
What was unexpected but interesting about this study is that having no X-ray Vi-
sualization did not do poorly at this task. Unexpectedly, participants with no X-ray
Visualization seemed to have performed fine throughout the study. No participants
commented that the objects felt like they were in the wrong place, which did not
prevent anyone from performing the task. While it did not aid people in the task
they were given, it was never a detriment. It seems like the stereoscopic effects
from the display may have fixed a major issue with X-ray Visualizations by provid-
ing more depth perception cues than other devices showing that indeed that just
enough reality may be enough in some instances. Showing that depth cues other
than occlusion may be just as important as occlusion for X-ray Vision.
    A plausible explanation for the None condition’s success lies in how participants
processed spatial information. Those exposed to the X-ray Visualizations may have
experienced divided attention between the virtual and physical environments, while
those without overlays could focus on the more consistent depth cues provided by
the stereoscopic display. For instance, motion parallax and binocular disparity are
potent depth cues that can be effectively utilized in the absence of occlusive overlays.
Display brightness may also have played a role, as the reduced contrast between
virtual and real content could have created a natural “transparency effect,” partially
replicating the purpose of an X-ray view. This might explain why some users still
valued the X-ray representations even though they were not strictly necessary for
task completion.
    The proposal here is that the AR effect for X-ray Visualizations effects should
be designed to be as unobtrusive as possible. Rather than trying to create a perfect
X-ray Visualization effect, the goal should be to create an effect that is just good
enough to provide the necessary information to the user without overwhelming them
or blocking their view of the real world. Using occlusion only to illustrate where an
item is compared to the physical object. It does not need to observe it, although
that would provide better depth perception.


3.8      Future Considerations
This study has several limitations that may have impacted this work. The gender
gap among participants may have influenced some of the findings however there is
no evidence of this. The large physical cube with a Voronoi pattern and the X-ray
Visualizations were designed with heavy consideration from Otsuki et al. [209] and
Santos et al. [218]. Experiments with different implementations and settings for
these conditions could result in different results, and it would be interesting to see


                                          121
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

if research like the work from Santos et al. [57] on illumination and opacity has the
same results on other devices.
    In future studies, I would consider restricting movement around the larger cube.
For this study, it was valuable to see how well a participant could place a virtual
object in an augmented world using X-ray Vision while being able to move in the
physical space, which mimics real-world settings. Although this allowed the partic-
ipants to employ their own problem-solving techniques, this made analysis difficult
as each participant undertook their own strategies, resulting in a wide range of re-
sults. This was a benefit to this research as it showcases data that could easily be
translated to a real-world setting. Still, it would have been possible to obtain more
consistent and statistically stronger results by restricting the users’ movement to a
smaller space.
    Great steps were taken to reduce latency for this study by developing a system
that presented a video feed where it was captured with a assumption this will not be
at real time (as presented in in Section 3.5.4), but it did not eliminate it completely.
Mounting a camera to the head of the participants was a useful attempt to transfer
VST AR effects to an OST AR headset, but it was not without difficulties as several
participants commented that they noticed some latency between their vision and
the camera. Camera technology is not yet on par with the precision you can gain
from using an Inertial Measurement Unit (IMU) like the one found on the HoloLens
(approx. 1000 fps) [203,294], and relying on cameras creates a noticeable lag between
the augmented world and the physical objects as participants move about. To some
extent, a lag of less than 16ms was purposeful since the experiment tested these
visualizations as they would need to run in a real-world environment. I could have
used a pre-generated texture that applied the effect to the outside of the box. This
would allow for these methods to be tested but would not be applicable to any real
world scenarios in the near future [29].
    A system that allowed direct access to the IMU would have been ideal. If it
was possible to know the exact millisecond or better when a photo was taken, the
system could have placed the image in the exact right spot rather than being several
milliseconds late. Further corrections could also be made if more information was
utilized between each frame by taking into account velocity or advanced machine
learning techniques for predicting user movements like the work from Gamage et
al. [295] and Lee et al. [296]. This allows the system to place the overlay texture in
almost the exact place for the appropriate millisecond. Another option around this
issue would have been not placing the camera sensor in a stationary position, similar
to [268]. This would have removed any issues regarding movement but would have
made the system much less flexible.
    Overall the lack of a time limit lead to a wide variety of strategies being used


                                          122
by participants causing a lot of variance in the time taken to complete the task.
This was purposeful to allow participants to take their time and ensure they were
accurate and demonstrate utilize their own sense of depth perception. This lead
which may have been a confounding factor in the results, as some participants took
their time to ensure they were accurate, while others rushed to complete the task
quickly.
    This study allowed for a wide range of interactions with the experimental setup.
While this does provide a more realistic scenario, it also indicates that we may
not have been researching spatial awareness as much as we were problem-solving or
another cognitive task. This could have been mitigated by restricting the movement
of the participants or by having a more structured task. Moving forward it would
make sense to run a more restricted version of this study.


3.9     Conclusion
This research has shown that the X-ray Visualization effects that work on VST
AR devices can be transferred to OST AR devices, but they have limited benefits
over real world overlay x-ray visualizations. This study has shown that there is
a large difference between different methods of X-ray Vision effects and how they
are displayed. This study found that occluding larger parts of the internal object
will improve depth perception but at the cost of vertical and horizontal accuracy.
When using stereoscopic Ocular See-Through (OST) Augmented Reality (AR) dis-
plays, users seem to value seeing the object’s geometric structure, which can be seen
through participants’ answers to the subjective questionnaires and their performance
with the edge-based visualization.
    The main contributions of this chapter are:

  • More occlusion can help with depth perception but hinders placement.

  • Users don’t want their vision to be occluded in an obtrusive way, meaning it
    needs to be genuinely salient (aligned with the user’s attention and task rele-
    vance, not just visual distinctiveness) to the user for saliency to work correctly.

  • The presence of occlusion for X-ray Vision is essential but unnecessary to cover
    the entire field.

  • A system design that allows computer vision effects to be overlaid on a user’s
    vision.

   This chapter has provided insights into which rendering techniques are effective
as X-ray Visualizations (such as Saliency, Edge-Based, Tessellation, and Random


                                         123
   3. SPATIAL ESTIMATION IN AUGMENTED REALITY AIDED
X-RAY VISION

Dot). It has also clarified which perceptual outcomes are important for achieving
X-ray Vision, namely accurate depth perception, alignment with the real world, and
unobtrusive integration into the user’s view. This study has clarified that while
occlusion can aid depth perception, it hinders general spatial awareness of a given
space. Making it less than an idea to find a usable solution for X-ray vision. This
study also showed that Computer Vision enabled X-ray Visualizations may not have
a place on OST AR HMDs and should likely not be considered until the video camera
hardware has progressed further so it can better keep up with human sight.




                                       124
Chapter 4

Designing X-ray Visualizations
with Volume Rendering

This chapter presents an investigation into the use of volume rendering to support
X-ray Visualizations by providing an explanation of the technical aspects of volume
rendering and motivating the opportunity of volume rendering. Then, it presents a
summary of volume rendering techniques and algorithms designed and demonstrated
for use on stereoscopic displays, which form the basis of the research in this disser-
tation. Finally, drawing from the knowledge from the literature review (Chapter 2)
and the previous study (Chapter 3) to create a new form of X-ray Visualizations
the Volumetric Illustrative Rendering Techniques (VIRT)s.
    The VIRTs are designed for Magnetic Resonance Imaging (MRI) and Computed
Tomography (CT) data when displayed using Direct Volume Rendering (DVR) and
how to view the inside of a solid object that they are captured from. The image
data from MRI and CT scanners has been chosen as the main focus as these devices
are the most commonly used forms of volume rendering that are used to look inside
of objects rather than trying to visualize or make other data readable.
    To understand what is inside solid objects, a 3D scanning technique will need to




Figure 4.1: A patient receiving a scan in the CT scanner. Left) shows the envi-
ronment within the Scanning Room. Right) is one example of an operation area for
a given CT scanner. Provided by NIH Clinical Center

                                         125
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.2: Several different types of 2D data: Left) MRI of a skull 1 ; Center)
Cerebral angiography, arteria vertebralis sinister injection 2 ; Right) CT scan of hu-
man lungs 3 . All images are licensed under Creative Commons Attribution licences


be utilized, allowing people to observe an area using a method that human sight can
use. Some options exist for the creation of this volume rendering data. Firstly, they
can be created manually or from the result of a simulation [297] or by using other
solutions like tracking the speed of signals between different areas [298]. Electronic
microscopes can visualize the data they see as a volume [2,299]. Ground Penetrating
Radar (GPR) can be used to understand what is below the ground, from pipes and
artifacts to different layers of ground sediments [119,300,301]. MRI and CT scanners
are more commonly used to see the inside of people [302, 303] and most other types
of materials [115, 116, 304].
    Medical practitioners’ current practice is to view medical data on a 2D display
near the patient. This is shown in Figure 4.1 where the practitioner is in a separate
room with a window between them and the patient looking at a series of slices
from the scan. Their concentration will be on this display, which will display images
shown in Figure 4.2, where they can view the data from a series of 2D planes of view
(axial, coronal, and sagittal). This type of exploration can be challenging to learn
and interpret methods of data interaction [305–307]. The experience on 3D displays
can be better suited for viewing 3D data [16, 21, 308], and studies have shown that
displaying this data over real objects can further aid this understanding [36,43,309].
This makes volume rendering more suitable for Mixed Reality (MR) Head-Mounted
Displays (HMDs) see as they utilize 3D displays.
    MRI and CT scanners are utilized to investigate areas inside objects that could
not usually be seen; however, this experience is not simple. Hole-like visualizations
work by obscuring most of the data collected, which is required to create the X-ray
  1
     https://www.flickr.com/photos/reighleblanc/3854685038
  2
     https://en.m.wikipedia.org/wiki/File:Cerebral_angiography,_arteria_
vertebralis_sinister_injection.JPG
   3
     https://commons.wikimedia.org/wiki/File:CT_scan_Iterative_reconstruction_
%28left%29_versus_filtered_backprojection_%28right%29.jpg


                                         126
Vision effect. This is problematic as MRI and CT scans generally only observe the
immediate area. Since they only show three axes, the only logical forms of entry are
along the three planes (axial, coronal, and sagittal) [310]. 3D objects also allow for
navigating challenging anatomy that can take on multiple different shapes, like the
liver [311].
    The radiation caused by CT scanners can be harmful to the health of patients.
Both MRI and CT scans are time-consuming and expensive, limiting the amount
of data that can be realistically collected when medical practitioners produce the
MRI or CT scans designed to diagnose and guide appropriate treatment. This will
generally result in a trade-off between the size and accuracy of the volumes these
machines create. Since this supply is already limited, using a method of X-ray
Vision that requires excess information to be provided to adequately a hole-like X-
ray Vision technique in practice would require a change in best practice to collect
more data, which would just be used to aid the illusion of depth perception.
    This chapter aims to enable methods of X-ray Vision that can be used with
DVR to create a method to view information created from MRI or CT scanners
using an Ocular See Through (OST) Augmented Reality (AR) HMD. This will
utilize artistic effects (VIRTs) because they do not obscure the focus of the user
while still presenting the illusion of depth within the real world. However, before
that goal is reached, there is a need to create and establish a method of real-time
DVR for stereoscopic displays. This chapter presents a novel apparatus to employ
DVR on MR, allowing VIRTs to be presented to users.


4.1     Fundamentals of Volume Rendering
Volume datasets represent information relating to a given physical space [312]. They
can either be viewed as a stack of 2D images, a singular 3D image, or a vector or
scalar field. Volume datasets are known for representing CT and MRI, but they
are also used for displaying geometric data that consists of information that can be
described in a voluminous way, such as meteorological data [312, 313].


4.1.1    Technical Description of Volume Rendering
Volumes are organized into a grid of Voxels, each with its own position and value.
These values can range in their purpose from Houndsvile units found in CT scans [314],
relaxation times for MRI scans [315], and they can either contain normals or veloc-
ity when looking at such as meteorological data [313]. Generally, a default value
is chosen when no value exists in a particular area. This makes volume data much
less flexible than point cloud data but allows for more flexibility with the rendering


                                         127
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING

process.
    Volumes can also be extended into having a fourth dimension, which will nor-
mally represent time, allowing them to change the visualization based on the amount
of time that has passed. This can be used in medicine to give the surgeon a clear
idea about how much organs in a patient may move normally [316, 317] or to allow
meteorologists to view the impact of phenomena like wind in real-time [5, 114].


4.1.2      Visualizing Volume Data
To visualize a volume in 3D, it is preprocessed into an iso-surface or rendered directly.
Creating an iso-surface makes it possible to use more traditional graphical rendering
methods to visualize the volume, allowing these visualizations to be viewed using less
computationally expensive at run time [318, 319]. Lowering the resulting polygon
count of these volumes makes it possible to allow them to work on even less powered
devices [320]. However, preprocessing a volume into an iso-surface requires running
time-consuming processes, which decreases and warps the amount of information
seen. Typically, this type of visualization will be used to see only a few different
surfaces of the volume [320]. Showing multiple surfaces transparently also causes
similar issues to traditional rendering, where an object is not entirely in front of or
behind, and another transparent object is not. When accurate results are required
or when dealing with complex structures, DVR is a more practical choice because
it provides additional information that can be filtered in real time [321].


4.1.3      Direct Volume Rendering (DVR)
DVR directly renders the data from the volume and displays it as a 3D image. This
allows for realistic images like those shown in Figure 4.3, which are accurate to the
source with very low preprocessing costs, which consist of reading in the files to the
Graphics Processing Unit (GPU). DVR has the drawback that it is slow to render
with the images in Figure 4.3, taking over 3 minutes to render [322].
    An advantage to DVR is that it provides a very flexible framework for rendering.
It is capable of allowing versions of DVR to focus on making the best image possible
and others to focus on providing smooth interactions on lower-powered devices [323–
326]. DVR has many methods to make it work efficiently on mobile devices, with
several recent studies getting DVR to work on the HoloLens 1 and 2 1 [327, 328].
  1
      https://www.microsoft.com/en-au/hololens




                                          128
4.1.4     Light Simulation Using Direct Volume Rendering
Generally, DVR simulates rays of light that travel through the volume. Mathemati-
cally, moving along a given ray is the same even when working with non-light-based
visualizations, such as determining the direction of wind from meteorological data.
    These photons will traverse times across different distances in each render. When
the photon collides with a value, several outcomes can occur. These can be cate-
gorized as Absorption, Out-Scattering, Emission, and In-Scattering. Absorption
characterizes the radiance of a given field, while emission is a separate radiance field
that calculates the radiance added to the field.
    The collision between the ray or photons triggers light propagation and deter-
mines how much radiance can be seen throughout a volume [329]. Any form of
volume rendering can provide some form of absorption and emission of light. Ab-
sorption and emission methods are regularly used in real-time volume rendering as
they are computationally inexpensive [330]. Scattering describes the phenomenon
where light is deflected in different directions as it interacts with particles within the
medium. Scattering is computationally expensive, requires more computing power,
and can be provided in real time.
    Even only utilizing one of these techniques can allow for some form of display
using volume rendering, and since they can take drastically different amounts of
time. This results in two different mechanisms for using DVRs. One of them is real-
time, where a user interacts with and manipulates the display in real-time and moves
around it. This version of DVR can be utilized using MR devices [327, 328, 331].
However, real-time DVR can tend to lack some realistic qualities. If creating realistic
images is the goal, it is possible to make them, but it may require more processing
time than would be accepted for a real-time application [322].


4.1.5     Cinematic Rendering
DVR, which looks realistic, is also known as Cinematic Rendering and is used to
create realistic images of a volume. To achieve an image close to what is presented in
Figure 4.3, some scattering processes must be considered. The scattering process will
allow other elements to gain the light properties of the nearby objects it has collided
with and those that collide with themselves [330]. This should change the angle at
which the ray moves and consider what rays are being redirected. However, this
will decrease the performance of the algorithm significantly, making it not sufficient
for real-time DVR and not possible for immersive MR HMDs yet [125, 332]. Still, it
may be possible in the near future to use more efficient algorithms [333].




                                           129
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.3: An example of cinematic rendering of a CT scan of a patient with a
sinus frontalis frontal bone fracture from different angles. The following image by
Eid et al. [322] is licensed under a creative-commons licence2

4.1.6     Real Time Direct Volume Rendering
Real-time DVR focuses on maximizing efficiency within tight time constraints, re-
quiring iterative simulation of light for every pixel in each frame. This approach
enables users to interact with data seamlessly, fostering a responsive user experience
and facilitating effective communication. Rendering a single light source using DVR
can be very computationally expensive [332]. This makes images like Figure 4.3,
which utilizes exterior lighting from multiple different sources, difficult to render in
real time. It would require recalling the rays’ trajectory from the light sources to
the camera(s) (viewport). However, it does allow for images like Figure 4.9 to be
viewed and interacted with in real-time [334]. The volumes seen in Figure 4.9 are
possible by skipping processes like scattering and removing exterior lighting from
the algorithm [334].
    Denser regions scatter light more because they contain more particles or materi-
als to interact with the light. For example, This causes areas of a CT scans shown
in Figure 4.3 to render the skull much more clearly than the rest of the head. It
also prevents the skull from becoming more occlusive as it interacts with the light.
Skipping the scattering process can be helpful when trying to create a transparent
volume as scattering will tend to obscure structures that lay under more solid ma-
terials [335, 336]. This can be seen in Figure 4.3 where the dense bone structure of
  2
   https://commons.wikimedia.org/wiki/File:3D_Cinematic_Rendering_
reconstructions\protect\@normalcr\relax_of_the_depressed_frontal_fracture.png


                                          130
the skull is entirely obscuring elements of the image like the brain.

Optimization Methods

Some approaches to DVR cache the final product to memory. This allows for the
fastest possible rendering time. If nothing has changed within a portion of an image
that is being rendered, the ray should not need to be cast again, and that pixel can
stay the same [337]. If the user’s viewpoint is relatively stable, rendering the object
at a lower resolution and gradually raising it piece by piece can also work. However,
these techniques struggle to function with immersive DVR. Research has been done
to get around this by pre-computing many of the angles that a user may look at the
object and caching and then estimating the difference between the viewing angle
and where the user is looking at [338].
    Early ray termination stops the ray before it can no longer change its color [339].
While GPUs will keep using resources until each parallel thread has stopped if all
the threads stop early, allowing for the overall processing time to be reduced. The
ray can either be terminated once it has passed the volume boundary or if it is inside
the volume. Optionally, the ray may also terminate when the pixel becomes wholly
opaque and is not able to change its color anymore [339].
    Other options for improving the speed of DVR will require some level of pre-
processing. This will consume resources when the data is loaded but will make
interacting with the visualization far more responsive. An intuitive method for this
is to use spatial data structures like Oct trees or KD trees to accelerate voxel look-up
times [340]. These can allow for processes that can skip sections of the volumes and
allow for a higher resolution at the cost of using more of the GPU’s memory [340].
    By utilizing spatial data structures, it is possible to skip space within the areas
of the volume that are not empty but contain a known value to the system. This is
called Adaptive Sampling [341]. This value can be found by segmenting the volume
into parts that you wish to visualize together. Which can be processed in real
time [342]. Adaptive sampling can be utilized to provide a similar visualization
to regular volume rendering, but it is not capable of producing the same output
because there is no way to be able to linearly predict what the outcome would what
the color should be for each voxel [324]. Recent research by Kraft et al. has shown
the effectiveness of calculating light effects like scattering with this effect [343].
    A common way of utilizing adaptive sampling is to perform it in areas that are
not being rendered. This is called "empty space skipping" [323, 324, 344]. Empty
space skipping tells the GPU that it has some amount of unrendered or empty space
left before it needs to start rendering [323, 325, 344]. Empty space within a volume
will usually bring no change to the presentation of the given pixel as most DVR
algorithms usually consider all empty space to have the value of zero [324, 326].

                                          131
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING

              Sphere Marching                                                                     Ray Marching
                           Transperent SDF
                                                                             Ray Marching Steps
 Sphere Marching Steps




                 Calculating 

                 Distance to 

                 The Nearest

                 Surface                                    Ray Trajectory                                               Ray Trajectory




                                             Solid Object                                                 Solid Object




Figure 4.4: Two different methods of moving along a ray. Right) shows a rational
volume rendering approach where the ray moves at a constant rate. Left) illustrates
how empty space skipping can be utilized by measuring the distance to the closest
object to determine the safe minimum amount it can move without hitting an edge.


    Some volumetric objects can utilize empty space skipping without requiring data
structures [340]. Sign Distance Fields (SDFs) are equations that represent the dis-
tance away from a shape from a point in space. Empty space skipping using SDFs
utilizes a technique called Sphere Marching, which looks at all of the SDFs in the
volume and determines what surface is the closest to it and moves that far forward as
illustrated in the left side of Figure 4.4. It typically terminates when it approaches
the SDF at a certain distance to ensure the ray meets its target. This allows for
opaque surfaces to be rendered directly.
    All of these methods allow for direct volume rendering, either by sacrificing image
quality and accuracy or requiring more memory and preprocessing time. For real-
time DVR to work in this dissertation, it must be tailored to run at a reasonable
frame rate without losing any visual quality.


4.2               Display Hardware for Stereoscopic Direct Vol-
                  ume Rendering
Stereoscopic displays bring new challenges to volume rendering, requiring different
methods to create them. This section details three systems that utilize DVR on
novel displays that stereoscopic displays bring CT and MRI data instead, with the
goal of better understanding how to best visualize DVR on these displays.


4.2.1                Designing Direct Volume Rendering for Volumetric
                     Displays
Volumetric displays such as the Voxon 3 generate a true 3D image by moving a
screen rapidly up and down. This creates a display that appears very similar to
   3
       https://voxon.co/


                                                                             132
Figure 4.5: An image of a set of bones from a CT scan displayed as an iso-surface
on a volumetric display (the Voxon). This image was provided by Voxon Ltd.


real-world 3D objects that can be viewed from almost any angle. A short project
was undertaken to determine if it was possible to display CT or MRI data using DVR
on a volumetric display. This included trying to visualize small Digital Imaging and
Communications in Medicine (DICOM) files on the device. This project quickly
failed due to limitations caused by this type of display and the required power.
    The lack of occlusion is a challenge with bringing DVR to this technology. This
is difficult due to the lack of occlusion possible on these displays, causing issues
in determining the depth of certain objects [29]. This is compounded by the fact
that these DVR objects require a dense grid of Voxels, requiring a good sense of
depth to distinguish the difference between different elements. The relatively low
refresh rate caused by all of these components makes this difficult, and this can be
more challenging when projecting more than one color. Volumetric displays tend to
require high-speed projectors, and many of these only produce a small amount of
colors and shades due to their low bit rate [345]. If it were possible to transfer a form
of direct volume rendering to this technology, all this would need to be considered.
    This type of display, however, could better be utilized by showing an adaptive
version of an iso-surface-like matching cubes as can be seen in Figure 4.5. The design
allowed for an interactive display that could show the most possible detail to the
end users. While DVR is not well suited to volumetric devices, iso-surfaces are a
viable option.




                                          133
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.6: A series of images showing the looking glass prototype utalizing DVR
(a and b) show pictures of the display looking the same volume from two different
angles (a) is from the left and b is from the right (c) shows the prototype’s interface
(d) shows the same image as seen in sections a and b. (e, f, and g) presents a
collection of users using the system simultaneously.


4.2.2      Autostereoscopic Displays
Auto-stereoscopic displays aim to let users see a 3D image without needing any
special eyewear. This makes auto-stereoscopic displays well-suited to shared expe-
riences. This method could allow patients and a doctor to talk and view a DVR
visualization together, allowing for clearer communication between medical practi-
tioners and patients [346, 347].
    The display used for this project was a Looking Glass 4 , which used a Lenticular-
Based Display. Lenticular Autostereoscopic displays present a series of images or
views that are sliced and interlaced. As the viewer changes their viewing angle,
different images are visible to each eye, creating a 3D effect.
    This project found that autostereoscopic displays are a good opportunity. They
are similar to traditional displays but take advantage of many different views that
humans require. Autostereoscopic technologies have a limited depth plane. Any-
thing in the foreground or background of these technologies would be blurred out
due to the binocular disparities caused by the mismatch between the two displays.
This allows for DVR to benefit from a more natural feeling of depth perception with
  4
      https://lookingglassfactory.com/



                                         134
very little work, as even if all the user can see is the first pane, the difference between
the displays each user can see will still create this stereoscopic effect.
     One challenge to note that DVR has when being used on an Autostereoscopic
display is rendering time. Displays like the Looking Glass may have up to 100 dif-
ferent viewpoints, each of which needs to be rendered in each frame. This can make
interacting with the volume directly a challenge. Early testing of the demonstration
and user testing of these systems show that frame rates over 20 Frames Per Second
(FPS) are tolerable or unnoticeable, and frame rates above 5 FPS can be tolerated.
     Since this system functioned similarly to a traditional desktop display, caching
was utilized to accelerate the frame rate when the DVR visualization was static.
This caching allowed the DVR to step through 256 times, even at a much higher-
than-normal resolution. Early ray termination was utilized once the color could no
longer be changed or when the ray left the Axis Aligned Bounding Box (AABB).
However, the step size was decreased to allow for more steps to allow a higher
resolution from the device.
     The prototype seen in Figure 4.6 was designed as a second step to help us un-
derstand how Autostereoscopic displays should be built for surgeons and medical
practitioners based on a prior investigation with two surgeons who wanted to learn
if a 3D visualization could change the method they prepared for dangerous surgeries.
The original prototype these surgeons interacted with utilized 3D visualization con-
sisting of a stack of 2D planes whose orientation could be viewed from the Axial,
Coronal, or Sagittal planes depending on the angle from which the volume was being
viewed. Most of this system’s design functionalities have been informed by surgeons
themselves. Including the need for DVR to be utilized along with traditional 2D
CT and MRI scans. They also wanted to restrict the volume they were looking into
and control the system either from the 3D perspective for the patient’s well-being
or from the 2D perspective, which they had more experience with and gave them
more precise control.
     As well as utilizing real-time DVR for this project, this system required several
more attributes. This system was required to be able to read any MRI or CT data
and render it as a volume while using the same dataset to power other options. This
prototype could load many different volumes at once and enabled quick switching
between them. It utilizes real-time clipping planes and lets users change some of
the properties of the transfer process (Opacity, Color, Gradient).
     In this state, it was demonstrated at Aus Medtech 2024, where it was shared
with the Australian medical community. From here, marketing and patient com-
munication cases were suggested for the following product. It was also considered
a mechanism for remote communication between medical practitioners and patients
to discuss their medical scans. This would help enable communication between


                                           135
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING

hospitals to collaborate on a given patient with little concern. More research is re-
quired to determine the steps required for its use in diagnosing patients and surgical
assistance.


4.2.3    Stereoscopic Head Mounted Displays
Stereoscopic HMDs encompass any AR or Virtual Reality (VR) HMD, which places
a different screen in each eye and tracks the user’s movement. Allowing the users
to be fully immersed in the virtual environment. Similar to the autostereoscopic
displays in Subsection 4.2.2, the volumes must be rendered at different levels, but
there are due to the screen’s position, some other tactics are required due to the
different methods of interactions possible.
    One difference between producing volume rendering on a HMD rather than a
screen-based device is that you can walk around the volume. They allow for better
depth perception since they allow for binocular depth cues and motion parallax-
based depth cues, which can be triggered even by utilizing the most minor move-
ments. Rendering all these changes to the volume caused by these little movements
makes using a DVR difficult to visualize, causing most people to contain the research
into this space into a cube [309]. Volume rendering is also usually visualized using
a plane, but it requires a 3D object when using a Stereoscopic HMDs.
    Figure 4.7 Illustrates the importance of utilizing the outer polygon shape to
match as closely as possible to the outer shape of the polygon case to aid the
system’s own calculated binocular distortions [122, 124, 198]. Most volumes use a
rectangular prism as their base shape due to its grid-like structure and the compu-
tational advantages it can assume. Cubes also utilize the fewest polygons possible,
which slightly improves rendering performance. Vertex operations only need to be
called 24 for any view, so any eye sees the box. This effect may not be evident when
viewing the volume from a distance, but when close to the visualization, the vol-
ume may appear slightly displaced [60]. Using a cube to display a volume generally
provides acceptable depth on a stereo headset. Some users might experience distor-
tion due to the absence of depth cues that the surrounding environment typically
provides. Additionally, placing the volume within a more closely fitting mesh to its
base shape can enhance near-field interactions, as the amount of white space con-
strains interactions with the volume or close examination on a conventional mesh.
The systems in this thesis employ closer-fitting meshes for Volume Rendering.

Challenges Regarding Frame Rate

Wang et al. [177] state that VR HMDs require a frame rate of 120 fps to avoid
simulator sickness and increase the speed of the user interaction. This is much


                                         136
Figure 4.7: A third person view of Direct Volume Rendering using three different
polygonal meshes. In the green box the outputs from the cameras by looking at
each of the different meshes. The straight lines coming from the camera represent
the camera frustum.


higher than a traditional display would be expected to run at, making it difficult
for DVR applications to run on stereoscopic HMDs. Having two separate displays
that are being shown to users makes the frame rate twice as difficult to manage as it
would generally be. The slower frame rate will cause the user to perceive a blurring
effect [348]. Users move quite regularly and require the visualization to be refreshed
as much as possible [177]. This is a challenging task on OST AR HMDs as many
of the devices are designed to be mobile. Calculating the position of real-world
objects is also a challenge for OST AR HMDs, which requires sensor information
to locate real world elements rather than just being able to rely on RGB camera
imagery [162].

Challenges related to Occular See-Through Head Mounted Devices

Realistic color representations are challenging on different displays, and OST AR
devices further complicate this issue. Unfortunately, each device brings its problems,
such as what color gambit they can display, the contrast they bring to the real world,
how much of the users’ real vision is occluded, and the specific configurations of how
the virtual and the real worlds are viewed [349–351]. Figure 4.8 shows the gradual
color change between a computer display and an OST AR display. The OST AR
display amplifies the brightness of the colors than intended, whereas the VST AR
overlay struggles to choose between the foreground and the background colors. The
observed changes come from several hardware limitations and user preferences on
the mentioned devices.


                                         137
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.8: This figure shows images from a computer monitor, an AR overlaid
image, and the HoloLens2’s display directly, featuring a box with a wireframe version
of X-ray Vision that can cast a shadow over the unseen area. A set of red, blue, and
green lights is directed at this object, showing almost the full-color gambit.


    Figure 4.8 highlights a range of colors that OST displays ca not properly display.
These relate to colors that are considered dull (colors that utilize a lot of brown or
black) as projecting these colors vibrantly is difficult [350, 351]. This makes these
colors appear both dull and translucent [349]. This means that bright and clean
colors are really the only viable solutions for these OST devices.


4.2.4    Implementation Details
Applications of DVR that were utilized in this dissertation utilized this version of
DVR, which can be viewed in Figure 4.9. This version of DVR was designed to be
very efficient but also allowed for multiple colors to be represented using different
ranges of Hounsfield Units (HU) and Tesla (T) values. These colors utilized a look-up
table, which held information relevant to the two different color blending techniques.
    Before running this system, each voxel was split into components either outside
of the main body (outside), inside of the main body (inside), or inside the main body
but touching the outside (surface) by running a volumetric recursive 12-directional
Breadth-First Search (BFS). The distance between all outside and surface bound-
aries was then calculated and stored within the volume. Then, the HU or the T
values were pre-allocated with a known min and max attributed to the DICOM
value before the system applied these values. All values in the volume were then
normalized and sent to the shader via a 3D texture. The color values were used to
store the voxel data, where 1 unit stored the HU or the T values, and another unit
stored the relative distance to the nearest surface voxel.
    The relative distance to the nearest surface was calculated by normalizing the
distance between two of the furthest corners on the volume boundaries and then
normalizing all other distances to the same coordinates. The shader also needed


                                         138
Figure 4.9: This image showcases some examples of what is possible for real-time
volume-rendered graphics working on an immersive MR HMD running at 60 fps on
a GeForce RTX 2700 GPU. a) heptane Gas simulation, b) CT scan of a monkey,
c) Visible Human Male, d) CT scan of an orange, e) Upwind information from
Hurricane Isabel, f) Visible Human Female.


to perform the same calculation to accurately correct this distortion, allowing it to
perform the space-skipping procedure shown in Figure 4.11. By having the relative
distance from any space on the volume, a form of empty space skipping that resem-
bled sphere marching (shown in Figure 4.10) was also utilized by calculating the
distance from the center of a voxel(c) to the center of a surface
                                                                qP
                                                                    voxel(s) allowing
                                                                    n
                                                                    i=1 (ci − si ) This
for large gaps to be quickly traversed [340]. distance(c, s) =                    2

requires preprocessing as all of the distances between the Voxels that are on the
surface (s) of the volume will need to be rendered, resulting in the Voxels acting as
an estimate to the next object, as shown in Figure 4.11.
    The same implementation used for the empty space skipping could have also been
utilized when traversing through the volume by grouping areas of similar Voxels to
allow for adaptive sampling. This could have been done by setting a tolerance value
and grouping all values that were of a similar HU or T value range, which could
have been skipped, and the color value would have been calculated to be identical
as if the system did the process the same. This was not utilized as it created
visual artifacts that made it distinct from regular DVR, potentially causing it to
hide information [324, 340]. Lowering the algorithm’s tolerance to react to smaller
changes could have increased its accuracy, but due to the high variability of the
data, it would have slowed down more than it was initially.


                                         139
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.10: A diagram of how ray marching works regarding this implementation
of SDFs. The dotted blue line circles are the ray logic, moving forward until the
nearest part of the ray enters the volume. The green dots (Step Positions) represent
each step the volume reads to produce the value for a single pixel. While outside
of the SDF, sphere marching was utilized whenever outside of the object to reduce
the number of steps required. If the ray is close to the object or inside of it, the
algorithm becomes ray marching, where we move forward by a specified amount for
each interaction.


    Two options for visualizing the colors: either it applied a flat range to a range of
HU to isolate different parts of the volume, or it performed Interpolation between
the different ranges to better highlight the different parts of the volume. The range
base would color space within a range of two HU. This allowed the range-based
method to ignore certain values within the volume and ignore areas like the bones.
The other option utilized linear interpolation between points. Each point would
correlate to its own color, and Voxels with that color would be the most colored
in their given value. Each other color would change gradually and appear closer to
its harboring color as the values grew closer to it. The linear interpolation volume
transformation sets two capstone values less than the minimum, which are black but
fully transparent, and any volume above the maximum, which is transparent and
clear.
    Getting DVR to run on stereoscopic OST AR devices requires displaying the
visualization on two displays and updating them at least 30 times a second. Most
OST HMDs are not able to run even the most efficient forms of DVR in real time
on these displays, as the ones used for this thesis utilized lower-powered mobile
hardware [327]. It is for this reason that this version of DVR is designed to run on
desktop hardware weathered to an OST AR HMD.
    To lower the processing time, the version of DVR scattering and other light
effects were removed from the algorithm. Early ray termination was activated when


                                          140
                                                               Ray Marching is applied
      Ray Marching Steps                                       when steps get too small




          Pixels are colored
         darker depending on
       their distances from the
            nearest object
                                                                       Ray Trajectory


                                           Solid Object


Figure 4.11: A 2D example of empty space skipping using a grid-based approach.
The distance that each moved forward would be equal to the distance to the nearest
object. It then moves that much further each time until the distance to the next
voxel is beneath the given threshold, where the ray will then move at the given
distance of that threshold.


the color had reached a point where it was not able to change from more steps, the
ray leaves the AABB, or if the ray required over 126 steps to complete. There was
no system to cache the rendering, as even slight movements from the headsets would
result in a different view of the volume.
    The version of DVR created for this system is designed to take advantage of tech-
niques that do not decrease the visualization quality in an immersive environment,
so elements like space skipping and early ray termination were kept. This selection
of parameters that controlled the quality of the visualization was tuned to produce
an accurate image that can be visualized and modified in real-time on a range of
different devices while still providing the best system results.


4.3      X-ray Vision Techniques for Direct Volume
         Rendering on Ocular See Through Augmented
         Reality Devices
OST AR allows users to see the real world directly with virtual content overlaid onto
it. However, in an OST, these virtual images can be washed out by light from the
real world, so X-ray Vision cues are not as strong in an OST AR device compared
to Video See Through (VST) AR [36]. Chapter 3 has shown that four issues need
to be considered when creating X-ray Vision visualizations for OST AR HMDs:

  • The X-ray Vision effect will not be affected by the user’s view of the real world
    but rather by static visualizations. In Chapter 3 participants found both the

                                        141
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING

      camera-based X-ray Vision (edge and saliency) effects to be more difficult to
      use, and they performed worse when using them.

  • The X-ray Vision effect still needs to occlude the interior objects partially,
    but not the users’ view inside of the object since this allows for additional
    depth cues like relative size and distance. The reference objects in Chapter 3
    proved to be beneficial to X-ray Vision as they help users locate objects nearby
    in relation to the interior of each object. This is likely to be very important
    in volumes as surfaces can be difficult to discern. This effect is made more
    difficult when you consider all the different surfaces that can exist in volume.

  • The X-ray Vision effect should also not use large objects as it is designed to
    be used up close to allow near-field interactions. Participants mentioned in
    Appendix E both Saliency and Random Dot occluded too much of the interior,
    causing participants to have issues interacting with the visualization when they
    were up close.

  • The effects would need to be reactive to the user and efficient enough to not
    impair the visualization’s quality. The None condition in Chapter 3 performed
    well in the subjective results. Participants noted this was due to it not occluding
    their vision in Appendix E. To compensate for this, the X-ray Visualizations
    in this chapter will only be observable within the peripheral vision of the par-
    ticipant.

Considering these, it is also important to focus on what could and could not be done
using volumetric rendering.
    The volumes know about their internal geometry, but they cannot adjust to the
real-world external shell. If the quality of the visualization is poor, the rays can also
step over the object’s surface, depending on the angle being viewed. This will result
in the visualizations not utilizing the device’s built-in binocular distortion and the
rest.
    It is also important to consider transparency’s impact on the interplay between
DVR X-ray Visualizations. Studies have shown that transparency can be detri-
mental to the user’s depth perception when done poorly, but it can be an effective
depth cue when done well [69, 252, 352]. Transperent DVR allows focus on im-
portant data while ensuring the overall information configuration remains easy to
understand [151]. The X-ray Visualizations presented in this section all ensure an
interplay between transparent and opaque visualizations by making the illustrative
effects opaque against a transparent volume. Using occlusion to represent a clear
foreground.


                                          142
Figure 4.12: Artistic images of anatomical images displayed as an X-ray Visu-
alization by Dr Joshua Luke Ameliorate. These images are titled: left) Hypnotic;
right) Infinite. They can be found at 5 . Used with permission from Dr. Ameliorate


    Due to the noticeable latency, performing a single rendering pass with no pro-
cessing stage was only possible [203, 294]. This means that a depth buffer could
not be calculated in time for these effects, which restricts the visualizations to only
understanding what information is related to the current voxel’s details.
    DVR is not as well designed for running on a GPU as traditional polygonal
assets are. To enable this, methods that can be calculated in advance and utilize
information from the texture memory have been prioritized. The memory is designed
to use the RGB values of the texture as normal values when they correspond to
edges, while the HU or magnetic radiance value is stored as part of the texture
information. Distance values are calculated only for the exterior surface, allowing
them to be pre-calculated with minimal computational overhead.
    Given all these considerations, illustrative effects would make the most sense as a
type of volumetric method because it allows for the appearance of transparency while
using occlusive objects [73], and it can be applied to volumetric objects [80]. Medical
journals commonly use hatching [74] and have commonly been used to translate
volumetric data to a 2D interface [353]. There are also methods of producing these
in monoscopic screens. However, if done in real-time, these should be able to provide
a similar effect to a X-ray Visualization.




                                         143
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.13: Examples of hand-drawn illustrations using the illustrative techniques
that inspired the various VIRTs as they would be depicted within art.


4.4      VIRT: Volumetric Illustrative Rendering Tech-
         niques
Throughout history, art has given depth to 2D surfaces; these works have simulated
transparency and manipulated physics. Inspiring this research to explore visualiza-
tions that covered all prior concerns was to utilize illustrative effects. Throughout
art, education, and pop culture, hatching and stippling have been used to discern
surface curvature and surface texture while still allowing the viewer to look inside.
Figure 4.12 illustrates the added benefit that these techniques allow you to see
through the surface of a girl and directly into her anatomy.
    The illustrations seen in Figure 4.12 present the effect of looking through a solid
matter using occlusive and semi-occlusive illustrative techniques by using partial
occlusion, just like the visualizations seen in Chapter 3. This shows that illustrative
effects utilize the same effects to present the effect of looking through objects as the
X-ray Visualizations seen in Chapter 3. Similar to tessellation and random dots,
they remain static in the real world. They can also be designed to allow for a
gradient of transparency, occluding less of the area where the user is looking and
more of the area where they are not.
    Utilizing effects that are traditionally designed to be seen in 2D should also
be more suitable for a DVR solution like direct volume rendering. Given the 3D
nature of many MR displays, this is not guaranteed, and the experiments in the
following chapters will investigate users’ preferences for these effects when used as
a visualization.


4.4.1     X-ray Vision of Empirical Volumes
We will be showcasing these techniques using visualizations of MRI and CT data.
The data is preprocessed to calculate the areas defined using a tetrahedra-defining
volume where surface normals would be expected and then calculate for all exterior
Voxels the distance away from the volume they were to allow for sphere marching.
By preprocessing the volume’s surface and all of the Voxels outside of this space,


                                          144
Figure 4.14: The Halo VIRT applied to the Visible Female data set overlaid over
the 3D printed dataset to provide an X-ray Vision effect. This image was taken
using the HoloLens2.


representing the skin, it is possible to determine the ray’s relation to the surface.
This shows an effect slightly over the top of the parts of the surface that are visible
by the final volume.


4.4.2     Halo
Halos’s outlines and feature lines have been used to improve depth perception [95,
354] and highlight areas of interest within volumes [355–357]. On its own, a halo
does not provide much depth perception, but when it is paired with a colored display,
then it can distinguish what is in front and behind clearer than transparent objects
can [356]. Halos have also been shown to illustrate where the surface of an object
is by highlighting the areas of high curvature [95]. Halos are also useful to give the
viewer a clear indication of borders, allowing them to see what is in front or behind
a given object. Allowing Halos to highlight the objects in the scene [95, 357–359].
Halo can also use these clear indications of borders to provide a clearer indication
of relative size and density, which was found to be a very powerful depth cue in
Chapter 3.

Implementation

Traditionally, the halo effect has been utilized using two graphics passes [95]. The
first is to build a volumetric depth map, and the second is to render the volume

                                         145
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING

where needed. On an OST AR display, this is problematic as minor delays in latency
are noticeable, so rather than rendering two passes of the volume, this dissertation
presents a system that could draw the halos without a depth map.
    To help prevent this latency, an alternate shader was utilized that recorded two
colors for the ray, one that would have the previous halo the ray intersected with
and one that did not. This allowed the system to only show the final halo that
was passed through. A halo would only be drawn when these rays got close to the
end and hit the surface at almost a 90°angle. This would be saved to the final
output color, but if it were to enter the same object again or become a halo for
another object, it would revert to the non-halo version of the output color. For the
purpose of visibility, all the halo’s However, it does have one caveat: it is possible
for it to show outlines on the outer areas of the foreground with a very high level of
curvature, presenting more information about the shape of the object but occluding
the volume very slightly.
    If the ray were to enter the same object it was nearby, the halo generated would
be discarded; if the ray entered another shape or came into the range of another halo,
it would be saved. Halos would be visualized when the dot product of the normal and
the ray direction was between -0.1 and 0.1, about 90 degrees away from the user’s
line of sight. This did result in a bug/feature that can be seen in Figure 4.14’s Halo
image where the curvature of the outer surfaces may show outlines in the foreground
around areas where there was a high curvature on the surface causing the Halo’s
color to be displayed on sharper contours of the volume facing the user.
    A Halo would be produced when the dot product of the normal and the ray
direction was between -0.1 and 0.1, about 90 degrees away from the user’s line of
sight. A white Halo would be drawn if the direct distance to the SDF was less
than the given Halo tolerance. All these variables were chosen to limit the number
of floating point artifacts that could appear while keeping the size of the lines as
uniform with the rest of the visualizations as possible.


4.4.3     Stippling
Stippling can be utilized to show the features within an image [360] and depth
by using several small dots and is capable of giving the artist a higher level of
control over showing the surface range than depth perception. Figure 4.13 shows
how Stippling can show curvature, surface depth, and different textures by changing
the size, frequency, and pattern of the stipples [361]. Using this effect to aid in the
perception of 3D data is not common [73], but it was chosen due to its similar depth
perception properties as Otoski et al.’s [209] Random Dot X-ray Vision effect [91,
362].


                                         146
    Stippling has not been as extensively tested for depth perception as any of the
other VIRTs [73]. It is more often used to demonstrate rougher surfaces to detail
intricate details and decrease distortions when viewing iso-surfaces [73, 363]. Stip-
pling allows the user to be able to see a series of textures that are laid over each
other due to the high range of control it gives artists but also gives them the ability
to show the roughness of the surfaces it is displayed upon [73, 91, 363].
    Stippling is used in graphics to showcase transparency [85], it is more frequently
used to show details in textures much more precise than can be seen by shading
alone [364]. In these cases, it would normally be referred to as dithering as it is used
in a much less random manner [365]. The particle occlusion formed by stippling
then clearly indicates what object is in front of another surface, avoiding screen
door effects, unlike what the effect of rendering multiple transparent polygons may
have on each other. This makes stippling a logical choice for an X-ray Visualization.

Implementation

This design of this effect took inspiration from two works by Lu et al. [84,86] focused
on a GPU version of stippling and Ma et al. [88], who developed a pre-calculated
version of stippling that could be used for dynamic environments. We also took
inspiration from the work by Kim et al. [360], who utilized different-sized dots to
portray curvature and importance within a 2D image.
    The stipple effect was applied over a grid using a converged distribution (or blue
noise) [366] to give the dots an evenly placed tone that was not too regular [364].
This was chosen over a feature-guided method as it would have required multiple
rendering passes or generating a 3D texture over the volume [360]. The blue noise
algorithm based on Heitz’s and Neyret’s [367] was used to determine where a dot
would be rendered over the shape’s surface because it was efficient and well suited for
GPU architecture and was more visually uniform than traditional white noise [365].
The final stippling design can be seen in Figure 4.15.
    Two major factors impacted the design: floating point errors and data legibility.
Made it impossible to render as many dots as those created by Lu et al. [84, 86]
and Ma et al. [88] due to a large number of floating point errors that seemed to be
caused by rapid movements and stereopsis. To overcome this drawback, a graphics
card with single or double-precision floating-point calculations would be required
rather than the half-precision that could be utilized. As a result, the approach of
"less is more" was adopted when rendering the dots while still keeping to guidelines
proposed by Lu et al. [84, 86], like having more dots when on parts of the volume’s
surface with high curvature. All the dots were housed in a grid whose sizes range
between 963 and 483 units 6 , ensuring that the outside of the grid was clear and easy
  6
      Real world sizes could vary depending on the volumes but were approximately 303 cm in this

                                               147
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.15: The Stippling VIRT applied to the Visible Female data set overlaid
over the 3D printed dataset to provide an X-ray Vision effect. This image was taken
using the HoloLens2.


to view while the inside was opaque with little to no floating point errors.
    All of the dots were placed randomly within the grid. to ensure that the dots
did not overlap with other dots To create the dots, first, a center point was picked
where the outer edges of the dot would not hit the edge of the cell. The dots
were scaled based on their opacity, as the ones closest to the user would be made
transparent, while the ones that were rendered further away or not facing the user
were displayed as more open. This technique was also utilized in the next section
(Subsection 4.4.4).When the surface curvature increases, the amount of stipples that
appear will decrease, similar to works by Lu et al. [84, 86]. This allows the dots to
better represent shade and intensity over the volume [364].


4.4.4        Hatching
Computer-generated hatching was originally formed by trying to make volumes or
images look like they have an artistically drawn look to them [79,368]. In an artistic
context, hatching is considered a good method of showcasing transparency [369].
Different objects will utilize different patterns and densities of the hatching [80].
diseration


                                         148
This allows for a large amount of flexibility and allows for more complex surfaces to
be hatched.
    Figure 4.13 shows how Hatching is similar in utility to stippling; it can show
differences between curvature position and lighting [361, 370]. It can be used to
great effect to showcase curvature within using 2D mediums like books [74] and
desktop applications [79, 361, 370], but there are very few examples of hatching
being utilized in AR. Current examples only show it being used as a medium to
help create a painterly atmosphere within mobile AR [371, 372].
    In an artistic context, hatching is considered a suitable method of showcasing
transparency, which makes it well suited as an X-ray Vision effect [369]. This is
achieved by having the hatching of different objects in the hierarchy display a dif-
ferent width, line angle, and density of the hatching [80, 373].

Implementation

Rather than try to make an artistically pleasing hatching algorithm, the implemen-
tation of hatching in this dissertation aims to create a version of hatching that is
both simple to implement and understand and functional within MR environments
for volumetric datasets. This method took advantage of DVR and rendered a simi-
lar implementation of hatching to work by Interrante et al. [76]. To do this, a grid
of rectangular prims as SDF and rendered the area that would not be where the
visualization could be presented, each with a deformed cube in the center to apply
the hatching effect, creating the line effect seen in Figure 4.16.
    The hatching effect was generated by creating a grid where every cell was 1.2%
of the size of the volume and had a box at the center of that which was 2.4%
of the volume. This visualization will be shaded lighter when facing the user’s
direction. This is done by altering the transparency by the angle between the surface
visualization and the user. This was done by tracking the dot product, which was
more transparent when the dot product grew lower. This caused the hatching effect
to disappear when it was not at the point it was closest to the user. Both of
these parameters decrease when representing smaller objects than the object it was
representing, causing the hatching effect to appear as a denser grid. After the dot
product was greater than 0.25 of the dot product, the hatching effect would incur a
tapering effect.
    How to orientate the hatching quickly became an issue. Generally, hatching is
designed to create a static image, so the lighting and shading would have been seen
in a static place, with the light source pre-calculated before each image. Instead, this
version of hatching treated the user’s viewpoint as if it were a light source and made
the effect from their point of view. This made the visualization react to the users’
behaviours and allowed them to have their view partially obscured while allowing

                                          149
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




Figure 4.16: The Hatching VIRT applied to the Visible Female data set overlaid
over the 3D printed dataset to provide an X-ray Vision effect. This image was taken
using the HoloLens2.


them to look inside the volume.
    Several aspects were considered to maintain the aesthetic elements required for
hatching when transitioning from a 2D to a 3D volume. The hatching effect utilized
an SDF of a deformable square grid with infinite depth on the z-axis. This grid was
deformed based on the relationship between volume and the viewer and to have a
larger, more opaque line, the further away from the user the normal was facing from
the viewer.


4.5      Conclusion
To create X-ray Visualizations for OST AR, an efficient method for rendering vol-
umes directly to the screen using DVR was developed. This method was successfully
demonstrated on autostereoscopic displays and provided additional rendering time
to generate three sketch-based X-ray Visions effects, collectively referred to as VIRT.
By leveraging sketch-based visualizations, observations were made that allow for the
configuration of these conditions for OST AR devices. Moving forward, this disser-
tation will further explore the utility of these X-ray Visualizations.
    The contributions of this chapter were:

                                         150
• An algorithm displaying how to run DVR on a stereoscopic display with min-
  imal distortions and low graphics requirements.

• A system designed using a stereoscopic display was to render DICOM using
  DVR while providing a reactive user interface, which allowed users to modify
  the volumes as they required.

• Three new X-ray Visualizations, the VIRTs, have been created to allow for
  X-ray Vision on volumes rendered in OST AR devices which adapt to user
  behaviours rather than using solely occlusion.




                                   151
   4. DESIGNING X-RAY VISUALIZATIONS WITH VOLUME
RENDERING




                        152
Chapter 5

Random Volume Generator:
Generating Irregular Hierarchical
Objects for Controlled User
Studies

Finding volumetric data that can be utilized reliably for quantitative experiments
is challenging. Methods to obtain volume data are limited. Two methods of creat-
ing volume data can occur by using natural means either: Patient data or artificial
data. Patient data is generally collected from a spread of patients with similar
symptoms [374] or collected data from relatively healthy patients [375]. The other
method of producing volumetric data is to collect artificial data is collected by uti-
lizing geometrical data to represent various natural phenomenon [376–378]. These
tend to take the form of calculating known an empirically assessed theoretical phys-
ical concept, which in turn can be computationally expensive to produce and to
manipulate.
    Controlled studies are challenging to perform with data that is limited in these
settings because they rely on having hundreds to thousands sets of similar data
based on a given condition. The data needed to be plentiful enough that conditions
could be replicated with different data sets over various conditions and conform to
the properties of MRI and CT scans while remaining intuitive enough for untrained
users to be able to interoperate it. This section details the methods focused on
creating extensible and flexible volumetric data that participants can understand
intuitively, regardless of their prior training.
    Volumetric data is commonly used to interpret the structure of an object, like
the human body, underground structures, or weather data. Typically, the informa-
tion utilized when requiring direct volume rendering is sourced from the real world


                                         153
      5. RANDOM VOLUME GENERATOR: GENERATING
IRREGULAR HIERARCHICAL OBJECTS FOR CONTROLLED
USER STUDIES
using either CT, MRI, or some form of radar technology. These methods of obtain-
ing volumetric data can be limited and challenging to produce in large quantities.
Access to the machines and equipment capable of producing this data is limited,
and the skills required make it difficult to automate. Most medical data needs to be
anonymized, and the patient authorizes the use of it so it can be used for research.
Furthermore, very little volumetric data is explicitly designed for more controlled or
quantized studies.
    The Open Scientific Visualization Database 1 has a wide selection of datasets
available for science, but they are very different and can result in unexpected
differences between the different visualizations when evaluated using a controlled
study [133]. Current solutions for generating artificial data require precise inputs,
are challenging to manage, and are computationally expensive [379–382]. This sec-
tion details a method of making simple-to-understand simulated volumetric data, a
method to validate that the data is comprehendible, and a modular system allowing
for flexible options to evaluate volumetric structures.
    Real-world volumetric data is challenging to share amongst other researchers due
to its large file size and privacy concerns [383]. Medical data privacy is paramount,
and there may be restrictions on what datasets can be distributed through re-
search [384, 385]. The size of the compressed data can easily be larger than one
gigabyte for a given volume. Making sharing datasets difficult [383]. This size be-
comes an issue for data sharing in the long term since storing large data sets will
likely cost either the publisher or the researchers [385], leading to many problems
with long-term data availability.
    Current alternatives for creating controlled studies using volumetric rendering
include adding extra information to the limited volumes via adding artificial data.
These settings do well in recreating scenarios relating to the introduction of medical
instruments to a volume and determining the reliability of these visualizations, but
the interactions between other objects become more complex. Another option can
be seen by viewing research that utilized synthetically created data [122, 135, 136].
    Synthetically created data has a couple of advantages. First, it allows you to
modify and control the data you are using in a given study, allowing for more
quantitative data to answer more specific questions. Allowing for more replicable
and focused studies to be possible. Englund et al. [135, 136] experiments generated
data designed especially for the purposes of testing their research and rendered as
static images of volumes for a range of two alternative forced-choice tasks. Kersten
et al. [122] utilized a fog-like volume made from Perlin noise within a cylindrical
object and rotated it to see if users could tell the direction in which it was rotating.
    Creating synthetic data is a common tactic in many other fields as data of any
  1
      https://klacansky.com/open-scivis-datasets


                                          154
type are hard to come by, and systems need to be tested using flexible mechanics
and a wide range of parameters [381]. These can range from systems designed to
create structured databases [381], networking data [379, 380], or even modifying the
original data for testing or publishing purposes [382].
    There have been several attempts at using the tools Generative Adversarial Nets
(GANs) [386] and stable diffusion [387] to create real examples of medical data [388–
391]. These datasets provide use cases for training staff without the need to provide
scans of real people, which can benefit tasks such as publishing and sharing medical
data with a broader audience [390], training radiologists [391], and research [389,
391]. These models utilize a large collection of existing data and aim to create a new
data set, which tends to be indistinguishable from the real data [389,390]. However,
controlling the specific qualities in a manner that would be required for a study is
not practical yet, and adjustments to this data for quantitative evaluations are not
possible yet.
    To solve these issues, the Random Volume Generator was created. Taking into
consideration the above parameters, I built a system to generate volumetric data
that can be generated for studies in a unique but controllable mannor. This software
is designed to act as a tool that can be used to generate a unique volume for each
interaction of your study that can be customised to fit any type of research that you
need to run. By utilizing SDFs set in a hierarchy of customizable options. Keeping
the system modular enabled it to swap parts in and out, making it work for any
study looking to evaluate volumetric graphics.


5.1     Noisy Hierarchical Spheres
To make these volumes as much like MRI and CT scans as possible, Perlin noise [123]
was used to deform a sphere object created via a spherical SDF (A noisy sphere) set
up in a hierarchical fashion. Perlin noise [123] is a close match for the noise found
in medical data, and it has been utilized to simulate and create artifacts in read
medical data for research purposes [392, 393]. This results in volumes that appear
like the ones in Figure 5.1. Since they are SDF, it is possible to accurately determine
the angle of normals in the volume, allowing visualization-based experiments like
X-ray Vision studies (as seen in Figure 5.4).
    We also considered the previous studies that have generated artificial volumes.
Englund et al.’s [135, 136] 2D static images would not be translated well to a stereo-
scopic display as they can not respond to motion parallax. Kersten et al.’s [122]
Perlin noise fog showed promise when using stereoscopic display. Perlin noise was
adopted to generate synthetic volumes, mimicking the noisy characteristics of volume-
rendered objects in applications such as MRI and CT scans. This approach is ad-

                                         155
      5. RANDOM VOLUME GENERATOR: GENERATING
IRREGULAR HIERARCHICAL OBJECTS FOR CONTROLLED
USER STUDIES
vantageous since it can be rendered effectively on a variety of surfaces in real time.
Volume data tends to be inherently noisy and messy, making it more challenging
to interpret, so Perlin noise-based objects make logical sense [120, 384]. The ability
to create a solid surface that could represent different materials or densities would
be beneficial when trying to create iso-surfaces, and the ability to have identifiable
pieces components is common in MRI and CT scans.


5.2     Random Volume Generator’s System Design
The Random Volume Generator took on a complex but modular design system in
Appendix C. Which is designed to produce an array of volumes that can conform
to a set of conditions. This results in the output, which can be seen in Figure 5.1,
whose shape is a "noisy sphere." [123].
    Noisy spheres, which are based on a sphere with random distortions, allow for
an almost infinite variety of shapes. As such, each shape is irregular in nature, and
a pseudo-random generation process determines the specific shape and appearance.
All while forcing the dataset to conform to a preconfigured set of requirements.
    The Random Volume Generator is primarily aimed at researchers conducting
human-computer interaction research investigating the traits and impacts of differ-
ent displays and rendering styles [94, 394]. The volumes generated by the Random
Volume Generator need to have generic qualities that other forms of volumes would
typically have. This might include rounded edges, lumps and bumps around the
edges, and a hierarchical set of objects. We chose to base these volumes on a similar
design to MRI and CT scans, but they also share qualities with electron microscopy
visualizations [118] and with minor modifications, they can be conformed to molec-
ular systems [2], meteorological data [5, 395], as well as ground penetrating radar
(GPR) data [120]. All these imaging techniques communicate information via geo-
metrical methods, require a high degree of precision to generate, and are subjective
to noise or inaccurate data.
    In each iteration of the generation process (shown in Figure 5.2), a top-level
volume is created and added to the scene. Children within the volume are created
recursively. The size and appearance of each volume are determined based on the
probability distributions given as input to the generation process, whereby the size
of the child volumes is constrained by their parent volume (if any). The number
of children in the volume is drawn from a probability distribution for each volume.
In addition, global constraints govern the overall number of objects of each type at
each level to ensure that the synthesized scene satisfies the desired properties.
    The dimensions and placement of objects are determined at random such that
each volume is wholly contained in its enclosing parent volume, and no objects are

                                         156
Figure 5.1: The types of volumes the Random Volume Generation System system
can produce. There are noisy spheres on the bottom and right-hand sides using
various illustrative effects (Outlines, Stippling, and Hatching).


allowed to touch each other. These placement constraints are verified by a voxel-
based algorithm that examines possible intersections between volumes. The system
provides a naive algorithm and a more efficient octree-based implementation. In
case an object is found to violate the constraints, the object is moved to repair the
situation.


5.2.1     System Design
The Random Volume Generator was designed to be modular by utilizing many
system design patterns, interfaces to allow for interchangeable classes, and the use
of functors to allow for customizable behaviours. The system can be split into
different parts, enabling adaptation to different requirements. The different parts
of the Random Volume Generator are classified as the Generator builder (used to
construct the final structure of the Generation system), the Random Volume Builder
and Validation (used to create and validate the success of the valid creation of a valid
volume), and the system Outputs (used to tailor the outputs to the various studies
that these volumes could be used for). Class diagrams showcasing the Random
Volume Generator’s structure can be found in the Chapter C.
    Four different types of noisy spheres can be made for the base version: outer,

                                          157
      5. RANDOM VOLUME GENERATOR: GENERATING
IRREGULAR HIERARCHICAL OBJECTS FOR CONTROLLED
USER STUDIES
composite, leaf and some multipurpose spheres can also be added. To improve
optical focus, using the method described in Chapter 4 where volumes were housed
in meshes that represented the exterior of their shape to some degree, these objects
are housed within a spherical mesh. Improving the distortion from the bi-optical,
allowing for better depth perception [122].
    The Unity game engine 2 limits the amount of threads available to the Random
Volume Generator is required to work within a single thread framework and uti-
lizes active interfaces in tandem with a stateful logic system. Systems that handle
the system’s logic are functors, allowing a system to progress statefully. Their be-
haviour is again modular, with the system’s logic separated from their unique tasks.
Allowing for a system capable of doing much more than its base functionality. Two
simple examples of the methods are recursively placing an SDF within its parents
so it takes as much space as possible. The other can produce several meshes for
3D printing(seen in Figure 5.3) while also providing their corresponding noise key,
allowing for comparisons with real-world objects.


5.2.2      Voxel-Based Verification of Volume Requirements
The Random Volume Generator utilized a modular verification system to ensure the
volume elements remained distinct to prevent any ambiguity between the objects.
By looking at each voxel and determining if they are sitting wholly inside each other
and not slipping outside their parent object or touching any other object that is not
a parent of theirs. The rules are checked via a C# based system that emulates the
same properties of Unity’s high-level shader language (HLSL). It is designed to be
extensible to allow for any SDF and potentially a different type of shader altogether.
    The verification methods for the root (outer) objects check that the volume fits
within the mesh within which it is rendering while remaining as large as possible.
This mesh can either be a cube mesh or a spherical mesh. If any SDF is outside of
the mesh, the object is discarded, and a completely new object is generated. The
sphere mesh utilized a Fibonacci sphere algorithm to determine if the large outer
sphere was within bounds [396] whereas others, like the cube meshes, tested if the
mesh was inside of AABB. In both conditions, if any point from any of these checks
were found to have any SDFs within a range of them, it would then shrink the
volume and perform the check again.
    The verification used for the leaf and composite objects can use two different
verification methods. Either a brute force or progressive oct-tree searching method.
This can either happen linearly or in parallel for each voxel. The other method
utilizes a partial linear oct-tree starting from one predefined depth in the oct-tree
  2
      https://unity.com/releases/2022-lts



                                            158
                                    Add Outer
                                    Objects To
                                      Array




                                   Verify Outer
                                     Objects



                                                                         Add Child
                    Shrink Outer                                       Objects Current
                     Volumes                                            Within Shape
                                                                         and Array




                                                                           Verify
                                                        Set Up Next
                                                                          Current
                                                          Shape
                                                                          Shape

            Set Up The                   Save SDF
               Next                      Volume To
            Participant                     File




Figure 5.2: An activity diagram showing the transition between the various states
of the Random Volume Generation System


from one depth to another deeper one. Studies in this thesis utilized an oct tree
starting with one layer beneath its root 1/83 to a leaf node that was of a voxel with
the comparable size of 1/5123 . This will perform a trimmed depth-first search. This
search will determine if a group of Voxels conform to SDF coordinates to ensure
that all child nodes are completely within the parent’s bounds. These brute force
and progressive oct-tree can be combined for both high perception and speed.

Random Volume Generation System Outputs

The output from these files can provide mesh file outputs (.obj and .stl) or show
DVR content in a JSON format. The meshes are created via marching, running the
iso-surface algorithm marching cubes [319] over the volume and can be created at
any resolution required. The volume can be saved as a single mesh or a collection of
smaller meshes (as shown in Figure 5.3) spaced appropriately apart allowing game
engines and most displays to render them natively. It can also create a 3D printable
model (as seen in Figure 5.4).
    The JSON file is designed to be read as input for a user study. It contains
instructions on what condition the visualization was built for, the noise key, and
any answers required for each volume, like volumetric information and the number
of volumes contained under a certain circumstance.



                                                  159
   5. RANDOM VOLUME GENERATOR: GENERATING
IRREGULAR HIERARCHICAL OBJECTS FOR CONTROLLED
USER STUDIES




Figure 5.3: A breakdown of the structures of the 3D meshes generated by the
Random Volume Generator. a) presents the volume using transparent colors to
show the different levels that meshes can be segmented, b) is the same volume but
covered in a wireframe, c) is a close-up shot of the center b.


5.2.3      Immersive User Interface Design
The Random Volume Generator’s immersive design component is where the volumes
are randomly generated while the user can change a set of parameters with the goal
of creating an input file for the final system. By immersing researchers in the same
environment as their users, they can experience the data set the same way their
participants would while choosing parameters for various instances of the project.
    The User Interface (UI) elements have been generated from the Mixed Reality
Tool Kit (MRTK) 3 and have been laid out to keep the users focused on the vol-
ume. These elements include changing two list menus between a predefined set of
conditions (this dissertation utilized VIRTs) and updating input parameters (shown
up the top and to the right of Figure 5.5). All areas that can be modified via the
selected parameters will be highlighted unless the "Custom Visualizations" option is
enabled, showing the set of pre-computed visualizations that have been developed
for the system. Most of the input parameters users can be changed via to sliders
(seen on the right side of Figure 5.5) that will allow them to choose between various
  3
      https://github.com/microsoft/MixedRealityToolkit-Unity


                                         160
Figure 5.4: (a) A 3D printed version of the model made from the Random Volume
Generator’s mesh output. (b and c) shows this model from the view of a Microsoft
Hololens2 to create an X-ray Vision effect using illustrative rendering.


sizes and amounts of objects that they want to exist at a time.
    Since this interface exists in 3D, it is important to create a color interface that
the interface suits this purpose. Unlike many color pickers used in VR, which focus
on a 2D input [397], it was believed that this interface would benefit from giving the
participants the ability to adjust the color with depth as of field as well. Requiring
a volumetric color picker (shown down the bottom of Figure 5.5). The colors for the
elements of the volume can be chosen from a 3D color picker in a similar style to
Kim et al.’s [154] color pickers designed for color blending.
    The Random Volume Generator is available on GitHub. The Random Volume
Generation system 4 is built as a modular system built in the Unity game engine 5 .


5.3        Conclusion
The Random Volume Generation System provides a method for generating volumes
designed for different types of user studies that look into volumetric research in the
3D space. It provides a solution to the limited number of volumetric datasets that
  4
      https://github.com/tomishninja/RandomlyGeneratingVolumes
  5
      https://unity.com/releases/editor/archive



                                         161
   5. RANDOM VOLUME GENERATOR: GENERATING
IRREGULAR HIERARCHICAL OBJECTS FOR CONTROLLED
USER STUDIES




     Figure 5.5: The HoloLens UI of the random volume generation system.


are available publicly and allows for more accurate results from controlled studies.
This now lays the groundwork for future research to determine what VIRTs the
effects as X-ray Visualizations have when used in tandem with DVR utilizing a set
of controls. Moving forward, the goal is to test the perception and depth perception
qualities of these visualizations on the general population by using these volumes
with a series of tests designed to test the functionality of the VIRTs.
    The Random Volume Generator provides researchers with:

  • A method for providing to recreate reproducible and a near-infinite amount
    of distinct volumes that can be easily transferred and stored for controlled
    studies.

  • An AR/VR interface to aid with the planning of these volumes.

  • Open source access to the system and a guide to the modular components,
    which can be tailored for use with other studies.




                                        162
Chapter 6

Perception of Volumetric
Illustrative Effects Visualizations
within OST AR

This chapter evaluates the VIRTs presented in Chapter 4 to see if a measurable
improvement can be observed when identifying individual graphical objects in a
mixed reality environment. This is critical for identifying foreign masses within
the human body, allowing more rapid and accurate diagnosis and determining how
compelling these visualizations would be for a given surgery. By evaluating how
participants comprehend the space and how they could categorize foreign masses,
this research aims to determine if VIRTs supports people’s ability to understand
data parameters.
    Illustrative rendering techniques enhance viewers’ understanding of precise vol-
ume boundaries, facilitating the identification of relationships between objects. Prac-
tical applications of these techniques are evident in scientific illustrations, partic-
ularly in fields such as entomology and medicine [363, 370]. Artistic renderings of
human anatomy have a long history, with "Gray’s Anatomy" [361, 398].
    Understanding the content within a volume through an X-ray Vision effect is
important, as being able to see an object clearly is required for any visualization.
The more an effect distracts from the information it is trying to present, the less
useful it becomes. The following study aims to evaluate the effect these X-ray
Visualizations have on data, and if VIRTs there are any issues that will need to be
overcome to utilize any given VIRT in a real-world scenario.




                                         163
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR




Figure 6.1: These images show the study environment in which the study was
found in and all four of the conditions listed in Chapter 4. All these images were
taken using an external HoloLens2’s camera while observing a user interacting with
the study. (a) Displays no Volumetric Illustrative Rendering Techniques (VIRT) and
shows the participant performing the task while being observed by the researcher.
(b) The Hatching VIRT illustrates a task perspective similar to the participants with
all major interfaces visible in the photo. (c) An image of the participant counting
the small green objects with their fingers while the visualization utilizes the stippling
VIRT.


6.1      Volumetric Illustrative Rendering Techniques
Artistic effects are generally chosen over utilizing transparent objects because trans-
parent objects can be difficult to place within a virtual space. This is because it is
difficult to even in the real world to determine if a transparent object is inside or
behind another object [352]. While realistically rendering a scene in Mixed Reality
(MR) will provide you with the best possible results regarding accuracy, research
has determined that better spatial understanding can be found by using a sketch or
cartoon-based rendering [399]. Neither stippling nor hatching has been used much
using any MR immersive devices, but it has seen a lot of use with 3D displays [89].
    Each different VIRT uses different techniques to indicate the shape and surface
of the object it is trying to represent. Halos work well to highlight objects and


                                          164
Figure 6.2: The conditions that were used in this study are displayed as the
volumes they are represented as.


have been shown to improve depth perception [73, 131, 359, 394]. Both Stippling
and Hatching are able to illustrate the location of major surfaces in relation to each
other, which should make it simpler to determine which objects are which. Stippling
accomplishes this by applying stippling to each different type of object, allowing a
better sense of space [400], while Hatching accomplishes this by changing the angles
of the lines [373].
    This Chapter focuses on the VIRTs revealed in Chapter 4: Halo, Hatching and
Stippling.The design of these VIRTs is shown in Figure 6.2, utilizing the volumes that
could be created by the Random Volume Generation system shown in Chapter 5.


6.2     User Study
To access the impact that VIRTs ability to identify spatial relationships between the
information within the volume two tasks were designed: one of them to determine
how well a user could identify objects within a volume using Ocular See Through
(OST) Augmented Reality (AR); while the other one aimed at determining how
well they could identify an a grouped hierarchy. Volumetric rendering can make
spatial perception difficult due to clustering, inaccurate depth perception, and ori-
entation [15,96,354]. MR Head-Mounted Displays (HMDs) see greatly benefit these
issues by enabling additional depth cues, including some stereotypical cues and mo-
tion parallax [60,71]. Still, OST AR HMDs tend to have a small color or gamut and
less bright and opaque colors than traditional video screens/devices [30, 151].


6.2.1    Research Questions
This study was designed to better understand how partial occlusion in the form
of VIRTs affected a user’s ability to understand the data within a volume. Most
works on the usability and perception of these effects in X-ray Vision have either
looked into where and when X-ray Vision is needed [177,224], what are the tolerable
parameters [218], or how to interact in a space that you cannot physically access [51,
401]. However, there has been limited work in general on the impact of the use of

                                         165
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR




Figure 6.3: A visual description of how the task users in this study were asked to
conduct using a volume with 22 green artifacts. On the left-hand side, users were
asked to count all of the green artifacts, while on the left-hand side, they were asked
to count all of the objects nested inside of the blue artifacts. All of the green cells
that participants were asked to count for each task are marked with a white outline
and counted with a number to the top right of each green artifact.


MR HMDs when using Direct Volume Rendering (DVR) techniques [22, 309, 402]
with very few papers focused on OST AR [28]. This has led to a great deal of
potential unknowns for how VIRTs being ended OST AR may impact how users
and the limited information which existed on the impact that VIRTs could have on
the perception of volumes answers, the following research questions were pursued
by this study where:

RQ.1 Can VIRTs aid a person’s comprehension of a volume when determining indi-
     vidual objects using direct volume rendering?

RQ.2 What is the impact of different VIRTs on participants’ self-reported cognitive
     load and usability?

   RQ.2.1 Is this effect noticeable via the differences between participant behaviour
          (hand, head, and eye movements) between VIRTs?


6.2.2     Tasks
The study design was based on work done in the field of visual analytics and
was influenced by studies within the field of volume rendering and visual analyt-
ics [126, 127, 129]. To understand what the participants could comprehend about


                                         166
the volumes with a range of given VIRTs. Investigating how well participants could
record information within the volume using VIRTs. This was compiled by generat-
ing a set of 7680 volumes that all utilized the same range of data, allowing a unique
volume that conformed to a set of predefined rules and had users report on the
frequency of artifacts in the object.

Count All

In the first task, participants were asked to summarize a visual data set (shown
in Figure 6.4) by asking them to "Count all of the small green objects throughout
the entire volume". This is called the "Count All" task for simplicity. The desired
behaviour participant is illustrated on the left-hand side of Figure 6.3. This task
was proposed in relation to DVR usability by Laha et al. [130], and a variation of
it was used by Munzener et al. [403]. The aim of the question was to determine
how easily people can identify individual objects within the volume and the effect
of VIRTs on their accuracy.

Count Nested

In the second task, participants were asked to count the "small green objects located
within the larger blue objects." This was called the "Count Nested" task. The desired
behaviour participant is illustrated on the right-hand side of Figure 6.3. This task
was intended to determine how accurately people understand the spatial relation-
ships within a volume and the effect of VIRTs on their accuracy. Specifically, the
goal of this research was to understand if VIRTs assists people in identifying if an
object is in front of, inside, or behind another object.


6.2.3    Hypotheses
 H.1 Participants will identify all of the objects more accurately using the halo
     VIRT in the Count All task (R1); Piringer et al. [356] have shown that the
     halo effect can allow for people to rapidly find objects within large 3D datasets
     and DVR visualizations

 H.2 Participants will identify all of the objects faster using the halo VIRT in the
     Count All task (R1). The same studies as above have shown that halo can
     improve identification time [356];

 H.3 Participants will identify all of the objects more accurately using the hatching
     VIRT in the Count Nested task (R.1). These visualizations have been designed
     to enable better depth perception for users and should be able to aid participants
     when counting large amounts of objects;

                                         167
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

 H.4 Participants will identify all of the objects fastest when either using the hatch-
     ing or stippling VIRTs in the Count Nested task (R.1). As in the previous
     study in this dissertation and in other similar studies, the visualizations should
     enable a more accurate view of the dataset. This effect can be seen without
     volume rendering in works that don’t use volume rendering, and I believe that
     this will carry over into this experiment [36];

 H.5 Participants will move their hands, eyes, and heads at a lower rate when
     they are using the halo VIRT (R.2.1). The embodied cognition field looks at
     instances when the body plays a significant causal role in a person’s cognitive
     processing [404, 405]. I expect that people would move their heads and eyes
     more rapidly when trying to solve a difficult problem [405]. This is likely to
     take the place of hand motions to help them count, like using their fingers to
     count and pointing at the objects [405], but could also be seen in with rapid eye
     movements;

 H.6 The halo VIRT will be the most preferable and least cognitively demanding
     for identification tasks (R2); When other studies have utilized similar meth-
     ods, they have found that a silhouette or outline has been more appealing to
     participants [253, 406];


6.2.4    Participants
24 participants were recruited for this study from a pool of students, faculty and
staff from the University of South Australia aged between 21 and 37 years old
(mean = 26.95, σ = 4.47), 6 female, 18 male, with little(9) to no (15) experience
with medical data. Their experience using MR systems varied, with 8 people using
them daily, 5 using them weekly, 6 using them monthly, 2 using them rarely, and
3 using them for the first time. All participants were asked to declare if they had
any major vision impairments that could not be corrected during the recruitment
process. If this was the case, they would have been asked not to participate in the
study.


6.2.5    Procedure
Participants were asked to sit down and complete eight training exercises to famil-
iarise them with the task of each condition containing either 13 or 14 objects to
count in total. No data was recorded during this phase, and the participants were
encouraged to talk to the examiner. After this, participants were allowed to take a
break before starting the study.



                                         168
Figure 6.4: 5 volumes each using no VIRT each with a different number of green
objects to count.


Task

Each participant was asked to count all of the small green objects throughout the
scene (Count All) or only the green objects located within one of the blue regions
(Count Nested). To ensure clarity, participants would be tasked with answering the
same question for each VIRT at the same time before moving forward to the other
question for that same VIRT. This group of 15 would consist of 3 repetitions of 5
conditions shown in Figure 6.4, each containing 14, 16, 18, 20, and 22 objects to
count. The order in which the VIRTs was presented was completely counterbalanced.
    This was done to ensure the participants were aware of which task they were
being asked to perform. Every time the question changed, the examiner would
verbally inform the participant. The order in which these questions were asked
throughout the study would be shuffled to remove possible learning effects. The
question would also be displayed on the nearby computer screen if the participant
was confused, and this would be changed for a prompt telling the participant the
question would change each time. This allowed us to test how well participants
could identify objects within the scene and the spatial location and hierarchy of the
objects within the scene.
    Participants were given a large red button (called the task button) that allowed
them to start each iteration presenting the visualization. Participants were required
to press the task button before giving their answer. Participants would then be
prompted by a computer monitor where they would input their answer via a key-
board. During the times between the task buttons being pressed, the time is taken
to start, and it would finish when they pressed it again to signal the end of the
condition. When Participants are prompted to provide their answer via the nearby
keyboard and monitor. When Participants pressed the task button again, the next
iteration of the study would begin. their hand, head, and gaze movements were
tracked.
    Between each condition, participants were asked to take a questionnaire that
consisted of a PAAS [407], a SUS [408], and a question promoting them to quantify


                                        169
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR




Figure 6.5: The layout of the study area (the participants desk space) the par-
ticipants had around them for the perception experiment. Distances are in center
meters


between 0(worst) and 5(best) how well they could observe all the objects in the
volume as well as giving them the option to tell us what they liked and disliked
about the VIRT which is reported in Chapter F. The post-study questionnaire asked
the participants to rank how they believed they performed with all of the VIRTs,
and how easy they were to use, and they were given the opportunity to explain
their answers (reported in Chapter F). These answers were then used to confirm a
correlation between how the participants answered each condition.

Pilot Study

The amount of objects in this study was decided by comparing this study research
to similar studies in graph visualizations that would use up to 20 nodes in their
visualizations [409]. A separate pilot study was conducted using 5 participants (1
female 4 male) who were aged between 21 - 57 years old. This study used a similar
procedure to the one listed in Section 6.2.5, which used a wider range of values from
9 green objects to 27. Only the no VIRT condition was used in the pilot study.
    The results from this study showed that they struggled to count past 18 when
presented with the baseline condition. All visualizations were randomly generated
for each study iteration, following a predefined set of rules laid out in the previous
chapter (Chapter 4). This caused the decision to have numbers ranging around 18
objects the participants needed to count.




                                         170
6.2.6    Study Environment
The study took place in a dimly lit room with multiple light sources (windows and
stage lighting) and utilized the space shown at the centre of the room shown and
detailed in Figure 6.5. The visualization would appear 15cm above the visualization
marker. Participants could move the computer screen, keyboard, and button to a
preferred location, but each user studied started in the same position. Participants
were seated at a desk in the center of the room and observed throughout the study
by a researcher as shown in Figure 6.1 a). The study was facilitated and built
upon using the Unity Engine1 , with the Microsoft Hololens 22 (Figure 6.1(b)) as the
display modality via a wired connection utilizing the Holographic Remoting Player3
to a desktop PC. The desktop PC featured an Intel i5 with an Nvidia Geforce GTX
2070 GPU, running at between 60-90 FPS during the study. Participants would
either be given a task button on either their left or right side for this study.


6.3     Results
This section contains the analysis of the data collected in both studies. All values
with a p-value < 0.1 have been reported, while only p-values < 0.05 are considered
significant.
    In this study, while the participant was observing the volume, the system tracked:

  • The participants’ accuracy. The absolute difference between the amount of
    green objects the participants counted(p) and the actual amount of objects
    there was(a);

  • The time required for a participant to count the number of green objects.

  • and user behaviour details were recorded this included information required
    to track:

        – Eye Movements
        – Head Movements
        – Hand Movements

For the analysis of errors and time, linear mixed models were conducted. This ap-
proach accounts for individual differences between participants in repeated measures
and allows us to also examine the effect of the number of objects. [251, 292]. Both
  1
     https://unity.com/releases/editor/whats-new/2019.4.3
  2
     https://www.microsoft.com/en-us/hololens/
   3
     https://learn.microsoft.com/en-us/windows/mixed-reality/develop/native/
holographic-remoting-player


                                         171
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

                                                   Analyzing User Error Margins
                                         Across VIRTs Conditions with Counting all Objects

                                     VIRTs:       ●
                                                          No VIRT    ●
                                                                             Halo         ●
                                                                                              Hatching           ●
                                                                                                                         Stippling


                 8
Range of Error



                 6                                                                                                                           ●
                                                                                                             ●


                                                                                                                                                 ●
                 4                                        ●
                                                                                  ●

                                                                                                                     ●
                             ●                                                        ●

                                 ●                            ●                                                                      ●

                 2   ●
                                              ●                          ●
                                                                                                    ●



                                                                                                         ●                               ●
                                                                              ●
                         ●                            ●

                 0

                         14                           16                      18                         20                              22
                                                              Amount Of Objects To Count

Figure 6.6: The accuracy, or difference between the participant’s and real answers,
when counting all green objects within the volume by the VIRTs and the number of
objects they were asked to count. Confidence intervals (CL = 95%) calculated from
the post-hoc emmeans between all conditions are shown as boxes on each error bar.
Significance bars have been omitted as all conditions are significant


models were specified with the fixed factors of the VIRTs designed in Chapter 4, the
number of objects the user was required to count, and the interaction effect between
them plus a random effect of the participant on the intercept. Significance values
were extracted using Type II Wald chi-square tests, and where appropriate, pairwise
post hoc comparisons were conducted using Tukey’s Honestly Significant Difference
(HSD) for multiple comparisons.


6.3.1                Accuracy (H.1 & H.3)
The accuracy of the participant responses was measured as the difference between
the correct answer and the answer given by the participant. The model for accuracy
in the Count All task showed a significant fixed effect between the VIRTs (χ2(3, N =
24) = 777.083, p < 0.0001), and the number of objects also showed a significant fixed
effect (χ2(4, N = 24) = 146.190, p < 0.0001), with a significant interaction effect
((χ2(3, N = 24) = 34.605, p < 0.0001)). These results are illustrated in Figure 6.6.
    The post hoc pairwise comparisons between the VIRTs showed significant dif-
ferences for all combinations (p < 0.0001). Halo had the highest accuracy, followed
by no VIRT, stippling, and hatching in descending order. Comparison between the
different numbers of objects showed significant differences between 14 and 22, 16
and 24, 16 and 22, and 18 and 24 objects (p < 0.0001), 14 and 28 (P = 0.0005), 16
and 18 objects (p = 0.0247), and 23 and 25 objects (p = 0.0033). The accuracy was
lower when the participant had more objects to count in all of these pairs.
    The Post hoc Pairwise comparison for the interaction between the conditions had
138 significant effects with a p-value < 0.05. To summarize these effects, most visu-

                                                                         172
                                   Comparing User Error Margins Across VIRTs
                                    Conditions When Counting Nested Objects
Range of Error


                 Stippling          ●




                 Hatching                ●




                     Halo                          ●   ●            ●   ●        ●   ●    ●




                 No VIRT      ●          ●         ●                             ●   ●    ●
                              .0




                                              .5




                                                              0




                                                                            5




                                                                                         0
                             −5




                                             −2




                                                             0.




                                                                            2.




                                                                                         5.
                                                             VIRT

Figure 6.7: The accuracy of participant responses when completing the Count
Nested task. The error bars indicate each X-ray Visualization’s confidence levels
(CL = 95%). Significance bars have been omitted as all conditions are significant


alizations showed a significant difference in themselves when the difference between
the number of objects the participant had to count was greater than 4, regardless
of VIRTs. The one exception to this rule was the halo visualization, which showed
no significant differences against itself when the participant was required to count
any number of objects. A full breakdown of the results can be found in Chapter D.
    For conditions with fewer objects to count, hatching was unlikely to be significant
when compared to conditions using stippling with more objects to count. There was
a high level of significance for the inverse, though (when there were fewer objects
to count using stippling than hatching). Generally, hatching and stippling showed
a very significant difference when compared to no VIRT and halo. All other in-
teraction pairs not previously mentioned showed a significant difference from one
another. A full table of these effects can be found in the supplemental materials.
    For the Count Nested task, the model showed a significant fixed effect between
the VIRTs (χ2(3, N= 24) = 347.053, p < 0.0001). No significant effect of the
number of objects or significant interaction effect was shown. The post hoc pairwise
comparison of VIRTs showed significant results between halo and hatching, halo and
stippling, hatching and no VIRT, hatching and stippling, no VIRT and stippling (p
< 0.0001), and halo and no VIRT (p = 0.0001). Halo had the highest accuracy,
followed by no VIRT, stippling, and hatching in descending order.


6.3.2                    Time Required (H.2 & H.4)
The model for the time spent on the Count All task showed a significant effect
between the number of objects viewed (χ2(4, N= 24) = 85.9544, p < 0.0001).
There was no significant fixed effect between the four conditions and no significant
interaction effect. The post hoc pairwise comparison of the different numbers of
objects showed significant differences between 14 and 22, 16 and 22 (p < 0.0001),

                                                       173
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

                                           Object Identification                           Object Identification
                                           Counting Everything                           Counting Within the Blue
Seconds Required for Task                                                        40
                                                ●                                                                           ●
                                    ●           ●                                                                ●
                                                ●         ●



                            40
                                                          ●                                                      ●


                                                                                                 ***
                                    ●                                                                                       ●
                                    ●           ●                                        ●
                                    ●                     ●                                                      ●
                                    ●                                  ●                 ●                       ●
                                                          ●                                                      ●          ●
                                                                                                  ●              ●
                                    ●           ●
                                    ●
                                                ●                                        ●
                                                ●

                                                                                                                ***
                                                          ●                              ●
                                                ●                                                                ●
                                                                                                                            ●
                                                ●
                                                ●                                                 ●              ●
                                                ●                                                                ●


                                                                                                       ***
                                                ●
                                                ●                                                                ●          ●
                                                                                                  ●                         ●
                                                ●                                                                ●          ●
                                                                                                                            ●
                                                ●
                                                ●                                        ●        ●              ●




                            30
                                                ●
                                                ●
                                                ●
                                                ●
                                                ●
                                                                                 30      ●


                                                                                         ●
                                                                                         ●
                                                                                         ●
                                                                                                  ●

                                                                                                  ●
                                                                                                  ●
                                                                                                  ●
                                                                                                                            ●
                                                                                                                            ●
                                                                                                                            ●

                                                                                                                            ●
                                                                                                  ●
                                                                                         ●        ●
                                                                                                  ●                         ●
                                                                                                                            ●
                                                                                                                            ●
                                                                                                  ●                         ●
                                                                                                                            ●
                                                                                         ●                                  ●
                                                                                                  ●
                                                                                                  ●




                            20                                                   20


                            10                                                   10

                             0                                                   0
                                 No VIRT       Halo    Hatching    Stippling          No VIRT   Halo         Hatching   Stippling


Figure 6.8: The time required to complete each interaction of a task for each
VIRT for both the Counting Everything and Counting Nested tasks. The error
bars indicate each X-ray Visualization’s confidence levels (CL = 95%). Significance
differences are displayed as the lines on the right side of the graph stars indicate
significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).


14 and 20 (p = 0.0008), and 18 and 22 (p = 0.0055).
    the participants had to count, the longer it took them to complete the task,
particularly when the count differed by four objects.
    The model for the time spent on the Count Nested task showed a significant fixed
effect between the four VIRTs (χ2(3, N= 24) = 85.9544, p < 0.0001). In contrast
to counting all objects, the fixed effect number of objects shown only showed some
variability (χ2(4, N= 24) = 8.5963, p = 0.07202), and no significant interaction
effects were found. The post hoc comparison between the VIRTs showed significant
differences between halo and hatching, halo and stippling, hatching and no VIRT (p
< 0.0001), hatching and stippling (p = 0.0010), and no VIRT and stippling (p =
0.0143).
    A Kendall correlation test was used to determine if the time spent by each
participant correlated to their accuracy. When only counting the green objects
contained within the larger blue objects, a significant relationship between the time
required and range of error was found with a low to moderate amount of correlation
τ = −0.18 (z = -9.3074, p < 0.0001), showing that participants tended to do better
when they took more time some of the time. When counting all of the objects, almost
no relationship was found between the time required and accuracy τ = −0.03831766
(z = -1.8507, p = 0.06422), suggesting that taking more time when performing this
task did not help improve accuracy.


6.3.3                               User Behavioural Analysis (H.5)
This section focuses on how the participants’ behaviour reacted to the four condi-
tions. Over the course of the overall distance, participants would have moved their


                                                                           174
                                     Velocity of Participants' Moved Their Head
                                                            **                                                     *
                                                      ***                                      ***
Average Velocity of



                      75          ***                                   75               **
                                               *                                                     ***
  Headset (cm/s)



                                                                                                                        ●



                              ●          ●                       ●
                              ●                                                                               ●

                                                       ●

                                                                 ●
                                                       ●
                                                       ●




                      50                                                50
                                                                 ●

                                                       ●




                      25                                                25


                      0                                                    0
                           No VIRT      Halo       Hatching Stippling          No VIRT        Halo         Hatching Stippling
                                  Counting Everything                              Counting Nested Objects

Figure 6.9: Box plots showing how much each participant moved their head to view
the volume between the different VIRT conditions for both the Counting Everything
and Counting Nested tasks. The error bars indicate each X-ray Visualization’s
confidence levels (CL = 95%). Significance differences are displayed as the lines on
the right side of the graph stars indicate significance (* = p < 0.05; ** = p < 0.01;
*** = p < 0.001).


heads, gaze, and hands where tracked. This is then divided by the time each condi-
tion took to allow us to view the average speed at which participants moved. The
headset’s and hand’s distance was tracked by calculating the amount its position had
moved relative to the visualisation between every 16ms and summing these results.
When the hands were not in front of the user, their distance was not tracked. Eye
Gaze’s distance was measured by tracking the distance a user’s eye gaze would move
within a 2m radius from the participant’s current position in each frame minus the
same distance that the head gaze would have provided.
    The model for the participants’ head motion velocity for the Counting Nested
task showed a significant fixed effect between the various VIRTs (χ2(3, N= 24) =
54.9095, p < 0.0001). The fixed effect number of objects and interaction effects both
found no significant effects. The post hoc comparison between the VIRTs showed
significant differences between hatching and no VIRT (p < 0.0001), no VIRT and
Stippling (p = 0.0001), Halo and no VIRT (p = 0.0047), Halo and hatching (p =
0.0005), Halo and no VIRT (p = 0.0049), Hatching and Stippling (p = 0.0205).
Figure 6.9’s Counting Nested objects graph shows that having no VIRT leads to
an unpredictable reaction from the user behaviour while adding a VIRT tends to
restrict this.
    The model for the participants’ head motion velocity for the Counting Everything
task showed a significant fixed effect between the various volumetric Illustrative Ren-
derings (χ2(3, N= 24) = 32.2482, p < 0.0001). In contrast to counting all objects,
the fixed effect number of objects and interaction effects were found to have no
significant effects. The post hoc comparison between the VIRTs showed significant

                                                                     175
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

                                          Velocity of Participants' Moved Their Hands
                                            ●                  ●                                   ●
                                                               ●                                   ●
                                                                                                   ●




Velocity of hands (cm/s)
                                   ●                                                                          ●
                                   ●
                                   ●                                                       ●
                                   ●        ●                                                      ●
                                   ●
                                                     ●
                                            ●                                              ●
                                            ●                                                                 ●
                                   ●                                                                          ●
                                                                                ●
                                                               ●                                              ●
                                            ●        ●
                                                     ●
                                   ●                 ●
                                                     ●
                                   ●                 ●         ●
                                            ●        ●         ●                                   ●          ●
                                                               ●


                                                                                                   **
                                                                                ●



                           75                                          75
                                   ●
                                   ●        ●                                                                 ●
                                                                                                              ●
                                   ●        ●        ●         ●
                                                     ●
                                                     ●                                     ●
                                   ●                                            ●
                                   ●        ●        ●                          ●
                                                               ●                                              ●
                                   ●                                            ●          ●
                                                                                           ●
                                   ●        ●        ●                                                        ●
                                                     ●         ●
                                                               ●
                                   ●        ●        ●         ●                ●
                                                                                ●
                                                               ●                           ●       ●
                                   ●                                                                          ●
                                                     ●                                             ●
                                            ●                  ●
                                   ●
                                            ●                                   ●                  ●
                                   ●                 ●                          ●
                                   ●                 ●
                                                     ●                                     ●
                                   ●                           ●
                                            ●        ●
                                                     ●         ●                ●
                                                     ●                                     ●
                                                     ●                          ●
                                            ●        ●
                                            ●                  ●                                   ●
                                            ●        ●                                             ●
                                   ●        ●
                                            ●                                   ●
                                   ●                           ●                ●                  ●
                                   ●        ●        ●
                                   ●                 ●         ●                           ●
                                            ●                                                      ●
                                                     ●                                     ●       ●
                                                               ●
                                                               ●
                                                               ●
                                                                                           ●
                                                               ●
                                                               ●



                           50                                          50
                                                     ●         ●
                                                               ●                           ●
                                                               ●                           ●
                                                     ●                                     ●
                                                     ●
                                                                                           ●

                                                                                           ●
                                                     ●
                                                     ●




                           25                                          25


                           0                                             0
                                No VIRT    Halo   Hatching Stippling         No VIRT     Halo   Hatching Stippling
                                       Counting Everything                          Counting Nested Objects

Figure 6.10: A graph showing the speed at which participants moved their hands in
this experiment when the different VIRTs were being utilized for both the Counting
Everything and Counting Nested tasks. The error bars indicate each X-ray Visual-
ization’s confidence levels (CL = 95%). Significance differences are displayed as the
lines on the right side of the graph stars indicate significance (* = p < 0.05; ** = p
< 0.01; *** = p < 0.001).


differences between halo and stippling, and hatching and stippling (p < 0.0001), no
VIRT and Hatching (p = 0.0042), Halo and no VIRT (p = 0.0047). Figure 6.9’s
Counting Everything graph shows that the Hatching and Stippling VIRTs required
participants to move more than other VIRT such as halo or no VIRT.
    The model for the participants’ head motion velocity for the Counting Everything
task showed no significant results. However, The model for the Counting Nested task
showed a significant fixed effect between the VIRTs (χ2(3, N= 24) = 11.7432, p =
0.0083), but both the fixed effect for the different amounts of countable objects
and the interaction effect showed no significant results. The post hoc comparison
between the various VIRTs showed that when participants were using the Halo
VIRT. Figure 6.10, they would move their hands significantly faster than they would
during the Stippling task (p = 0.0057).
    The model for the velocity of the participants’ eye gaze for Counting Everything
task showed a significant fixed effect between the different VIRTs (χ2(3, N= 24) =
19.347, p = 0.0002) and the number of objects in total (χ2(4, N= 24) = 30.759, p <
0.0001). No significant interaction effect was found. The post-hoc comparison be-
tween the VIRTs showed that participants significantly moved their eyes more when
using stippling compared to Halo (p = 0.0007) and Hatching (p = 0.0010). Some
variation was also found between No VIRT and Stippling (p = 0.0918). Whereas
the posthoc comparison for the number of objects 14 countable objects showed par-
ticipants moved their eyes significantly less compared to 18 (p = 0.0013), 20 (p =
0.0153) and 22 (p = 0.0013). The same phenomenon was found with 16 countable
objects as participants moved their eyes significantly fewer results against 20 (p =

                                                                   176
                                          Velocity of Participants' Moved Their Eyes
                           25                                         25
Eye Gaze Velocity (cm/s)

                                                                                                          ●
                                                               ●
                                            ●


                                                               ●                           ●              ●
                                                                                ●                                  ●
                                            ●
                                                               ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                ●
                                                                                                ***       ●




                                   ●
                                            ●
                                                          **                    ●


                                                                                ●
                                                                                           *              ●        ●



                                                    ***                                                  ***
                                   ●                                                       ●



                           20      ●
                                            ●
                                            ●
                                            ●

                                            ●
                                                               ●

                                                               ●
                                                               ●
                                                                      20        ●



                                                                                ●
                                                                                ●
                                                                                           ●
                                                                                           ●
                                                                                                          ●


                                                                                                          ●
                                            ●                                              ●
                                                                                ●          ●
                                                                                           ●
                                                     ●                                     ●                       ●
                                            ●                  ●                           ●              ●
                                   ●                           ●
                                                               ●
                                            ●        ●                                                    ●        ●
                                   ●                           ●                                          ●
                                                               ●
                                                                                                          ●        ●
                                   ●
                                            ●
                                                               ●
                                   ●                           ●                                                   ●
                                                     ●




                           15                                         15
                                            ●
                                            ●                  ●
                                   ●
                                   ●        ●
                                            ●
                                   ●                           ●
                                   ●                 ●
                                            ●        ●
                                   ●
                                            ●
                                            ●
                                   ●
                                            ●        ●
                                                     ●
                                            ●
                                            ●        ●
                                                     ●
                                                     ●
                                            ●        ●
                                                     ●
                                            ●        ●




                           10                                         10

                            5                                            5

                            0                                            0
                                No VIRT   Halo   Hatching Stippling          No VIRT     Halo         Hatching Stippling
                                       Counting Everything                          Counting Nested Objects

Figure 6.11: Box plots relating to the speed at which participants moved over
different eye gazes. The error bars indicate each X-ray Visualization’s confidence
levels (CL = 95%). Significance differences are displayed as the lines on the right
side of the graph stars indicate significance (* = p < 0.05; ** = p < 0.01; *** = p
< 0.001).


0.0177) and 22 (p = 0.0024).
    The model for the velocity of the participants’ eye gaze for Counting Nested
task showed a significant fixed effect between the different VIRTs (χ2(3, N= 24)
= 28.1722, p < 0.0001) and no significant fixed effect was found with the number
of objects to count and the interaction effect. The post-hoc comparison between
the VIRTs (shown in right hand side of Figure 6.12) showed that Stippling was
significantly faster compared to Halo (p = 0.0004) and no VIRT (p < 0.0001).
Participants moved significantly slower when using Hatching when compared to no
VIRT (p = 0.0108). Some variation could also be seen between Hatching and Halo
(p = 0.0602).

Subjective Results (H.6)

This section focuses on how the participants themselves perceive the various VIRTs.
By asking participants to complete the PAAS Cognitive load scale and a System
Usability Scale (SUS) questionnaire. Paired with the asking participants "How easy
was it to look at objects inside of other objects using this visualization?". It was
possible to gain further insights into the participants’ actions and behaviours.
    The results from the PAAS questionnaire for the counting only within the blue
showed a significant difference between different illustrative visualizations using a
Friedman rank sum test (χ2(3, N = 24) = 46.493, p < 0.0001). Post-hoc analysis
with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction
applied, comparisons showed significantly increased cognitive load between hatching
and halo (p < 0.0001), no VIRT, and halo (p = 0.0008), stippling and hatching (p


                                                                   177
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

                                            Analyzing Eye Gaze Velocity (cm/s)
                                     Across VIRTs Conditions with Counting all Objects
                           20                                         **
                                                   *
Eye Gaze Velocity (cm/s)                                    ***
                                                   *
                           15            **




                           10

                                ●
                                         ●

                            5                                ●                   ●
                                                                                         ●




                            0
                                14      16                  18                  20       22
                                       Amount of Objects Required To Be Counted

Figure 6.12: A plot showing the difference in velocity between the user’s eye
movements when different amounts of objects existed for them to see when counting
everything. The error bars indicate each X-ray Visualization’s confidence levels (CL
= 95%). Significance differences are displayed as the lines on the right side of the
graph stars indicate significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).


= 0.0003), no VIRT and hatching (p = 0.0031). The PAAS questionnaire for the
counting everything question showed a significant difference between different illus-
trative visualizations using a Friedman rank sum test (χ2(3, N= 24)= 43.674, p <
0.0001). Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted
with a Bonferroni correction applied, comparisons showed significantly increased
cognitive load between Hatching and Halo (p < 0.0001), no VIRT and Halo (p =
0.0008), Stippling and Hatching (p = 0.0041), no VIRT and Hatching (p = 0.03618),
and Hatching and no VIRT (p = 0.0064). All of the PAAS results found, shown
in Figure 6.13, indicate that participants feel that the halo VIRT required the least
cognitive load, while the hatching and no VIRT VIRTs were felt to require a much
greater cognitive load.
    All the PASS results for both were highly correlated τ = 0.77 (z = 9.9632,
p < 0.001) to each other. showing that regardless of the task the participants
were presented with, they felt these visualizations took a similar cognitive load.
The correlation between the PAAS Questionnaire Results and how well participants
believed they did on each condition. A significant negative relationship between
them τ = −0.49 (z = -6.4262, p < 0.001). Showing participants’ feelings at the end
of the study and showcasing an accurate representation of how they viewed their
performance.
    The results from the SUS questionnaire showed a significant difference between
different VIRT using a Friedman rank sum test (χ2(3, N = 24) = 54.728, p < 0.0001)
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a
Bonferroni correction applied, comparisons showed significantly increased cognitive
load between hatching and halo (p < 0.0001), no VIRT and halo (p < 0.0089),
stippling and halo (p = 0.015) stippling and hatching (p = 0.0019), and no VIRT

                                                        178
       Stippling




                                                                                         ***
                                                                                   ***
       Hatching
VIRT




                                                                             ***

                                                                                       ***
           Halo




                                                                                 ***
       No VIRT
                   0.0            2.5                5.0               7.5
                                          PAAS Score
Figure 6.13: Outcomes from the PAAS questionnaire in the counting study. Lower
results indicate lower cognitive load and higher results indicate higher cognitive load.
Significance differences are displayed as the lines on the right side of the graph stars
indicate significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).


and hatching (p = 0.0022) (see Figure 6.14).
    The correlation between the SUS Questionnaire Results and how easily par-
ticipants found the visualizations to use was tested where there was a significant
relationship between them τ = 0.48 (z = 6.2974, p < 0.0001). Showing a mildly
strong positive correlation between participants’ feelings about their ability to use
the VIRTs for this task.
    Responses for the question "How easy was it to look at objects inside of other
objects using this visualization?" were analyzed using a Friedman rank sum test
and showed a significant difference between the various VIRT (χ2(3, N= 24)=
49.392, p < 0.0001). Post-hoc analysis with pairwise Wilcoxon signed-rank tests
was conducted with a Bonferroni correction applied. Comparisons showed partici-


       Stippling
                                                                                       ***
                                                                                 ***




       Hatching
VIRT




                                                                           ***

                                                                                   ***




           Halo
                                                                             ***




       No VIRT
                   0              30                60                90
                                           SUS Score
Figure 6.14: Results of the SUS questionnaire for the counting study. Significance
differences are displayed as the lines on the right side of the graph stars indicate
significance (* = p < 0.05; ** = p < 0.01; *** = p < 0.001).



                                          179
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR


       Stippling




                                                                                        ***
                                                                                  ***
       Hatching
VIRT




                                                                            ***

                                                                                    ***
           Halo




                                                                              ***
       No VIRT
                   0                    2                       4
                                      Ease of Viewing
Figure 6.15: Results for the question "How easy was it to look at objects inside of
other objects using this visualization?"


pants found it significantly more challenging to see through the hatching and halo, no
VIRT and halo (p < 0.0001), stippling and halo (p = 0.0012), stippling and hatching
(p = 0.0005), and no VIRT and hatching (p = 0.0030) VIRTs (see Figure 6.15).


6.4        Discussion
The main findings from the Section 6.3 show that the Halo VIRT was the most
efficient and preferred of all of the visualizations when using OST AR devices. Both
the Hatching and Stippling seemed detrimental to this task. The following sections
will detail how these conclusions were reached based on the five hypotheses.


6.4.1       Accuracy (H.1 & H.3)
The results in Subsection 6.3.1 support H.1 with the halo effect improving the
counting task performance. In the Count Nested task, when counting only the
objects within the larger blue regions, H.3 was unsupported as the halo did not
significantly affect performance. Results in both tasks seem to indicate similar
results between the VIRT conditions. This indicates that participants did not find
the extra information too distracting when using different VIRTs, but also shows
that the halo VIRT is constantly easy to use.
    Halos gave participants a better understanding of the volume. Especially when
identifying the approximate location of the green objects. In the Count Nested
task, participants could not count the wrong amount of objects. This effect can be
seen in the lack of significant interaction effects between the different amounts of
objects shown in Figure 6.6. Compared to using no VIRT at all, the halo technique
performs significantly better with high amounts of data. This indicates that the
halo visualization supports understanding a much larger amount of objects than

                                         180
any of the other VIRTs. Participants mentioned that halos gave them a clear and
easy-to-understand way to conceive the volume and had almost no critiques.
    Hatching is not intuitive when used in MR, as participants seem to have found it
misleading or confusing. Figure 6.6 indicates Hatching effect adds to the complexity
just as much as adding four more objects to count. It leads to much poorer results
than stippling, a visualization that was hypothesized to be similar. As it covers the
object, participants seem to find it hard to track what participants are looking at
while they move their heads around.


6.4.2    Time Required (H.2 & H.4)
The results found in Subsection 6.3.2 do not support H.2 as no significant differences
were found regarding the time results for the Count All task. However, in the
Count Nested task, Figure 6.8 showed participants were fastest when using the halo
visualization and slowest using the hatching visualization, which also did not support
H.4. It showed us that the volume rendering effect was quite strong as participants
were more accurate when using no Volumetric Illustrative Rendering Techniques
(VIRT) than hatching and stippling, but they were also significantly faster when
using this VIRTs. This may mean that these visualizations gave them some false
information regarding where the objects were.


6.4.3    Participant Behaviour (H.5)
Overall, Subsection 6.3.3 showed that embodied cognition was not a major factor
for this study. Participants tended to move faster with the Counting Nested task
than they did for the Counting Everything across all of the behaviours that were
measured. Participants did not seem to point at any objects to count them or to
use their hands in any context. Participants tended to move their heads less when
there were more objects to view. Overall, (H.5) was shown to be false. The exact
types of tasks that are required to trigger Embodied Cognition when looking at
visualizations are still unknown [410]. It is likely that the task chosen was either
not cognitively difficult enough for the participants or the metaphors and clustering
chosen to utilize did not trigger a correct metaphor [405]. Although Figure 6.9 head
motion seems to have been utilized. lowering the amount of movement required for
participants to be able to see the volume of all the objects in the volume, but it
seems to have required them to move more to confirm their location in the Counting
Nested task.
    All the graphs in Figure 6.9, Figure 6.10, and Figure 6.11 show that Counting
Nested and Counting Everything caused participants’ tended to some difference be-
tween both tasks these results can be seen in Figure 6.8, Halo where it can be seen

                                         181
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

that Halo caused the participants to move around the slowest/least. Figure 6.9 illus-
trates for the no VIRT condition, participants expressed a large range of hand and
head speeds when they felt that they needed to move around more with the Hatch-
ing and Stippling VIRT. This change in behaviour is likely because depth perception
was not a huge concern when counting all of the items, so they just needed to focus
on identifying various objects. It does identify that participant’s movements are less
erratic when VIRTs are utilized.
    The Velocity of participants’ Eye Gaze seems to have functioned differently on
both their hand and head velocities. Figure 6.12 indicates the participants’ eye
movements are not reliant on the number of objects in the scene as their eye move-
ment velocity seems to stabilize when they have more objects to count. This can also
be noted as Figure 6.11 showed participants moved their eyes less on the counting
everything task, but Stippling VIRT seemed to require participants to move their
eyes more. This may have been to get a better idea of the structure the participants
were looking at or to look through various stipples due to their larger sizes, or it
could have been due to the stippling task having more, requiring more movement
to see around.
    The increased movement in the eye gaze, which is seen in Figure 6.12, may
indicate a sign of embodied cognition where the users seem to be struggling to count
the largest numbers for the counting everything task [404, 405, 411]. However, these
results are not almost inverted for the Counting Nested task, where stippling causes
the least amount of eye movement. This may be caused by having the occlusion
showing what objects are nested more clearly than they would have been shown
with the Halo or no VIRT conditions.
    Figure 6.10 There seems to be little difference between the VIRTs when partici-
pants move their hands. When performing the Counting Nested task, it seems the
participants moved their hands much more when stippling VIRT. This may have
been to compensate for the lack of eye movement, indicating when counting fewer
objects users prefer to track objects by using their hands rather than moving their
eyes when they are using the stippling VIRT.


6.4.4    Subjective Results (H.6)
The results in Section 6.3.3 showed that both Hatching and Stippling were perceived
to be much harder to complete the tasks with. This was a reasonable response
since users seemed to struggle most with these tasks. The correlation between the
PAAS and SUS questionnaires with the post-study questionnaire seems to have also
indicated that participants were sure about this decision in the moment regardless
of the order they experienced the VIRTs.


                                         182
    In Chapter F, participants were asked to explain their responses. Participants
who disliked the hatching visualization mentioned that it was distracting, some of
them had to move their heads in strange ways to be able to tell what objects were
what, and overall the visualizations were criticized for being too opaque, partici-
pants were pleased however that is was easy to tell the boundary of the shapes, the
depth within the objects and Participants made it clear what objects where distinct.
Interestingly, the halo and hatching VIRTs moved dynamically, but only the hatch-
ing VIRT was criticized for it. This is likely due to the lack of occlusion that the
halo has, while the hatching VIRT covers a larger part of the face of the surface.
When Fisher et al. [253] performed their user calibration study, they also found that
participants preferred to see an outline over all other conditions. If an athletically
pleasing way of statically using the hatching VIRT is found, it would likely show
different results.
    Stippling was still significantly less accurate and slower than using none of the
VIRTs. Performing better than hatching, this seems to be because it was static,
and the dots would not move or rearrange. In all cases, stippling was worse than
having no VIRT or using the halo VIRT when counting objects. A large issue
mentioned by the participants about this visualization was that it made it difficult
to tell where the edges of the objects were. Several participants mentioned that
the dots allowed them to easily identify the exact location of the objects within the
volume. This was because this visualization was relatively stable. While various
dots would appear and disappear, they would not move like hatching, allowing the
participants to identify patterns of the objects.


6.5     Conclusion
The Halo VIRT is very well suited to allowing people to determine OST AR while
other VIRTs seem to make it. This chapter showed the Halo VIRT improved the
participants’ performance when interoperating the volume. Using either hashing or
stippling with these effects made it considerably harder, especially when they were
asked to find more objects. This shows the Halo VIRT is the most useful X-ray
Visualization to use if you, a user, are required to inspect a volume. Allowing for
better communication of diagnosis for medical professionals [9] and clearer provide
more detailed information for fields like education [13, 410], material science [115,
116,304] and geology [3,120]. Moving forward, this disertaion will look at the impact
of VIRTs on depth perception and test their viability with being used for a more
interactive purpose.
    This research showed:
  • Participants can better observe objects within a volume when using the Halo

                                         183
   6. PERCEPTION OF VOLUMETRIC ILLUSTRATIVE EFFECTS
VISUALIZATIONS WITHIN OST AR

   VIRT, and this effect seems to become stronger when more items are being
   visualized.

 • The Stippling and Hatching VIRTs impaired the user’s ability to access what
   objects were inside of the object based on what was in the volume.

 • Participants found the Halo VIRT to be preferable compared to the other
   VIRTs, but the Hatching VIRT seemed to cause some discomfort with partic-
   ipants while performing this task.




                                    184
Chapter 7

The Limits of Depth Perception
when Using Volumetric
Illustrative Rendering Techniques

It has been shown that X-ray visualizations have improved the perceived depth
mismatch caused by a visual misalignment when a virtual object is rendered behind
a real object [37,172,199,412]. Chapter 3 shows that X-ray Visualizations techniques
can impact depth perception, which is important to understand before they are
proposed for activities that require precise hand-eye coordination tasks, especially
activities like surgery. Occlusion is a powerful depth cue that uses virtual graphics
to block physical world objects to influence the perceived depth. However, it can
also obscure key information in some scenarios. The study presented in this chapter
advances the current research knowledge by exploring the impact of Volumetric
Illustrative Rendering Techniques (VIRT)s on a user’s depth perception when paired
with Direct Volume Rendering (DVR).
    DVR on desktop displays provides a good sense of depth when looking through
an object. Our understanding of how it is affected when using a Augmented Reality
(AR) Head Mounted Device (HMD) is relatively unknown past the understanding
that the previous devices struggled to run the visualizations efficiently enough [28].
Prior literature that focused on how humans understood how VIRTs affects depth
perception tends to include the following:

  • Spatial awareness is improved in VR when removing distracting details from
    the physical environment [399];

  • Illustrative geometric effects like those found in X-ray Vision can better join
    the virtual world and the virtual world together by providing a clear reference
    where both of the relationship between them [406];


                                         185
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES




Figure 7.1: This figure shows the environment in which this study took place
and presents each of the VIRT conditions utalized in this study. All these images
were taken using a HoloLens2 camera, viewing a user engaging with the study.
(a) Illustrates the Halo VIRT and shows a participant pressing one of the task
buttons present in the study. (b) Shows the Stippling VIRT being observed by the
participant. (c) Displays the No VIRT condition from the participant’s perspective.
(d) Is an image of the Hatching VIRT.

  • Illustrative effects are utilized in medical AR as they are more tested at dealing
    with the depth mismatch when displaying structures that are located within
    each other since they can illustrate depth perception in ways that are not
    possible with transparency alone [81, 413];

  • Figure 7.1 Shows illustrative effects are also pleasing to see rendered inside of
    other objects as they are better designed to showcase [83];

  • Illustrative effects can convey curvature clearer than shaded meshes [414];

However, the literature has not discussed the impact of illustrative effects in tandem
with DVR regarding depth perception. If VIRTs are to be found useful for high-
precision tasks, their accuracy with depth perception should not decrease depth

                                         186
perception and ideally improve or remove any mismatch. Chapter 3, demonstrated
that having any X-ray Vision effect impaired depth perception. Chapter 4, it was
shown that both Stippling and Hatching may be detrimental to understanding the
layout of a given volume.
    This chapter’s investigation required suitable methodologies to measure the im-
pact of VIRTs, with the expectation that they would be used in a precise and
stressful situation. Similar to the study design used by Nagata [415] and Chen et
al. [416], a Two-alternative forced choice (2AFC) psycho-physical experiment was
conducted. Nagata [415] showed the participants an image or video displayed, either
on an image, television, or stereoscopic display between two separate experiments
to determine the impact of motion and stereopsis on depth perception. The results
from Nagata [415] formed the foundations upon which a lot of depth perception
research is based today, detailing the maximum thresholds of motion, binocular
parallax, accommodation, and convergence while also stating some of the conditions
that affect these factors. Chen et al. [416] conducted a depth perception experiment
examining the impact of binocular distortion using motion and focusing on creating
depth cues that conflicted with each other by presenting slightly different phenom-
ena in each eye showing that a more occlusive element would normally appear closer
to the viewer if only shown in one eye.


7.1     Volumetric Illustrative Rendering Techniques
        Impact on Depth Perception on Ocluar See
        Though Devices
There are several challenges using current Ocular See Through (OST) displays that
make it difficult to present virtual information with intuitive depth cues. Current
OST AR displays display blacks and darker colors as transparent, making shading
realistically difficult. Since VIRTs can utilize bright occlusive colors, they can act
as an alternative to shadows when used for OST AR displays. VIRTs can mitigate
many of the issues regarding transparency with OST AR displays. This allows
VIRTs to convey a sense of depth that would not otherwise be possible when only
using DVR graphics.
    Previous literature shows that VIRTs are able to provide a better opportunity
to convey depth perception within 2D mediums. Stippling has been shown to allow
the user to better understand an object’s surface and how it is shaped and enables
a user to see layers between objects [73,363]. Hatching has been used to show depth
in several textbooks [74, 417, 418] and communicates depth by overlapping lines and
using different angles, leading to darker objects that are deeper into the object and

                                         187
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES

hidden from the light [419]. Halo’s and feature lines have been shown not just to be
an effective X-ray Vision technique to comprehend and is also a method for more
clearly indicating parts of the various parts of the systems [96] that can also be used
to determine the approximate depth of objects [362].


7.2      User Study
7.2.1     Research Questions
This study investigates the accuracy that is achievable when using VIRTs in DVR
to answer the following research questions:

RQ.1 What is the minimum difference in depth that participants can reliably dis-
     tinguish between volumetric objects, independent of any given VIRT?

RQ.2 What impact do the VIRTs have on the participant’s ability to distinguish the
     difference in depth of volumetric objects reliably?

RQ.3 What is the impact of different VIRTs on the participants’ self-reported cog-
     nitive load and usability when determining the depth of a volumetric object?

   RQ.3.1 Is there a noticeable effect between participant’s behavior (hand, head,
          and eye movements) between different VIRTs?

    To determine the performance of each VIRT, this study conducts a 2AFC psycho-
physical study detailing the limits of depth perception when utilizing both DVR and
VIRTs on an ocular see-through device. The participants are asked to judge what
is closer to them in a virtual scene with only a single noisy sphere. Each of these
noisy spheres is presented using one of the three different VIRTs and a baseline (no
VIRT).
    A psycho-physical 2AFC questionnaire study presents participants with pairs of
options and requires them to select the one that best corresponds to their judgment.
This study design utilized a 2AFC questionnaire to determine at what depth a user
could no longer judge the difference in depth between two objects. This choice was
influenced by research done in X-ray Vision investigating similar research investigat-
ing the effect visual cues can have on depth perception in AR [209,420] and research
looking at methods capable of influencing depth perception in general [416, 421].
This type of study enables us to determine what level of depth perception is achiev-
able when using an OST AR headset with volume rendering.
    The threshold at which a user can no longer reliably judge the difference between
two conditions in a 2AFC experiment is referred to as the Just Noticeable Difference
(JND). The point where the two objects look identical to the Point of Subjective

                                         188
                    Figure 7.2: The VIRTs used for this study


Equality (PSE). Hence, these studies are aimed at finding the limits of human
perception and have been performed in many depth perception studies to determine
the thresholds of human perception of various objects [415, 416].


7.2.2    Hypothesis
 H.1 participants will be unable to distinguish differences in depth less than 1.25cm
     with any VIRT (R1). Studies observed depth perception using AR within the
     near field can achieve accuracy of just over 2cm [33] to just under 3mm [244].
     Our study aimed to improve depth perception through volume rendering alone.
     There is a chance that the transparency of the volume will hinder the precise
     nature of nature possible;

 H.2 Either the hatching or Stippling VIRTs will have the lowest Just Noticeable
     Difference (JND). The use of hatching-like visualizations to show depth per-
     ception is a helpful depth perception cue when trying to detail the interior of
     an object [36];

 H.3 Either the hatching or Stippling VIRTs will have the closest Point of Subjective
     Equality (PSE) to zero (R2). The PSE and the JND are likely going to be
     correlated in this experiment

 H.4 Participants will be able to determine depth faster using the Hatching VIRT
     (R2). Previous work by Martin-Gomez et al. [36] has shown that Hatching
     can aid depth perception in AR X-ray Vision when used to demonstrate depth
     through/within an object, and results form Chapter 3 showed that a similar
     technique was the most accurate among the investigated techniques;

 H.5 Participants will move their heads, hands, and eyes faster when using the Halo.
     VIRT (R.3.1). Embodied cognition did seem to place a role in Chapter 6 where
     we found high rates of movement on more challenging conditions. It is likely
     that a difficult problem [405]. Embodied condition in this study would likely
     indicate that the participant was struggling and trying to align the virtual world
     with something real [405], but could also be seen in rapid eye movements;

                                         189
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES




Figure 7.3: An example of what the volume looked like from the outside for each
depth (displayed in the columns shown in cm) from three sides (a front-on view
(Front), a birds-eye view(Top), and a side-on view(Side)). All of these images use
the same noise calculation. The reference object is the blue object on the right-hand
side of each volume. The left side of this image shows the blue variable object when
it is closest to the participant, while the right side shows when it is the furthest
away.


 H.6 The halo VIRT will be the most preferable and the least cognitively demanding
     for the depth perception task (R3). When other studies have utilized similar
     methods, they have found that a silhouette or outline has been more appealing
     to participants [253, 406].;


7.2.3     Participants
24 participants were recruited for this study from a pool of students, faculty, and
staff from the University of South Australia, each aged between 19 and 37 years old
(mean = 26.79, σ = 4.96), 7 female, 17 male, with little (8) to no (16) experience
with medical data. Their experience using Mixed Reality (MR) systems varied,
with 8 who used MR daily, 5 who used MR weekly, 7 who used MR monthly, 1 who
used MR rarely, and three participants whose it was their first time using MR. All
participants had to be asked to declare if they had any major vision impairments
that could not be corrected during the recruitment process. If this were the case,
they would have been asked not to attend the study.


7.2.4     Study Design
This study utilizes the noisy sphere detailed in Chapter 5 and asks participants to
assess what blue object in the volume was closest to them. This not only allows
for validation of the structure of the volumes but also controls the noise and shape
of the various cells within the volume. It also allows the creation of volumes with
elements of similar but slightly differing artifacts, creating a linear range of volumes
shown in Figure 7.3. Every distinct iteration of this study utilized a unique volume

                                          190
that could be adjusted to suit any condition. The row labelled "Front" in Figure 7.3
shows how difficult this choice made it to tell various depths when they are displayed
in 2D. However, MR allows for a much greater sense of the depth of these volumes
than this image can convey. This study aims to learn how good this sense of depth
is [169].

Task

Two task buttons were made available to participants to press with either of their
hands. Participants would press the task button on the same side as the blue object
they found closer to them to answer this. The 3D object was rendered in one utilizing
one of the depths shown in Figure 7.3 that participants would be asked to judge
would sit at various points along a depth axis and move between 2.75cm and -2.75
cm, with 0 being used as a baseline for each comparison. In comparison, the other
side was variable and could be placed within 2.75cm on either side of the reference
object.
    Figure 7.3 shows all the different conditions from three different angles. The
reference object would randomly be swapped half the time to avoid bias from hand-
edness. The system would save and convert the participant’s answer to whether or
not the participant thought the variable side was in front of or behind the reference
object (Shown in Figure 7.4), as the participant was not informed what side the
reference object was on. They used the technique detailed in Subsection 7.2.6, and
both objects in each condition appeared identical.

Procedure

Participants repeated this process for 80 iterations (ten different depths, two sides,
repeated four times) for each condition, after which they were asked to answer a
questionnaire. At the end of each condition, they would perform a SUS and PAAS
questionnaire. At the end of the study, they would be asked to complete a post-study
questionnaire to gain their final understanding of the entire study.
    Before the study, participants were required to read and fill out an information
sheet detailing the study, sign a consent form, and complete the demographic ques-
tionnaire. The structure of the study was explained to the participants. For the
training task, they were then asked to sit down and sent through 16 different train-
ing exercises that covered conditions that were 5cm away from the reference object
(both behind and in front of the reference object). No data was recorded during
this phase, and the participants were encouraged to ask the examiner questions.
Subsequently, participants were allowed to take a break before starting the actual
study.


                                         191
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES




Figure 7.4: The furthest extremes of the visualization shown in enlarged using
the No VIRT condition. (Columns) represent the depth of the objects; and (Rows)
represent different viewpoints of the volume. (Top) the viewpoint the participants
could see; and the (bottom) a bird’s eye view of the volume.


Determining Maximum Thresholds

Since there was little prior work on depth perception using quantitative methods
utilizing DVR on similar devices, a pilot study was run to inform the selection of
parameters for the main study. While the pilot study was conducted under similar
conditions to the main study, its primary purpose was to refine the experimen-
tal parameters by identifying the depth thresholds at which participants began to
struggle with determining whether an object was in front or behind the baseline
object. This study consisted of four participants, each presented with the same
three VIRTs (Stippling, Hatching, and Halo) and the baseline condition. The most
extreme depths were over 5cm away from the reference object (in front and behind).
This study ran for a total of 36 iterations comprising nine different evenly spaced
depth levels, presented from two sides and repeated twice.
    The data from the pilot study showed participants tended to struggle to accu-
rately determine depths within 2.5cm of the reference object. This informed our
choice to place both endpoints 2.75 cm away from the reference object. We also
chose to increase the number of depths the participant would view to 10 because
participants showed frustration when the objects seemed to be at an identical depth,
and the data gathered from this was not as useful as participants could not deter-

                                        192
Figure 7.5: Top-down view of the layout of the participants’ study area (their
desk).


mine in this case which volume was the baseline as they where both identical and
the baseline was on a random side. The longest it took to determine any depth was
just under 20 seconds, and iterations for the study were based on the findings from
this pilot.


7.2.5     Study Environment
This study occurred in the same lit study room as the previous one in Chapter 6. It
contained multiple light sources (windows and stage lighting) and utilized the space
shown at the center of the room shown in Figure 7.5. Participants were seated at a
desk in the center of the room and observed throughout the study by a researcher.
The visualization would appear 15cm above the visualization marker on the desk.
Participants could move the buttons to any area on the desk they chose, but they
always started in the position identified in Figure 7.5. This study was facilitated and
built upon using the Unity Engine1 , with the Microsoft Hololens 22 (Figure 6.1(b))
as the display modality via a wired connection utilizing the Holographic Remoting
Player3 to a desktop PC. The desktop PC featured an Intel i5 with an Nvidia Geforce
GTX 2070 GPU, running at 60-90 FPS during the study. The computer allowed
participants to answer questionnaires without leaving their seats, but they were free
to do so.
  1
     https://unity.com/releases/editor/whats-new/2019.4.3
  2
     https://www.microsoft.com/en-us/hololens/
   3
     https://learn.microsoft.com/en-us/windows/mixed-reality/develop/native/
holographic-remoting-player


                                         193
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES




Figure 7.6: An Example of Euclidean DVR and non-Euclidean DVR of the same
volume. The left side shows the direction of the Euclidean method of the transparent
sphere casting method, whereas the right side shows the non-euclidean version of ray
tracing that was used. Blue circles show the sphere marching into the Sign Distance
Field (SDF), green dots show the ray marching steps, and the black line shows the
ray these elements are following.


7.2.6     Generation and Placement of Volumes
This study aims to allow us to understand the limits of depth perception using
DVR on an OST AR HMD. This required a visualization that could be modified in
a controlled way but would still appear to look individual each time it was created.
The volumes utilized in this study used a set of controllable parameters, enabling
us to conduct a highly controlled depth perception study. This section explains the
methods that were used to create and manage the volumes required for such a study.
    Applying Perlin noise throughout the volume led to differently shaped objects
appearing within the volume at different areas. This would have made testing depth
perception to an accurate level a difficult feat since differently shaped objects would
have presented a different front, back, and center of a shape. To ensure a consistent
comparison between both objects, noise was applied to them as if they were in the
center, while they appeared in different positions visually.
    This required several changes to how the volume was rendered compared to how
it was detailed in Chapter 5. Adjusting the offset of the ray’s position left and right
by approximately a quarter of the diameter of the volume allowed us to render the
same shape in two separate positions using the same noise. This triggered each of
these sides to render at the same time when the ray would display the volume when
the ray when the offset was acceptable. This created two identical sides, a variable
and a reference object side.
    Both the variable object and the reference object were placed in a position where

                                         194
they would not extend beyond the outer red volume (Seen in Figure 7.6) and could
not intersect with each other. Each side of this volume consisted of an outer green
object identical to the other side and a smaller blue object that could be moved closer
or further away from the participant. If the small blue object and other objects had
the same noise and were in the same location, they would look identical. However,
since they are always positioned differently, they appear as objects with distinct
shapes. Regardless of their shape, these objects were consistently placed, causing
them to appear either in front of or behind the reference object.
    The overall implementation is unchanged regarding the red outer volume from
what was mentioned in Chapter 5. In the previous chapter, the color green was
chosen because the contrast between red and blue colors was not as great as that
between green and both the red and blue colors. This same logic was applied to this
study, but since there were fewer objects for the participants to consider and they
all used the same template, contrasting the green against the blue and red colors
ensured the volume was easy to view. For this study, placing the blue object inside
the green one further aided the contrast between the colors, making the blue object
stand out more.


7.3      Results
This section presents the values collected throughout this user study. All values
within this with a p-value < 0.1 have been noted. All p-values < 0.05 are considered
significant.


7.3.1     Psychometric Analysis of Depth Perception
A psychometric analysis of the participant response data was conducted to inves-
tigate the 2AFC results. The amount of time that was required to determine the
difference between the depths of the objects and the baseline used a linear mixed
model and their details surrounding how much the participants’ heads, hands, and
eyes moved during this study. This approach accounts for individual differences
between participants in repeated measures and allows us to examine the effect of
the differences between the variable object and the baseline one [251, 292].

Psychometric Analysis(H.1 & H.2)

A sigmoid function was fitted to the participant responses as shown in Figure 7.7.
This allows us to determine at which point they could not tell the difference in depth
between two virtually created volumetric objects. This method was chosen because
it provides a good approximation of human behaviour [422].

                                         195
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES

                                                        Illustrative Visual Effect       No VIRT         Halo/Outline        Hatching          Stippling

                                                           No VIRT                                                                        Halo/Outline
                    1.00


                    0.75


                    0.50
Mean of Responces




                    0.25


                    0.00
                                                           Hatching                                                                         Stippling
                    1.00


                    0.75


                    0.50


                    0.25


                    0.00
                                         −2




                                                                                                                        −2
                                                               0




                                                                                     2




                                                                                                                                               0




                                                                                                                                                           2
                                                                               Distance from Ground Truth (cm)

Figure 7.7: Four graphs which show impact to the depth perception caused by
each condition in this study’s response data, and sigmoid functions for each VIRT.
The top and bottom grey lines indicate the area where the participants could not
accurately determine the difference in depth. The cross in the center of the image
shows the exact point of zero, where participants should be unable to determine a
difference. The confidence interval (CL 95%) is shaded for each condition, and each
point on the plot represents a participant’s mean answer for this condition.


    In Figure 7.7, the distance in front of or behind the ground truth is plotted on the
horizontal axis. This value increases as the test object moves further away from the
participant than the reference object at position zero. Likewise, the value decreases
into the negative range as the object gets closer to the participant. In theory, the
further the absolute distance between the reference object and the test object, the
more easily the participants can tell if it is in front or behind.

                                  Illustrative Effect (VIRT)              No VIRT
        Halo/Outline           Hatching               Stippling
                       1.00







                      0.75




       Mean of responces




                      0.50







                      0.25







                      0.00
                                  −3           −2                  −1                0                     1                  2                     3

                                                                        Distance From Ground Truth


Figure 7.8: Magnified view of the sigmoid functions showing the offset from zero
and is the largest divergence for the JND at the 75% line.


                                                                                               196
Table 7.1: The results from the point of subjective equality (PSE) in cm across all
of the VIRTs. The standard error of the mean (SEM) and the inferior and superior
confidence interval (CI) are calculated.

           Conditions     No VIRT     Halo/Outline   Hatching Stippling
            PSE(cm)        -0.0855         -0.0127    -0.0849   -0.1353
           SEM (cm)         0.0468          0.0446     0.0447    0.0447
          CI Superior       0.0025          0.0800    0.05240   -0.0476
           CI Inferior       -0.172        -0.1055    -0.1724   -0.2231


   We computed the point of subjective equality (PSE) of all the conditions. This is
where the sigmoid function intersects at 50% between in front or behind, indicating
a position in participants where participants viewed the objects as equal. This
indicates if the object was viewed to be in front or behind where it was expected
to be. The results collected are displayed in Table 7.1. Using a pairwise T-test,
which showed a significant difference between Halo and stippling (p = 0.0303) and
hatching and stippling (p = 0.0490).
   We additionally computed the perceptual limits of the JND of all the VIRTs.
This limit was set for both 75% of the time they guessed that the object was behind
the reference object and 25% The results collected are displayed in Table 7.2. Using
a pairwise comparison, I found significance between halo and hatching p = (0.0498)

Time Required (H.3)

A Linear Mixed Effects Models (LMM) was used to evaluate the different times
required for each interaction between the different VIRTs and depths experienced
by the participants. Using each participant as their random variable will reduce any
differences in behaviours between participants unique to the participants [292]. The
time required to determine what object was in front for each task showed a significant
fixed effect between the VIRTs (χ2(3, N= 24) = 32.2482, p < 0.0001). The different
depths showed no significant fixed effect, and no significant interaction effects were
found. Post hoc pairwise comparisons were done using Tukey’s HSD for multiple
comparisons to further evaluate these findings. The post hoc pairwise comparison

Table 7.2: Results for the just noticeable difference (JND) in cm across all of
the VIRTs. The standard error of the mean (SEM) and the inferior and superior
confidence interval (CI) are calculated.

            Conditions    No VIRT     Halo/Outline   Hatching Stippling
             JND(cm)       -0.7691         -0.8506    -0.7349   -0.7613
          SEM (SEM)         0.0460          0.0473    0.04326    0.0459
           CI Superior     -0.6700          0.0800    -0.6501   -0.6712
            CI Inferior    -0.8600         -0.1055    -0.8197   -0.8513


                                         197
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES


                               Stippling                                ●
                                                                        ●●
                                                                        ●  ●
                                                                           ●
                                                                          ●●●●●
                                                                              ●●●●
                                                                                 ●
                                                                                 ●●    ●
                                                                                       ●●
                                                                                   ●●●●● ●●
                                                                                         ● ●
                                                                                           ●
                                                                                           ●●● ●
                                                                                               ●●   ●●
                                                                                                 ●● ● ●    ●●● ●●
                                                                                                           ●    ●●●● ●● ● ● ●●● ● ●●       ●●
                                                                                                                                           ● ● ●
                                                                                                                                               ●     ●       ●
                                                                                                                                                           ● ●            ●    ●           ●●
                                                                                                                                                                                           ●                  ●                                ●




                               Hatching                                              ●
                                                                                     ●●● ●
                                                                                     ●   ●●●
                                                                                           ●
                                                                                           ●    ●
                                                                                                ●●
                                                                                            ● ●●● ●
                                                                                                  ●
                                                                                                  ● ●●●●
                                                                                                   ●●      ●● ●●
                                                                                                               ●
                                                                                                               ●   ●●
                                                                                                                ●●●●  ●●
                                                                                                                     ●● ●●●
                                                                                                                          ●
                                                                                                                          ●   ●
                                                                                                                           ● ●●  ● ●● ●
                                                                                                                               ● ●    ●●● ●●●
                                                                                                                                            ●● ● ●
                                                                                                                                             ●                        ●
                                                                                                                                                                  ●●●●●       ●● ●●    ●                 ●
                                                                                                                                                                                                    ● ● ●●●       ●● ● ●
                                                                                                                                                                                                                       ● ●       ●●      ●         ●●
                                                                                                                                                                                                                                                   ●
VIRT

                          Halo/Outline                                                           ●
                                                                                                 ● ●●
                                                                                                  ●● ●
                                                                                                     ●
                                                                                                     ●● ●●●
                                                                                                          ●●●
                                                                                                            ●
                                                                                                            ●●● ●
                                                                                                              ● ●
                                                                                                                ● ●●
                                                                                                                 ●● ●
                                                                                                                    ●●● ●●
                                                                                                                         ●               ●●
                                                                                                                                ●● ●● ●● ●            ●●
                                                                                                                                                     ●●       ●   ●●      ●●●●     ● ● ●●●      ●      ●●     ●       ●●              ● ●● ●                ● ●




                               No VIRT                                            ●●
                                                                                   ●
                                                                                   ●
                                                                                   ●●●
                                                                                     ●  ●●
                                                                                      ●●●●●●
                                                                                           ●
                                                                                           ●●●●●●●●
                                                                                             ●●   ●● ●
                                                                                                  ●  ●●
                                                                                                      ●●●
                                                                                                        ●●     ●●●● ●●●●● ●●●   ●
                                                                                                                                ●●   ●●
                                                                                                                                  ● ●●      ● ●●●●    ●
                                                                                                                                                      ●
                                                                                                                                                      ●● ●●         ●
                                                                                                                                                                    ●
                                                                                                                                                                  ● ●     ●   ●       ●
                                                                                                                                                                                   ●●●●             ● ●                      ●   ●       ●     ●        ●




                                                0          5            10                                                                   15                                                                            20                                     25
                                                                                  Seconds
Figure 7.9: The mean time required for each VIRT in the depth perception study.


between VIRTs showed significant differences between Halo and no VIRT (p =
0.0047), halo and stippling (p < 0.0001), hatching and no VIRT (p = 0.0042),
hatching and stippling (p < 0.0001).

User Behavioral Analysis (H.4)

This section focuses on how participants moved when the various VIRTs were being
used, with a focus on known behaviours that exhibit embodied cognition [404, 405].
This was done by measuring the overall distance participants moved their heads,
gazes, and hands each iteration. This is then divided by the time each condition took,
allowing us to view the average speed at which participants moved. Tracking the
average velocity is important for this particular study as there was no real task to be
completed other than looking at two objects and pressing a button. A higher average


                                                                                                                                                                                                    *
Headset Velocity (cm/s)




                                                                                                                                ***
                                                     ***
                          60               ●
                                                                ●



                                                                ●
                                                                             ***                                                                                                                                                                            ●



                                                                ●
                                           ●                                                                                                                                                                                                                ●
                                           ●                                                                                                                                                                                                                ●
                                           ●                    ●
                                           ●
                                                                ●
                                                                ●                                                                                                                                                                                           ●
                                           ●                    ●
                                           ●
                                           ●                    ●                                                                                                                                                                                           ●
                                                                ●                                                                                                                                                                                           ●
                                           ●
                                           ●                                                                                                                                                                                                                ●
                                                                                                                                       ●                                                                                                                    ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●
                                           ●                                                                                           ●
                                           ●                                                                                                                                                                                                                ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●                                                                                                                                                                                                                ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●
                                           ●                                                                                                                                                                                                                ●
                                           ●                                                                                           ●                                                                                                                    ●
                                                                                                                                                                                                                                                            ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●                                                                                           ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                                                                                                                                            ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                                                                                                                                                                                                                                            ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                                                                                                                                                                                                                                            ●
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                                                                                                                                                                                                                                            ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●
                                           ●                                                                                                                                                                                                                ●
                                           ●                                                                                           ●



                          40
                                           ●
                                           ●                                                                                           ●                                                                                                                    ●
                                           ●
                                           ●                                                                                           ●
                                                                                                                                       ●                                                                                                                    ●
                                           ●                                                                                           ●                                                                                                                    ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                       ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                       ●                                                                                                                    ●
                                                                                                                                                                                                                                                            ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●
                                                                                                                                       ●




                          20


                           0
                                    No VIRT                    Halo                                         Hatching                                                                                                             Stippling
                                               Volumetric Illustrative Rendering Technqiues
Figure 7.10: Box plots showing the average head movement velocity for each
condition


                                                                      198
                                                        ●              ●                 ●
Hand Motion Velocity (cm/s)           ●
                                      ●
                                      ●
                                      ●
                                                        ●
                                                        ●
                                                        ●
                                                                       ●
                                                                       ●
                                                                       ●
                                                                                         ●
                                                                                         ●
                                                                                         ●
                                                                                         ●
                                      ●
                                      ●                 ●              ●
                                                                       ●
                                      ●                 ●
                                                        ●              ●                 ●

                                                                              **
                                      ●
                                      ●                 ●              ●                 ●
                                                                                         ●
                                      ●                 ●              ●                 ●
                                                                                         ●
                                      ●
                                      ●                 ●
                                                        ●              ●                 ●
                                                                                         ●
                                      ●
                                      ●                 ●              ●
                                                                       ●                 ●
                                      ●                 ●              ●                 ●
                                                                                         ●
                                      ●                 ●
                                                        ●              ●
                                                                       ●                 ●
                                                                                         ●
                                      ●
                                      ●                 ●              ●                 ●
                                      ●                 ●              ●
                                                                       ●                 ●
                                      ●
                                      ●                 ●              ●                 ●
                                                                                         ●
                                                        ●              ●                 ●
                                                                                         ●
                                                        ●              ●
                                                                       ●                 ●
                                                        ●
                                                        ●              ●                 ●
                                                        ●              ●
                                                                       ●
                                                        ●
                                                        ●
                                                        ●              ●
                                                                       ●
                                                        ●              ●
                                                                       ●
                                                        ●
                                                        ●              ●
                                                                       ●
                                                        ●              ●
                                                        ●
                                                        ●              ●
                                                                       ●



                              75
                                                                       ●
                                                                       ●




                              50



                              25



                               0
                                   No VIRT            Halo         Hatching        Stippling
                                          Volumetric Illustrative Rendering Technqiues
Figure 7.11: Box plots relating to the speed at which participants tended to move
their hands.


velocity would have indicated a struggle with using the visualization [404,405]. The
headset and hand distances were tracked by calculating the amount of their position
that had moved relative to the visualization between every 16ms and summing
these results. Their distance was not tracked when the hands were not in front
of the participant. Eye Gaze’s distance was measured by tracking the distance a
participant’s eye gaze would move within a 2m radius from the participant’s current
position in each frame minus the same distance the head gaze would have provided.
    Each set of analyses for this section utilizes a LMM to evaluate the different
Speeds of hand motion, eye gaze, and head gaze between the different VIRTs and
depths experienced by the participants. Using each participant as their random
variable will reduce any differences in behaviours between participants unique to
the participants [292]. Post hoc pairwise comparisons are run using Tukey’s HSD
for multiple comparisons on all significant findings found using the LMM.
    The LMM for the participants’ head motion velocity while performing the task
across all of the conditions showed a significant fixed effect between the various
VIRTs (χ2(3, N= 24) = 123.8529, p < 0.0001). The different depths showed no
fixed effect, and no interaction effect was found. The post hoc comparison between
the VIRTs showed significant differences between Halo and no VIRT (p = 0.0001),
Halo and Hatching (p < 0.0001), Halo and Stippling (p = 0.0001), Hatching and
Stippling (p = 0.0205), No VIRT and Stippling (p = 0.0492). These differences can
be seen in Figure 7.10 as they moved considerably more when they were using the
Halo VIRT than in other conditions.
    The LMM for the participants’ hand motion velocity across the conditions showed


                                                             199
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES

                             25      ●
                                     ●
                                     ●
                                     ●
                                     ●
                                     ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                       ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                        ●
                                                                                        ●
                                                                                        ●
                                                                                        ●




Eye Motion Velocity (cm/s)
                                     ●                 ●
                                                       ●                                ●
                                     ●                                  ●               ●
                                     ●                 ●                                ●
                                     ●                 ●                ●
                                                                        ●               ●
                                     ●                 ●


                                                                       ***
                                     ●                 ●
                                                       ●                ●
                                                       ●                ●               ●
                                     ●                 ●                                ●
                                     ●                                  ●               ●
                                     ●                 ●                ●
                                     ●                 ●                ●               ●
                                     ●                 ●                ●               ●
                                     ●                 ●                ●               ●
                                     ●                 ●
                                                       ●                ●               ●
                                                       ●                ●

                                               ***
                                     ●                 ●
                                                       ●                ●               ●
                                     ●                 ●
                                                       ●                ●
                                                       ●                ●
                                                                        ●               ●
                                     ●                 ●
                                                       ●                ●               ●
                                     ●                 ●                                ●
                                     ●
                                     ●                 ●                ●
                                                                        ●               ●
                                     ●
                                     ●                 ●
                                                       ●                                ●
                                                                                        ●
                                                       ●
                                                       ●
                                     ●                 ●                ●
                                                                        ●               ●
                                                                                        ●
                                     ●                 ●                ●


                                                              ***
                                                       ●
                                                       ●                ●



                             20
                                     ●                 ●                ●
                                                                        ●               ●
                                     ●                 ●
                                                       ●                                ●
                                                       ●                                ●
                                     ●
                                     ●                 ●                ●
                                                                        ●
                                     ●
                                     ●                                  ●               ●
                                     ●
                                     ●                 ●                ●               ●
                                     ●                 ●                ●               ●
                                                                                        ●
                                                       ●
                                                       ●                ●
                                                                        ●
                                     ●
                                     ●                 ●                ●               ●
                                                       ●                ●
                                                       ●
                                     ●
                                     ●                                  ●               ●
                                                                                        ●
                                     ●
                                     ●                 ●
                                                       ●                ●               ●
                                     ●                 ●
                                                       ●                                ●
                                     ●
                                     ●                 ●                ●
                                     ●                 ●
                                                       ●                ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                     ●                 ●                ●
                                                                        ●
                                                                        ●               ●
                                                                                        ●
                                     ●                 ●
                                                       ●                ●               ●
                                     ●
                                     ●                 ●
                                                       ●                ●               ●
                                     ●
                                     ●                 ●
                                                       ●
                                     ●                 ●                ●
                                                       ●                ●               ●
                                     ●
                                     ●                 ●
                                                       ●                ●
                                                                        ●               ●
                                     ●                 ●
                                                       ●                ●               ●
                                                                                        ●
                                     ●                 ●                ●               ●
                                     ●                 ●
                                                       ●                ●
                                                                        ●               ●
                                                       ●
                                                       ●                                ●
                                                                                        ●
                                     ●
                                     ●                 ●                ●               ●
                                     ●
                                     ●
                                     ●                                  ●               ●
                                                                        ●               ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●               ●
                                                                        ●               ●
                                     ●                                  ●



                             15
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                     ●                                  ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●                                  ●               ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                     ●
                                     ●                                  ●               ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●                                  ●
                                                                        ●               ●
                                     ●                                                  ●
                                                                                        ●
                                     ●                                  ●               ●
                                                                                        ●
                                     ●                                  ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                        ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●                                  ●
                                                                        ●
                                                                        ●               ●
                                                                                        ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●
                                     ●                                  ●
                                                                        ●               ●
                                                                                        ●
                                     ●                                  ●               ●
                                     ●
                                     ●
                                     ●
                                     ●                                  ●
                                                                        ●
                                     ●                                  ●
                                                                        ●
                                     ●                                  ●
                                     ●
                                     ●
                                     ●
                                     ●



                             10
                                     ●




                              5

                              0
                                  No VIRT            Halo           Hatching     Stippling
                                         Volumetric Illustrative Rendering Technqiues
Figure 7.12: The difference in velocity between the participant’s eye movements.


a significant fixed effect between the VIRTs (χ2(3, N= 24) = 23.4039, p < 0.0001),
no significant fixed effect for the different depths was found. The interaction ef-
fect showed no significant results. With the post hoc comparison between all of
the VIRTs showing No VIRT and Hatching (p < 0.0001), Stippling and Hatching
(p = 0.0062), and Halo and Hatching (p = 0.0185). These results can be seen in
Figure 7.11 where it can be seen that participants moved their hands slower when
using Hatching.
    The LMM for the velocity of the participants’ eye gaze velocity across all condi-
tions showed a significant fixed effect between the different VIRTs (χ2(3, N= 24) =
60.5829, p = 0.0001) but no significant effects were found when analyzing the dif-
ferent depths and the interaction effect. Figure 7.12 post-hoc comparison between
the VIRTs showed that participants moved their eyes significantly more when using.
Halo compared to all other VIRTs, revealing: Halo and No VIRT (p < 0.0001),
Halo and Hatching (p = 0.0001), and Halo and Stippling (p < 0.0001).

Subjective Results (H.5)

The analysis of responses to the PAAS questionnaire showed a significant differ-
ence between different illustrative visualizations using a Friedman rank sum test
(χ2(3, N = 24) = 22.109, p = 0.0001). Post-hoc analysis with pairwise Wilcoxon
signed-rank tests were conducted with a Bonferroni correction applied, and compar-
isons showed significantly increased cognitive load for Stippling when compared to
no VIRT (p < 0.0073) (see Figure 7.13).
    We tested the correlation between the PAAS Questionnaire Results to identify
if cognitive load correlated with how well participants believed they did on each


                                                            200
       Stippling
       Hatching
VIRT




                                                                           **
           Halo
       No VIRT
                   0.0            2.5              5.0               7.5
                                        PAAS Score
Figure 7.13: Results from the PAAS questionnaire for the depth perception study.


condition. I found a significantly negative correlation between both of these con-
ditions τ = −0.32 (z = -4.2966, p < 0.0001), showing that participants tended to
think they did better when they felt the task required less cognitive load regardless
of their actual performance.
    The results from the SUS questionnaire showed no significant difference between
different illustrative visualizations using a Friedman rank sum test (χ2(3, N = 24) =
5.8969, p = 0.0957) (see Figure 7.14).
    We tested the correlation between the SUS Questionnaire Results and how well
participants believed they did on each condition and found a significant relationship
between them τ = −0.22 (z = -6.0749, p < 0.0017). This showed a mismatch
between the system usability scores for each condition and how the participants felt
about the usability when looking back on all the conditions. This lower correlation
may indicate that the participants were unsure about their preferences for this task.
    Responses for the question "How easy was it to look at objects inside of other
objects using this visualization?" were analyzed using a Friedman rank sum test and
showed a significant difference between the various VIRTs (χ2(3, N= 24)= 21.44, p



       Stippling
       Hatching
VIRT




           Halo
       No VIRT
                   0               30              60               90
                                         SUS Score
  Figure 7.14: Results from the SUS questionnaire for depth perception study.


                                        201
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES


       Stippling
       Hatching
VIRT




                                                                               ***
           Halo




                                                                             **
                                                                           *
       No VIRT
                   0                     2                     4
                                     Ease of Viewing
Figure 7.15: Results for the question "How easy was it to look at objects inside of
other objects using this visualization?"


< 0.0001) (see Figure 7.15).
   Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with
a Bonferroni correction applied. Which found a significant difference when trying to
perform this task using Hatching compared to no VIRT (p = 0.0061) and Stippling
when compared to no VIRT (p = 0.0005). Figure 7.15 showed that for this task,
participants found it much more difficult to comprehend the internal structure of
the volume when using no VIRTs.


7.4       Discussion
The following sections present the findings of this study based on a comprehensive
analysis of the participant’s performance when using different VIRTs across accuracy,
time requirements, behavioral patterns, and subjective feedback. Overall, we found
that all conditions performed similarly in accuracy. However, participants noted
that Halo was more difficult to utilize inside of this particular study.


7.4.1      Psychometric Analysis (H.1 & H.2)
Participants could reliably tell if a volumetric object was in front or behind an ob-
ject when it was less than 1 cm away from the reference object, which is relatively
consistent with all VIRTs, which seem to all follow a similar curve. This exceeds
the estimates shown in H.1. Figure 7.7 and Table 7.2 shows that utilizing no VIRT
performed very similarly to both the Stippling and hatching VIRT, meaning that the
depth cue from the coming from the volume rendering was much stronger than the
effect of the partial occlusion they provided. Another possibility is that the depth
cues Hatching and Stippling provided were not good enough to overcome the draw-
back of occluding a small subsection volume, resulting in minimal improvement. In


                                        202
my opinion, while difficult to test, I assume it is for both of these reasons. Hatching
and Stippling provide a better depth cue, but the partial occlusion is not working
as well as the DVR itself. Leading to the depth cue of aerial perspective, which may
provide a more precise form of depth perception than partial occlusion.
    While the results for H.2 showed a slight improvement to the JND when hatching
or Stippling was used, I could not support either H.2. Instead, both Hatching and
Stippling conditions showed similar comparable patterns in Figure 7.7 except for a
significant difference between Stippling and halo ’s Just Noticeable Difference (JND).
Figure 7.8 shows us that participants struggled to tell the difference between the two
objects more when the variable object was behind the reference object by over 1 mm.
Participants also took significantly longer to determine the depth when answering
these questions, leading us to find that the Halo visualizations may harm depth
perception.
    Interestingly, the Halo VIRT also had the Point of Subject Equality (PSE) closest
to zero (0.0127), indicating that participants had equally no perception of where the
object was on either side of zero. The other visualizations (> 0.8mm) and especially
stippling(> 1.3mm) were considered to be in front of where the variable objects were.
A virtual object appearing slightly closer than its actual position is a common effect
in AR [31, 231]. The halo effect made it almost equally ambiguous where the center
was, as it was the least accurate most consistently. In theory, this could be because
the participant was still underestimating; it is possible that they are using the Halo
as a depth cue and placing, which would move the center of the object about 0.5mm
forward (a similar effect should happen to the other VIRTs, though). The significant
difference between the PSE between the Halo and hatching and Stippling shows that
Stippling seems to cause participants to systematically underestimate depth relative
to the Hatching and Halo VIRTs. This highlights an improvement that Halo and
Hatching both seem to improve depth perception when the objects are near the user,
but they tend to perform worse when it is far away. In contrast, Stippling performed
worse when it was closer to the participant.


7.4.2     Time Required (H.3)
Figure 7.9 shows participants took longer viewing the both halo and hatching VIRTs.
The hatching VIRT was likely due to the time how distracting participants found
it, which is further described in Subsection 7.4.4 and Chapter G whereas the Halo
seems to be due to the challenges experienced in Section 7.3.1. Highlighting the
issues participants had distinguishing the depth, which is reflected in Figure 7.8. In
summary, the findings relating to the time taken for each section indicate the nu-
anced relation between the VIRTs and how they noticeably impact depth perception


                                         203
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES

and usability.


7.4.3     User Behavioral Analysis (H.4)
Much like the previous chapter (Chapter 6), participants demonstrated a clear in-
crease in activity when they used the Halo VIRT, where they felt like they had
less control. This supports H.5 by showing the impact of embodied cognition in
regards to depth perception as participants commented that it was challenging to
perform the task in Appendix G. Generally, the impact of hand-based movement
was much lower in this study, as hand motions provided little aid for this type of
task. However, the impact of the accelerated gaze and head motion was consider-
ably accelerated when they were trying to better understand the depth of various
objects in the scene, and the accelerated movement can be seen as a side effect of
that notion [60].


7.4.4     Subjective Results (H.5)
Participants indicated in Appendix G that the stippling visualization was the most
straightforward visualization to use for this task. This goes against our original
hypothesis and shows that while participants understand the difference between the
different VIRTs, participants performed much better when using the Halo visualiza-
tion. However, for both time and accuracy (JND), hatching outperformed the halo
VIRT. This does not support H.5 as we found that participants did not prefer the
Halo VIRT when trying to perform depth perception.
     It seems that the Hatching VIRT in Section 7.3.1 was disliked because it followed
the participant around similar comments relating to slow rendering and instability
were noted inside of this study similar to Chapter 6. A similar phenomenon has
been found in previous studies on other devices, which have noted that Hatching
looked like viewing a screen in front of the image [73]. While this implementation
seemed to remove the screen door effect, having the Hatching follow the user was
still not ideal, according to the participant feedback in Appendix G. It is quite
possible that the results for Hatching could be improved if a different design for
real-time 3D Hatching was used. This would likely require a move away from the
2D artistic approach rather than using volumetric geometry relying on a surface
based geometric approach like Gerl et al. [82]’s or the purely geometrical one similar
to the tessellation X-ray Vision effect used in Chapter 3. By providing a version
of the Hatching VIRT that could remain static, finding a method to improve these
results may be possible.




                                         204
7.5     Conclusion
The Stippling VIRT was seen as the best VIRT to use for depth perception, espe-
cially when considering participant feedback. Technically, the Hatching VIRT may
have slightly outperformed Stippling, but participants spent more time working with
Hatching and found it more challenging to judge depth to use than Stippling. Over-
all, it seems that the advantage that depth perception grants when using DVR is
quite high, and this is likely too high to see much more improvement with other
depth perception effects. This is positive regarding X-ray Vision effects as this
shows that stippling and Hatching can both function as X-ray Vision techniques
for DVR without hindering depth perception below what the devices are capable of
delivering.
    Overall, this research showed that:

  • This study has showed that the Halo VIRT may be slightly detrimental to
    precision tasks;

  • While hatching and stippling perform similarly but are better suited to precise
    tasks;

  • I observed that utilizing the transparent nature of depth perception may make
    for a more precise depth cue than any of the others.




                                       205
   7. THE LIMITS OF DEPTH PERCEPTION WHEN USING
VOLUMETRIC ILLUSTRATIVE RENDERING TECHNIQUES




                        206
Chapter 8

Conclusion

This thesis adapted three new X-ray visualization techniques (Halo, Hatching, Stip-
pling) for use in Augmented Reality (AR), testing their strengths and weaknesses
and showcasing the environments in which they can be used. The research in this
dissertation shows that Volumetric Illustrative Rendering Techniques (VIRT)s pro-
vides X-ray Visualizations, which can both aid or hinder a user’s comprehension of
a volume. This chapter summarizes this thesis’s novel contributions and is followed
by potential future research and future research it has enabled.


8.1      X-ray Vision Evaluation
The field of AR enabled X-ray Vision is one of the oldest research areas in AR.
This thesis has provided literature information on collating research done in prior
fields to influence our choices regarding the research on X-ray Visualizations. Which
informed the initial study presented in Chapter 3 Plenty of studies have researched
depth perception. The initial goal of this research was to determine how X-ray
Visualizations affects a user in a more ecologically relevant scenario, leading us to
answer the following question: "What is the impact on spatial estimation when using
X-ray Vision effects who use different design methodologies?".
    This dissertation presents a study in Chapter 3 aimed at judging depth percep-
tion and how X-ray visualization affects a user’s ability to place an object accurately.
This is one of the first to allow users to view computer vision-enabled (Saliency or
Edge detection) X-ray Visualizations on Ocular See Through (OST) AR devices.
This experiment showed that X-ray Visualizations hampered spatial estimation and
that this effect grows larger as they occlude more.
    Chapter 2 stated many different X-ray effects. This dissertation focused on
allowing users to experience as much visualization as possible without obscuring
the data. This is essential because Medical Data can be dangerous and expensive.
Current procedures only allow data to work with what is currently possible. To

                                          207
      8. CONCLUSION


that end, this dissertation focused on visualizations that allowed the user to view
most of the visualizations. A study focused on auxiliary effects was conducted,
motivated by situations found in medical environments. High and low occlusion
models and different saliency techniques were compared. Four diverse visualizations
were chosen, and a back face was applied to improve depth perception.
    Being able to adapt Video See Through (VST) AR techniques to OST AR alone
may allow for many extensions of many systems in the future. This system did
allow us to utilize Computer Vision Enabled X-ray Vision techniques. While im-
provements could be made to this system, it does show that it is possible to visualize
Computer Vision Enabled X-ray Vision techniques on OST AR displays.
    To ensure this study’s ecological relevance, a placement study was performed,
which had participants accurately replicate a physical scene inside a large box. This
study showed that Auxiliary Augmentation effectively improved spatial estimation
and provided an X-ray Vision cue. These results led us to learn that while visual-
izations that occluded more provided a better sense of depth perception, they did
not, and the spatial awareness they provided was reduced. It also became clear that
the 90fps of the Microsoft HoloLens was not fast enough to provide a good user
experience for the Computer Vision-Enabled X-ray Vision Technologies. This told
us that Real-world overlays were still required and could be tailored to provide more
accuracy, but the participants’ feedback also informed us that they did not require
the whole visualization to be covered.


8.2     X-ray Visualizations for Direct Volume Ren-
        dering
The lessons learned from Chapter 3 and the lessons found in the prior literature in
Chapter 2 several requirements were found that enforced the design of the X-ray
Visualizations. Direct Volume Rendering (DVR) also required visualizations that
could take the effect of an object that utilized a ray-cast geometry rather than a
polygonal one. These constraints indicated the utility of specific VIRTs that took
inspiration from illustration techniques that could depict see-through or glass-like
objects.
   This required modifying several illustrative techniques to work with DVR and
adjusting the parameters of X-ray Vision. Resulting in the question of "How can
Volumetric Illustrative Rendering Techniques be adopted to become OST OST X-ray
Vision effects for DVR visualizations?" To answer this, the techniques of Saliency,
Hatching, and Halo were adapted to work with OST AR Head-Mounted Displays
(HMDs) see. Implementations of all these effects previously existed, but they needed


                                         208
to be extensively adapted to be displayed using DVR and ready to use for X-ray
Visualizations.


8.3      Evaluation of Volumetric Illustrative Render-
         ing Techniques
Volumetric data for quantitative evaluation makes it difficult to acquire the required
data since finding unique but repeatable data is not realistic. Furthermore, design-
ing a set of initial products to scan and use is a time-consuming and expensive task
that comes at the time of these in-demand machines. To remedy this, this thesis
contributes the design of the Random Volume Generation System, which was de-
signed to produce pseudo-random generated volumes for use for Human-Computer
Interaction (HCI) studies. These volumes were in the form of a hierarchical place-
ment of random noisy spheres but could also be replaced with any required shape
because this system was designed to be modular. This consideration was made to
allow for easy modification in many different experiments.
    By using the Random Volume Generation System, it made it possible for two
studies to be run using an OST AR HMD. One of these was focused on determin-
ing how well a person could recall information through these effects. In contrast,
the other determined the minimum threshold depth participants could reliably tell
between two volumetric objects while using am OST AR HMD.


8.3.1     Perception
Comments from the X-ray Vision user study indicated that all of the X-ray Vision
effects impair the user’s ability to view a system. This issue is one of the primary
reasons for visualizing data using DVR. This led us to ask the question, "Can an X-
ray Vision effect facilitate a user’s understanding of spatial relationships when using
Direct Volume Rendering?" To determine what VIRTs were better able to convey
the information within the volume.
    Testing how clearly users could see into the volume when a VIRT was applied
to it, I conducted a test that could view a volume. The participants’ perceptions
were then evaluated by having them perform a counting task under two different
conditions. One had participants count all of the objects that fit the criteria within
the volume, whereas the other had them count them while they were in a particular
subgroup.
    This study showed that the halo VIRT could give users a more intuitive view
of a volume. The results from this study seem to indicate that this effect will keep
being useful even with complex datasets. The ability to accurately perceive what

                                         209
      8. CONCLUSION


is inside of the volume is diminished when using Stippling and Hatching VIRTs.
However, this shows that it can improve a user’s understanding of a volume while
X-ray Vision is utilized.


8.3.2     Depth Perception
Depth perception plays an important role when interacting naturally with data
as the user needs to be able to perceive their location within it. To be able to
ensure at what range depth perception is possible, depth perception of DVR on
immersive stereoscopic HMDs, and if there is a way that VIRTs can provide an even
better quality of depth perception. This led to the following question proposed:
"Can Volumetric Illustrative Effects improve the perceived depth reported on virtual
objects in OST AR?" The following experiment in this dissertation looked at how
VIRTs affects depth perception.
    Depth perception using immersive stereoscopic HMD had not been researched
to this point. Similar research had shown that the results on immersive stereoscopic
HMDs had far greater results, leading to our study needing to focus on at what
given threshold on these devices what depth detectable and what impact VIRTs
provide. A psycho-physical experiment was utilized, consisting of a Two-alternative
forced choice (2AFC) questionnaire. By using this type of questionnaire, the lower
limit where a user could decide on depth perception when using DVR on an OST
AR display was discovered.
    This study showed that the effect on VIRT was very small (< 2mm), showing
that while they may not have been beneficial to DVR, they were not detrimental.
However, both the Hatching and Stippling VIRTs allowed participants to answer
faster, indicating that the sense of depth was clearer when using VIRTs. However,
it was discovered that the Halo VIRT seemed to reduce participants’ ability to
determine the depth within a volume and that users took longer to answer the
questionnaire when it was available.


8.4      Future Work
This Dissertation has covered a lot of ground when it comes to working to find ways
to integrate X-ray Vision applications utilizing DVR on OST AR devices. However,
this is likely just the start of this work on all these topics. This section first looks at
the related work that can still be done in AR enabled X-ray Vision, DVR on OST
AR displays, and then an example of work that will be done for future work.




                                           210
8.4.1     Augmented Reality Enabled X-ray Vision
AR X-ray Vision is becoming a well-explored field of research. There is a reasonable
understanding of how X-ray Visualizations work, especially on mobile devices [213,
218]. Previous literature showed what techniques were required to provide a user
with the perceived ability to look through an object in AR [171]. Several open
research challenges with XRV seek to find methods to improve its presentation’s
precision, accuracy, and depth perception.
    Recent research in the space has focused on XRV performance and our natural
vision. This includes challenges like improving depth and spatial perception [36,182]
and the creation of new interaction techniques so people are better able to use XRV
to support everyday work tasks and recreational activities [176].
    Research on how to visualize other types of non-camera data, such as radar and
sonar, will likely be important in the future. This thesis has primarily looked at ways
to visualize non-photo data like Magnetic Resonance Imaging (MRI) and Computed
Tomography (CT) scans behind X-ray Visualizations. There are methods for viewing
3D Sonar and Radar [301, 423, 424] that can be used to view things through walls
and the surroundings of objects and could be translated to interfaces for summaries
and other naval vessels. Moreover, the recent findings of being able to visualize
the interference from WIFI signals to allow for X-ray Vision become a household
utility [298].
    The systemic literature review in Chapter 2 noted that there had been quite a
lack of work on X-ray Vision effects. This dissertation has tried to integrate the
impact of having more than one item in the visible scene (Auxiliary Augmentation).
Still, it may be fair to expect that using a combination of X-ray Visualizations
and other depth cues may provide much stronger effects for OST AR displays.
One that this thesis did not investigate in this thesis was vergence-based AR; this
requires much better eye tracking where it is possible to track the vergence and
accommodation of the user’s eyes, allowing users to control the AR space naturally.


8.4.2     Direct Volume Rendering Displayed Using Ocular See
          Through Augmented Reality
This work used synthetic data to create generalizable results across many volume
data forms. They are designed to look like CT or MRI scans, but there is a real
chance that a sparse dataset like an angiogram might produce different results.
Collecting this data may require using deep learning to generate a set of ventricles
within a space that conforms to a set of given parameters [425]. However, more work
is still required to ensure that these models can create models to a set of parameters
to allow for a controlled study.

                                         211
     8. CONCLUSION


     This thesis also did not investigate the effects of applying these effects to medical
data to see if there was any real change. In the future, I would prefer to use a set
of participants with a more diverse skill set. While this would be difficult to make
into a controlled study, stable diffusion may offer a method to provide large enough
datasets to enable a more controlled study than was previously possible [426].
     The work in this thesis marks the beginning of work in this field, and there is
a plethora of studies possible with this technology, including: Introducing static
noise [427], 4D objects [316], and more complicated data would all be interesting
areas for exploration in this space and could greatly impact the ways it is possible
to visualize real-world data.Different styles of studies, like density observations or
trying to analyze small imperfections using these visualizations, could also make for
interesting research moving forward [130]. While also looking into rendering these
effects on different displays to more intuitive methods of viewing this data [29, 30].
     A major limitation of this work is that the volumes that are used are solid and
have properties similar to those of a cell or a medical scan. There is a real chance that
a sparse dataset like an angiogram might produce different results. The acquisition
of larger datasets for research use could be generated using deep learning to create
a set of ventricles within a space that conforms to a set of given parameters [425].
However, more work is still required to ensure that these models can create models
to a set of parameters to allow for a controlled study.
     More research investigating the effects of applying VIRTs to medical data is
still required to investigate if there was any real change between the artificial noisy
spheres and real data. This could be arranged moving forward by recruiting partici-
pants with a more diverse skill set. This would be difficult to make into a controlled
study, so an empirical evaluation may be necessary. If a controlled study is required,
stable diffusion may offer a method to provide large enough datasets to be able to
enable a more controlled study than was previously possible [426].
     This paper discusses the limitations of viewing volumetric data in augmented
reality without utilizing real-world counterparts to determine the accuracy possible
when viewing volumetric data on an OST AR device. The next step that will be
needed to be taken for this research would be to start working with studies like
blind reaching and perceptual matching tasks to determine further what accuracy
is possible when interacting with these volume renderings in the real world [31].
     Introducing static noise [427], 4D objects [316], and more complicated data would
all be interesting areas for exploration in this space. Different styles of studies, like
density observations or trying to analyze small imperfections using these visualiza-
tions, could also make for interesting research moving forward [130]. While also
looking into rendering these effects on different displays to more intuitive methods
of viewing this data [29, 30]. A plethora of research can still be done in this space.


                                           212
Figure 8.1: A prototype of a new form of accommodation-convergence based X-ray
effect designed for OST AR devices. This image shows the effect that are focusing on
the objects within the x-ray field., whereas the outside will grow more transparent
and the inside will allow for a greater focus.


8.4.3     Further Evaluations
To continue the research proposed in this thesis, I propose investigating if it is pos-
sible to incorporate using eye gazes to select aspects of an X-ray Visualization while
also distorting the binocular distortion, making the foreground and background
lighter to create a similar effect as focusing on an object located in a fog. This
would enable the system to adapt the volumetric instances to the face they are
looking at and lock them to the correct position of the user. This system would
also need to be able to estimate the users’ bones. Further work in this type of
collaboration is essential.
    This is based on previous research based on Zannoli et al. [66], who looked at the
benefits of blurring the foreground plane to enhance the viewer’s depth perception.
This technique resembles the effect of Accommodation and Convergence, so it is
possible to improve depth perception further by mimicking the effect. By utilizing
the equations laid out in their research, I hypothesize it is possible to extend the
work done by Kitajima et al. [184] to create a vergence-based visualization and by
utilizing similar interaction as mentioned in Jing et al. [428] research it should be
possible to develop VIRTs further to take advantage of the weakness of X-ray Vision.
    This type of X-ray Vision would utilize VIRTs when the user was not focused on
it but would then dissolve them to allow the user to navigate through the volume.
This utilizes the vergence accommodation definancy (described in Chapter 4) with
OST AR HMDs.
    I have developed a prototype of this system, which works within a controlled
environment. Figure 8.1 shows that this system uses the tesselation model and only
utilizes a couple of shapes. However, early tests have shown some promise using this


                                         213
      8. CONCLUSION


technique. However, this would need to be made to work both with the randomly
generating spheres and with CT and MRI data sets.
    Moving forward, this technique should be expanded to work within a volumetric
environment, and its impacts should be reviewed by user studies. This research will
enable us to understand how accommodation convergences can be simulated based
on artifacts in space or if they need to be represented more closely with real-life
phenomena. It could also look at verifying the hypothesis initially stated by Zannoli
et al. [66].


8.5     Final Remarks
This thesis set out to determine how X-ray Vision on OST AR devices should be
created and use that information to develop new ones. The systematic literature
review in Chapter 2 and the user study comparing different X-ray Visualizations in
Chapter 3. This required learning to calibrate a user’s sight to the display itself,
allowing the visualization to overlap the real world. Real-time DVR were tested
and made for different stereoscopic displays, and then the lessons from comparing
different X-ray Visualizations was utilized to create the three VIRTs (Halo, Hatching,
and Stippling). These VIRTs were then tested the participant’s ability to determine
information from within the volume and the ability to locate a specific location
within the volume. Moving beyond this research, utilizing and combining VIRTs to
find a method that enables users to convey information from the volume while still
understanding the information regarding the depth perception of the object they
are looking inside.




                                         214
Bibliography

[1] H. B. Nguyen, T. Q. Thai, S. Saitoh, B. Wu, Y. Saitoh, S. Shimo, H. Fujitani,
    H. Otobe, and N. Ohno, “Conductive resins improve charging and resolution of
    acquired images in electron microscopic volume imaging,” Scientific Reports,
    vol. 6, no. March, pp. 1–10, 2016.

[2] D. S. Goodsell, I. S. Mian, and A. J. Olson, “Rendering volumetric data in
    molecular systems,” Journal of Molecular Graphics, vol. 7, no. 1, pp. 41–47,
    1989.

[3] D. Mathiesen, T. Myers, I. Atkinson, and J. Trevathan, “Geological visual-
    isation with augmented reality,” Proceedings of the 2012 15th International
    Conference on Network-Based Information Systems, NBIS 2012, no. Septem-
    ber, pp. 172–179, 2012.

[4] L. Liu, D. Silver, K. Bemis, D. Kang, and E. Curchitser, “Illustrative Visual-
    ization of Mesoscale Ocean Eddies,” Computer Graphics Forum, vol. 36, no. 3,
    pp. 447–458, 2017.

[5] W. Hibbard L., “4-D Display of Meterological Data,” Interactive 3D Graphics,
    pp. 23–36, 1986.

[6] O. Oren, B. J. Gersh, and D. L. Bhatt, “Artificial intelligence in
    medical imaging: switching from radiographic pathological data to clinically
    meaningful endpoints,” The Lancet Digital Health, vol. 2, no. 9, pp. e486–e488,
    2020. [Online]. Available: http://dx.doi.org/10.1016/S2589-7500(20)30160-6

[7] A. Hosny, C. Parmar, J. Quackenbush, L. H. Schwartz, and
    H. J. W. L. Aerts, “Artificial intelligence in radiology,” Na-
    ture reviews. Cancer, vol. 18, no. 8, pp. 500–510, aug 2018.
    [Online]. Available: https://www.ncbi.nlm.nih.gov/pubmed/29777175https:
    //www.ncbi.nlm.nih.gov/pmc/articles/PMC6268174/

[8] J. Jurgaitis, M. Paškonis, J. Pivoriunas, I. Martinaityte, A. Juška, R. Jurgai-
    tiene, A. Samuilis, I. Volf, M. Schöbinger, P. Schemmer, T. W. Kraus, and


                                      215
    BIBLIOGRAPHY


    K. Strupas, “The comparison of 2-dimensional with 3-dimensional hepatic vi-
    sualization in the clinical hepatic anatomy education,” Medicina, vol. 44, no. 6,
    pp. 428–438, 2008.

 [9] V. B. H. Mandalika, A. I. Chernoglazov, M. Billinghurst, C. Bartneck,
     M. A. Hurrell, N. Ruiter, A. P. Butler, and P. H. Butler, “A
     Hybrid 2D/3D User Interface for Radiological Diagnosis,” Journal of
     Digital Imaging, vol. 31, no. 1, pp. 56–73, 2018. [Online]. Available:
     http://dx.doi.org/10.1007/s10278-017-0002-6

[10] M. Mast, I. Kaup, S. Krüger, C. Ullrich, R. Schneider, and S. Bay, “Exploring
     the Benefits of Holographic Mixed Reality for Preoperative Planning with 3D
     Medical Images,” Mensch und Computer 2019 - Workshopband, pp. 590–594,
     2019.

[11] V. Dicken, J. M. Kuhnigk, L. Bornemann, S. Zidowitz, S. Krass, and H. O.
     Peitgen, “Novel CT data analysis and visualization techniques for risk assess-
     ment and planning of thoracic surgery in oncology patients,” International
     Congress Series, vol. 1281, pp. 783–787, 2005.

[12] C. Rieder, M. Schwier, A. Weihusen, S. Zidowitz, and H.-O. Peitgen, “Visu-
     alization of risk structures for interactive planning of image guided radiofre-
     quency ablation of liver tumors,” Medical Imaging 2009: Visualization, Image-
     Guided Procedures, and Modeling, vol. 7261, p. 726134, 2009.

[13] C. C. Cheung, S. M. Bridges, and G. L. Tipoe, “Why is Anatomy Difficult to
     Learn? The Implications for Undergraduate Medical Curricula,” Anatomical
     Sciences Education, vol. 14, no. 6, pp. 752–763, 2021.

[14] C. K. Abbey, M. A. Lago, and M. P. Eckstein, “Comparative observer effects
     in 2D and 3D localization tasks,” Journal of Medical Imaging, vol. 8, no. 04,
     pp. 1–17, 2021.

[15] L. Zhou, M. Fan, C. Hansen, C. R. Johnson, and D. Weiskopf, “A Review of
     Three-Dimensional Medical Image Visualization,” Health Data Science, vol.
     2022, 2022.

[16] J. P. McIntire, P. R. Havig, and E. E. Geiselman, “What is 3D good for? A
     review of human performance on stereoscopic 3D displays,” Head- and Helmet-
     Mounted Displays XVII; and Display Technologies and Applications for De-
     fense, Security, and Avionics VI, vol. 8383, no. February, p. 83830X, 2012.




                                       216
                                                               BIBLIOGRAPHY


[17] G. Ahlberg, L. Enochsson, A. G. Gallagher, L. Hedman, C. Hogman, D. A.
     McClusky III, S. Ramel, C. D. Smith, and D. Arvidsson, “Proficiency-based
     virtual reality training significantly reduces the error rate for residents
     during their first 10 laparoscopic cholecystectomies,” The American Journal
     of Surgery, vol. 193, no. 6, pp. 797–804, jun 2007. [Online]. Available:
     https://doi.org/10.1016/j.amjsurg.2006.06.050

[18] G. Zhang, X.-j. Zhou, C.-z. Zhu, Q. Dong, and L. Su, “Usefulness of Three-
     dimensional(3D) simulation software in hepatectomy for pediatric hepatoblas-
     toma,” Surgical Oncology, vol. 25, 2016.

[19] I. J. Akpan and M. Shanker, “A comparative evaluation of the effectiveness
     of virtual reality, 3D visualization and 2D visual interactive simulation: an
     exploratory meta-analysis,” Simulation, vol. 95, no. 2, pp. 145–170, 2019.

[20] M. Vetter, P. Hassenpflug, M. Thorn, C. Cardenas, L. Grenacher, G. M.
     Richter, W. Lamade, C. Herfarth, and H.-P. Meinzer, “Superiority of au-
     tostereoscopic visualization for image-guided navigation in liver surgery,” Med-
     ical Imaging 2002: Visualization, Image-Guided Procedures, and Display, vol.
     4681, no. May 2002, pp. 196–203, 2002.

[21] L. Merino, A. Bergel, and O. Nierstrasz, “Overcoming Issues of 3D Software
     Visualization through Immersive Augmented Reality,” Proceedings - 6th IEEE
     Working Conference on Software Visualization, VISSOFT 2018, pp. 54–64,
     2018.

[22] H. Cecotti, M. Callaghan, B. Foucher, and S. Joslain, “Serious Game for Med-
     ical Imaging in Fully Immersive Virtual Reality,” TALE 2021 - IEEE Inter-
     national Conference on Engineering, Technology and Education, Proceedings,
     pp. 615–621, 2021.

[23] Z. Asadi, M. Asadi, N. Kazemipour, É. Léger, and M. Kersten-Oertel, “A
     decade of progress: bringing mixed reality image-guided surgery systems in
     the operating room,” Computer Assisted Surgery, vol. 29, no. 1, pp. –, 2024.
     [Online]. Available: https://doi.org/10.1080/24699322.2024.2355897

[24] G. Jha, L. shm Sharma, and S. Gupta, Future of Augmented Reality in
     Healthcare Department. Springer Singapore, 2021, vol. 203 LNNS. [Online].
     Available: http://dx.doi.org/10.1007/978-981-16-0733-2_47

[25] R. Beams, E. Brown, W. C. Cheng, J. S. Joyner, A. S. Kim, K. Kontson,
     D. Amiras, T. Baeuerle, W. Greenleaf, R. J. Grossmann, A. Gupta,
     C. Hamilton, H. Hua, T. T. Huynh, C. Leuze, S. B. Murthi, J. Penczek,

                                        217
    BIBLIOGRAPHY


    J. Silva, B. Spiegel, A. Varshney, and A. Badano, “Evaluation Challenges
    for the Application of Extended Reality Devices in Medicine,” Journal of
    Digital Imaging, vol. 35, no. 5, pp. 1409–1418, 2022. [Online]. Available:
    https://doi.org/10.1007/s10278-022-00622-x

[26] J. Rosen, B. Hannaford, and R. M. Satava, “Surgical robotics: Systems ap-
     plications and visions,” Surgical Robotics: Systems Applications and Visions,
     pp. 1–819, 2011.

[27] C. Bichlmeier, T. Sielhorst, S. M. Heining, and N. Navab, “Improving depth
     perception in medical AR a virtual vision panel to the inside of the patient,”
     Informatik aktuell, pp. 217–221, 2007.

[28] T. Sielhorst, C. Bichlmeier, S. M. Heining, and N. Navab, “Depth perception
     - A major issue in medical AR: Evaluation study by twenty surgeons,” Lecture
     Notes in Computer Science (including subseries Lecture Notes in Artificial
     Intelligence and Lecture Notes in Bioinformatics), vol. 4190 LNCS, pp. 364–
     372, 2006.

[29] J. Geng, “Three-dimensional display technologies,” Advance Optical Photon-
     ics, pp. 456–535, 2013.

[30] J. Xiong, E. L. Hsiang, Z. He, T. Zhan, and S. T. Wu, “Augmented reality
     and virtual reality displays: emerging technologies and future perspectives,”
     Light: Science and Applications, vol. 10, no. 1, pp. 1–30, 2021.

[31] F. E. Jamiy and R. Marsh, “Distance Estimation In Virtual Reality And
     Augmented Reality: A Survey,” in 2019 IEEE International Conference on
     Electro Information Technology (EIT), 2019, pp. 63–68.

[32] C. S. Rosales, G. Pointon, H. Adams, J. Stefanucci, S. Creem-Regehr, W. B.
     Thompson, and B. Bodenheimer, “Distance judgments to on- and off-ground
     objects in augmented reality,” 26th IEEE Conference on Virtual Reality and
     3D User Interfaces, VR 2019 - Proceedings, pp. 237–243, 2019.

[33] M. Al-Kalbani, M. Frutos-Pascual, and I. Williams, “Virtual object grasping
     in augmented reality: Drop shadows for improved interaction,” 2019 11th In-
     ternational Conference on Virtual Worlds and Games for Serious Applications,
     VS-Games 2019 - Proceedings, p. 1DUUMY, 2019.

[34] C. Armbrüster, M. Wolter, T. Kuhlen, W. Spijkers, and B. Fimm, “Depth
     perception in virtual reality: Distance estimations in peri- and extrapersonal
     space,” Cyberpsychology and Behavior, vol. 11, no. 1, pp. 9–15, 2008.


                                       218
                                                            BIBLIOGRAPHY


[35] V. Krevelen, Ric and Poelman Ronald, “A Survey of Augmented Reality
     Technologies, Applications, and Limitations,” International journal of
     virtual reality, vol. 9, no. 2, pp. 10–20, 2010. [Online]. Available:
     http://www.arvika.de/

[36] A. Martin-Gomez, J. Weiss, A. Keller, U. Eck, D. Roth, and N. Navab, “The
     Impact of Focus and Context Visualization Techniques on Depth Perception
     in Optical See-Through Head-Mounted Displays,” IEEE Transactions on Vi-
     sualization and Computer Graphics, vol. XX, no. X, pp. 1–16, 2021.

[37] M. Bajura, H. Fuchs, and R. Ohbuchi, “Merging virtual objects with the real
     world: seeing ultrasound imagery within the patient,” Computer Graphics
     (ACM), vol. 26, no. 2, pp. 203–210, 1992.

[38] B. Avery, C. Sandor, and B. H. Thomas, “Improving Spatial Perception for
     Augmented Reality X-Ray Vision,” in 2009 IEEE Virtual Reality Conference,
     2009, pp. 79–82.

[39] D. Kalkofen, E. Veas, S. Zollmann, M. Steinberger, and D. Schmalstieg,
     “Adaptive ghosted views for Augmented Reality,” 2013 IEEE International
     Symposium on Mixed and Augmented Reality, ISMAR 2013, vol. 1, no. c, pp.
     1–9, 2013.

[40] D. Parsons and K. Maccallum, “Current perspectives on augmented reality
     in medical education: Applications, affordances and limitations,” Advances in
     Medical Education and Practice, vol. 12, pp. 77–91, 2021.

[41] A. E. Kaufman, “Introduction to volume graphics,” Siggraph, vol. 99, no.
     Section 3, pp. 24–47, 1999.

[42] J. D. Kasprzak, J. Pawlowski, J. Z. Peruga, J. Kaminski, and P. Lipiec,
     “First-in-man experience with real-time holographic mixed reality display of
     three-dimensional echocardiography during structural intervention: balloon
     mitral commissurotomy,” European Heart Journal, 2019-04. [Online].
     Available: https://doi.org/10.1093/eurheartj/ehz127

[43] P. Pratt, M. Ives, G. Lawton, J. Simmons, N. Radev, L. Spyropoulou, and
     D. Amiras, “Through the HoloLens™ looking glass: augmented reality for
     extremity reconstruction surgery using 3D vascular models with perforating
     vessels,” European Radiology Experimental, vol. 2, no. 1, pp. 0–6, 2018.

[44] M. G. Hanna, I. Ahmed, J. Nine, S. Prajapati, and L. Pantanowitz, “Aug-
     mented reality technology using microsoft hololens in anatomic pathology,”


                                      219
    BIBLIOGRAPHY


    Archives of Pathology and Laboratory Medicine, vol. 142, no. 5, pp. 638–644,
    2018.

[45] V. Garciá-Vázquez, F. Von Haxthausen, S. Jäckle, C. Schumann, I. Kuhle-
     mann, J. Bouchagiar, A. C. Höfer, F. Matysiak, G. Hüttmann, J. P. Goltz,
     M. Kleemann, F. Ernst, and M. Horn, “Navigation and visualisation with
     HoloLens in endovascular aortic repair,” Innovative Surgical Sciences, vol. 3,
     no. 3, pp. 167–177, 2020.

[46] M. Unger, D. Black, N. M. Fischer, T. Neumuth, and B. Glaser,
     “Design and evaluation of an eye tracking support system for the scrub
     nurse,” The International Journal of Medical Robotics and Computer
     Assisted Surgery, vol. 15, no. 1, p. e1954, 2019. [Online]. Available:
     https://onlinelibrary.wiley.com/doi/abs/10.1002/rcs.1954

[47] A. Mewes, F. Heinrich, B. Hensen, F. Wacker, K. Lawonn, and C. Hansen,
     “Concepts for augmented reality visualisation to support needle guidance
     inside the MRI,” Healthcare technology letters, vol. 5, no. 5, pp. 172–
     176, sep 2018. [Online]. Available: https://www.ncbi.nlm.nih.gov/pubmed/
     30464849https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6222244/

[48] C. A. Agten, C. Dennler, A. B. Rosskopf, L. Jaberg, C. W. Pfirrmann, and
     M. Farshad, “Augmented Reality-Guided Lumbar Facet Joint Injections,” In-
     vestigative Radiology, vol. 53, no. 8, pp. 495–498, 2018.

[49] J. Li, Q. Li, X. Dai, J. Li, and X. Zhang, “Does pre-scanning training
     improve the image quality of children receiving magnetic resonance imaging?:
     A meta-analysis of current studies,” Medicine, vol. 98, no. 5, pp.
     e14 323–e14 323, 2019-02. [Online]. Available: https://www.ncbi.nlm.nih.gov/
     pubmed/30702613https://www.ncbi.nlm.nih.gov/pmc/PMC6380694/

[50] W. Si, X. Liao, Y. Qian, and Q. Wang, “Mixed Reality Guided Radiofrequency
     Needle Placement: A Pilot Study,” IEEE Access, vol. 6, pp. 31 493–31 502,
     2018.

[51] T. Blum, R. Stauder, E. Euler, and N. Navab, “Superman-like X-ray vision:
     Towards brain-computer interfaces for medical augmented reality,” ISMAR
     2012 - 11th IEEE International Symposium on Mixed and Augmented Reality
     2012, Science and Technology Papers, pp. 271–272, 2012.

[52] R. Booij, R. P. J. Budde, M. L. Dijkshoorn, and M. van Straten, “Accuracy
     of automated patient positioning in CT using a 3D camera for body contour


                                       220
                                                            BIBLIOGRAPHY


    detection,” European Radiology, vol. 29, no. 4, pp. 2079–2088, apr 2019.
    [Online]. Available: https://doi.org/10.1007/s00330-018-5745-z

[53] M. Kania, H. Rix, M. Fereniec, H. Zavala-Fernandez, D. Janusek, T. Mroczka,
     G. Stix, and R. Maniewski, “The effect of precordial lead displacement on ECG
     morphology,” Medical and Biological Engineering and Computing, vol. 52,
     no. 2, pp. 109–119, 2014.

[54] A. Hadjiantoni, K. Oak, S. Mengi, J. Konya, and T. Ungvari, “Is the Correct
     Anatomical Placement of the Electrocardiogram (ECG) Electrodes Essential
     to Diagnosis in the Clinical Setting: A Systematic Review,” Cardiology and
     Cardiovascular Medicine, vol. 05, no. 02, pp. 182–200, 2021.

[55] K. Petri, K. Witte, N. Bandow, P. Emmermacher, S. Masik, M. Dannenberg,
     S. Salb, L. Zhang, and G. Brunnett, Development of an Autonomous Character
     in Karate Kumite, 2018, vol. 663, no. Iacss.

[56] R. Wang, Z. Geng, Z. Zhang, R. Pei, and X. Meng, “Autostereoscopic
     augmented reality visualization for depth perception in endoscopic
     surgery,” Displays, vol. 48, pp. 50–60, 2017. [Online]. Available:
     http://dx.doi.org/10.1016/j.displa.2017.03.003

[57] M. E. C. Santos, M. Terawaki, T. Taketomi, G. Yamamoto, and H. Kato, “De-
     velopment of handheld augmented reality X-Ray for K-12 settings,” Lecture
     Notes in Educational Technology, no. 9783662444467, pp. 199–219, 2015.

[58] R. L. Kanodia, L. Linsen, and B. Hamann, “Multiple transparent material-
     enriched isosurfaces,” 13th International Conference in Central Europe on
     Computer Graphics, Visualization and Computer Vision 2005, WSCG’2005
     - In Co-operation with EUROGRAPHICS, Full Papers, no. January, pp. 23–
     30, 2005.

[59] H. Guo, X. Yuan, J. Liu, G. Shan, X. Chi, and F. Sun, “Interference mi-
     croscopy volume illustration for biomedical data,” IEEE Pacific Visualization
     Symposium 2012, PacificVis 2012 - Proceedings, vol. Ill, pp. 177–184, 2012.

[60] J. C. Vishton and P.M., “chapter Perceiving Layout and Knowing Distances
     : The Integration, Relative Potency, and Contextual Use of Different
     Information about Depth,” Perception of Space and Motion, vol. 22, no. 5,
     pp. 69–117, 1995. [Online]. Available: http://doi.apa.org/getdoi.cfm?doi=10.
     1037/0096-1523.22.5.1299




                                      221
    BIBLIOGRAPHY


[61] G. T. Fechner and W. M. Wundt, Elemente der Psychophysik. Erster Theil /
     von Gustav Theodor Fechner ; [herausgeber: W. Wundt]., 2nd ed. Leipzig:
     Breitkopf & Hartel, 1889.

[62] E. G. Boring, Sensation and perception in the history of experimental psychol-
     ogy. Oxford, England: Appleton-Century, 1942.

[63] R. L. French and G. C. Deangelis, “Scene-relative object motion biases
     depth percepts,” Scientific Reports, pp. 1–17, 2022. [Online]. Available:
     https://doi.org/10.1038/s41598-022-23219-4

[64] D. N. Lee, H. Kalmus, H. C. Longuet-Higgins, and N. S. Sutherland, “The
     optic flow field: the foundation of vision,” Philosophical Transactions of
     the Royal Society of London. B, Biological Sciences, vol. 290, no. 1038, pp.
     169–179, 1980. [Online]. Available: https://royalsocietypublishing.org/doi/
     abs/10.1098/rstb.1980.0089

[65] S. Nagata, How to reinforce perception of depth in single two-dimensional
     pictures - A comparative study of various cues for depth perception. National
     Aeronautics and Space Administration (NASA), 01 1991, pp. 527–545.

[66] M. Zannoli, G. D. Love, R. Narain, and M. S. Banks, “Blur and the perception
     of depth at occlusions,” Journal of Vision, vol. 16, no. 6, pp. 1–25, 2016.

[67] G. Berkeley, “An essay toward a new theory of vision, 1709.” in Readings in
     the history of psychology., ser. Century psychology series. East Norwalk, CT,
     US: Appleton-Century-Crofts, 1948, pp. 69–80.

[68] J. S. Watson, M. S. Banks, C. von Hofsten, and C. S. Royden, “Gravity as
     a monocular cue for perception of absolute distance and/or absolute size.”
     Perception, vol. 21, no. 1, pp. 69–76, 1992.

[69] J. Ping, B. H. Thomas, J. Baumeister, J. Guo, D. Weng, and Y. Liu, “Effects
     of shading model and opacity on depth perception in optical see-through aug-
     mented reality,” Journal of the Society for Information Display, no. May, pp.
     1–13, 2020.

[70] D. Aoi, K. Hasegawa, L. Li, Y. Sakano, and S. Tanaka, Improving depth
     perception using multiple iso-surfaces for transparent stereoscopic visualization
     of medical volume data. Springer Singapore, 2020, vol. 192. [Online].
     Available: http://dx.doi.org/10.1007/978-981-15-5852-8_6

[71] J. E. Cutting, “How the eye measures reality and virtual reality,” Virtual
     Reality, vol. 29, no. 1, pp. 27–36, 1997.

                                        222
                                                              BIBLIOGRAPHY


[72] M. Siegel and S. Nagata, “Just enough reality: comfortable 3-D viewing via
     microstereopsis,” IEEE Transactions on Circuits and Systems for Video Tech-
     nology, vol. 10, no. 3, pp. 387–396, 2000.

[73] K. Lawonn, I. Viola, B. Preim, and T. Isenberg, “A Survey of Surface-Based
     Illustrative Rendering for Visualization,” Computer Graphics Forum, vol. 37,
     no. 6, pp. 205–234, 2018.

[74] H. Gray, Anatomy. Chrysalis Books plc, 1877.

[75] V. Interrante, H. Fuchs, and S. Pizer, “Enhancing transparent skin surfaces
     with ridge and valley lines,” Proceedings of the IEEE Visualization Conference,
     pp. 52–59, 1995.

[76] V. Interrante, H. Fuchs, and S. M. Pizer, “Conveying the 3D shape of smoothly
     curving transparent surfaces via texture,” IEEE Transactions on Visualization
     and Computer Graphics, vol. 3, no. 2, pp. 98–117, 1997.

[77] V. Interrante, “Illustrating surface shape in volume data via principal
     direction-driven 3D line integral convolution,” Proceedings of the 24th Annual
     Conference on Computer Graphics and Interactive Techniques, SIGGRAPH
     1997, vol. D, pp. 109–116, 1997.

[78] A. Hertzmann and D. Zorin, “Illustrating smooth surfaces,” SIGGRAPH 2000
     - Proceedings of the 27th Annual Conference on Computer Graphics and In-
     teractive Techniques, no. Section 5, pp. 517–526, 2000.

[79] E. Praun, H. Hoppe, M. Webb, and A. Finkelstein, “Real-Time Hatching,”
     SIGGRAPH 2001, vol. 1, p. 581, 2001. [Online]. Available: https:
     //blendernpr.org/cross-hatch-shader/https://doi.org/10.1145/383259.383328

[80] R. V. Pelt, A. Vilanova, and H. V. D. Wetering, “GPU-based Particle Systems
     for Illustrative Volume Rendering,” Volume- and Point-based Graphics, vol. vi,
     pp. 2–9, 2008.

[81] K. Lawonn, T. Moench, and B. Preim, “Computer Graphics Forum - 2013 - La-
     wonn - Streamlines for Illustrative Real-Time Rendering,” Computer Graphics
     Forum, vol. 32, no. 3, pp. 312–330, 2013.

[82] M. Gerl and T. Isenberg, “Interactive Example-based Hatching,” Computers
     & Graphics, 2012.

[83] K. Lawonn, M. Luz, and C. Hansen, “Improving spatial perception of vascular
     models using supporting anchors and illustrative visualization,” Computers

                                       223
    BIBLIOGRAPHY


    and Graphics (Pergamon), vol. 63, pp. 37–49, 2017. [Online]. Available:
    http://dx.doi.org/10.1016/j.cag.2017.02.002

[84] A. Lu, J. Taylor, M. Hartner, D. S. Ebert, and C. D. Hansen,
     “Hardware-Accelerated Interactive Illustrative Stipple Drawing of Polygonal
     Objects,” Proceedings of Vision, Modeling, and Visualization 2002 (VMV
     2002, November 20–22, 2002, Erlangen, Germany), no. June 2014, pp. 61–68,
     2002. [Online]. Available: http://www.ecn.purdue.edu/purpl/level2/papers/
     stipple_VMV_2002.pdf

[85] O. M. Pastor and T. Strotthote, “Graph-based point relaxation for 3D stip-
     pling,” Proceedings of the Fifth Mexican International Conference in Computer
     Science, ENC 2004, pp. 141–150, 2004.

[86] A. Lu, C. J. Morris, D. S. Ebert, P. Rheingans, and C. Hansen, “Non-
     photorealistic volume rendering using stippling techniques,” Proceedings of
     the IEEE Visualization Conference, no. May 2013, pp. 211–218, 2002.

[87] O. E. Meruvia Pastor and T. Strothotte, “Frame-Coherent Stippling,” Pro-
     ceedings of Eurographics 2002, Short Presentations, pp. 145–152, 2002.

[88] L. Ma, J. Guo, D. M. Yan, H. Sun, and Y. Chen, “Instant Stippling on 3D
     Scenes,” Computer Graphics Forum, vol. 37, no. 7, pp. 255–266, 2018.

[89] M. T. Bui, J. Kim, and Y. Lee, “3D-look shading from contours and hatching
     strokes,” Computers and Graphics (Pergamon), vol. 51, pp. 167–176, 2015.

[90] N. A. Svakhine and D. S. Ebert, “Interactive volume illustration and feature
     halos,” Proceedings - Pacific Conference on Computer Graphics and Applica-
     tions, vol. 2003-Janua, pp. 347–354, 2003.

[91] E. B. Lum and K.-L. Ma, “Hardware-Accelerated Parallel Non-Photorealistic
     Volume Rendering,” in Association for Computing Machinery, 2002, pp.
     67–74. [Online]. Available: https://doi.org/10.1145/508530.508542

[92] K. Lawonn, “Feature Lines for Illustrating Medical Surface Models: Mathe-
     matical Background Feature Lines for Illustrating Medical Surface,” Visual-
     ization in Medicine and Life Sciences, no. January, 2015.

[93] E. Ozgur, A. Lafont, and A. Bartoli, “Visualizing In-Organ Tumors in Aug-
     mented Monocular Laparoscopy,” Adjunct Proceedings of the 2017 IEEE Inter-
     national Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017,
     pp. 46–51, 2017.


                                      224
                                                             BIBLIOGRAPHY


 [94] P. Rheingans and D. Ebert, “Volume illustration: Nonphotorealistic render-
      ing of volume models,” IEEE Transactions on Visualization and Computer
      Graphics, vol. 7, no. 3, pp. 253–264, 2001.

 [95] S. Bruckner and M. Eduard Gröller, “Enhancing depth-perception with flex-
      ible volumetric halos,” IEEE Transactions on Visualization and Computer
      Graphics, vol. 13, no. 6, pp. 1344–1351, 2007.

 [96] S. Bruckner, S. Grimm, A. Kanitsar, and M. E. Gröller, “Illustrative context-
      preserving exploration of volume data,” IEEE Transactions on Visualization
      and Computer Graphics, vol. 12, no. 6, pp. 1559–1569, 2006.

 [97] W. E. Lorensen and H. E. Cline, “Marching cubes: A high resolution 3D sur-
      face construction algorithm,” SIGGRAPH Computer Graphics, vol. 21, no. 4,
      pp. 163–169, Aug. 1987.

 [98] R. A. Drebin, L. Carpenter, and P. Hanrahan, “Volume rendering,” Proceed-
      ings of the 15th Annual Conference on Computer Graphics and Interactive
      Techniques, SIGGRAPH 1988, no. August 1988, pp. 65–74, 1988.

 [99] E. Keppel, “Approximating complex surfaces by triangulation of contour
      lines,” IBM Journal of Research and Development, vol. 19, no. 1, pp. 2–11,
      1975.

[100] H. Fuchs, Z. M. Kedem, and S. P. Uselton, “Optimal surface reconstruction
      from planar contours,” Commun. ACM, vol. 20, no. 10, p. 693–702, oct 1977.
      [Online]. Available: https://doi.org/10.1145/359842.359846

[101] H. N. Christiansen and T. W. Sederberg, “Conversion of complex contour line
      definitions into polygonal element mosaics,” Proceedings of the 5th Annual
      Conference on Computer Graphics and Interactive Techniques, SIGGRAPH
      1978, pp. 187–192, 1978.

[102] G. Herman and J. Udupa, “Display of three-dimensional discrete surfaces,”
      Proceedings of the Society of Photo-Optical Instrumentation Engineers, vol.
      283, pp. 90–97, 1981.

[103] D. Meagher, “Geometric modeling using octree encoding,” Computer Graphics
      and Image Processing, vol. 19, no. 2, pp. 129–147, 1982. [Online]. Available:
      https://www.sciencedirect.com/science/article/pii/0146664X82901046

[104] B. C. Wünsche, “A toolkit for visualizing biomedical data sets,” Proceedings
      of the 1st International Conference on Computer Graphics and Interactive


                                       225
     BIBLIOGRAPHY


      Techniques in Australasia and South East Asia, GRAPHITE ’03, no. June,
      2003.

[105] B. Liu, B. Wünsche, and T. Ropinski, “Visualization by example: A construc-
      tive visual component-based interface for direct volume rendering,” GRAPP
      2010 - Proceedings of the International Conference on Computer Graphics
      Theory and Applications, pp. 254–259, 2010.

[106] D. H. DOUGLAS and T. K. PEUCKER, “Algorithms for the reduction of
      the number of points required to represent a digitized line or its caricature,”
      Cartographica, vol. 10, no. 2, pp. 112–122, 1973. [Online]. Available:
      https://doi.org/10.3138/FM57-6770-U75U-7727

[107] M. Meißner, H. Pfister, R. Westermann, and C. M. Wittenbrink, “Volume
      Visualization and Volume Rendering Techniques,” Eurographics, vol. Vi, no.
      June, 2000. [Online]. Available: http://ismusicmake.googlecode.com/svn-hi/
      trunk/rc_Paper/10.1.1.93.2014.pdf

[108] E. J. Farrell, “Color display and interactive interpretation of three-dimensional
      data,” IBM Journal of Research and Development, vol. 27, no. 4, pp. 356–366,
      1983.

[109] D. R. Ney, R. A. Drebin, and D. Magid, “Volumetric Rendering of Computed
      Tomography Data: Principles and Techniques,” IEEE Computer Graphics and
      Applications, vol. 10, no. 2, pp. 24–32, 1990.

[110] A. E. Kaufman, Volume Visualization in Medicine. Academic Press, 2000,
      vol. Vi. [Online]. Available: http://dx.doi.org/10.1016/B978-012077790-7/
      50050-3

[111] K. Engel, M. Kraus, and T. Ertl, “High-quality pre-integrated volume ren-
      dering using hardware-accelerated pixel shading,” Proceedings of the ACM
      SIGGRAPH Conference on Computer Graphics, no. WORKSHOP, pp. 9–16,
      2001.

[112] P. J. MacDougall, C. E. Henze, and A. Volkov, “Volume-rendering on a 3D
      hyperwall: A molecular visualization platform for research, education and
      outreach,” Journal of Molecular Graphics and Modelling, vol. 70, pp. 1–6,
      2016. [Online]. Available: http://dx.doi.org/10.1016/j.jmgm.2016.09.002

[113] K. Riley, D. Ebert, C. Hansen, and J. Levit, “Visually Accurate Multi-Field
      Weather Visualization,” Proceedings of the IEEE Visualization Conference,
      pp. 279–286, 2003.


                                         226
                                                                BIBLIOGRAPHY


[114] Y. Wang, S. Zhang, B. Wan, W. He, and X. Bai, “Point cloud and visual
      feature-based tracking method for an augmented reality-aided mechanical as-
      sembly system,” International Journal of Advanced Manufacturing Technol-
      ogy, 2018.

[115] E. Okuyan, U. Güdükbay, C. Bulutay, and K. H. Heinig, “MaterialVis: Mate-
      rial visualization tool using direct volume and surface rendering techniques,”
      Journal of Molecular Graphics and Modelling, vol. 50, pp. 50–60, 2014.

[116] B. Gröger, D. Köhler, J. Vorderbrüggen, J. Troschitz, R. Kupfer, G. Meschut,
      and M. Gude, “Computed tomography investigation of the material
      structure in clinch joints in aluminium fibre-reinforced thermoplastic sheets,”
      Production Engineering, vol. 16, no. 2-3, pp. 203–212, 2022. [Online].
      Available: https://doi.org/10.1007/s11740-021-01091-x

[117] S. Grottel, M. Krone, K. Scharnowski, and T. Ertl, “Object-space ambient oc-
      clusion for molecular dynamics,” IEEE Pacific Visualization Symposium 2012,
      PacificVis 2012 - Proceedings, pp. 209–216, 2012.

[118] N. Nguyen, F. Liang, D. Engel, C. Bohak, P. Wonka, T. Ropinski, and
      I. Viola, “Differentiable Electron Microscopy Simulation: Methods and
      Applications for Visualization,” ArXiv, no. 1, pp. 1–22, 2022. [Online].
      Available: http://arxiv.org/abs/2205.04464

[119] G. S. Baker, T. E. Jordan, and J. Pardy, “An introduction to ground pene-
      trating radar (GPR),” Special Paper of the Geological Society of America, vol.
      432, pp. 1–18, 2007.

[120] B. Zehner, “On the visualization of 3D geological models and their uncer-
      tainty,” Zeitschrift der Deutschen Gesellschaft fur Geowissenschaften, vol. 172,
      no. 1, pp. 83–98, 2021.

[121] Y. Hui and W. Ng, “3D cursors for volume rendering applications,” IEEE
      Access, pp. 1243–1245, 1993.

[122] M. A. Kersten, A. J. Stewart, N. Troje, and R. Ellis, “Enhancing depth percep-
      tion in translucent volumes,” IEEE Transactions on Visualization and Com-
      puter Graphics, vol. 12, no. 5, pp. 1117–1123, 2006.

[123] K. Perlin and E. M. Hoffert, “Hypertexture,” ACM SIGGRAPH Computer
      Graphics, vol. 23, no. 3, pp. 253–262, 1989. [Online]. Available:
      htps://doi.org/10.1145/74334.74359



                                         227
     BIBLIOGRAPHY


[124] M. Kersten-Oertel, S. J. S. Chen, and D. L. Collins, “An evaluation of depth
      enhancing perceptual cues for vascular volume visualization in neurosurgery,”
      IEEE Transactions on Visualization and Computer Graphics, vol. 20, no. 3,
      pp. 391–403, 2014.

[125] A. Corcoran and J. Dingliana, “Real-time illumination for two-level volume
      rendering,” Lecture Notes in Computer Science (including subseries Lecture
      Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 7431
      LNCS, no. PART 1, pp. 544–555, 2012.

[126] B. Laha, D. A. Bowman, and J. D. Schiffbauer, “Validation of the MR Simu-
      lation Approach for Evaluating the Effects of Immersion on Visual Analysis of
      Volume Data,” IEEE TRANSACTIONS ON VISUALIZATION AND COM-
      PUTER GRAPHICS, vol. 9, no. 4, pp. 529 – 538, 2013.

[127] B. Laha, K. Sensharma, J. D. Schiffbauer, and D. A. Bowman, “Effects of
      immersion on visual analysis of volume data,” IEEE Transactions on Visual-
      ization and Computer Graphics, vol. 18, no. 4, pp. 597–606, 2012.

[128] B. Laha and D. A. Bowman, “Identifying the Benefits of Immersion in Virtual
      Reality for Volume Data Visualization,” Immersive Visualization Revisited
      Workshop of the IEEE VR conference, vol. D, no. March, pp. 1–2, 2012.

[129] B. Laha, D. A. Bowman, and J. J. Socha, “Effects of VR system fidelity on
      analyzing isosurface visualization of volume datasets,” IEEE Transactions on
      Visualization and Computer Graphics, vol. 20, no. 4, pp. 513–522, 2014.

[130] B. Laha, D. A. Bowman, D. H. Laidlaw, and J. J. Socha, “A classification of
      user tasks in visual analysis of volume data,” 2015 IEEE Scientific Visualiza-
      tion Conference, SciVis 2015 - Proceedings, vol. D, pp. 1–8, 2016.

[131] E. Shen, S. Li, X. Cai, L. Zeng, and W. Wang, “Sketch-based interactive
      visualization: a survey,” Journal of Visualization, vol. 17, no. 4, pp. 275–294,
      2014.

[132] X.-M. Feng, L.-D. Wu, R.-H. Yu, and C. Yang, “Enhanced depth perception
      grid-projection algorithm for direct volume rendering,” Dianzi Yu Xinxi Xue-
      bao/Journal of Electronics and Information Technology, vol. 37, no. 11, pp.
      2548 – 2554, 2015.

[133] A. V. Grosset, M. Schott, G. P. Bonneau, and C. D. Hansen, “Evaluation
      of depth of field for depth perception in DVR,” IEEE Pacific Visualization
      Symposium, pp. 81–88, 2013.


                                         228
                                                                BIBLIOGRAPHY


[134] J. P. Roberts, T. R. Fisher, M. J. Trowbridge, and C. Bent, “A design thinking
      framework for healthcare management and innovation,” Healthcare, vol. 4,
      no. 1, pp. 11–14, 2016.

[135] R. Englund and T. Ropinski, “Evaluating the perception of semi-transparent
      structures in direct volume rendering techniques,” SA 2016 - SIGGRAPH
      ASIA 2016 Symposium on Visualization, 2016.

[136] ——, “Quantitative and Qualitative Analysis of the Perception of Semi-
      Transparent Structures in Direct Volume Rendering,” Computer Graphics Fo-
      rum, vol. 37, no. 6, pp. 174–187, 2018.

[137] P. Milgram and F. Kishimo, “A taxonomy of mixed reality,” IEICE Transac-
      tions on Information and Systems, vol. 77, no. 12, pp. 1321–1329, 1994.

[138] I. E. Sutherland, “A Head-Mounted Three Dimentional Display,” Proceedings
      of the AFIPS Fall Joint Computer Conference, pp. 295–302, 1968.

[139] O. Baus and S. Bouchard, “Moving from virtual reality exposure-based therapy
      to augmented reality exposure-based therapy: A review,” Frontiers in Human
      Neuroscience, vol. 8, no. MAR, pp. 1–15, 2014.

[140] S. M. Pizer, “Systems for 3D Display in Medical Imaging,” Pictorial Informa-
      tion Systems in Medicine, pp. 235–249, 1986.

[141] J. fang Wang, V. Chi, and H. Fuchs, “Real-time optical 3D tracker for head-
      mounted display systems,” Computer Graphics (ACM), vol. 24, no. 2, pp.
      205–215, 1990.

[142] S. Mann, “Wearable computing: A first step toward personal imaging,” Com-
      puter, vol. 30, no. 2, pp. 25–32, 1997.

[143] S. Park, S. Bokijonov, and Y. Choi, “Review of microsoft hololens applications
      over the past five years,” Applied Sciences (Switzerland), vol. 11, no. 16, 2021.

[144] A. Marto and A. Gonçalves, “Augmented Reality Games and Presence: A
      Systematic Review,” Journal of Imaging, vol. 8, no. 4, 2022.

[145] K. Kim, M. Billinghurst, G. Bruder, H. B. Duh, and G. F. Welch, “Revis-
      iting Trends in Augmented Reality Research: A Review of the 2nd Decade
      of ISMAR (2008–2017),” IEEE Transactions on Visualization and Computer
      Graphics, vol. 24, no. 11, pp. 2947–2962, 2018.




                                         229
     BIBLIOGRAPHY


[146] T. Hoang, M. Reinoso, Z. Joukhadar, F. Vetere, and D. Kelly, “Augmented
      Studio: Projection Mapping on Moving Body for Physiotherapy Education,”
      Proceedings of the 2017 CHI Conference on Human Factors in Computing
      Systems, pp. 1419–1430, 2017.

[147] M. M. Knodel, B. Lemke, M. Lampe, M. Hoffer, C. Gillmann, M. Uder,
      J. Hillengaß, G. Wittum, and T. Bäuerle, “Virtual reality in advanced
      medical immersive imaging: a workflow for introducing virtual reality
      as a supporting tool in medical imaging,” Computing and Visualization
      in Science, vol. 18, no. 6, pp. 203–212, 2018. [Online]. Available:
      https://doi.org/10.1007/s00791-018-0292-3

[148] X. Ding and Z. Li, “A review of the application of virtual reality technology
      in higher education based on Web of Science literature data as an example,”
      Frontiers in Education, vol. 7, 2022.

[149] B. Xie, H. Liu, R. Alghofaili, Y. Zhang, Y. Jiang, F. D. Lobo, C. Li, W. Li,
      H. Huang, M. Akdere, C. Mousas, and L. F. Yu, “A Review on Virtual Reality
      Skill Training Applications,” Frontiers in Virtual Reality, vol. 2, no. April, pp.
      1–19, 2021.

[150] E. A. L. Lee and K. W. Wong, “A review of using virtual reality for learning,”
      Lecture Notes in Computer Science (including subseries Lecture Notes in Ar-
      tificial Intelligence and Lecture Notes in Bioinformatics), vol. 5080 LNCS, pp.
      231–241, 2008.

[151] T. Zhan, K. Yin, J. Xiong, Z. He, and S. T. Wu, “Augmented
      Reality and Virtual Reality Displays: Perspectives and Challenges,”
      iScience, vol. 23, no. 8, p. 101397, 2020. [Online]. Available: https:
      //doi.org/10.1016/j.isci.2020.101397

[152] “Microsoft hololens: Mixed reality technology for business.” [Online].
      Available: https://www.microsoft.com/en-us/hololens

[153] A. Karambakhsh, A. Kamel, B. Sheng, P. Li, P. Yang, and D. D. Feng, “Deep
      gesture interaction for augmented anatomy learning,” International Journal
      of Information Management, vol. 45, no. March, pp. 328–336, 2019.

[154] J. Kim, J. I. Hwang, and J. Lee, “VR Color Picker: Three-Dimensional Color
      Selection Interfaces,” IEEE Access, vol. 10, pp. 65 809–65 824, 2022.

[155] M. Cordeil, A. Cunningham, T. Dwyer, B. H. Thomas, and K. Marriott,
      “ImAxes: Immersive axes as embodied affordances for interactive multivari-


                                          230
                                                              BIBLIOGRAPHY


     ate data visualisation,” UIST 2017 - Proceedings of the 30th Annual ACM
     Symposium on User Interface Software and Technology, pp. 71–83, 2017.

[156] Y. S. Tanagho, G. L. Andriole, A. G. Paradis, K. M. Madison, G. S. Sandhu,
      J. E. Varela, and B. M. Benway, “2D versus 3D visualization: Impact on
      laparoscopic proficiency using the fundamentals of laparoscopic surgery skill
      set,” Journal of Laparoendoscopic and Advanced Surgical Techniques, vol. 22,
      no. 9, pp. 865–870, 2012.

[157] T. S. Mujber, T. Szecsi, and M. S. Hashmi, “Virtual reality applications in
      manufacturing process simulation,” Journal of Materials Processing Technol-
      ogy, vol. 155-156, no. 1-3, pp. 1834–1838, 2004.

[158] H. Qu, Q. Zhu, M. Guo, and Z. Lu, “Simulation of carbon-based model
      for virtual plants as complex adaptive system,” Simulation Modelling
      Practice and Theory, vol. 18, no. 6, pp. 677–695, 2010. [Online]. Available:
      http://dx.doi.org/10.1016/j.simpat.2010.01.004

[159] N. Guo, T. Wang, B. Yang, L. Hu, H. Liu, and Y. Wang, “An online calibration
      method for Microsoft Hololens,” IEEE Access, vol. 7, pp. 101 795–101 803,
      2019.

[160] C. M. Andrews, A. B. Henry, I. M. Soriano, M. K. Southworth, and J. R.
      Silva, “Registration Techniques for Clinical Applications of Three-Dimensional
      Augmented Reality Devices,” IEEE Journal of Translational Engineering in
      Health and Medicine, vol. 9, no. November 2020, 2021.

[161] D. G. Rodrigues, A. Jain, S. R. Rick, S. Liu, P. Suresh, and N. Weibel, “Ex-
      ploring Mixed Reality in specialized surgical environments,” Conference on
      Human Factors in Computing Systems - Proceedings, vol. Part F1276, pp.
      2591–2598, 2017.

[162] G. Zari, S. Condino, F. Cutolo, and V. Ferrari, “Magic Leap 1 versus Microsoft
      HoloLens 2 for the Visualization of 3D Content Obtained from Radiological
      Images,” Sensors, vol. 23, no. 6, p. 3040, 2023.

[163] D. Mejías, I. Yeregui, R. Viola, M. Fernández, and M. Montagud,
      “Remote Rendering for Virtual Reality: performance comparison of
      multimedia frameworks and protocols,” 2025. [Online]. Available: http:
      //arxiv.org/abs/2507.00623

[164] S. Gao, J. Liu, Q. Jiang, F. Sinclair, W. Sentosa, B. Godfrey, and S. Adve,
      “XRgo: Design and Evaluation of Rendering Offload for Low-Power Extended


                                        231
     BIBLIOGRAPHY


     Reality Devices,” MMSys 2025 - Proceedings of the 16th ACM Multimedia
     Systems Conference, pp. 124–135, 2025.

[165] E. Pelanis, R. P. Kumar, D. L. Aghayan, R. Palomar, Å. A. Fretland,
      H. Brun, O. J. Elle, and B. Edwin, “Use of mixed reality for improved
      spatial understanding of liver anatomy,” Minimally Invasive Therapy and
      Allied Technologies, vol. 29, no. 3, pp. 154–160, 2020. [Online]. Available:
      https://doi.org/10.1080/13645706.2019.1616558

[166] B. Stadlinger, H. Essig, P. Schumann, H. van Waes, S. Valdec, and S. Win-
      klhofer, “Cinematic rendering in der digitalen volumentomografie: Fotoreal-
      istische 3d-rekonstruktion dentaler und maxillofazialer pathologien,” SWISS
      DENTAL JOURNAL SSO–Science and Clinical Topics, vol. 131, no. 2, pp.
      133–139, 2021.

[167] B. Stadlinger, S. Valdec, L. Wacht, H. Essig, and S. Winklhofer, “3d-cinematic
      rendering for dental and maxillofacial imaging,” Dentomaxillofacial Radiology,
      vol. 49, p. 20190249, 07 2019.

[168] T. Steffen, S. Winklhofer, F. Starz, D. Wiedemeier, U. Ahmadli, and
      B. Stadlinger, “Three-dimensional perception of cinematic rendering versus
      conventional volume rendering using CT and CBCT data of the facial
      skeleton,” Annals of Anatomy, vol. 241, p. 151905, 2022. [Online]. Available:
      https://doi.org/10.1016/j.aanat.2022.151905

[169] F. Heinrich, L. Schwenderling, M. Streuber, K. Bornemann, K. Lawonn, and
      C. Hansen, “Effects of Surface Visualizations on Depth Perception in Projec-
      tive Augmented Reality,” Proceedings of the 2021 IEEE International Confer-
      ence on Human-Machine Systems, ICHMS 2021, pp. 0–5, 2021.

[170] C. Furmanski, R. Azuma, and M. Daily, “Augmented-reality visualizations
      guided by cognition: Perceptual heuristics for combining visible and ob-
      scured information,” Proceedings - International Symposium on Mixed and
      Augmented Reality, ISMAR 2002, pp. 215–224, 2002.

[171] S. Ghasemi, “An Investigation of Using Random Dot Patterns to Achieve
      X-Ray Vision for Near-Field Applications of Stereoscopic Video Based Aug-
      mented Reality Displays,” MIT Press, 2018.

[172] C. Sandor, A. Cunningham, A. Dey, and V. V. Mattila, “An augmented reality
      X-ray system based on visual saliency,” 9th IEEE International Symposium
      on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010
      - Proceedings, pp. 27–36, 2010.

                                        232
                                                                BIBLIOGRAPHY


[173] M. Fischer, J. Rosenberg, C. Leuze, B. Hargreaves, and B. Daniel, “The Im-
      pact of Occlusion on Depth Perception at Arm ’ s Length,” IEEE Transactions
      on Visualization and Computer Graphics, vol. PP, pp. 1–9, 2023.

[174] H. Li, R. R. Corey, U. Giudice, and N. A. Giudice, “Assessment of visualization
      interfaces for assisting the development of multi-level cognitive maps,” Lecture
      Notes in Computer Science (including subseries Lecture Notes in Artificial
      Intelligence and Lecture Notes in Bioinformatics), vol. 9744, no. 1, pp. 308–
      321, 2016.

[175] A. Dey, G. Jarvis, C. Sandor, and G. Reitmayr, “Tablet versus phone: Depth
      perception in handheld augmented reality,” ISMAR 2012 - 11th IEEE In-
      ternational Symposium on Mixed and Augmented Reality 2012, Science and
      Technology Papers, no. November, pp. 187–196, 2012.

[176] Z. Wang, Y. Zhao, and F. Lu, “Gaze-Vergence-Controlled See-Through Vision
      in Augmented Reality,” IEEE Transactions on Visualization and Computer
      Graphics, vol. 28, no. 11, pp. 3843–3853, 2022.

[177] J. Wang, R. Shi, W. Zheng, W. Xie, D. Kao, and H. N. Liang, “Effect of Frame
      Rate on User Experience, Performance, and Simulator Sickness in Virtual
      Reality,” IEEE Transactions on Visualization and Computer Graphics, vol. 29,
      no. 5, pp. 2478–2488, 2023.

[178] S. Liao, Y. Zhou, and V. Popescu, “AR Interfaces for Disocclusion - A Com-
      parative Study,” Proceedings - 2023 IEEE Conference Virtual Reality and 3D
      User Interfaces, VR 2023, pp. 530–540, 2023.

[179] S. Côté and A. Mercier, “Augmentation of Road Surfaces with Subsurface
      Utility Model Projections,” in 2018 IEEE Conference on Virtual Reality and
      3D User Interfaces (VR), 2018, pp. 535–536.

[180] M. T. Eren, M. Cansoy, and S. Balcisoy, “Multi-view augmented reality for
      underground exploration,” Proceedings - IEEE Virtual Reality, pp. 117–118,
      2013.

[181] M. Z. Muthalif, D. Shojaei, and K. Khoshelham, “Resolving Perceptual Chal-
      lenges of Visualizing Underground Utilities in Mixed Reality,” International
      Archives of the Photogrammetry, Remote Sensing and Spatial Information Sci-
      ences - ISPRS Archives, vol. 48, no. 4/W4-2022, pp. 101–108, 2022.

[182] U. Gruenefeld, Y. Brück, and S. Boll, “Behind the Scenes: Comparing X-Ray
      Visualization Techniques in Head-mounted Optical See-through Augmented
      Reality,” ACM International Conference Proceeding Series, pp. 179–185, 2020.

                                         233
     BIBLIOGRAPHY


[183] M. Kytö, A. Mäkinen, J. Häkkinen, and P. Oittinen, “Improving relative depth
      judgments in augmented reality with auxiliary augmentations,” ACM Trans-
      actions on Applied Perception, vol. 10, no. 1, 2013.

[184] Y. Kitajima, S. Ikeda, and K. Sato, “[POSTER] Vergence-based AR X-ray
      vision,” Proceedings of the 2015 IEEE International Symposium on Mixed and
      Augmented Reality, ISMAR 2015, pp. 188–189, 2015.

[185] D. Dunn, Q. Dong, H. Fuchs, and P. Chakravarthula, “Mitigating vergence-
      accommodation conflict for near-eye displays via deformable beamsplitters,”
      p. 104, 2018.

[186] Y. Kameda, T. Takemasa, and Y. Ohta, “Outdoor see-through vision utilizing
      surveillance cameras,” ISMAR 2004: Proceedings of the Third IEEE and ACM
      International Symposium on Mixed and Augmented Reality, no. Ismar, pp.
      151–160, 2004.

[187] Y. Ohta, Y. Kameda, I. Kitahara, M. Hayashi, and S. Yamazaki, “See-through
      vision: A visual augmentation method for sensing-web,” Communications in
      Computer and Information Science, vol. 81 PART 2, pp. 690–699, 2010.

[188] T. Tsuda, H. Yamamoto, Y. Kameda, and Y. Ohta, “Visualization methods
      for outdoor see-through vision,” ACM International Conference Proceeding
      Series, vol. 157, pp. 62–69, 2005.

[189] J. Aaskov, G. N. Kawchuk, K. D. Hamaluik, P. Boulanger, and J. Hartvigsen,
      “X-ray vision: The accuracy and repeatability of a technology that allows
      clinicians to see spinal X-rays superimposed on a person’s back,” PeerJ, vol.
      2019, no. 2, pp. 1–11, 2019.

[190] L. T. De Paolis and V. De Luca, “Augmented visualization with depth percep-
      tion cues to improve the surgeon’s performance in minimally invasive surgery,”
      Medical and Biological Engineering and Computing, vol. 57, no. 5, pp. 995–
      1013, 2019.

[191] O. Erat, O. Pauly, S. Weidert, P. Thaller, E. Euler, W. Mutschler, N. Navab,
      and P. Fallavollita, “How a surgeon becomes superman by visualization of
      intelligently fused multi-modalities,” Medical Imaging 2013: Image-Guided
      Procedures, Robotic Interventions, and Modeling, vol. 8671, no. March 2013,
      p. 86710L, 2013.

[192] O. Pauly, A. Katouzian, A. Eslami, P. Fallavollita, and N. Navab, “Supervised
      classification for customized intraoperative augmented reality visualization,”


                                        234
                                                                BIBLIOGRAPHY


      ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented
      Reality 2012, Science and Technology Papers, pp. 311–312, 2012.

[193] S. K. Feiner and D. D. Seligmann, “Cutaways and ghosting: satisfying visibil-
      ity constraints in dynamic 3D illustrations,” The Visual Computer, vol. 8, no.
      5-6, pp. 292–302, 1992.

[194] S. Habert, J. Gardiazabal, P. Fallavollita, and N. Navab, “RGBDX: First
      design and experimental validation of a mirror-based RGBD X-ray imaging
      system,” Proceedings of the 2015 IEEE International Symposium on Mixed
      and Augmented Reality, ISMAR 2015, pp. 13–18, 2015.

[195] N. Phillips, F. A. Khan, B. Kruse, C. Bethel, and J. E. Swan, “An X-ray
      vision system for situation awareness in action space,” Proceedings - 2021
      IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and
      Workshops, VRW 2021, pp. 593–594, 2021.

[196] G. Avveduto, F. Tecchia, and H. Fuchs, “Real-world occlusion in optical see-
      through AR displays,” Proceedings of the ACM Symposium on Virtual Reality
      Software and Technology, vol. Part F1319, 2017.

[197] F. Heinrich, F. Joeres, K. Lawonn, and C. Hansen, Comparison of Projective
      Augmented Reality Concepts to Support Medical Needle Insertion. IEEE, jan
      2019.

[198] F. Heinrich, L. Schwenderling, F. Joeres, and C. Hansen, “2D versus 3D:
      A Comparison of Needle Navigation Concepts between Augmented Reality
      Display Devices,” Proceedings - 2022 IEEE Conference on Virtual Reality and
      3D User Interfaces, VR 2022, pp. 260–269, 2022.

[199] B. Avery, B. H. Thomas, and W. Piekarski, “User evaluation of see-through
      vision for mobile outdoor augmented reality,” Proceedings - 7th IEEE Interna-
      tional Symposium on Mixed and Augmented Reality 2008, ISMAR 2008, pp.
      69–72, 2008.

[200] M. Lerotic, A. J. Chung, G. Mylonas, and G. Z. Yang, “Pq-space based non-
      photorealistic rendering for augmented reality,” in Lecture Notes in Computer
      Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
      Notes in Bioinformatics), vol. 4792 LNCS, no. PART 2, 2007, pp. 102–109.

[201] O. Erat, W. A. Isop, D. Kalkofen, and D. Schmalstieg, “Drone-Augmented
      human vision: Exocentric control for drones exploring hidden areas,” IEEE
      Transactions on Visualization and Computer Graphics, vol. 24, no. 4, pp.
      1437–1446, 2018.

                                         235
     BIBLIOGRAPHY


[202] S. Zollmann, G. Schall, S. Junghanns, and G. Reitmayr, “Comprehensible and
      interactive visualizations of GIS data in augmented reality,” Lecture Notes in
      Computer Science (including subseries Lecture Notes in Artificial Intelligence
      and Lecture Notes in Bioinformatics), vol. 7431 LNCS, no. PART 1, pp. 675–
      685, 2012.

[203] H.-J. Guo and B. Prabhakaran, “HoloLens 2 Technical Evaluation as Mixed
      Reality Guide,” Lecture Notes in Computer Science, 2022. [Online]. Available:
      http://arxiv.org/abs/2207.09554

[204] M. Otsuki, Y. Kamioka, Y. Kitai, M. Kanzaki, H. Kuzuoka, and H. Uchiyama,
      “Please show me inside: Improving the depth perception using virtual mask
      in stereoscopic AR,” SIGGRAPH Asia 2015 Emerging Technologies, SA 2015,
      no. February 2018, pp. 2–5, 2015.

[205] M. Otsuki and K. Hideaki, “Effect of translucent random dot mask on depth
      perception in stereo AR environment,” Research report Human-computer in-
      teraction (HCI) (Research Report Human-Computer Interaction (HCI)), vol.
      2016, no. 2, pp. 1–8, 2016.

[206] C. Becher, S. Bottecchia, and P. Desbarats, “Projection Grid Cues: An Effi-
      cient Way to Perceive the Depths of Underground Objects in Augmented Real-
      ity,” Lecture Notes in Computer Science (including subseries Lecture Notes in
      Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 12932 LNCS,
      pp. 611–630, 2021.

[207] A. S. Johnson, J. Sanchez, A. French, and Y. Sun, “Unobtrusive augmentation
      of critical hidden structures in laparoscopy.” Studies in health technology and
      informatics, vol. 196, pp. 185–191, 2014.

[208] W. Chen, X. Luo, Z. Liang, C. Li, M. Wu, Y. Gao, and X. Jia, “A uni-
      fied framework for depth prediction from a single image and binocular stereo
      matching,” Remote Sensing, vol. 12, no. 3, pp. 1–13, 2020.

[209] M. Otsuki, P. Milgram, and R. Chellali, “Use of Random Dot Patterns
      in Achieving X-Ray Vision for Near-Field Applications of Stereoscopic
      Video-Based Augmented Reality Displays,” Presence: Teleoperators &
      Virtual Environments, vol. 26, no. 1, pp. 42–65, 2017. [Online]. Available:
      http://www.mitpressjournals.org/doi/pdf/10.1162/PRES_a_00135

[210] D. C. Rompapas, N. Sorokin, A. I. W. Lübke, T. Taketomi, G. Yamamoto,
      C. Sandor, and H. Kato, “Dynamic augmented reality X-ray on google glass,”


                                        236
                                                              BIBLIOGRAPHY


     SIGGRAPH Asia 2014 Mobile Graphics and Interactive Applications, SA
     2014, p. 2010, 2014.

[211] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Focus and Context Visual-
      ization for Medical Augmented Reality Focus and Context Visualization for
      Medical Augmented Reality,” ISMAR ’07: Proceedings of the 2007 6th IEEE
      and ACM International Symposium on Mixed and Augmented Reality, vol. 6,
      pp. 1–10, 2007.

[212] J. Chen, X. Granier, and N. Lin, “On-Line Visualization of Underground
      Structures using Context Features,” Proceedings of the ACM Symposium on
      Virtual Reality Software and Technology, VRST, pp. 167–170, 2010.

[213] A. Dey and C. Sandor, “Lessons learned: Evaluating visualizations for
      occluded objects in handheld augmented reality,” International Journal of
      Human Computer Studies, vol. 72, no. 10-11, pp. 704–716, 2014. [Online].
      Available: http://dx.doi.org/10.1016/j.ijhcs.2014.04.001

[214] R. Cong, J. Lei, H. Fu, M. M. Cheng, W. Lin, and Q. Huang, “Review of visual
      saliency detection with comprehensive information,” IEEE Transactions on
      Circuits and Systems for Video Technology, vol. 29, no. 10, pp. 2941–2959,
      2019.

[215] Ren and Malik, “Learning a classification model for segmentation,” in Pro-
      ceedings Ninth IEEE International Conference on Computer Vision, 2003, pp.
      10–17 vol.1.

[216] S. Zollmann, D. Kalkofen, E. Mendez, and G. Reitmayr, “Image-based ghost-
      ings for single layer occlusions in augmented reality,” 9th IEEE International
      Symposium on Mixed and Augmented Reality 2010: Science and Technology,
      ISMAR 2010 - Proceedings, pp. 19–26, 2010.

[217] S. Zollmann, R. Grasset, G. Reitmayr, and T. Langlotz, “Image-based X-ray
      visualization techniques for spatial understanding in outdoor augmented real-
      ity,” Proceedings of the 26th Australian Computer-Human Interaction Confer-
      ence, OzCHI 2014, pp. 194–203, 2014.

[218] M. E. C. Santos, I. de Souza Almeida, G. Yamamoto, T. Taketomi,
      C. Sandor, and H. Kato, “Exploring legibility of augmented reality X-ray,”
      Multimedia Tools and Applications, vol. 75, no. 16, pp. 9563–9585, 2016.
      [Online]. Available: http://dx.doi.org/10.1007/s11042-015-2954-1




                                        237
     BIBLIOGRAPHY


[219] S. K. Feiner, A. C. Webster, T. E. Krueger, B. MacIntyre, and
      E. J. Keller, “Architectural Anatomy,” Presence: Teleoper. Virtual
      Environ., vol. 4, no. 3, pp. 318–325, 1995-01. [Online]. Available:
      https://doi.org/10.1162/pres.1995.4.3.318

[220] M. T. Eren and S. Balcisoy, “Evaluation of X-ray visualization techniques
      for vertical depth judgments in underground exploration,” Visual Computer,
      vol. 34, no. 3, pp. 405–416, 2018.

[221] A. Dey, G. Jarvis, C. Sandor, A. Wibowo, and M. Ville-Veikko, “An Evaluation
      of Augmented Reality X-Ray Vision for Outdoor Navigation,” In Proceedings
      of International Conference on Artificial Reality and Telexistence, pp. 28–32,
      2011.

[222] L. F. Maia, W. Viana, and F. Trinta, “A real-time X-ray mobile application
      using augmented reality and google street view,” Proceedings of the ACM
      Symposium on Virtual Reality Software and Technology, VRST, vol. 02-04-
      Nove, pp. 111–119, 2016.

[223] G. Yamamoto and A. W. L, “A See-through Vision with Handheld,” Dis-
      tributed, Ambient, and Pervasive Interactions, pp. 392–399, 2014.

[224] H. J. Guo, J. Z. Bakdash, L. R. Marusich, O. E. Ashtiani, and B. Prabhakaran,
      “User Evaluation of Dynamic X-Ray Vision in Mixed Reality,” Proceedings -
      2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts
      and Workshops, VRW 2023, pp. 851–852, 2023.

[225] N. Phillips, B. Kruse, F. A. Khan, and J. E. S. Ii, Window for Law
      Enforcement Operations. Springer International Publishing, 2020. [Online].
      Available: http://dx.doi.org/10.1007/978-3-030-49695-1_40

[226] H. F. Al Janabi, A. Aydin, S. Palaneer, N. Macchione, A. Al-Jabir,
      M. S. Khan, P. Dasgupta, and K. Ahmed, “Effectiveness of the HoloLens
      mixed-reality headset in minimally invasive surgery: a simulation-based
      feasibility study,” Surgical Endoscopy, vol. 34, no. 3, pp. 1143–1149, 2020.
      [Online]. Available: https://doi.org/10.1007/s00464-019-06862-3

[227] M. Kalia, N. Navab, and T. Salcudean, “A real-time interactive augmented re-
      ality depth estimation technique for surgical robotics,” Proceedings - IEEE In-
      ternational Conference on Robotics and Automation, vol. 2019-May, pp. 8291–
      8297, 2019.




                                        238
                                                              BIBLIOGRAPHY


[228] A. Martin-Gomez, J. Weiss, A. Keller, U. Eck, D. Roth, and N. Navab, “The
      Impact of Focus and Context Visualization Techniques on Depth Perception
      in Optical See-Through Head-Mounted Displays,” IEEE Transactions on Vi-
      sualization and Computer Graphics, vol. XX, no. X, pp. 1–16, 2021.

[229] H. M. Nguyen, B. Wünsche, P. Delmas, E. Zhang, C. Lutteroth, and W. Van
      Der Mark, “High-definition texture reconstruction for 3D image-based model-
      ing,” 21st International Conference in Central Europe on Computer Graphics,
      Visualization and Computer Vision, WSCG 2013 - Full Papers Proceedings,
      pp. 39–48, 2013.

[230] M. Pereira, D. Orfeo, W. Ezequelle, D. Burns, T. Xia, and D. R. Huston,
      “Photogrammetry and augmented reality for underground infrastructure sens-
      ing, mapping and assessment,” International Conference on Smart Infrastruc-
      ture and Construction 2019, ICSIC 2019: Driving Data-Informed Decision-
      Making, vol. 2019, pp. 169–175, 2019.

[231] F. E. Jamiy and R. Marsh, “Survey on depth perception in head mounted
      displays: distance estimation in virtual reality, augmented reality, and mixed
      reality,” IET Image Processing, vol. 13, no. 5, pp. 707–712, 2019.

[232] W. C. Fan, B. Brown, and M. K. Yap, “A new stereotest: the double
      two rod test.” Ophthalmic & physiological optics : the journal of the
      British College of Ophthalmic Opticians (Optometrists), vol. 16, no. 3, pp.
      196–202, may 1996. [Online]. Available: https://www.ptonline.com/articles/
      how-to-get-better-mfi-resultshttp://www.ncbi.nlm.nih.gov/pubmed/8977882

[233] S. R. Ellis and B. M. Menges, “Localization of Virtual Objects in the Near
      Visual Field,” Human Factors, vol. 40, no. 3, pp. 415–431, sep 1998. [Online].
      Available: https://doi.org/10.1518/001872098779591278

[234] J. W. McCanless, S. R. Ellis, and B. D. Adelstein, “Localization of a Time-
      Delayed, Monocular Virtual Object Superimposed on a Real Environment,”
      Presence: Teleoperators and Virtual Environments, vol. 9, no. 1, pp. 15–24,
      2000.

[235] J. P. Rolland, C. Meyer, K. Arthur, and E. Rinalducci, “Method of Adjust-
      ments versus Method of Constant St imuli in the Quantification of Accuracy
      and Precision of Rendered Depth in Head-Mounted Displays,” Technology,
      vol. 11, no. 6, 2002.




                                        239
     BIBLIOGRAPHY


[236] G. Mather and D. R. Smith, “Combining depth cues: Effects upon accuracy
      and speed of performance in a depth-ordering task,” Vision Research, vol. 44,
      no. 6, pp. 557–562, 2004.

[237] J. Wither and T. Höllerer, “Pictorial depth cues for outdoor augmented real-
      ity,” Proceedings - International Symposium on Wearable Computers, ISWC,
      vol. 2005, pp. 92–99, 2005.

[238] A. Murgia and P. M. Sharkey, “Estimation of Distances in Virtual Environ-
      ments Using Size Constancy,” International Journal of Virtual Reality, vol. 8,
      no. 1, pp. 67–74, 2009.

[239] J. E. Swan, A. Jones, E. Kolstad, M. A. Livingston, and H. S. Smallman,
      “Egocentric depth judgments in optical, see-through augmented reality,” IEEE
      Transactions on Visualization and Computer Graphics, vol. 13, no. 3, pp. 429–
      442, 2007.

[240] J. A. Jones, J. E. Swan, G. Singh, and S. R. Ellis, “Peripheral visual informa-
      tion and its effect on distance judgments in virtual and augmented environ-
      ments,” Proceedings - APGV 2011: ACM SIGGRAPH Symposium on Applied
      Perception in Graphics and Visualization, pp. 29–36, 2011.

[241] A. Jones, J. E. Swan, G. Singh, and E. Kolstad, “The effects of virtual real-
      ity, augmented reality, and motion parallax on egocentric depth perception,”
      Proceedings - IEEE Virtual Reality, no. August, pp. 267–268, 2008.

[242] G. Singh, J. E. Swan, J. A. Jones, and S. R. Ellis, “Depth judgment tasks and
      environments in near-field augmented reality,” Proceedings - IEEE Virtual
      Reality, no. March, pp. 241–242, 2011.

[243] ——, “Depth judgments by reaching and matching in near-field augmented
      reality,” Proceedings - IEEE Virtual Reality, pp. 165–166, 2012.

[244] J. E. Swan, G. Singh, and S. R. Ellis, “Matching and Reaching Depth Judg-
      ments with Real and Augmented Reality Targets,” IEEE Transactions on
      Visualization and Computer Graphics, vol. 21, no. 11, pp. 1289–1298, 2015.

[245] D. Medeiros, M. Sousa, D. Mendes, A. Raposo, and J. Jorge,
      “Perceiving depth: Optical versus video see-through,” Proceedings of
      the ACM Symposium on Virtual Reality Software and Technology,
      VRST, vol. 02-04-Nove, pp. 237–240, 2016. [Online]. Available: https:
      //dl.acm.org/doi/10.1145/2993369.2993388{#}d9578050e1



                                        240
                                                              BIBLIOGRAPHY


[246] D. Cheng, Q. Hou, Y. Li, T. Zhang, D. Li, Y. Huang, Y. Liu, Q. Wang,
      W. Hou, T. Yang, Z. Feng, and Y. Wang, “Optical design and pupil swim
      analysis of a compact, large EPD and immersive VR head mounted display,”
      Optics Express, vol. 30, no. 5, p. 6584, 2022.

[247] G. Singh, S. R. Ellis, and J. E. Swan, “The Effect of Focal Distance, Age,
      and Brightness on Near-Field Augmented Reality Depth Matching,” IEEE
      Transactions on Visualization and Computer Graphics, vol. 26, no. 2, pp.
      1385–1398, 2018.

[248] C. Hua and J. E. Swan, “The effect of an occluder on near field depth matching
      in optical see-through augmented reality,” Proceedings - IEEE Virtual Reality,
      pp. 81–82, 2014.

[249] P. J. Edwards, L. G. Johnson, D. J. Hawkes, M. R. Fenlon, A. J. Strong, and
      M. J. Gleeson, “Clinical experience and perception in stereo augmented reality
      surgical navigation,” Lecture Notes in Computer Science (including subseries
      Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),
      vol. 3150, pp. 369–376, 2004.

[250] M. Whitlock, E. Harnner, J. R. Brubaker, S. Kane, and D. A. Szafir, “Inter-
      acting with Distant Objects in Augmented Reality,” 25th IEEE Conference
      on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings, pp. 41–48,
      2018.

[251] H. Adams, J. Stefanucci, S. Creem-Regehr, and B. Bodenheimer, “Depth Per-
      ception in Augmented Reality: The Effects of Display, Shadow, and Position,”
      Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Inter-
      faces, VR 2022, pp. 792–801, 2022.

[252] J. Ping, D. Weng, Y. Liu, and Y. Wang, “Depth perception in shuffleboard:
      Depth cues effect on depth perception in virtual and augmented reality sys-
      tem,” Journal of the Society for Information Display, vol. 28, no. 2, pp. 164–
      176, 2020.

[253] M. Fischer, C. Leuze, S. Perkins, J. Rosenberg, B. Daniel, and A. Martin-
      Gomez, “Evaluation of Different Visualization Techniques for Perception-
      Based Alignment in Medical AR,” Adjunct Proceedings of the 2020 IEEE
      International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct
      2020, pp. 45–50, 2020.

[254] M. Berning, D. Kleinert, T. Riedel, and M. Beigl, “A study of depth perception
      in hand-held augmented reality using autostereoscopic displays,” ISMAR 2014

                                        241
     BIBLIOGRAPHY


      - IEEE International Symposium on Mixed and Augmented Reality - Science
      and Technology 2014, Proceedings, pp. 93–98, 2014.

[255] J. L. Gabbard, G. M. Fitch, and H. Kim, “Behind the glass: Driver challenges
      and opportunities for AR automotive applications,” Proceedings of the IEEE,
      vol. 102, no. 2, pp. 124–136, 2014.

[256] J. P. Rolland and H. Fuchs, “Optical Versus Video See-Through Head-
      Mounted Displays in Medical Visualization,” Presence: Teleoperators and Vir-
      tual Environments, vol. 9, no. 3, pp. 287–309, 2000.

[257] N. Karlsson, E. Di Bernardo, J. Ostrowski, L. Goncalves, P. Pirjanian, and
      M. E. Munich, “The vSLAM algorithm for robust localization and mapping,”
      Proceedings - IEEE International Conference on Robotics and Automation,
      vol. 2005, no. April, pp. 24–29, 2005.

[258] G. Klein and D. Murray, “Parallel tracking and mapping for small AR
      workspaces,” 2007 6th IEEE and ACM International Symposium on Mixed
      and Augmented Reality, ISMAR, pp. 225–234, 2007.

[259] M. A. Livingston, J. E. Swan, J. L. Gabbard, T. H. Hollerer, D. Hix, S. J.
      Julier, Y. Baillot, and D. Brown, “Resolving multiple occluded layers in aug-
      mented reality,” Proceedings - 2nd IEEE and ACM International Symposium
      on Mixed and Augmented Reality, ISMAR 2003, pp. 56–65, 2003.

[260] A. Webster, S. Feiner, B. MacIntyre, W. Massie, and T. Krueger, “Augmented
      reality in architectural construction, inspection, and renovation,” Computing
      in Civil Engineering (New York), no. September 2000, pp. 913–919, 1996.

[261] G. J. Hettinga, P. J. Barendrecht, and J. Kosinka, “A comparison of GPU
      tessellation strategies for multisided patches,” European Association for Com-
      puter Graphics - 39th Annual Conference, EUROGRAPHICS 2018 - Short
      Papers, no. April, pp. 45–48, 2018.

[262] J. Flick, “Tessellation,” https://catlikecoding.com/unity/tutorials/advanced-
      rendering/tessellation/, Nov 2017, accessed: 2020-10-06.

[263] M. Tian, S. Wan, and L. Yue, “A color saliency model for salient objects
      detection in natural scenes,” Lecture Notes in Computer Science (including
      subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioin-
      formatics), vol. 5916 LNCS, pp. 240–250, 2009.

[264] G. Valensi, “System of television in colors,” pp. 0–9, 1945.


                                        242
                                                             BIBLIOGRAPHY


[265] G. Saravanan, G. Yamuna, and S. Nandhini, “Real time implementation of
      RGB to HSV/HSI/HSL and its reverse color space models,” International
      Conference on Communication and Signal Processing, ICCSP 2016, pp. 462–
      466, 2016.

[266] I. K. Y. Li, E. M. Peek, B. C. Wunshe, and C. Lutteroth, “Enhancing 3D
      Applicaitons Using Stereoscipic 3D and Motion Parallax,” Proceedings of the
      Thirteenth Australasian User Interface Conference, vol. 340, pp. 59–68, 2012.

[267] E. Peek, B. Wünsche, and C. Lutteroth, “Using integrated GPUs to perform
      image warping for HMDs,” ACM International Conference Proceeding Series,
      vol. 19-21-November-2014, pp. 172–177, 2014.

[268] I. Hamadouche, “AUGMENTED REALITY X-RAY VISION ON OPTICAL
      SEE-THROUGH HEAD-MOUNTED,” Ph.D. dissertation, University of
      Oulu, 2018. [Online]. Available: https://pdfs.semanticscholar.org/de04/
      a705f11ebdb9a7e913e5dd86914d346a9444.pdf

[269] D. Brown, “Decentering Distortion of Lenses - The Prism Effect Encountered
      in Metric Cameras can be Overcome Through Analytic Calibration,” Photo-
      metric Engineering, vol. 32, no. 3, pp. 444–462, 1966.

[270] Zhengyou Zhang, “A Flexible New Technique for Camera Calibration
      Zhengyou,” Technical Report MSR TR-98-71 Microsoft, no. last updated on
      Aug. 13, 2008, 1998.

[271] J. P. de Villiers, F. W. Leuschner, and R. Geldenhuys, “Centi-pixel accurate
      real-time inverse distortion correction,” Optomechatronic Technologies 2008,
      vol. 7266, no. 1, p. 726611, 2008.

[272] S. B. Kang, “Radial distortion snakes,” IEICE Transactions on Information
      and Systems, vol. E84-D, no. 12, pp. 1603–1611, 2001.

[273] Q. Liu, X. Sun, and Y. Peng, “A Distortion Image Correction Method for
      Wide-Angle Cameras Based on Track Visual Detection,” Photonics, vol. 12,
      no. 8, p. 767, 2025.

[274] D. C. Brown, “Close-Range Camera Calibration,” Photogramm. Eng, vol. 8,
      no. 37, pp. 855–866, 1971.

[275] R. Gruen, E. Ofek, A. Steed, R. Gal, M. Sinclair, and M. Gonzalez-Franco,
      “Measuring System Visual Latency through Cognitive Latency on Video See-
      Through AR devices,” Proceedings - 2020 IEEE Conference on Virtual Reality
      and 3D User Interfaces, VR 2020, pp. 791–799, 2020.

                                       243
     BIBLIOGRAPHY


[276] J. M. Van Waveren, “The asynchronous time warp for virtual reality on con-
      sumer hardware,” Proceedings of the ACM Symposium on Virtual Reality Soft-
      ware and Technology, VRST, vol. 02-04-Nove, pp. 37–46, 2016.

[277] M. Landvoigt and J. Cardoso, “Analysis about publications on Facebook
      pages: finding of important characteristics,” Sage Research Methods Cases,
      no. February, p. 8, 2017. [Online]. Available: http://www.kdd.org/kdd2016/
      papers/files/Paper_799.pdf

[278] S. Martinez-Conde, J. Otero-Millan, and S. L. MacKnik, “The impact of mi-
      crosaccades on vision: Towards a unified theory of saccadic function,” Nature
      Reviews Neuroscience, vol. 14, no. 2, pp. 83–96, 2013.

[279] G. Casiez, N. Roussel, and D. Vogel, “1 Euro filter: A simple speed-based
      low-pass filter for noisy input in interactive systems,” Conference on Human
      Factors in Computing Systems - Proceedings, pp. 2527–2530, 2012.

[280] M. Wilson, “Six views of embodied cognition.” Psychometric Bulletin & Re-
      view, vol. 9, no. 4, pp. 625–636, 2002.

[281] M. Kytö, A. Mäkinen, T. Tossavainen, and P. Oittinen, “Stereoscopic depth
      perception in video see-through augmented reality within action space,” Jour-
      nal of Electronic Imaging, vol. 23, no. 1, p. 11006, 2014.

[282] N. D. Bruce and J. K. Tsotsos, “Saliency, attention and visual search: An
      information theoretic approach,” Journal of Vision, vol. 9, no. 3, pp. 1–24,
      2009.

[283] L. E. van Dyck, R. Kwitt, S. J. Denzler, and W. R. Gruber, “Comparing Object
      Recognition in Humans and Deep Convolutional Neural Networks—An Eye
      Tracking Study,” Frontiers in Neuroscience, vol. 15, no. October, pp. 1–15,
      2021.

[284] F. G. Paas, “Training Strategies for Attaining Transfer of Problem-Solving
      Skill in Statistics: A Cognitive-Load Approach,” Journal of Educational Psy-
      chology, vol. 84, no. 4, pp. 429–434, 1992.

[285] O. Bimber and R. Raskar, Spatial Augmented Reality Merging Real and Virtual
      Worlds, 1st ed. A K Peters Ltd., 2004.

[286] J. B. Brooke, “Brooke1996susa,” in Usability Evaluation In Industry.    CRC
      Press, 1996.




                                       244
                                                               BIBLIOGRAPHY


[287] L. Meteyard and R. A. Davies, “Best practice guidance for linear
      mixed-effects models in psychological science,” Journal of Memory and
      Language, vol. 112, no. January, p. 104092, 2020. [Online]. Available:
      https://doi.org/10.1016/j.jml.2020.104092

[288] A. Bell, M. Fairbrother, and K. Jones, “Fixed and random effects models:
      making an informed choice,” Quality and Quantity, vol. 53, no. 2, pp. 1051–
      1074, 2019. [Online]. Available: https://doi.org/10.1007/s11135-018-0802-x

[289] H. Singmann and D. Kellen, An Introduction to Mixed Models for Experimental
      Psychology. Tailor and Frances, 2019, no. October.

[290] Y. Li and J. Baron, “Behavioral research data analysis with R,” Behavioral
      Research Data Analysis with R, no. 1973, pp. 1–245, 2012.

[291] L. M.-e. Models and B. Concepts, “Nonlinear Mixed-effects Models: Basic
      Concepts and Motivating Examples,” Mixed-Effects Models in S and S-PLUS,
      pp. 273–304, 2006.

[292] M. Kaptein, “Using Generalized Linear (Mixed) Models in HCI,” in Modern
      Statistical Methods for HCI, J. Robertson and M. Kaptein, Eds. Cham:
      Springer International Publishing, 2016, pp. 251–274. [Online]. Available:
      https://doi.org/10.1007/978-3-319-26633-6_11

[293] A. Cameron and P. Trivedi, Microeconometrics: Methods and Applications.
      Cambridge University Press, 05 2005.

[294] I. Matyash, R. Kutzner, T. Neumuth, and M. Rockstroh, “Accuracy mea-
      surement of HoloLens2 IMUs in medical environments,” Current Directions in
      Biomedical Engineering, vol. 7, no. 2, pp. 633–636, 2021.

[295] N. M. Gamage, D. Ishtaweera, M. Weigel, and A. Withana, So Predictable!
      Continuous 3D Hand Trajectory Prediction in Virtual Reality. Association
      for Computing Machinery, 2021, vol. 1, no. 1.

[296] D. Lee, M. Choi, and J. Lee, “Prediction of head movement in 360-degree
      videos using attention model,” Sensors, vol. 21, no. 11, pp. 1–22, 2021.

[297] R. Avila, T. He, L. Hong, A. Kaufman, H. Pfister, C. Silva, L. Sobierajski, and
      S. Wang, “VolVis: A diversified system for volume research and development,”
      in Proceedings Visualization ’94, oct 1994, pp. 31–38.




                                        245
     BIBLIOGRAPHY


[298] F. Adib and D. Katabi, “See through walls with WiFi!” SIGCOMM 2013 -
      Proceedings of the ACM SIGCOMM 2013 Conference on Applications, Tech-
      nologies, Architectures, and Protocols for Computer Communication, pp. 75–
      86, 2013.

[299] R. Khlebnikov, B. Kainz, M. Steinberger, and D. Schmalstieg, “Noise-based
      volume rendering for the visualization of multivariate volumetric data,” IEEE
      Transactions on Visualization and Computer Graphics, vol. 19, no. 12, pp.
      2926–2935, 2013.

[300] P. M. Maloca, J. E. R. de Carvalho, T. Heeren, P. W. Hasler, F. Mushtaq,
      M. Mon-Williams, H. P. Scholl, K. Balaskas, C. Egan, A. Tufail, L. Witthauer,
      and P. C. Cattin, “High-performance virtual reality volume rendering of orig-
      inal optical coherence tomography point-cloud data enhanced with real-time
      ray casting,” Translational Vision Science and Technology, vol. 7, no. 4, 2018.

[301] R. Van Son, S. W. Jaw, J. Yan, H. S. Khoo, W. K. Loo, S. N. Teo, and
      G. Schrotter, “A framework for reliable three-dimensional underground utility
      mapping for urban planning,” International Archives of the Photogrammetry,
      Remote Sensing and Spatial Information Sciences - ISPRS Archives, vol. 42,
      no. 4/W10, pp. 209–214, 2018.

[302] A. Alhazmi, “3D Volume Visualization in Medical Application,” Virtual Real-
      ity & Intelligent Hardware, vol. Vi, pp. 58–64, 2018.

[303] Q. Zhang, R. Eagleson, and T. M. Peters, “Volume visualization: A technical
      overview with a focus on medical applications,” Journal of Digital Imaging,
      vol. 24, no. 4, pp. 640–664, 2011.

[304] M. A. Vicente, J. Mínguez, and D. C. González, “The Use of
      Computed Tomography to Explore the Microstructure of Materials in Civil
      Engineering: From Rocks to Concrete,” in Computed Tomography, A. M.
      Halefoglu, Ed. Rijeka: IntechOpen, 2017, ch. 10. [Online]. Available:
      https://doi.org/10.5772/intechopen.69245

[305] R. Brath, “3D InfoVis is here to stay: Deal with it,” 2014 IEEE VIS Interna-
      tional Workshop on 3DVis, 3DVis 2014, pp. 25–31, 2015.

[306] T. Vernon and D. Peckham, “The benefits of 3D modelling and animation
      in medical teaching,” Journal of Visual Communication in Medicine, vol. 25,
      no. 4, pp. 142–148, 2002.




                                        246
                                                               BIBLIOGRAPHY


[307] K. S. Tang, D. L. Cheng, E. Mi, and P. B. Greenberg, “Augmented reality in
      medical education: a systematic review,” Canadian Medical Education Jour-
      nal, vol. 11, no. 1, pp. 81–96, 2019.

[308] B. H. Thomas, M. Marner, R. T. Smith, N. A. M. Elsayed, S. Von Itzstein,
      K. Klein, M. Adcock, P. Eades, A. Irlitti, J. Zucco, T. Simon, J. Baumeister,
      and T. Suthers, “Spatial augmented reality - A tool for 3D data visualization,”
      2014 IEEE VIS International Workshop on 3DVis, 3DVis 2014, no. September
      2016, pp. 45–50, 2015.

[309] R. Fischer, K. C. Chang, R. Weller, and G. Zachmann, “Volumetric Medi-
      cal Data Visualization for Collaborative VR Environments,” Lecture Notes in
      Computer Science (including subseries Lecture Notes in Artificial Intelligence
      and Lecture Notes in Bioinformatics), vol. 12499 LNCS, no. March 2021, pp.
      178–191, 2020.

[310] A. Joshi, S. Dustin, K. Vives, D. Spencer, L. Staib, and P. Xenophon, “Novel
      Interaction Techniques for Neurosurgical Planning and Stereotactic Naviga-
      tion,” IEEE Transactions on Visual Computing Graphics, vol. 23, no. 1, pp.
      1–7, 2013.

[311] L. T. De Paolis and F. Ricciardi, “Augmented visualisation in the
      treatment of the liver tumours with radiofrequency ablation,” Computer
      Methods in Biomechanics and Biomedical Engineering: Imaging and
      Visualization, vol. 6, no. 4, pp. 396–404, 2018. [Online]. Available:
      http://dx.doi.org/10.1080/21681163.2017.1287598

[312] A. KAUFMAN and K. MUELLER, “Overview of volume rendering,”
      in Visualization Handbook, C. D. Hansen and C. R. Johnson, Eds.
      Burlington: Butterworth-Heinemann, 2005, pp. 127–174. [Online]. Available:
      https://www.sciencedirect.com/science/article/pii/B9780123875822500095

[313] A. Joshi, J. Caban, P. Rheingans, and L. Sparling, “Case Study on Visual-
      izing Hurricanes Using Illustration-Inspired Techniques,” IEEE Transactions
      on Visualization and Computer Graphics, vol. 15, no. 5, pp. 709–718, 2009.

[314] T. D. DenOtter and J. Schubert, “Hounsfield Unit.” in StatPearls. StatPearls
      Publishing, jan 2024.

[315] P. A. Rinck, Magnetic Resonance in Medicine, 14th ed. The Basic Textbook
      of the European Magnetic Resonance Forum, 2024. [Online]. Available:
      https://www.magnetic-resonance.org/


                                        247
     BIBLIOGRAPHY


[316] U. W. Langner and P. J. Keall, “Prospective displacement and velocity-based
      cine 4D CT,” Medical Physics, vol. 35, no. 10, pp. 4501–4512, 2008.

[317] G. Gill and R. R. Beichel, “Lung Segmentation in 4D CT Volumes Based on
      Robust Active Shape Model Matching,” International Journal of Biomedical
      Imaging, vol. 2015, 2015.

[318] B. Liu, G. J. Clapworthy, F. Dong, and E. Wu, “Parallel Marching Blocks: A
      Practical Isosurfacing Algorithm for Large Data on Many-Core Architectures,”
      Computer Graphics Forum, vol. 35, no. 3, pp. 211–220, 2016. [Online].
      Available: https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12897

[319] W. Lorensen and H. E. Cline, “Marching Cubes: A High Resolution 3D Surface
      Construction Algorithm,” ACM SIGGRAPH Computer Graphics, vol. 21, pp.
      163–, 1987.

[320] T. S. Newman and H. Yi, “A survey of the marching cubes algorithm,” Com-
      puters and Graphics (Pergamon), vol. 30, no. 5, pp. 854–879, 2006.

[321] H. Dai, Y. Tao, X. He, and H. Lin, “IsoExplorer: an isosurface-driven
      framework for 3D shape analysis of biomedical volume data,” Journal
      of Visualization, vol. 24, no. 6, pp. 1253–1266, 2021. [Online]. Available:
      https://doi.org/10.1007/s12650-021-00770-2

[322] M. Eid, C. N. De Cecco, J. W. Nance, D. Caruso, M. H. Albrecht, A. J.
      Spandorfer, D. De Santis, A. Varga-Szemes, and U. Joseph Schoepf, “Cine-
      matic rendering in CT: A novel, lifelike 3D visualization technique,” American
      Journal of Roentgenology, vol. 209, no. 2, pp. 370–379, 2017.

[323] W. Li, K. Mueller, and A. Kaufman, “Empty Space Skipping and Occlusion
      Clipping for Texture-based Volume Rendering,” Proceedings of the IEEE Vi-
      sualization Conference, vol. 4400, no. Cvc, pp. 317–324, 2003.

[324] N. Morrical, W. Usher, I. Wald, and V. Pascucci, “Efficient Space Skipping
      and Adaptive Sampling of Unstructured Volumes Using Hardware Accelerated
      Ray Tracing,” 2019 IEEE Visualization Conference, VIS 2019, vol. 1, pp. 256–
      260, 2019.

[325] M. Hadwiger, A. K. Al-Awami, J. Beyer, M. Agus, and H. Pfister, “Sparse-
      Leap: Efficient Empty Space Skipping for Large-Scale Volume Rendering,”
      IEEE Transactions on Visualization and Computer Graphics, vol. 24, no. 1,
      pp. 974–983, 2018.



                                        248
                                                                 BIBLIOGRAPHY


[326] L. Deakin and M. Knackstedt, “Accelerated volume rendering with Chebyshev
      distance maps,” SIGGRAPH Asia 2019 Technical Briefs, SA 2019, pp. 25–28,
      2019.

[327] H. Jung, Y. Jung, and J. Kim, “Understanding the Capabilities of the
      HoloLens 1 and 2 in a Mixed Reality Environment for Direct Volume Ren-
      dering with a Ray-casting Algorithm,” Proceedings - 2022 IEEE Conference
      on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW
      2022, pp. 698–699, 2022.

[328] B. Cetinsaya, C. Neumann, and D. Reiners, “Using Direct Volume
      Rendering for Augmented Reality in Resource-constrained Platforms,”
      2022 IEEE Conference on Virtual Reality and 3D User Interfaces
      Abstracts and Workshops (VRW), pp. 768–769, 2022. [Online]. Available:
      https://ieeexplore.ieee.org/document/9757418

[329] S. Chandrasekhar, “Radiative transfer. By S. Chandrasekhar. London (Oxford
      University Press) 1950. 8vo. Pp. 393, 35 figures. 35s,” Quarterly Journal of the
      Royal Meteorological Society, vol. 76, no. 330, p. 498, 1950. [Online]. Available:
      https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49707633016

[330] J. Fong, M. Wrenninge, C. Kulla, and R. Habel, “Production volume render-
      ing SIGGRAPH 2017 course,” ACM SIGGRAPH 2017 Courses, SIGGRAPH
      2017, 2017.

[331] B. TUKORA, “Effective volume rendering on mobile and standalone vr head-
      sets by means of a hybrid method,” Pollack Periodica, vol. 15, no. 2, pp. 3–12,
      2020.

[332] T. Ropinski, C. Döring, and C. Rezk-Salama, “Interactive volumetric lighting
      simulating scattering and shadowing,” IEEE Pacific Visualization Symposium
      2010, PacificVis 2010 - Proceedings, pp. 169–176, 2010.

[333] M. Van Damme, L. Vanderstraeten, J. De Nardis, J. Haegeman, and F. Ver-
      straete, “Real-time scattering of interacting quasiparticles in quantum spin
      chains,” Physical Review Research, vol. 3, no. 1, 2021.

[334] B. Li, L. Tian, and S. Ou, “An optical model for translucent volume ren-
      dering and its implementation using the preintegrated shear-warp algorithm,”
      International Journal of Biomedical Imaging, vol. 2010, 2010.

[335] J. Kniss, S. Premoze, C. Hansen, P. Shirley, and A. McPherson, “A model
      for volume lighting and modeling,” IEEE Transactions on Visualization and
      Computer Graphics, vol. 9, no. 2, pp. 150–162, 2003.

                                          249
     BIBLIOGRAPHY


[336] B. Li, L. Tian, and S. Ou, “An optical model for translucent volume ren-
      dering and its implementation using the preintegrated shear-warp algorithm,”
      International Journal of Biomedical Imaging, vol. 2010, 2010.

[337] S. Jabłoński and T. Martyn, “Real-time voxel rendering algorithm based on
      Screen Space Billboard Voxel Buffer with Sparse Lookup Textures,” 24th In-
      ternational Conference in Central Europe on Computer Graphics, Visualiza-
      tion and Computer Vision, WSCG 2016 - Full Papers Proceedings, pp. 27–36,
      2016.

[338] B. F. Tomandl, P. Hastreiter, C. Rezk-Salama, K. Engel, T. Ertl, W. J. Huk,
      R. Naraghi, O. Ganslandt, C. Nimsky, and K. E. Eberhardt, “Local and remote
      visualization techniques for interactive direct volume rendering in neuroradi-
      ology,” Radiographics, vol. 21, no. 6, pp. 1561–1572, 2001.

[339] M. Matsui, F. Ino, and K. Hagihara, “Parallel volume rendering with early ray
      termination for visualizing large-scale datasets,” Lecture Notes in Computer
      Science (including subseries Lecture Notes in Artificial Intelligence and Lecture
      Notes in Bioinformatics), vol. 3358, pp. 245–256, 2004.

[340] M. Pharr and R. Fernando, GPU Gems 2: Programming Techniques for
      High-Performance Graphics and General-Purpose Computation (Gpu Gems).
      Addison-Wesley Professional, 2005.

[341] F. Heide, G. Wetzstein, R. Raskar, and W. Heidrich, “Adaptive image
      synthesis for compressive displays,” ACM Trans. Graph., vol. 32, no. 4, Jul.
      2013. [Online]. Available: https://doi.org/10.1145/2461912.2461925

[342] S. Schenke, C. W. Burkhard, and J. Denzler, “GPU-Based Volume
      Segmentation,” Proceedings of IVCNZ 2005, vol. 11, no. 0959-4388 LA - eng
      PT - Journal Article PT - Review PT - Review, Tutorial, pp. 59–65, 2005.
      [Online]. Available: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.
      1.1.112.3023&rep=rep1&type=pdf

[343] V. Kraft, F. Link, A. Schenk, and C. Schumann, “Adaptive Illumination
      Sampling for Direct Volume Rendering,” in Lecture Notes in Computer
      Science (including subseries Lecture Notes in Artificial Intelligence and
      Lecture Notes in Bioinformatics), vol. 12221 LNCS. Springer International
      Publishing, 2020, pp. 107–118. [Online]. Available: http://dx.doi.org/10.
      1007/978-3-030-61864-3_10




                                         250
                                                                 BIBLIOGRAPHY


[344] S. Zellmann, “Comparing Hierarchical Data Structures for Sparse Volume
      Rendering with Empty Space Skipping,” ArXiv, 2019. [Online]. Available:
      http://arxiv.org/abs/1912.09596

[345] S. Nakagawa and Y. Watanabe, “High-Frame-Rate Projection with Thousands
      of Frames Per Second Based on the Multi-Bit Superimposition Method,” Pro-
      ceedings - 2023 IEEE International Symposium on Mixed and Augmented Re-
      ality, ISMAR 2023, pp. 741–750, 2023.

[346] T. Ni, A. K. Karlson, and D. Wigdor, “AnatOnMe: Facilitating doctor-patient
      communication using a projection-based handheld device,” Conference on Hu-
      man Factors in Computing Systems - Proceedings, pp. 3333–3342, 2011.

[347] A. Abildgaard, A. K. Witwit, J. S. Karlsen, E. A. Jacobsen, B. Tennøe,
      G. Ringstad, and P. Due-Tønnessen, “An autostereoscopic 3D display can
      improve visualization of 3D models from intracranial MR angiography,” In-
      ternational Journal of Computer Assisted Radiology and Surgery, vol. 5, no. 5,
      pp. 549–554, 2010.

[348] C. Zhao, A. S. Kim, R. Beams, and A. Badano, “Spatiotemporal image quality
      of virtual reality head mounted displays,” Scientific Reports, vol. 12, no. 1, pp.
      1–13, 2022. [Online]. Available: https://doi.org/10.1038/s41598-022-24345-9

[349] A. Erickson, K. Kim, G. Bruder, and G. F. Welch, “Exploring the Limitations
      of Environment Lighting on Optical See-Through Head-Mounted Displays,”
      Proceedings - SUI 2020: ACM Symposium on Spatial User Interaction, no.
      October, 2020.

[350] K.-K. Lee, J.-W. Kim, J.-H. Ryu, and J.-O. Kim, “Ambient light robust gamut
      mapping for optical see-through displays,” Optics Express, vol. 28, no. 10, p.
      15392, 2020.

[351] O. Ashtiani, H. J. Guo, and B. Prabhakaran, “Impact of motion cues, color,
      and luminance on depth perception in optical see-through AR displays,” Fron-
      tiers in Virtual Reality, vol. 4, no. December, pp. 1–11, 2023.

[352] A. P. Pisanpeeti and E. Dinet, “Transparent objects: Influence of shape
      and color on depth perception,” ICASSP, IEEE International Conference on
      Acoustics, Speech and Signal Processing - Proceedings, pp. 1867–1871, 2017.

[353] M. Gerl, “Volume Hatching for Illustrative Visualization,” TU Wien Library,
      no. November 2006, 2006. [Online]. Available: http://www.cg.tuwien.ac.at/
      research/publications/2006/gerl-2006-vhi/gerl-2006-vhi-pdf.pdf


                                          251
     BIBLIOGRAPHY


[354] L. Zheng, Y. Wu, and K. L. Ma, “Perceptually-based depth-ordering enhance-
      ment for direct volume rendering,” IEEE Transactions on Visualization and
      Computer Graphics, vol. 19, no. 3, pp. 446–459, 2013.

[355] J. Díaz, H. Yela, and P.-P. Vázquez, “Vicinity Occlusion Maps: Enhanced
      depth perception of volumetric models,” Proccedings of Computer Graphics
      International, no. January 2008, pp. 56–63, 2008.

[356] H. Piringer, R. Kosara, and H. Hauser, “Interactive Focus + Context Visual-
      ization with Linked 2D / 3D Scatterplots,” Proceedings. Second International
      Conference on Coordinated and Multiple Views in Exploratory Visualization,
      2004.

[357] K. Marriott, F. Schreiber, T. Dwyer, K. Klein, N. Henry, R. Takayuki,
      W. Stuerzlinger, B. H. Thomas, and D. Hutchison, Immersive Analytics 123.
      Springer, 2018.

[358] J. Baumeister, M. R. Marner, R. T. Smith, M. Kohler, B. H. Thomas, t. I.
      I. S. o. M. 2015, and A. R. W. J. . S. . October, “Visual subliminal cues
      for spatial augmented reality,” in Proceedings of the 2015 IEEE International
      Symposium on Mixed and Augmented Reality Workshops, ISMARW 2015, no.
      7344747. IEEE, 2015, pp. 4–11.

[359] A. Joshi, X. Qian, D. P. Dione, K. R. Bulsara, C. K. Breuer, A. J. Sinusas,
      and X. Papademetris, “Effective visualization of complex vascular structures
      using a non-parametric vessel detection method,” IEEE Transactions on Vi-
      sualization and Computer Graphics, vol. 14, no. 6, pp. 1603–1610, 2008.

[360] D. Kim, M. Son, Y. Lee, H. Kang, and S. Lee, “Feature-guided image stip-
      pling,” Computer Graphics Forum, vol. 27, no. 4, pp. 1209–1216, 2008.

[361] B. Preim, C. Tietjen, and C. Dörge, “NPR, Focussing and Emphasis in Medical
      Visualizations,” Simulation und Visualisierung 2005, no. January, pp. 139–
      152, 2005.

[362] Z. Salah, D. Bartz, W. Straßer, and M. Tatagiba, “Expressive anatomical
      illustrations based on scanned patient data,” ArXiv, pp. 1–11, 2006.

[363] R. Maciejewski, T. Isenberg, W. M. Andrews, D. S. Ebert, M. C. Sousa,
      and W. Chen, “Measuring stipple aesthetics in hand-drawn and computer-
      generated images,” IEEE Computer Graphics and Applications, vol. 28, no. 2,
      pp. 62–74, 2008.



                                       252
                                                              BIBLIOGRAPHY


[364] D. Martín, G. Arroyo, A. Rodríguez, and T. Isenberg, “A survey of digital
      stippling,” Computers and Graphics (Pergamon), vol. 67, no. October, pp.
      24–44, 2017.

[365] R. A. Ulichney, “Dithering with Blue Noise,” Proceedings of the IEEE, vol. 76,
      no. 1, pp. 56–79, 1988.

[366] S. P. Lloyd, “Least Squares Quantization in PCM,” IEEE Transactions on
      Information Theory, vol. 28, no. 2, pp. 129–137, 1982.

[367] E. Heitz and F. Neyret, “High-Performance By-Example Noise using a
      Histogram-Preserving Blending Operator,” Proceedings of the ACM on Com-
      puter Graphics and Interactive Techniques, vol. 1, no. 2, pp. 1–25, 2018.

[368] I. A. Salvetti and C. A. Gran, “Non-Photorealistic Rendering: Cross Hatch-
      ing,” Ph.D. dissertation, Universitat Politecnica De Catalunya, 2020.

[369] X. Yuan, M. X. Nguyen, N. Zhang, and B. Chen, “Stippling and
      Silhouettes Rendering in Geometry-Image Space,” Proceedings of Eurographics
      Symposium on Rendering 2005 (EGSR’05, June 29–July 1, 2005, Konstanz,
      Germany), no. May, pp. 193–200, 2005. [Online]. Available: http:
      //www-users.cs.umn.edu/$\sim$xyuan/research/publication/egsr05.htm

[370] G. Philbrick and C. S. Kaplan, “Defining Hatching in Art,” EXPRESSIVE
      2019 - ACM/EG Expressive Symposium, pp. 111–121, 2019.

[371] J. Chen, G. Turk, and B. MacIntyre, “Painterly rendering with coherence for
      augmented reality,” ISVRI 2011 - IEEE International Symposium on Virtual
      Reality Innovations 2011, Proceedings, pp. 103–110, 2011.

[372] ——, “A non-photorealistic rendering framework with temporal coherence for
      augmented reality,” ISMAR 2012 - 11th IEEE International Symposium on
      Mixed and Augmented Reality 2012, Science and Technology Papers, pp. 151–
      160, 2012.

[373] T. E. French and T. E. French, A manual of engineering drawing for students
      & draftsmen. Cornell University, 1960.

[374] C. A. Cocosco, V. Kollokian, R. K.-S. Kwan, and A. C. Evans, “Brainweb:
      Online interface to a 3d mri simulated brain database,” NeuroImage, 1997.
      [Online]. Available: https://api.semanticscholar.org/CorpusID:14864257

[375] M. J. Ackerman, “The Visible Human Project,” Proceedings of the IEEE,
      vol. 86, no. 3, pp. 504–511, 1998.

                                        253
     BIBLIOGRAPHY


[376] R. Avila, T. He, L. Hong, A. Kaufman, H. Pfister, C. Silva, L. Sobierajski, and
      S. Wang, “VolVis: A diversified system for volume research and development,”
      in Proceedings Visualization ’94, oct 1994, pp. 31–38.

[377] M. Lee and R. D. Moser, “Direct numerical simulation of turbulent channel
      flow up to Reτ ≈ 5200,” Journal of Fluid Mechanics, vol. 774, pp. 395–415,
      jul 2015.

[378] R. W. Grout, A. Gruber, H. Kolla, P.-T. Bremer, J. C. Bennett, A. Gyu-
      lassy, and J. H. Chen, “A direct numerical simulation study of turbulence and
      flame structure in transverse jets analysed in jet-trajectory based coordinates,”
      Journal of Fluid Mechanics, vol. 706, pp. 351–383, 2012.

[379] J. Bossek, “grapherator: A Modular Multi-Step Graph Generator,” The Jour-
      nal of Open Source Software, vol. 3, no. 22, p. 528, 2018.

[380] S. Haghighi, “Pyrgg: Python Random Graph Generator,” The Journal of
      Open Source Software, vol. 2, no. 17, p. 331, 2017.

[381] D. Meyer, T. Nagler, and R. J. Hogan, “Copula-based synthetic
      data augmentation for machine-learning emulators,” Geoscientific Model
      Development, vol. 14, no. 8, pp. 5205–5215, 2021. [Online]. Available:
      https://doi.org/10.5194/gmd-14-5205-2021

[382] N. Patki, R. Wedge, and K. Veeramachaneni, “The synthetic data vault,” Pro-
      ceedings - 3rd IEEE International Conference on Data Science and Advanced
      Analytics, DSAA 2016, pp. 399–410, 2016.

[383] B. Fabian, T. Ermakova, and P. Junghanns, “Collaborative and secure
      sharing of healthcare data in multi-clouds,” Information Systems, vol. 48,
      no. October 2022, pp. 132–150, 2015. [Online]. Available:            http:
      //dx.doi.org/10.1016/j.is.2014.05.004

[384] C. Gillmann, N. N. Smit, E. Groller, B. Preim, A. Vilanova, and T. Wischgoll,
      “Ten Open Challenges in Medical Visualization,” IEEE Computer Graphics
      and Applications, vol. 41, no. 5, pp. 7–15, 2021.

[385] Nature, “Data sharing is the future,” Nature Methods, vol. 20, no. 4, p. 471,
      2023. [Online]. Available: https://doi.org/10.1038/s41592-023-01865-4

[386] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
      A. Courville, and Y. Bengio, “Generative adversarial networks,” Communica-
      tions of the ACM, vol. 63, no. 11, pp. 139–144, 2020.


                                         254
                                                              BIBLIOGRAPHY


[387] P. Dhariwal and A. Nichol, “Diffusion Models Beat GANs on Image Synthesis,”
      Advances in Neural Information Processing Systems, vol. 11, pp. 8780–8794,
      2021.

[388] W. H. L. Pinaya, P.-d. Tudosiu, J. Dafflon, P. F. Da, V. Fernandez, P. Nachev,
      S. Ourselin, and J. M. Cardoso, “Brain Imaging Generation with Latent Dif-
      fusion Models,” arXiv, pp. 1–10, 2022.

[389] Z. Ren, S. X. Yu, and D. Whitney, “Controllable medical image generation
      via generative adversarial networks,” IS and T International Symposium on
      Electronic Imaging Science and Technology, vol. 2021, no. 11, 2021.

[390] R. Togo, T. Ogawa, and M. Haseyama, “Synthetic gastritis image generation
      via loss function-based conditional pggan,” IEEE Access, vol. 7, pp. 87 448–
      87 457, 2019.

[391] L. X. Nguyen, P. S. Aung, H. Q. Le, S. B. Park, and C. S. Hong, “A New
      Chapter for Medical Image Generation: The Stable Diffusion Method,” Inter-
      national Conference on Information Networking, vol. 2023-January, pp. 483–
      486, 2023.

[392] H. Tomic, A. C. Costa, A. Bjerkén, M. A. Vieira, S. Zackrisson, A. Tingberg,
      P. Timberg, M. Dustler, and P. R. Bakic, “Simulation of breast lesions based
      upon fractal Perlin noise,” Physica Medica, vol. 114, no. July, 2023.

[393] B. A. Lawson, C. Drovandi, P. Burrage, A. Bueno-Orovio, R. W. dos
      Santos, B. Rodriguez, K. Mengersen, and K. Burrage, “Perlin noise
      generation of physiologically realistic cardiac fibrosis,” Medical Image
      Analysis, vol. 98, no. October 2023, p. 103240, 2024. [Online]. Available:
      https://doi.org/10.1016/j.media.2024.103240

[394] N. A. Svakhine, D. S. Ebert, and W. M. Andrews, “Illustration-inspired depth
      enhanced volumetric medical visualization,” IEEE Transactions on Visualiza-
      tion and Computer Graphics, vol. 15, no. 1, pp. 77–86, 2009.

[395] D. Lin, C. Wyman, and C. Yuksel, “Fast volume rendering with spatiotemporal
      reservoir resampling,” ACM Transactions on Graphics, vol. 40, no. 6, pp. 1–18,
      2021.

[396] Á. González, “Measurement of Areas on a Sphere Using Fibonacci and
      Latitude-Longitude Lattices,” Mathematical Geosciences, vol. 42, no. 1, pp.
      49–64, 2010.



                                        255
     BIBLIOGRAPHY


[397] M. Alex, D. Lottridge, J. Lee, S. Marks, and B. Wüensche, “Discrete versus
      Continuous Colour Pickers Impact Colour Selection in Virtual Reality Art-
      Making,” ACM International Conference Proceeding Series, no. Figure 2, pp.
      158–169, 2020.

[398] Ö. Coşkun, G. Nteli Chatzioglou, and A. Öztürk, “Henry Gray (1827–1861):
      the great author of the most widely used resource in medical education,”
      Child’s Nervous System, vol. 38, no. 8, pp. 1421–1423, 2022.

[399] I. A. Wijayanto, S. V. Babu, C. C. Pagano, and J. H. Chuang, “Comparing the
      Effects of Visual Realism on Size Perception in VR versus Real World Viewing
      through Physical and Verbal Judgments,” IEEE Transactions on Visualization
      and Computer Graphics, vol. 29, no. 5, pp. 2721–2731, 2023.

[400] S. Busking, A. Vilanova, and J. J. van Wijk, “VolumeFlies: Illustrative
      Volume Visualization using Particles,” Thirteenth Annual Conference of the
      Advanced School for Computing and Imaging, Proceedings, vol. Vi, no.
      September, pp. 51–58, 2007. [Online]. Available: http://graphics.tudelft.nl/
      $\sim$stef/publications/Busking2007-ASCI.pdf

[401] T. Blum, V. Kleeberger, C. Bichlmeier, and N. Navab, “Mirracle: Augmented
      reality in-situ visualization of human anatomy using a magic mirror,” Proceed-
      ings - IEEE Virtual Reality, pp. 169–170, 2012.

[402] JunYoung Choi, Hae Jin Jeong, and Jeong Wonki, “Improvement Depth Per-
      ception of Volume Rendering using Virtual Reality,” pp. 29–40, 2018.

[403] T. Munzner, “Why: Task Abstraction,” in Visual Analysis and Design,
      1st ed. New York, NY, USA: CRC Press, 2014, ch. 3, pp. 43–62. [Online].
      Available: https://doi.org/10.1201/b17511

[404] M. Raab and D. Araújo, “Embodied cognition with and without mental repre-
      sentations: The case of embodied choices in sports,” Frontiers in Microbiology,
      vol. 10, no. AUG, 2019.

[405] A. D. Wilson and S. Golonka, “Embodied Cognition is Not What you Think
      it is,” Frontiers in Psychology, vol. 4, no. February, pp. 1–13, 2013.

[406] A. Martin-Gomez, U. Eck, and N. Navab, “Visualization techniques for precise
      alignment in VR: A comparative study,” 26th IEEE Conference on Virtual
      Reality and 3D User Interfaces, VR 2019 - Proceedings, pp. 735–741, 2019.




                                        256
                                                               BIBLIOGRAPHY


[407] F. Paas, J. E. Tuovinen, H. Tabbers, and P. W. Van Gerven, “Cognitive
      load measurement as a means to advance cognitive load theory,” Educational
      Psychologist, vol. 38, no. 1, pp. 63–71, 2003.

[408] J. R. Lewis, “The System Usability Scale: Past, Present, and Future,” In-
      ternational Journal of Human-Computer Interaction, vol. 34, no. 7, p. 577,
      2018.

[409] V. Yoghourdjian, D. Archambault, S. Diehl, T. Dwyer, K. Klein, H. C. Pur-
      chase, and H. Y. Wu, “Exploring the limits of complexity: A survey of em-
      pirical studies on graph visualisation,” Visual Informatics, vol. 2, no. 4, pp.
      264–282, 2018.

[410] S. Jang, J. M. Vitale, R. W. Jyung, and J. B. Black, “Direct manipulation
      is better than passive viewing for learning anatomy in a three-dimensional
      virtual reality environment,” Computers and Education, vol. 106, pp. 150–165,
      2017. [Online]. Available: http://dx.doi.org/10.1016/j.compedu.2016.12.009

[411] L. Shapiro and S. Spaulding, “Embodied Cognition,” in The Stanford Ency-
      clopedia of Philosophy, E. N. Zalta, Ed. Metaphysics Research Lab, Stanford
      University, 2021.

[412] D. Kalkofen, E. Mendez, and D. Schmalstieg, “Comprehensible visualization
      for augmented reality,” IEEE Transactions on Visualization and Computer
      Graphics, vol. 15, no. 2, pp. 193–204, 2009.

[413] C. Hansen, J. Wieferich, F. Ritter, C. Rieder, and H. O. Peitgen, “Illustrative
      visualization of 3D planning models for augmented reality in liver surgery,”
      International Journal of Computer Assisted Radiology and Surgery, vol. 5,
      no. 2, pp. 133–141, 2010.

[414] K. Lowonn, R. Gasteiger, and B. Preim, “Qualitative Evaluation of Feature
      Lines on Anatomical Surfaces,” in Bildverarbeitung für die Medizin, 2013, no.
      May 2014, pp. 187–193.

[415] S. Nagata, “How To Reinforce Perception of Depth in Single Two-Dimensional
      Pictures.” Proceedings of the SID, vol. 25, no. 3, pp. 239–246, 1983.

[416] Z. Chen, R. N. Denison, D. Whitney, and G. W. Maus, “Illusory occlusion
      affects stereoscopic depth perception,” Scientific Reports, vol. 8, no. 1, pp.
      1–9, 2018. [Online]. Available: http://dx.doi.org/10.1038/s41598-018-23548-3

[417] J. C. Vinson, Thomas Nast. Political Cartoonist, 1st ed.     Atlanta: Athens:
      University of Georgia Press, 1967.

                                        257
     BIBLIOGRAPHY


[418] G. Philbrick and C. S. Kaplan, “A Primitive for Manual Hatching,” ACM
      Transactions on Graphics, vol. 41, no. 2, 2022.

[419] F. Ritter, C. Hansen, V. Dicken, O. Konrad, B. Preim, and H. O. Peitgen,
      “Real-time illustration of vascular structures,” IEEE Transactions on Visual-
      ization and Computer Graphics, vol. 12, no. 5, pp. 877–884, 2006.

[420] B. Krajancich, P. Kellnhofer, and G. Wetzstein, “Optimizing depth perception
      in virtual and augmented reality through gaze-contingent stereo rendering,”
      ACM Transactions on Graphics, vol. 39, no. 6, 2020.

[421] H. Adams, J. Stefanucci, S. H. Creem-Regehr, G. Pointon, W. B. Thompson,
      and B. Bodenheimer, “Shedding Light on Cast Shadows: An Investigation of
      Perceived Ground Contact in AR and VR,” IEEE Transactions on Visualiza-
      tion and Computer Graphics, no. July, 2021.

[422] L. Harvey, “Efficient estimation of sensory thresholds,” Behavior Research
      Methods, Instruments, & Computers, vol. 18, pp. 623–632, 11 1986.

[423] X. Gilliam, K. Manross, and M. Gamel, “Visualization of Radar Data in Three-
      Dimensions,” Wind Engineering, 1996.

[424] S. D. Macdonald, P. J. Fields, and M. D. Stroup, “REAL-TIME 3D SONAR
      MODELING AND VISUALIZATION,” Ph.D. dissertation, Navel Postgradu-
      ate School, Monterey, California, 1997.

[425] M. Danu, C.-I. Nita, A. Vizitiu, C. Suciu, and L. M. Itu, “Deep learning
      based generation of synthetic blood vessel surfaces,” in 2019 23rd International
      Conference on System Theory, Control and Computing (ICSTCC), 2019, pp.
      662–667.

[426] L. X. Nguyen, P. Sone Aung, H. Q. Le, S.-B. Park, and C. S. Hong, “A new
      chapter for medical image generation: The stable diffusion method,” in 2023
      International Conference on Information Networking (ICOIN), 2023, pp. 483–
      486.

[427] R. Ratcliff and P. L. Smith, “Perceptual Discrimination in Static and Dynamic
      Noise: The Temporal Relation Between Perceptual Encoding and Decision
      Making,” Journal of Experimental Psychology: General, vol. 139, no. 1, pp.
      70–94, 2010.

[428] A. Jing, K. May, G. Lee, and M. Billinghurst, “Eye See What You See: Ex-
      ploring How Bi-Directional Augmented Reality Gaze Visualisation Influences


                                         258
                                                             BIBLIOGRAPHY


     Co-Located Symmetric Collaboration,” Frontiers in Virtual Reality, vol. 2, no.
     June, pp. 1–17, 2021.

[429] R. Booij, M. van Straten, A. Wimmer, and R. P. Budde, “Influence of breath-
      ing state on the accuracy of automated patient positioning in thoracic CT
      using a 3D camera for body contour detection,” European Radiology, vol. 32,
      no. 1, pp. 442–447, 2022.




                                       259
BIBLIOGRAPHY




               260
List of Figures

 1.1   Images of Computed Tomography (CT) and Magnetic Resonance
       Imaging (MRI) films being utilized in a medical setting. . . . . . . . . 2
 1.2   Two similar slices of different human heads. . . . . . . . . . . . . . . 3
 1.3   A image of Hanna et al.’s [44] sterile system for Pathologists . . . . . 6
 1.4   The Holographic Overlay system in use with a CT scanner. . . . . . 7
 1.5   This is a image of a cube rendered to show the same data shown in
       Figure 1.4 but using Direct Volume Rendering (DVR). . . . . . . . . 9
 1.6   An example of where it is difficult to interpret depth due to the ab-
       sence of depth cues. The circular object is a virtual object displayed
       against the wall in the next room. Left) shows the room with no X-
       ray vision; center) shows the same room with X-ray vision enabled.
       However, the lack of depth cues makes it impossible to determine
       where the circular object in the next room is located. Right) shows
       the same rooms as on the left, displayed using an isometric perspec-
       tive to illustrate where the items are actually located. . . . . . . . . 10

 2.1   This graph of depth cues and distance provides guidelines for depth
       perception in relation to the distance and key perception parameters.           16
 2.2   Several images showing various cases of Monoscopic forms of depth
       perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    17
 2.3   Aerial Perspective, Relative Size, and Relative Density: An Image of
       a mountain view in Bavaria, Germany, with indicators to explain how
       monoscopic depth perception functions . . . . . . . . . . . . . . . . .         18
 2.4   Depth Cues and Motion: Schematic illustration of motion compo-
       nents arising from observer translation and scene-relative object mo-
       tion. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   19
 2.5   A depiction of how convergence and accommodation work in the real
       world using Mixed Reality and OST AR. . . . . . . . . . . . . . . . .           20
 2.6   Three images of the Stanford bunny sitting behind a wall, each using
       a different X-ray Vision effect. . . . . . . . . . . . . . . . . . . . . . .    21



                                         261
  LIST OF FIGURES


2.7  Examples detailing the stippling algorithm created by Pastor and
     Strotthote [85]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     25
2.8 A example of Cuberilles. . . . . . . . . . . . . . . . . . . . . . . . . .        29
2.9 Examples of Hui et al.’s cursors on 3D planes. . . . . . . . . . . . . .          32
2.10 The 6 datasets used in Grosset et al.’s [133] research. . . . . . . . . .        35
2.11 Conditions used in Englund et al.’s [135, 136] studies. . . . . . . . . .        36
2.12 A simple view of Milgram et al.’s Mixed Reality Continuum [137]. . .             37
2.13 Ivan Sutherland’s [138] "The Sword of Damocles" one of the first head-
     mounted Augmented Reality (AR) systems. . . . . . . . . . . . . . .              39
2.14 The virtual scene created by Cecotti et al. . . . . . . . . . . . . . . .        43
2.15 The protocol utilized for this literature review with the matching
     results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     45
2.16 A bar graph representing the number of papers that were published
     between 1990 and 2023 focusing on X-ray Vision. . . . . . . . . . . .            46
2.17 A plot showing the studies that have been run on X-ray Vision by
     device type. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .     48
2.18 Examples of X-ray Vision techniques that don’t function using tra-
     ditional techniques but rather label or indicate where objects should
     be located. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    49
2.19 Armadillos sitting behind a corkboard in AR with the same orienta-
     tion using hole-in-the-worlds effects to function. . . . . . . . . . . . .       51
2.20 A image of Bajura et al.’s [37] X-ray Vision technique. . . . . . . . . .        52
2.21 This image contains the X-ray Vision study environment from Avve-
     duto et al. [196]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   53
2.22 The conditions used in Heinrich et al.’s [197] investigation in to Spatial
     Augmented Reality (SAR) based virtual holes. . . . . . . . . . . . . .           53
2.23 A image of the tunneling x-ray visualization used to look through
     multiple walls. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    54
2.24 Wireframe, Random Dot, and Grid X-ray Visualization over colorful
     patterned box with a virtual cube, sphere icosahedron, and a Stanford
     bunny rendered inside. . . . . . . . . . . . . . . . . . . . . . . . . . .       56
2.25 The conditions utilized for Heinrich et al.’s [198] research. . . . . . . .      57
2.26 Ozgur et al.’s [93] X-ray Vision system using Halos. . . . . . . . . . .         58
2.27 X-ray Vision showing alpha blending, Saliency, and edge-based Visu-
     alizations (Shown left to right) looking through a building. . . . . . .         59
2.28 A description of the X-ray Vision system used by Kalkofen et al. [39]            60
2.29 A heat map illustrating how often different X-ray Vision effects are
     compared to each other in the literature. . . . . . . . . . . . . . . . .        61



                                        262
                                                              LIST OF FIGURES


2.30 Eren et al.’s [180] different visualizations of underground pipe net-
     works using X-ray Vision techniques. . . . . . . . . . . . . . . . . . .         63
2.31 A promotional Image from Dr Grordborts Invaders made by Weta
     Workshop for the Magic Leap. . . . . . . . . . . . . . . . . . . . . . .         64
2.32 A bar graph of the type of display used to visualize the various X-ray
     Vision effects. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .    65
2.33 A bar graph displaying The types of AR that was used to visualize
     the different types of X-ray Vision effects. . . . . . . . . . . . . . . .       66
2.34 Devices examined for X-ray vision across the literature over time. . .           67
2.35 A plot showing nine devices found in the literature. . . . . . . . . . .         69
2.36 The Ocular See Through (OST) AR display was utilized for Swan et
     al.’s [239] study. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .   73
2.37 Swan et al.’s [244] study design. . . . . . . . . . . . . . . . . . . . . .      75
2.38 OST AR Head Mounted Device (HMD) which was utilized in Singh
     et al. [247] experiments. . . . . . . . . . . . . . . . . . . . . . . . . .      76
2.39 Several images from Al-Kalbani et al.’s [33] study set up. . . . . . . .         77

3.1  a) Otuski et al.’s [204] version Random Dot b) A image of the Random
     Dot visualization used in this chapter. . . . . . . . . . . . . . . . . . 84
3.2 a) Livingston et al.’s [259] initial version of the wireframe X-ray Vision
     effect b) A image of the Tessellation visualization used in this chapter. 85
3.3 a) Kalkofen et al.’s [211] version of X-ray Vision b) A image of the
     Edge-Based Visualization used in this study. . . . . . . . . . . . . . . 87
3.4 Two images that use saliency detection. a) Sandor et al.’s [172] version
     of Saliency. b) An image of the Saliency Visualization used in this
     study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
3.5 This image illustrates how different fields of view (FOV) were man-
     aged to work with each other. . . . . . . . . . . . . . . . . . . . . . . 90
3.6 This diagram explains the frame-by-frame process that the system
     used to process images . . . . . . . . . . . . . . . . . . . . . . . . . . 92
3.7 A image of the Baseline (None) condition . . . . . . . . . . . . . . . . 95
3.8 A image of the hardware used for this study on a glass mannequin
     head from three different angles. . . . . . . . . . . . . . . . . . . . . . 99
3.9 A image of the prototype models developed to prototype the final
     ZED MINI camera mount . . . . . . . . . . . . . . . . . . . . . . . . 100
3.10 A diagram of the base dimensions for the HoloLens2 calculated uti-
     lizing 3D prototyping. The top of the HoloLens2 Mount can be seen
     shaded in yellow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
3.11 The study environment used for spatial estimation experiment. . . . . 102


                                        263
 LIST OF FIGURES


3.12 A top down view of the study setup for the spatial estimation exper-
     iment color coded in to the different areas of note . . . . . . . . . . . 105
3.13 A graph representing the accuracy of participants’ placement based
     on the distance the object was placed from the goal position within
     the large physical cube with a Voronoi pattern using the X-ray Visu-
     alizations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
3.14 A Graphs representing the accuracy of participants’ placement based
     on the distance they are from the goal position on the X-axis within
     the large, physical cube with a Voronoi pattern . . . . . . . . . . . . 108
3.15 Graphs representing the accuracy of participants’ placement based on
     the distance they are from the goal position on the Y-axis within the
     large physical cube with a Voronoi pattern. . . . . . . . . . . . . . . . 109
3.16 Graphs representing the accuracy of participants’ placement based
     on the distance the object was placed from the goal position on the
     Z-axis within the large physical cube with a Voronoi pattern . . . . . 110
3.17 Two box plots analyzing the time it took to complete the task com-
     pared between different (Right) X-ray Visualizations and (Left) with
     and without the reference objects. . . . . . . . . . . . . . . . . . . . . 112
3.18 Graphs representing the percentage of time participants spent moving
     the icosahedron for each task for each X-ray Vision effect. . . . . . . . 113
3.19 The distance participants walked for each when (Left) different X-ray
     Visualizations were displayed to them or (Right) the differences when
     reference objects were or were not available. . . . . . . . . . . . . . . 114
3.20 Box plots represent the distance participants walked for each iteration.114
3.21 Box plots representing the speed/velocity in which the object was
     moved based on the icosahedron. . . . . . . . . . . . . . . . . . . . . 116
3.22 Three plots displaying the subjective results collected throughout the
     course of this study. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117

4.1   A patient receiving a scan in the CT scanner . . . . . . . . . . . . . . 125
4.2   Left) MRI of a skull; Center) Cerebral angiography, arteria vertebralis
      sinister injection; Right) CT scan of human lungs. . . . . . . . . . . . 126
4.3   An example of cinematic rendering of a CT scan of a patient with a
      sinus frontalis frontal bone fracture from different angles. . . . . . . . 130
4.4   Two different methods of moving along a ray. . . . . . . . . . . . . . 132
4.5   An image of a set of bones from a CT scan displayed as an iso-surface
      on a volumetric display (the Voxon). . . . . . . . . . . . . . . . . . . 133
4.6   A series of images showing the looking glass prototype utalizing DVR. 134




                                       264
                                                             LIST OF FIGURES


4.7  A third person view of Direct Volume Rendering using three different
     polygonal meshes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
4.8 This figure shows images from a computer monitor, an AR overlaid
     image, and the HoloLens2’s display directly, featuring a box with a
     wireframe version of X-ray Vision that can cast a shadow over the
     unseen area. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
4.9 This image showcases some examples of what is possible for real-time
     volume-rendered graphics working on an immersive Mixed Reality
     (MR) HMD running at 60 fps on a GeForce RTX 2700 Graphics
     Processing Unit (GPU). . . . . . . . . . . . . . . . . . . . . . . . . . 139
4.10 A diagram of how ray marching works regarding this implementation
     of SDFs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
4.11 A 2D example of empty space skipping using a grid-based approach. . 141
4.12 Artistic images of anatomical images displayed as an X-ray Visual-
     ization by Dr Joshua Luke Ameliorate. . . . . . . . . . . . . . . . . . 143
4.13 Examples of hand-drawn illustrations using the illustrative techniques
     that inspired the various Volumetric Illustrative Rendering Techniques
     (VIRT)s as they would be depicted within art. . . . . . . . . . . . . . 144
4.14 The Halo VIRT applied to the Visible Female data set overlaid over
     the 3D printed dataset to provide an X-ray Vision effect. . . . . . . . 145
4.15 The Stippling VIRT applied to the Visible Female data set overlaid
     over the 3D printed dataset to provide an X-ray Vision effect. . . . . 148
4.16 The Hatching VIRT applied to the Visible Female data set overlaid
     over the 3D printed dataset to provide an X-ray Vision effect. . . . . 150

5.1   The types of volumes the Random Volume Generation System system
      can produce. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.2   An activity diagram showing the transition between the various states
      of the Random Volume Generation System . . . . . . . . . . . . . . . 159
5.3   A breakdown of the structures of the 3D meshes generated by the
      Random Volume Generator. . . . . . . . . . . . . . . . . . . . . . . . 160
5.4   (a) A 3D printed version of the model made from the Random Vol-
      ume Generator’s mesh output. (b and c) shows this model from the
      view of a Microsoft Hololens2 to create an X-ray Vision effect using
      illustrative rendering. . . . . . . . . . . . . . . . . . . . . . . . . . . . 161
5.5   The HoloLens User Interface (UI) of the random volume generation
      system. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162

6.1   This image shows the study environment in which the study was
      found in and all four of the conditions listed in Chapter 4. . . . . . . 164

                                       265
 LIST OF FIGURES


6.2  The conditions that were used in this study are displayed as the vol-
     umes they are represented as. . . . . . . . . . . . . . . . . . . . . . . 165
6.3 A visual description of how the task users in this study were asked to
     conduct using a volume with 22 green artifacts. . . . . . . . . . . . . 166
6.4 5 volumes each using no VIRT each with a different number of green
     objects to count. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
6.5 The layout of the study area (the participants desk space) the par-
     ticipants had around them for the perception experiment. . . . . . . . 170
6.6 The accuracy, or difference between the participant’s and real an-
     swers, when counting all green objects within the volume by the
     VIRTs and the number of objects they were asked to count. . . . . . 172
6.7 The accuracy of participant responses when completing the Count
     Nested task. The error bars indicate each X-ray Visualization’s con-
     fidence levels (CL = 95%). Significance bars have been omitted as all
     conditions are significant . . . . . . . . . . . . . . . . . . . . . . . . . 173
6.8 The time required to complete each interaction of a task for each
     VIRT for both the Counting Everything and Counting Nested tasks. . 174
6.9 Box plots showing how much each participant moved their head to
     view the volume between the different VIRT conditions for both the
     Counting Everything and Counting Nested tasks. . . . . . . . . . . . 175
6.10 A graph showing the speed at which participants moved their hands
     in this experiment when the different VIRTs were being utilized for
     both the Counting Everything and Counting Nested tasks. . . . . . . 176
6.11 Box plots relating to the speed at which participants moved over
     different eye gazes. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177
6.12 A plot showing the difference in velocity between the user’s eye move-
     ments when different amounts of objects existed for them to see when
     counting everything. . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
6.13 Outcomes from the PAAS questionnaire in the counting study. Lower
     results indicate lower cognitive load and higher results indicate higher
     cognitive load. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
6.14 Results of the SUS questionnaire for the counting study. . . . . . . . 179
6.15 Results for the question "How easy was it to look at objects inside of
     other objects using this visualization?" . . . . . . . . . . . . . . . . . 180

7.1   This figure shows the environment in which this study took place and
      presents each of the VIRT conditions utalized in this study. . . . . . . 186
7.2   The VIRTs used for this study . . . . . . . . . . . . . . . . . . . . . . 189




                                       266
                                                             LIST OF FIGURES


7.3  An example of what the volume looked like from the outside for each
     depth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 190
7.4 The furthest extremes of the visualization shown in enlarged using
     the No VIRT condition. . . . . . . . . . . . . . . . . . . . . . . . . . 192
7.5 Top-down view of the layout of the participants’ study area (their
     desk). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
7.6 An Example of Euclidean DVR and non-Euclidean DVR of the same
     volume. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
7.7 Four graphs which show impact to the depth perception caused by
     each condition in this study’s response data, and sigmoid functions
     for each VIRT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
7.8 Magnified view of the sigmoid functions showing the offset from zero
     and is the largest divergence for the JND at the 75% line. . . . . . . 196
7.9 The mean time required for each VIRT in the depth perception study. 198
7.10 Box plots showing the average head movement velocity for each con-
     dition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
7.11 Box plots relating to the speed at which participants tended to move
     their hands. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
7.12 The difference in velocity between the participant’s eye movements. . 200
7.13 Results from the PAAS questionnaire for the depth perception study. 201
7.14 Results from the SUS questionnaire for depth perception study. . . . 201
7.15 Results for the question "How easy was it to look at objects inside of
     other objects using this visualization?" . . . . . . . . . . . . . . . . . 202

8.1   A prototype of a new form of accommodation-convergence based X-
      ray effect designed for OST AR devices. . . . . . . . . . . . . . . . . 213

C.1 The Class Diagram showing the main dependencies of the Random
    Generating Volumes System. . . . . . . . . . . . . . . . . . . . . . . . 276




                                       267
LIST OF FIGURES




                  268
List of Tables

 2.1   Advantages and Disadvantages of Devices Used for X-ray Vision . . . 70

 3.1   Table of X-ray Visualizations used in this study. Showing the mean
       (µ), standard deviation (σ), and the median (M) of the distance from
       the correct target position of all the X-ray Visualizations from the
       user’s sight. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108

 7.1   The results from the point of subjective equality (PSE) in cm across
       all of the VIRTs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
 7.2   Results for the just noticeable difference (JND) in cm across all of
       the VIRTs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197




                                        269
LIST OF TABLES




                 270
Appendix A

Holographic Overlay System
Details

The Digital Imaging and Communications in Medicine (DICOM) data generated by
the Computed Tomography (CT) scanner was displayed using a real-time rendered
iso-surface to represent 3D models by compiling the DICOM data from real-time
using marching cubes [319] from DICOM information generated by fo-DICOM 1 .
Running marching cubes, even in parallel blocks, was deemed too slow and imprecise.
This was then encased within an avatar of the patient for reviewing processes or
could be viewed.
    Later iterations of this project aimed at helping to communicate tasks for phys-
iotherapists by communicating with them where electrodes should be placed on the
patient. Predefined locations were based on previous data to simulate the concept
of predefined instructions and the system’s body type using the later work of Booij
et al. [429], where a more accurate model of the body could be formed to counteract
issues like the patient’s breathing. This allowed us to correct the difference between
different body types in different patients.
    The final part of this system looked at making the CT DICOM data more inter-
active. A two-way clipping plane that would show the current slice with adjustable
contrast was included. These six clipping planes were placed on the scan’s vertical,
horizontal, and depth axes. These would each have their own data set that could be
displayed, communicating what slice was being looked at in either slice/pixel range,
percentage through the volume, or center meters through the volume. This system
also came with several different methods of interacting with it, depending on the
type of task required by the radiologist. The final version of the product can be
seen in Figure 1.4.
    This system was then extended to the present version of this project and moved
  1
      https://github.com/fo-dicom/fo-dicom



                                             271
     A. HOLOGRAPHIC OVERLAY SYSTEM DETAILS


over to having a Direct Volume Rendering (DVR) system attached. DVR works
by directly rendering the data from the DICOM files. This allows for far greater
data flexibility, rendering the visualization with almost no performance overheads.
The trade-off for using DVR is that it requires a much higher level of performance
overall, requiring us to use a wireless theater using holographic remoting to a nearby
Personal Computer (PC) to run the system. This was an acceptable trade-off as
this system to have both performed more reliably, delivered a smoother frame rate,
improved the quality of graphics, and allowed for much faster network connectivity.




                                         272
Appendix B

X-ray Vision Literature Review
Methodology




               273
B. X-RAY VISION LITERATURE REVIEW METHODOLOGY




                    274
Appendix C

Class Diagram for Random
Generating Volumes




              275
   C. CLASS DIAGRAM FOR RANDOM GENERATING
VOLUMES




Figure C.1: The Class Diagram showing the main dependencies of the Random
Generating Volumes System.




                                  276
Appendix D

Chapter 5: Counting Everything
Post Hoc

               LHS Amount                    RHS Amount
LHS VIRT                     RHS VIRT                      estimate      t.ratio   p.value
                Of Objects                    Of Objects
Halo/Outline   14            Hatching        14            -2.76573    -8.68107    0.00000
Halo/Outline   14            No Effect       14            -1.04346    -3.28904    0.11078
Halo/Outline   14            Stippling       14            -2.10695    -6.64124    0.00000
Halo/Outline   14            Halo/Outline    16            -0.04346    -0.13697    1.00000
Halo/Outline   14            Hatching        16            -3.13869    -9.89337    0.00000
Halo/Outline   14            No Effect       16            -1.23393    -3.88944    0.01542
Halo/Outline   14            Stippling       16            -2.18631    -6.89140    0.00000
Halo/Outline   14            Halo/Outline    18            -0.39266    -1.23770    0.99960
Halo/Outline   14            Hatching        18            -3.93234   -12.39501    0.00000
Halo/Outline   14            No Effect       18            -1.37679    -4.33973    0.00254
Halo/Outline   14            Stippling       18            -2.78712    -8.97394    0.00000
Halo/Outline   14            Halo/Outline    20            -0.55139    -1.73802    0.97364
Halo/Outline   14            Hatching        20            -5.02791   -15.96877    0.00000
Halo/Outline   14            No Effect       20            -1.82812    -5.78551    0.00000
Halo/Outline   14            Stippling       20            -3.34638   -10.50360    0.00000
Halo/Outline   14            Halo/Outline    22            -0.61488    -1.93816    0.92538
Halo/Outline   14            Hatching        22            -5.20219   -16.39764    0.00000
Halo/Outline   14            No Effect       22            -2.36092    -7.44176    0.00000
Halo/Outline   14            Stippling       22            -4.10695   -12.94537    0.00000
Hatching       14            No Effect       14             1.72228     5.38580    0.00002
Hatching       14            Stippling       14             0.65878     2.06011    0.87628
Hatching       14            Halo/Outline    16             2.72228     8.51293    0.00000
Hatching       14            Hatching        16            -0.37296    -1.16630    0.99983
Hatching       14            No Effect       16             1.53180     4.79015    0.00033
Hatching       14            Stippling       16             0.57942     1.81192    0.95998
Hatching       14            Halo/Outline    18             2.37307     7.42092    0.00000
Hatching       14            Hatching        18            -1.16661    -3.64816    0.03631
Hatching       14            No Effect       18             1.38894     4.34342    0.00250
Hatching       14            Stippling       18            -0.02139    -0.06822    1.00000
Hatching       14            Halo/Outline    20             2.21434     6.92455    0.00000
Hatching       14            Hatching        20            -2.26217    -7.12720    0.00000
Hatching       14            No Effect       20             0.93761     2.94296    0.26525
Hatching       14            Stippling       20            -0.58065    -1.80864    0.96068
Hatching       14            Halo/Outline    22             2.15085     6.72600    0.00000
Hatching       14            Hatching        22            -2.43645    -7.61913    0.00000



                                            277
    D. CHAPTER 5: COUNTING EVERYTHING POST HOC




               RHS Amount                   RHS Amount
LHS VIRT                     RHS VIRT                     estimate      t.ratio   p.value
                Of Objects                   Of Objects
Hatching       14            No Effect      22             0.40482     1.26592    0.99946
Hatching       14            Stippling      22            -1.34122    -4.19417    0.00468
No Effect      14            Stippling      14            -1.06349    -3.33926    0.09595
No Effect      14            Halo/Outline   16             1.00000     3.13990    0.16561
No Effect      14            Hatching       16            -2.09524    -6.57885    0.00000
No Effect      14            No Effect      16            -0.19048    -0.59808    1.00000
No Effect      14            Stippling      16            -1.14286    -3.58846    0.04431
No Effect      14            Halo/Outline   18             0.65079     2.04343    0.88396
No Effect      14            Hatching       18            -2.88889    -9.07083    0.00000
No Effect      14            No Effect      18            -0.33333    -1.04663    0.99997
No Effect      14            Stippling      18            -1.74367    -5.58808    0.00001
No Effect      14            Halo/Outline   20             0.49206     1.54503    0.99286
No Effect      14            Hatching       20            -3.98445   -12.60529    0.00000
No Effect      14            No Effect      20            -0.78467    -2.47333    0.60495
No Effect      14            Stippling      20            -2.30292    -7.20155    0.00000
No Effect      14            Halo/Outline   22             0.42857     1.34567    0.99875
No Effect      14            Hatching       22            -4.15873   -13.05801    0.00000
No Effect      14            No Effect      22            -1.31746    -4.13670    0.00592
No Effect      14            Stippling      22            -3.06349    -9.61907    0.00000
Stippling      14            Halo/Outline   16             2.06349     6.47917    0.00000
Stippling      14            Hatching       16            -1.03175    -3.23958    0.12711
Stippling      14            No Effect      16             0.87302     2.74119    0.39865
Stippling      14            Stippling      16            -0.07937    -0.24920    1.00000
Stippling      14            Halo/Outline   18             1.71429     5.38269    0.00002
Stippling      14            Hatching       18            -1.82540    -5.73157    0.00000
Stippling      14            No Effect      18             0.73016     2.29263    0.73955
Stippling      14            Stippling      18            -0.68018    -2.17982    0.81246
Stippling      14            Halo/Outline   20             1.55556     4.88429    0.00021
Stippling      14            Hatching       20            -2.92096    -9.24080    0.00000
Stippling      14            No Effect      20             0.27882     0.87887    1.00000
Stippling      14            Stippling      20            -1.23943    -3.87587    0.01622
Stippling      14            Halo/Outline   22             1.49206     4.68494    0.00054
Stippling      14            Hatching       22            -3.09524    -9.71875    0.00000
Stippling      14            No Effect      22            -0.25397    -0.79744    1.00000
Stippling      14            Stippling      22            -2.00000    -6.27981    0.00000
Halo/Outline   16            Hatching       16            -3.09524    -9.71875    0.00000
Halo/Outline   16            No Effect      16            -1.19048    -3.73798    0.02665
Halo/Outline   16            Stippling      16            -2.14286    -6.72837    0.00000
Halo/Outline   16            Halo/Outline   18            -0.34921    -1.09647    0.99993
Halo/Outline   16            Hatching       18            -3.88889   -12.21074    0.00000
Halo/Outline   16            No Effect      18            -1.33333    -4.18654    0.00483
Halo/Outline   16            Stippling      18            -2.74367    -8.79286    0.00000
Halo/Outline   16            Halo/Outline   20            -0.50794    -1.59487    0.98967
Halo/Outline   16            Hatching       20            -4.98445   -15.76891    0.00000
Halo/Outline   16            No Effect      20            -1.78467    -5.62540    0.00000
Halo/Outline   16            Stippling      20            -3.30292   -10.32869    0.00000
Halo/Outline   16            Halo/Outline   22            -0.57143    -1.79423    0.96365
Halo/Outline   16            Hatching       22            -5.15873   -16.19792    0.00000
Halo/Outline   16            No Effect      22            -2.31746    -7.27660    0.00000
Halo/Outline   16            Stippling      22            -4.06349   -12.75897    0.00000
Hatching       16            No Effect      16             1.90476     5.98077    0.00000
Hatching       16            Stippling      16             0.95238     2.99038    0.23834




                                        278
               LHS Amount                    RHS Amount
LHS VIRT                     RHS VIRT                      estimate      t.ratio   p.value
                Of Objects                    Of Objects
Hatching       16            Halo/Outline    18             2.74603     8.62228    0.00000
Hatching       16            Hatching        18            -0.79365    -2.49199    0.59037
Hatching       16            No Effect       18             1.76190     5.53221    0.00001
Hatching       16            Stippling       18             0.35157     1.12671    0.99990
Hatching       16            Halo/Outline    20             2.58730     8.12388    0.00000
Hatching       16            Hatching        20            -1.88921    -5.97675    0.00000
Hatching       16            No Effect       20             1.31057     4.13100    0.00605
Hatching       16            Stippling       20            -0.20768    -0.64946    1.00000
Hatching       16            Halo/Outline    22             2.52381     7.92452    0.00000
Hatching       16            Hatching        22            -2.06349    -6.47917    0.00000
Hatching       16            No Effect       22             0.77778     2.44215    0.62916
Hatching       16            Stippling       22            -0.96825    -3.04022    0.21205
No Effect      16            Stippling       16            -0.95238    -2.99038    0.23834
No Effect      16            Halo/Outline    18             0.84127     2.64151    0.47344
No Effect      16            Hatching        18            -2.69841    -8.47276    0.00000
No Effect      16            No Effect       18            -0.14286    -0.44856    1.00000
No Effect      16            Stippling       18            -1.55319    -4.97764    0.00013
No Effect      16            Halo/Outline    20             0.68254     2.14311    0.83363
No Effect      16            Hatching        20            -3.79397   -12.00269    0.00000
No Effect      16            No Effect       20            -0.59419    -1.87294    0.94517
No Effect      16            Stippling       20            -2.11245    -6.60591    0.00000
No Effect      16            Halo/Outline    22             0.61905     1.94375    0.92347
No Effect      16            Hatching        22            -3.96825   -12.45994    0.00000
No Effect      16            No Effect       22            -1.12698    -3.53862    0.05211
No Effect      16            Stippling       22            -2.87302    -9.02099    0.00000
Stippling      16            Halo/Outline    18             1.79365     5.63189    0.00000
Stippling      16            Hatching        18            -1.74603    -5.48237    0.00001
Stippling      16            No Effect       18             0.80952     2.54183    0.55123
Stippling      16            Stippling       18            -0.60081    -1.92547    0.92957
Stippling      16            Halo/Outline    20             1.63492     5.13349    0.00006
Stippling      16            Hatching        20            -2.84159    -8.98972    0.00000
Stippling      16            No Effect       20             0.35819     1.12903    0.99990
Stippling      16            Stippling       20            -1.16006    -3.62768    0.03890
Stippling      16            Halo/Outline    22             1.57143     4.93413    0.00016
Stippling      16            Hatching        22            -3.01587    -9.46955    0.00000
Stippling      16            No Effect       22            -0.17460    -0.54824    1.00000
Stippling      16            Stippling       22            -1.92063    -6.03061    0.00000
Halo/Outline   18            Hatching        18            -3.53968   -11.11426    0.00000
Halo/Outline   18            No Effect       18            -0.98413    -3.09006    0.18781
Halo/Outline   18            Stippling       18            -2.39446    -7.67373    0.00000
Halo/Outline   18            Halo/Outline    20            -0.15873    -0.49840    1.00000
Halo/Outline   18            Hatching        20            -4.63524   -14.66415    0.00000
Halo/Outline   18            No Effect       20            -1.43546    -4.52468    0.00113
Halo/Outline   18            Stippling       20            -2.95372    -9.23667    0.00000
Halo/Outline   18            Halo/Outline    22            -0.22222    -0.69776    1.00000
Halo/Outline   18            Hatching        22            -4.80952   -15.10144    0.00000
Halo/Outline   18            No Effect       22            -1.96825    -6.18013    0.00000
Halo/Outline   18            Stippling       22            -3.71429   -11.66250    0.00000
Hatching       18            No Effect       18             2.55556     8.02420    0.00000
Hatching       18            Stippling       18             1.14522     3.67018    0.03369
Hatching       18            Halo/Outline    20             3.38095    10.61586    0.00000




                                            279
    D. CHAPTER 5: COUNTING EVERYTHING POST HOC




               LHS Amount                    RHS Amount
LHS VIRT                     RHS VIRT                      estimate      t.ratio   p.value
                Of Objects                    Of Objects
Hatching       18            Hatching        20            -1.09556    -3.46594    0.06557
Hatching       18            No Effect       20             2.10422     6.63264    0.00000
Hatching       18            Stippling       20             0.58597     1.83240    0.95539
Hatching       18            Halo/Outline    22             3.31746    10.41651    0.00000
Hatching       18            Hatching        22            -1.26984    -3.98718    0.01066
Hatching       18            No Effect       22             1.57143     4.93413    0.00016
Hatching       18            Stippling       22            -0.17460    -0.54824    1.00000
No Effect      18            Stippling       18            -1.41033    -4.51982    0.00116
No Effect      18            Halo/Outline    20             0.82540     2.59167    0.51213
No Effect      18            Hatching        20            -3.65112   -11.55075    0.00000
No Effect      18            No Effect       20            -0.45134    -1.42264    0.99741
No Effect      18            Stippling       20            -1.96959    -6.15917    0.00000
No Effect      18            Halo/Outline    22             0.76190     2.39231    0.66721
No Effect      18            Hatching        22            -3.82540   -12.01138    0.00000
No Effect      18            No Effect       22            -0.98413    -3.09006    0.18781
No Effect      18            Stippling       22            -2.73016    -8.57244    0.00000
Stippling      18            Halo/Outline    20             2.23573     7.16503    0.00000
Stippling      18            Hatching        20            -2.24078    -7.23731    0.00000
Stippling      18            No Effect       20             0.95900     3.08777    0.18888
Stippling      18            Stippling       20            -0.55925    -1.78354    0.96574
Stippling      18            Halo/Outline    22             2.17224     6.96155    0.00000
Stippling      18            Hatching        22            -2.41506    -7.73975    0.00000
Stippling      18            No Effect       22             0.42621     1.36590    0.99848
Stippling      18            Stippling       22            -1.31982    -4.22975    0.00404
Halo/Outline   20            Hatching        20            -4.47651   -14.16199    0.00000
Halo/Outline   20            No Effect       20            -1.27673    -4.02435    0.00923
Halo/Outline   20            Stippling       20            -2.79499    -8.74030    0.00000
Halo/Outline   20            Halo/Outline    22            -0.06349    -0.19936    1.00000
Halo/Outline   20            Hatching        22            -4.65079   -14.60304    0.00000
Halo/Outline   20            No Effect       22            -1.80952    -5.68173    0.00000
Halo/Outline   20            Stippling       22            -3.55556   -11.16410    0.00000
Hatching       20            No Effect       20             3.19978    10.16259    0.00000
Hatching       20            Stippling       20             1.68153     5.29782    0.00003
Hatching       20            Halo/Outline    22             4.41302    13.96112    0.00000
Hatching       20            Hatching        22            -0.17428    -0.55135    1.00000
Hatching       20            No Effect       22             2.66699     8.43734    0.00000
Hatching       20            Stippling       22             0.92096     2.91356    0.28284
No Effect      20            Stippling       20            -1.51825    -4.76549    0.00037
No Effect      20            Halo/Outline    22             1.21324     3.82422    0.01960
No Effect      20            Hatching        22            -3.37406   -10.63526    0.00000
No Effect      20            No Effect       22            -0.53279    -1.67939    0.98165
No Effect      20            Stippling       22            -2.27882    -7.18300    0.00000
Stippling      20            Halo/Outline    22             2.73149     8.54175    0.00000
Stippling      20            Hatching        22            -1.85581    -5.80337    0.00000
Stippling      20            No Effect       22             0.98546     3.08167    0.19175
Stippling      20            Stippling       22            -0.76057    -2.37841    0.67764
Halo/Outline   22            Hatching        22            -4.58730   -14.40369    0.00000
Halo/Outline   22            No Effect       22            -1.74603    -5.48237    0.00001
Halo/Outline   22            Stippling       22            -3.49206   -10.96474    0.00000
Hatching       22            No Effect       22             2.84127     8.92131    0.00000
Hatching       22            Stippling       22             1.09524     3.43894    0.07126
No Effect      22            Stippling       22            -1.74603    -5.48237    0.00001




                                            280
Appendix E

Chapter 3’s Participant Comments
on X-Ray Vision

This appendix displays users’ comments on the study in Chapter 3. Results like
space and N/A have been omitted from all of the sections in this appendix and have
been anonymized.


E.1      Favorite Visualizations and Why
  • Favorite: Wire Frame, Why: Consistent lines and i could see all objects
    clearly.

  • Favorite: Random Dot, Why: I could use it as a grid to assist in placing the
    shape.

  • Favorite: None, Why: Easy to see the pointer and object

  • Favorite: Saliency, Why: It was easier to position the objects accurately as
    opposed to the others.

  • Favorite: None, Why: They are pretty much all the same to me minus none
    does not have a bothersome outline

  • Favorite: Wire Frame, Why: It created a better frame of reference in my
    opinion

  • Favorite: Random Dot, Why: Didn’t freak out as much as edge based and
    saliency and gave me a good idea of what I was looking at

  • Favorite: Wire Frame, Why: Helped me to place the object with more
    confident.


                                       281
    E. CHAPTER 3’S PARTICIPANT COMMENTS ON X-RAY
VISION

 • Favorite: None, Why: No obstacles to obstruct vision

 • Favorite: Saliency, Why: Because it was fun to try to place the object with
   certain sections blacked out

 • Favorite: Wire Frame, Why: It helped to have corners I could use for refer-
   ence. I thought it might be better though if the mesh was more of a grid.

 • Favorite: Wire Frame, Why: I felt like it gave me a better understanding of
   the location of objects

 • Favorite: Random Dot, Why: Gave a good impression of the outer edge of
   the box, the cubes matching the shape helped distinguish edges and the gave
   good reference points for placement

 • Favorite: None, Why: The back plane in the none condition gave me enough
   cue and least cluttered view.

 • Favorite: Edge based, Why: Gave me a reference to where I was pointing (I
   felt like I was being guided more). Other than that none provided the least
   amount of visual clutter.

 • Favorite: Random Dot, Why: Not too sure why. It did not occlude my view.
   I felt that it just easier to use than the others.

 • Favorite: Edge based, Why: I feel that the wireframe and random dot con-
   ditions felt the most natural due to the uniform nature of the pattern designs.
   The edge-based and saliency patterns were a little bit too confusing visually
   and I felt that I couldn’t judge the depth as well with these patterns for some
   reason.

 • Favorite: None, Why: There was no distraction lines, etc. so it was much
   easier to place the object.

 • Favorite: Edge based, Why: Edge base, as it has some points of references,
   but not enough to hide the position of the objects.

 • Favorite: Wire Frame, Why: This one gave the best 3d visual reference to
   tell where I was placing the object compared to the others.


E.2    General Comments
 1. Very interesting. I think having some form of guidance helps a lot compared
    to none. Saliency was very frustrating though as I couldn’t see the objects
    from the views I wanted to.

                                      282
 2. Saliency was the hardest one because my sight of the shapes were often blocked.

 3. Everything’s are interesting and satisfied. But saliency bit hard to recognise
    object

 4. Cool experiment! It would be good if the box position was higher around the
    waist or chest for ease of use.

 5. I personally don’t fine myself using any of the vis as I think the box and a
    hexagon or ngon is enough to judge the xyz positions

 6. Edge based and saliency made the task harder than with no visualisation

 7. Very interesting experience, looking forward to more AR research!

 8. I found even the slight delay from moving the headset disorientating (especially
    when holding the orb thing)

 9. Wireframe or none are close, with none the back edge is clearer and the front
    of the box is like the wire frame

10. If there were no back plane in the none condition then I would have chosen
    the wire frame condition as my preference.

11. I think a grid pattern could be interesting.

12. The tracking was cutting out when I was standing in front of the cube so I
    was limited to manipulating the object from one specific angle. The precision
    was not great using the fishing reel manipulation and was abit jittery.

13. I was not always sure if I could see the back faces of the cubic workspace when
    working in every condition, but I think that the system could benefit from
    a brighter or easier-to-see backface edge design. This way I could relate the
    placement of objects relative to not only the faces positioned toward me, but
    relative to the Cuba’s rear sides/edges as well.

14. Controller issues, but that probably can be resolved at a later stage.

15. I really enjoyed participating in the study and getting to use the hololens
    technology.




                                       283
    E. CHAPTER 3’S PARTICIPANT COMMENTS ON X-RAY
VISION




                        284
Appendix F

Feedback from Chapter 5 Examing
the Effect of Perception on VIRTs

This section of the appendix displays the comments users had on the study in
Chapter 3. Results like empty space and N/A have been omitted from all of the
sections in this appendix and have been anonymized.


F.1     Things That Were Liked and Disliked About
        VIRTs
F.1.1    Halo
I liked this visualization because.

  • The outline effect was much clearer than the no effect as it made counting the
    nodes easier.

  • The objects are easyly to see

  • it have the outline to idicated the obh=ject which allows me to identiflied
    where is the object and how many so i won’t miss the object now

  • outline makes it very easy to count green shapes

  • It was very easy to count the green objects because of the white outline.

  • Objects towards the back of the frame were clearly highlighted, whereas in
    other conditions they were dim and sometimes impossible to see from the
    opposite side

  • it outline the objects, i can just sit and see the objects easily without moving
    around

                                       285
   F. FEEDBACK FROM CHAPTER 5 EXAMING THE EFFECT
OF PERCEPTION ON VIRTS

  • The halo effect added a lot of clarity to each object, which made it obvious an
    object existed even if it was very small. Combined with the inherant colour
    changes of the object made it quite clear to me.

  • it’s easy to use and because it’s 3D interface, I can check in different angle

  • I could easily idenity all shapes.

  • The outline effect made it easier to identify and differentiate betwen the blue
    circles.

  • Simple and effective. Easy to read, smooth.

  • There was a good amount of clarity to identify the green things

  • The shapes of the objects are very clear

  • Very easy to count green objects (counting total condition)

  • It made it easier to count and view objects that were overlapping with others

  • outline makes things a lot clearer

  • the outline made it way easier to spot the blobs

  • it was easy to notice the shape of the object so I could quickly count the
    numbers.

  • It’s easy and clear to count

  • with just outline it was much easier to count the green objects.

  • It is easy to locate all the green objects (in total). It is easy to seperate the
    green objects because there is a boundary around each of them. The appearnce
    of the material changes slightly with respect to the viewpoint.

  • I needed to move my head less often

I disliked this visualization because.

  • Some of the objects are not easy to see

  • it is still hard to tell if the onejct was inside other object or not , because of
    the colour and sometimes , other object block the sign of those object behind
    it includet he outline.

  • Dont know


                                         286
  • no

  • Sometimes on very small objects the halo effect was much stronger than the
    base colour of the object, making it sometimes difficult to determine the ob-
    ject’s colour.

  • Sometimes it was hard to identify the depth of a shape. The outline made
    them look a bit flat.

  • Good visualisation, but it requires alot of concentration to keep track of the
    blue circles while counting. (especially as the number of blue circles in the
    visualisation is high).

  • -

  • It is hard to recognize whether the white dots in the back are in the boundary
    of blue object

  • Little bit hard to find green objects when it placed behind blue cells.

  • I didnt have any dislikes

  • sometimes made me too confident so I had to recount

  • none

  • it was sometimes hard to detect when the object was too small.

  • It is a little hard to see how deep is the green object inside the blue objects.
    The green object has to be far away from the blue object, in order to decide
    that it is definetly not inside. But when the green object is just infront of
    the blue object, I felt it is hard to say if it is inside it or outside it. I would
    prefered that the material of the object could tell more about its depth or 3D
    structure.


F.1.2    Hatching
I liked this visualization because.

  • Some of the objects are easy to see

  • It have a web to identiflied the border of the whole red object so I won’t
    missunderstand the background outside of the object as the green object

  • easy to catch the edges of shapes



                                         287
   F. FEEDBACK FROM CHAPTER 5 EXAMING THE EFFECT
OF PERCEPTION ON VIRTS

  • It highlight the boundary of the objects, that help me to find out the objects,
    but not helping much.

  • The alternating cross-hatching density (direction?) occasionally made it clear
    where there were two separate objects.

  • It is visually the most unique.

  • It was still kind of useful for identifying the green objects I think but it would
    be hard for me to use long-term.

  • The shape of blue object is very easy to recognize

  • This condition is easy to count total green objects.

  • I didn’t have any likes for this visulization

  • Sometimes when overlapping crosshatching helped

  • it helps to notice depth information at some degree.

  • The opacity of the color of the object seems to change as a function of dis-
    tance, which helped in identifying the objects. The strides (or lines) on the
    object assisted in separating the objects (I could tell the nearby objects were
    disconncted rather than perceiving them as a one big object)

I disliked this visualization because.

  • This visualization was least preferred as the nodes were not easy to count at
    all. It was straineous to count them.

  • Some objects are difficults

  • the web strongly interupt my adjustment on green obejct’s position adn exis-
    tance.

  • still difficult to make out distant shapes, slightly worse performance compared
    to no effect

  • the billboard view of the objects, which make me difficult to see the side of
    the red objects and count the object number. Also, the cross-line could be
    slightly transparent, and the inside object cross-line could be more obvious.

  • The Hatching was very heavy and often felt like it was occluding the already
    sometimes faint colours of distant/deep objects. The effect helped to show the
    blue objects but did not help represent the green ones. Additionally, the effect

                                        288
   felt relative to my view rather than the world, meaning looking around the
   visualisation was not very natural and a bit confusing.

• it’s hard to count the number because the cross hatching make me confuse

• I really struggled to tell anything about and when I moved my head, because
  the visual changed, I couldn’t be certain that I hadn’t already counted a
  shape. I had no real idea what any shapes actually looked like. If the grid
  didn’t move with my head, maybe it would have been easier. This was the
  only visualization that gave me eye strain.

• The grid patterns was starting to give me a headache. I had to try to keep my
  head still to avoid physical discomfort.

• To my eyes it seemed glitchy (but maybe that’s how it’s supposed to be).
  Slightly laggy unfrotunately but overall really really hard to use.

• The objects were much harder to identify due to their shapes and their lack
  of clarity

• It is really lagging and makes me feel sick, the green object in the depth faded
  in the blue object.

• Difficult to distinguish when green objects are in blue cells or placing close to
  other objects

• The visualisation made the objects seem very custered and made me confused
  during counting objects that were within close proximity to each other

• Crosshatching the big red blob itself made it way too cluttered. I think if only
  the green and blue were I would have had an easier time and maybe even rated
  it highly. But with the red one crosshatched there is just too much going on

• Hatching made it difficult to see into objects

• the lines made it distracting to see the blobs, i had difficulties in determining
  for both within blue blob and whole volume

• I had to rotate my view to find out whether the green objects were overlapped
  or not.

• it’s not a little clear to see

• Some of the overlapping blue objects were hard to figure out.



                                      289
   F. FEEDBACK FROM CHAPTER 5 EXAMING THE EFFECT
OF PERCEPTION ON VIRTS

  • The opacity of the objects in the furthest point (too far away) was fading that
    I couldn’t decisively determine if it was an object or not. I wish I could rotate
    the scene to look behind the big blue objects.

  • It seemed to me that I had too much information


F.1.3    No VIRT
I liked this visualization because.

  • It was clear and easy to differentiate the colors of the artifacts

  • it is simple enough for everyone to use it,

  • It’s simple and clean. Transparencies are a good way to perceive depth

  • It was easy to figure out which green objects were inisde the blue shapes and
    which weren’t

  • Straightforward, interesting colours

  • 3d view of the objects

  • Without additional effects the visualisation was very clear to view. I think
    this made aspects of the visualisation more comfortable to parse.

  • It was easy to see green shapes through green shapes (as in they looked off
    color and it help identify some hiding behind others).

  • Everything felt clearer on the screen, so think I performed better at keeping
    track when counting green objects. Because of this I felt like the task was eas-
    ier, despite depth probably being harder to interpret. Additionally, I probably
    took my time abit longer and used more head movement to carefully interpret
    the data with this one compared to the grid visualisation (because the grid
    was causing physical discomfort when I moved too much).

  • Simple, smooth

  • It was easier to identify each object individually, there was nothing reducing
    clarity

  • The green objects are obviously in the blue object

  • Low complexity and relatively easy to learn.

  • Didnt have any particular likes for the visualizations


                                        290
  • No clutter, generally easy to see.

  • There were less distractions which made it easier for me to spot the green
    blobs for both within the blue volumes and whole volume

  • somehow seems natural for me.

  • In some cases it was easier to identify number of green objects due to color
    shades.

  • It is relatively easy to identidy the little green objects (in total) as the space is
    rather empty of everything but colors. The opacity or color seems to change
    relative to the depth and compostion of the object.

I disliked this visualization because.

  • Most of the objects are difficult to see and count

  • it is hard to identified which one is green, and sometimes blue objects were
    blocking the green one, causing me need more time and attention to identify,
    sometimes I even missed it

  • Don’t know

  • I couldn’t see the green objects far in the back

  • Colours can bleed together, it can be difficult to determine blue pockets to-
    wards the back of the volume

  • Sometimes, when the objects are far behind the objects, they can’t really see
    them clearly.

  • Very small or far back green objects became difficult to see without significant
    head movement as their coloring blended a lot with the surrounding volume.
    Also, looking through green objects produced an "xray" effect, which was in-
    consistent with the external visualisation.

  • This visualization do not have an outline. Sometimes it’s hard to count the
    number, and the 3D object is not too clear.

  • I really struggled with being confident that I was identifying all the shapes
    near the back. Also some shapes would kind of blend together and I’d have to
    move my head a lot to see them clearer.




                                          291
   F. FEEDBACK FROM CHAPTER 5 EXAMING THE EFFECT
OF PERCEPTION ON VIRTS

  • I felt that the depth was probably harder to interpret compared to some of the
    other visualizations (specifically the visualization where objects were outlined
    & also the staple one).

  • A bit hard to count with it. Sometimes, green thingies hide behind other green
    thingies, and this visualization does not help in realizing that.

  • THe green objects in the depth are hardly recognized

  • It is Difficult to percept depth, so I couldn’t recognize whether green objects
    are in the blue or not.

  • It felt like i had to strain my eyes too much in order to try and correclty count
    objects as it was hard to determine where one object started and another
    ended if they were overlapping

  • Sometimes confused if a green blob is half in a blue one or fully in one

  • after a while, it did made me a bit dizzy looking at it

  • It was difficult to figure out whether the green object is inside or behind the
    blue object.

  • some of the opposite sided objects are hard to identify if they are single or
    double (one over other)

  • It is harder to identify the relation between the blue and the green objects. In
    other words, it is hard to say if the green object is inside or outside the blue
    object.

  • I might have missed some objects due to occlusion and thickness of red and
    blue ’fluids’


F.1.4    Stippling
I liked this visualization because.

  • Almost all the objects are easy to see and count

  • because it has the dot to identify the size of the object so I tell it’s position
    once I spotted it

  • LIttle dots help see green elelemts

  • easier to spot the green objects because of the white spots on them

  • More legible than the others so far

                                        292
  • The dots on the objects helped me to differenciate different objects

  • The Stippling seemed to serve to add texture to the objects, which helped
    with perceiving object depth, and also identifying the shape of objects, which
    was important due to some green blobs overlapping in various ways.

  • it’s clear

  • It was easy on the eyes. Seemed sort of magical.

  • I feel like the task would probably be harder without the visualisation because
    it overall made the green objects easier to identify.

  • I felt more confident answering. It feels like the green thingies are more visible.

  • With the dots on the objects, it is easy to recognize the shapes of them.

  • It has low mental load and low complexity.

  • It was easy to notice objects that were behind others

  • Sometimes the white dots help difference between overlapping ones

  • the green blobs were easy to count

  • glitering surfaces were helpful for depth perception.

  • It is easy to locate the objects in total. The material is helping alot I felt that
    the reflectance changes based on the viewpoint and the dotts on the surface
    of the object are helping as well. It is easy to differentiate between the green
    objects (even when many green objects are too close, I can tell they are not
    one object but three that are too close). It is easy to tell whether the green
    object is inside or outside the blue object.

I disliked this visualization because.

  • It was not clear as compared to the outline effect. At times, I doubted if I had
    seen the node properly.

  • it is still confusing about the green object is within or not of the blue object
    because sometime the dot mixed up together and looks like half is within and
    half is outside

  • When having several shapes it gets confusing.

  • Slight lag, but forgiveable


                                         293
   F. FEEDBACK FROM CHAPTER 5 EXAMING THE EFFECT
OF PERCEPTION ON VIRTS

 • sometimes. the objects are way behind other objects, then i need to move
   around to see it.

 • The effect was very noisy and was uncomfortable at first due to the amount
   of detail in the scene.

 • I found it hard to identify which dots where for each shape.

 • The main issue was that sometimes when green objects were close together it
   was difficult for me to identify whether it was a single or multiple objects &
   as a result had to make a guess sometimes since it was unclear to me.

 • Slightly laggy

 • Like the cross-hatching, it lowered the clarity and made it harder to identify
   between objects

 • The white dots sometimes confuse me about the boundaries.

 • Difficult to distinguish when objects are grouped close.

 • Felt clustered

 • Still wasn’t too clear overall sometimes, though. can’t describe much but from
   training I’m guessing outline is going to be my fav

 • I wasn’t really sure if the number I put in is right because I’m counting all the
   blobs that touch the blue space

 • sometimes it distracts me for counting the number.

 • due to too much noise (dots), it was creating confusion.

 • My depth perception of point clouds was not as good as with NOTHING qand
   OUTLINE conditions




                                       294
Appendix G

Feedback from Chapter 6
Examining the limits of Depth
Perception

This Appendix focuses of the written feedback that was collected during the study
form Chapter 7. Results like empty space and N/A have been omitted from all of
the sections in this appendix and have been anonymized.


G.1       Things That Were Liked and Disliked About
          VIRTs
This section contains a list of all of the user comments related to things participants
liked and disliked about the Volumetric Illustrative Rendering Techniques (VIRT)s
can be found here.


G.1.1      Halo
I liked this visualization because.

  • some objects are obvious to see the differences

  • it is simple, the outlines do help to see depth

  • Different colors make identifying objects’ boundaries an easy task.

  • This visualization method has low complexity, easy to learn.

  • The visulisation was easy to comprehend and I think I could easily figure out
    which blue orb was closer



                                         295
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • it’s easy to see the areas and how close they are.

 • It was easy to understand straight away

 • I have no opinion

 • The shape of the object is very clear.

 • Halo helps readability and distinguishing between "particles"

 • the outline helped with the depth a bit

 • I felt confident that I could guage depth using the fadedness of the lines.

 • Simpe, easy to read, easy to compare colours

 • The scene was very clear and comfortable to view. The outlines defined the
   objects even when small and deep within the green objects.

 • the outline made it easier to see the different layers and to determine how close
   the blob was to me

 • it gives me a shape information which is helpful for depth perception.

 • It was good for counting

 • This the most preferred visualization because its clear and easier to compare
   both objects to decide depth perception

 • I think that the white lines are good to recognize.

 • The curve of the edge can support visibility

 • it was well prepared

 • there’s no much visual distraction that could happen in this visualization

 • it was easy to tell objects apart

 • outlines made it easier to see the red and green objects but so much for the
   blue objects.

 • Not too complicated

 • contouring helps with colorblindness




                                       296
I disliked this visualization because.

  • it is really hard to tell which one is closer in ar, especially it’s limit ur movement
    and graphics were rendered in x ray

  • Some object are not obvius ti see the distane

  • Some of the pairs felt the same

  • Hard to distinguish depth level.

  • sometimes, when the two objects are in the similar distance to me, then i need
    to move around to be able to see it.

  • I found it difficult to interpret depth at times.

  • It is hard to tell the depth of the objects and also hard to compare the depth
    of them.

  • I am not convinced the halo helps distinguishing depth levels but I guess my
    results will speak for themselves...

  • Sometimes the headsets resolution made the lines look to pixel-y.

  • Difficult to get a lot of contrast between shapes

  • I don’t think the outlines helped with the depth aspect very much.

  • some of the outlines looked identical that it made it hard to determine which
    one was closer so i could only guess

  • sometimes the outline reminds me a face, so it seems distracting.

  • It was a lot harder in this experiment to determine which was closer. It made
    the usual method of how much green is in front of the blue more difficult to
    determine I felt

  • Probably less compatability when using Holo2 Lens with another glassess

  • some of them were hard to distinguish whether right one is closer or the left
    one.

  • it’s similar to the basic "No Effect" visualization therefore had to rely on the
    color difference and other hint to tell the difference

  • it was hard to gauge the distance of the objects

  • Blue objects sometimes was hard to see it as 3D object

                                          297
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

  • Hard to tell the distance

  • the depth perception was not enhanced as much than with the dots or wire-
    frame visu


G.1.2     Hatching
I liked this visualization because.

  • That easy to see the objects’ different

  • Its the best one for describing shapes and it does help for depth perception a
    lot

  • Defining the blue object made it easier to localize it

  • This method is suitable to recognize 3D shape of tissue.

  • It was very easy to figure out which blue object was closest because of the
    grid.

  • it has the linein the objects which makes little bit easier to see the distance of
    the blue object

  • it’s more clear than outline one

  • Having the grid patterns made it easier to interpret depth than the 4th con-
    dition (standard visualisation).

  • The cross hatching gives a good reference of comparing depth

  • I think the parallel lines helped me "read" the depth

  • Its easy to get an idea of how high an object is.

  • I could tell the shape of each blue thing really quickly and it made me much
    more confident in my answer.

  • High contrast with crosshatching, nade discerning shapes easy

  • The cross hatching gave a sense of the topology of the object which I think
    helped when comparing the blue objects against each other.

  • at first i thought it was easy to use as i would choose the grids that "pop
    out"more as the ones that are closer

  • it gives me some texture information of the object.


                                        298
  • The hatching effect allows me to differentiate depth perception of the objects
    clearer

  • It looks intutive and quick to use

  • The hashing made it easier to tell which was closer as it provided more 3d
    space definition.

  • this was a bit easier to distinguese between compare to past 2 tests.

  • I could predict and estimate the depth based on the curve "angle" of the
    wireframe

  • it was easy to tell objects apart

  • Fun and vivid.

  • I could aprehend the shapes better

I disliked this visualization because.

  • i don’t like the x ray render texture on it

  • It can be a bit confusing. It would be awesome to be able to turn it on and
    off.

  • Sometimes difficult to see inside tissue(blue) because of occlusion of visualiza-
    tion effects.

  • no.

  • Due the the hatching on the outer objects, it added additional challenge to
    identifying the inner objects.

  • It’s slightly uncomfortable on the eyes, but didn’t have any impact on my
    performance. However, I think if I had to use it for a longer duration (i.e. an
    hour) then it would start to probably impact my performance.

  • It is lagging sometimes.

  • The angles of the lines move with the viewer’s position, which is distracting

  • It is visually overloading. Just a lot going on.

  • The crosshatching made comparing depth more difficult. I ended up relying
    on comparing colour



                                         299
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

  • While I got used to the view-dependent effect, I think I would like it more if
    the grid effect stayed static on the object like a wireframe. There was also
    some artefecting when looking at the objects too close.

  • afterwards as i progressed through this visualisation, the grids that used to
    assist me made it more difficult to determine how close it is to me, i had to
    focus really hard and stare at it a lot to determine the smallest details to guess
    which one is closer.

  • I felt like it oclude my eyesight sometimes.

  • It is somehow frozen automatically

  • The red blob would sometimes have hashing in front of the green/blue and
    cluttered it a bit. Not often though.

  • visualizations were difficult to see or observe when there’s too many wires on
    the wireframe

  • didnt have any dislikes

  • Compared to stippling the lines more heavier which made it bit more difficult
    to see,

  • Nothing that I can think of. My favorite so far.

  • it was a bit too much


G.1.3     No VIRT
I liked this visualization because.

  • n/a nothing good, just easy to use , nothing else then that

  • The colour is obvius

  • it is the cleanest but hard to tell depth

  • This method does not cause mental effort and eye fatigue.

  • simple and clean

  • The shape of the objects is very clear.

  • Simple, no distracting elements

  • It was easier to tell the difference when the difference was smaller


                                        300
  • I could usually tell straight away which one was closer.

  • Clean visuals made colour comparisons easy

  • This was the clearest way to view the scene, and I found it emphasized the
    colour and shading of each object making it easier to use that as a cue for
    depth. I am unsure if that was more accurate or not however.

  • it was quite easy to determine which bob was closer to me, at first I just kept
    staring at both of them to see which one was closer, but afterwards i noticed
    that it would be easier if i just stared between the two blobs, it makes it way
    obvious

  • it is intuitive and quite easier to understand.

  • the vagueness of the edge can be easily observed

  • Less clutter so I could focus on the green blob and how much I thought was
    closer to be compared to the blue blob

  • it was nice to look for the objects in 3rd dimentions.

  • its simple

  • didnt have any likes

  • compared to the other ones, this one didnt have any lines or dots which made
    it a bit easier to look at.

  • It’s simpler

  • it was simple

I disliked this visualization because.

  • i feel the xray texture is very disrupting

  • Some of the objests are really difficult to see the distance

  • It is very difficult to asses depth withoute any guides

  • I found it harder to perceive the blue object. I think the scene looked more
    planar (like clouds, there is depth that is hard to see) and my brain could not
    differentiate between the objects.

  • Some of images impossible to percept depth.



                                        301
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • In most cases both the visualisations loked the same thus, it was difficult to
   figure out which one was the closest.

 • somethings the transparent of the color confuses me, it make me feels like with
   high opacity is closer but sometimes it is not.

 • It was harder to visualise depth when compared to other visualisations

 • it’s not clear

 • I think that having the stippling/outline effects (cond 1/2) helped more with
   understanding depth.

 • It hard to compare the depth of two objects.

 • Harder to guess depth without depth cues

 • When I couldn’t tell straight away, no ammount of moving my head made me
   feel any more confident in my guess.

 • Difficult to discern relative depths between the two fields

 • Without additional effects, I sometimes found it hard to determine the shape
   of objects when further behind the green containers as they were very faint.

 • at first when i kept staring atthe blobs it was hard to differenciate and the
   quality i was seeing was a bit low but afterwards once i got used to it, it was
   fine.

 • I wasn’t sure to find correct answer for some difficult questions. Not much
   visual information to decide the answer.

 • This visualization was hard to use because the depth could not be differenti-
   ated.

 • It is difficult to judge the distance when the volume of both blue object is
   almost same with similar shade.

 • When the blue blobs were close together is was almost impossible to tell with-
   out having some form of hashing or outline etc

 • there isn’t much clue or information that I could use to help with the depth
   perception

 • found it hard to see object in other objets and to tell distances of objects



                                      302
  • At the same time, not having any white indicators made it hard to recongnize
    the object as 3D object. Sometimes it would be seen as a hole in the green
    object.

  • Hard to tell the difference

  • it was not obvious to determine which was closer


G.1.4     Stippling
I liked this visualization because.

  • the dot did helped me to didentiflied whch one is closer to me, and the system
    is easy to use

  • The colour is obvius to see

  • It felt easier than the outline/halo condition. The dots help see parallax when
    moving to the sides and this makes easier to determine depth.

  • It is much easier to determine how far is the blue object inside the green one.

  • This method seems easy to learn and not too much affect to original image.

  • The dots made it easy to understand the depth of the blue orbs

  • it gives me more feeling of how blue object close.

  • Changes in the pattern could be discerned by distance

  • the white dot is very helpful

  • No idea why but I felt that depth was easier to interpret than the first condition
    (halo/outline).

  • The dots on the surface give me a good reference to compare the depth

  • Simple to understand and I feel like it gave me a good idea of the depth of the
    objects

  • It was easy to visualize vertex points in 3d space

  • I could almost always tell straight away which was the right one. I had a good
    idea of the shape of each blue thing.

  • The white dots offered a point of contrast for comparison



                                        303
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

  • The "static" nature of the dots added extra detail that I felt made it easier to
    judge depth and shape.

  • the dots made it a bit easier to determine how close the blobs were to me

  • it gives me clearer depth information.

  • sometimes the bubbles helped determine one was closer

  • Similar to hatching, the Stippling was easy to visualize depth of the objects

  • The white dots help to understand the depth.

  • The stips may be visually easy to count

  • in this test it was way easier to identify the distance between both the blue
    objects. I was able to identify by the color density of the white dots and size
    of the white dots.

  • the dots were useful for telling the depth perception

  • didnt have any particular likes for the visualization

  • It looked very cool

  • I think visualising things helps me to better understand things

  • the paralax was easier to see

I disliked this visualization because.

  • the x ray texture is really disrupting

  • Some of the objects are difficult to see the differents

  • The dots could get in the way of other info., but very effective.

  • Little bit difficult to percept depth.

  • sometimes, when the distances between objects and my eyes are similar, then
    it will be little bit diffcult to make decision

  • Depth is still hard for me to interpret.

  • I like it

  • The dots were a bit inconsistent

  • Sometimes the dots looked a little cut off.

                                        304
 • The varying appearance of the dots (as in where they appeared on the green)
   obfuscated comparison to an extent

 • While the dots were static, there was some artefacting during head movement
   that was slightly confusing.

 • i had to keep moving my head sideways to determine which one was closer and
   due to that it made me a bit dizzy in the process

 • it was sometimes hard to discriminate shapes.

 • But at the same time, sometimes they didn’t feel as impactful. I mainly relied
   on how much green was layered over the blue

 • It is less supportive than the glid or line.

 • sometimes a cluster of dots could bring negative effect than helping

 • it was sometimes hard to view overlapping objects

 • It was a bit hard to see things because it was bit blurry sometimes. It took a
   while for my eyes to adjust but it was fine afterwards. Looking at it for a long
   time might hurt my eyes a bit.

 • Eyes gets exhausted in a while.

 • the dots density didn’t seem consistent


G.2     Validation for Answers Given in the Post-
        Study Questionaire
G.2.1    What Made The Visualization Easy to use?
 • The clarity of the nodes and visual was clear

 • I can see all the objrcts obviusly

 • it shows me the border/size i can tell where is it located at the volumn without
   interupt by colour

 • The outline helped count green shapes and wasnt too confusing

 • The white outline in the Halo condition made it really easy to spot the green
   objects

 • Clear outlines, seperating the shapes from the volumes

                                        305
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • the outlines of the objects helped me to differenciate different objects even
   they are far away from me, i still can use the outline to see it.

 • In terms of use, the halo effect was the most logical and quick to understand.
   It did not make the scene more complicated and helped add definition. I would
   put no effect ahead of stippling as it is also simple and not overwhelming, even
   though the stippling was advantagous (see below).

 • the outline is clear

 • I could pick out shapes easier with the outline and stippling. I could easily see
   shapes at the back aswell.

 • The readability of the visualisations.

 • No technical knowledge required. Task can be accomplished without learning
   any skill, it’s just a matter of persistence really.
    No effect and halo are the easiest, they feel comfortable and are aligned with
    our vision of the world every day. Stippling is not what we are used to be is
    still quite easy to use after using it for a while.

 • The halo and no effect were easier as they did not hinder the visual differences
   between the object, hence improving clarity and allowing me to identify each
   individual.

 • The clear boundary of the objects makes me easy to recognize the shapes of
   them

 • When visualization method can assist to ditinguish which one is closer.

 • The outlines allowed me to easily tell overlapping objects apart from each
   other

 • Outline made it obvious how many green blobs there were from various angles

 • simplicity, lightlights blobs behind others (halo)

 • outline/halo easily slows me which are the blobs and all i had to do was count

 • The outlines were helpful to discriminate the object so i could easity count the
   number.

 • It’s easy to figure out the shape of green object.

 • It was easier to count green objects on the no effect because there was no noice
   at all and I was able to identify objcts based on color density.

                                       306
 • the material of the objects and the ability to deduce the 3D shape.

 • Reduced the amount of head movement to attain the same level of certainty
   before vlidating the result


G.2.2   What Made The Visualization Difficult to use?
 • The complexity of the image displayed did not make counting easy

 • Almost all the objects are difficult to see and count

 • the cros hatch become an interuption while i trying to determine the postion
   of that object, sometimes i try to find out is there any object behind other
   object, the cross hatch block my sign and cause confusion

 • The cross Hatching was extremely confusing with a lot of shapes

 • The grid in the cross hatching condition were covering the green objects making
   them dificult to spot

 • No outlines, making the shapes difficult to discerne against long views of the
   volume

 • too much overlapping with each other, and the far, the color is too transparent,
   which makes the objects are even harder to see it.

 • The cross-hatching visualisation made the task much harder as the effect was
   very dense and moved with my view, which wasn’t "realistic" and made under-
   standing the visualisation much harder. I would say any of the dense effects
   added "more" to parse for the user.

 • too much elements and hard to count

 • Cross hatching made it hard to see shapes.

 • Stippling & Cross Hatching made it more difficult for me to identify whether
   close objects together was a single object or actually separate multiple ob-
   jects. Cross-hatching was by far the worst because it was causing me physical
   discomfort when I moved my head too much.

 • Mainly occlusion. When the green thingies are deep inside the volume, or
   when they are behind each others, they can be very hard to see. The stippling
   helps a lot with that but caused (for me) a slight discomfort at the beginning.
   It goes away after a while. Cross-hatching feels very weird and uncomfortable.
   I could not get used to it. On top of that, it was hard to see the added value.

                                      307
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

    Counting was the hardest with it, because of what I mentioned before but also
    because sometimes it would "merge" green thigies depending on the angle of
    view.

 • The stippling and cross hatching were present on all objects and made it harder
   to identify between each object, specifically with the cross hatching blocking
   the identification of objects beind it.

 • The objects in the depth will faded in the backgruond color.

 • Some visualization method have too much visual effect.

 • Lack of guidance and very clustered

 • Crosshatching helped with overlapping but it was really noisy. I think if the
   red blob itself wasn’t crosshatched it would be a better. It did help with
   overlapping blobs sometimes though

 • Clutter, some vis techniques just added too much clutter

 • there were many distractions in cross hatching, there were so many lines that
   it made it hard to see if there was a lightly coloured blob or even any blobs
   at all, so i had to count, and keep asking myself, "does that count as a blob?"
   and just guess

 • The visualiztion seems to much so it made harder to perceive depth informa-
   tion.

 • The boundary is blurred.

 • cross hatching and stippling were a bit hard to work with because there too
   many lines or dots on the ojects which were creating confusions sometimes.

 • I think Cross Hatching made the scene complicated so it was hard to count
   because there was alot of things (lines, specularity). The No effects lacks depth
   information so it is harder to locate the realtive depth of objects and also it is
   hard to seperate nearby objets.

 • The depth perception needed "parala’ help with me moving my head


G.2.3    Why Do You Think You Performed Better With These
         Visualization?
 • The outline effect was best because it was clear to count the nodes in both
   cases

                                       308
• Because I can count the objects

• becuse i can see the border of the object clerly so i can dtermine it’s postion

• I felt the most confident with Outlines and the No effect.

• In the Halo condition it was really easy to locate the green objects that’s I
  think I could accurately count the number of the green objects

• Easy identification of small shapes. Clear identification of shapes within the
  volume

• i can clearly see and differenciating different objects.

• Both the halo and stippling effects helped me perform better becuase they
  added additional information to the objects which assisted where objects where
  occuluded or deep. The halo effect felt better to use as the white colouring
  seemed stronger with depth, whereas the stippling still required me to move
  around a lot more to detirmine overlapping objects apart.

• the visulization is clear

• I could keep keep track of shapes with the outline and stippling.

• I think no effect was best because it was overall easier on me to mentally keep
  track of the green objects when counting. I felt like I made alot more mistakes
  (For example - forgetting whether I had already counted an object and having
  to start over) in comparison to the other visualisations. I believe Outline was
  the best for depth-perception.

• I think halo was good but not as effective in terms of helping counting the
  green thingies than the stippling one. Halo just provides a contour, so you it
  highlights the separate green thingies really well, but stippling’s random dots
  sometime reveals occluded green thigies I might not have seen otherwise. As
  much as I hated the cross hatching, I still think it helped me count compared
  to the no effect condition.

• Halo and no effect as I believe it was easier to identify and hence count each
  object

• I can finish the counting faster. It is easier for me recognize the shapes and
  locations of the objects.

• The Outlines \Halo visualization is most comfort interms of mental effort and
  esay to count total number of green objects.

                                      309
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • Felt it was easier to count

 • Outline made me more confident. No effect there was less noise so I could
   focus.

 • Same as above

 • I was more confident when the blobs were specifically outlined, it made count-
   ing much easier

 • The outlines highlights the object’s surface so I could answer faster with con-
   fidence.

 • The boundaries were clear and easy to count

 • with outlines I was easily able to count the number of green objects there was
   not much going on I was able to recognise if the object is inside the blue object
   or outside.

 • I think I had no difficulty with the two questions under the stiplling.

 • The easier the task seems to me, the better is my percieved performance


G.2.4    Why Do You Think You Performed Worse With These
         visualization?
 • The clarity was poor

 • I cannot see the objects

 • becasue the cross hatch cause interuption hwile i trying to locate the object ,
   especially hwne the object is behind other object

 • I got mostly confused with these stippling and cross hatching

 • The grid in the cross hatching condition made it really difficult to locate the
   green objects thus, I feel i couldn’t count accurately

 • Difficult to discerne shapes, worse performance made it harder to pay attention
   too

 • cannot easily differenciating the objects which might cause double counting or
   miss counting




                                       310
• The cross hatching effect was view dependent which did not feel natural and
  I don’t think helped determine object depth or positions. This caused me to
  want to look closer, but this would cause the effect to become heavier in my
  mind.

• it’s hard to count

• I had more trouble remembering which shapes I had already counted with "no
  effect" and cross hatching.

• I think I performed reasonably poor with stippling/cross-hatching due to read-
  ability and mental discomfort.

• I believe that no effect makes it really difficult distinguish two green thingies
  that are close together, but also two green thingies occluding each others.
  Cross hatching causes lots of headache; feels like there was too much infor-
  mation that was barely useful in the end. The "merging" effect I mention in
  the previous question also makes it difficult to distinguish between two green
  thigies that are too close together.

• Stippling and cross hatching as I found it harder to identify these and some-
  times felt like I was guessing

• It is hard to tell the location and the shapes of the objects, I need more time
  to double confirm.

• Too difficult to count grouped objects and hard to percept depth.

• Felt like i struggled with them due to clustered infromation

• Crosshatch was too noisy and made me less focused. Stippling to a lesser
  extent but it wasn’t that bad. I just feel I could focus more on the no effect
  and outline.

• Same as above

• i couldn’t determine if what i saw was classified as a green blob making it very
  hard to count

• It was quite demanding for me to count the number in the cross hatching
  condition.

• It was hard tounderstand if the green object was one or several attached to it.

• cross hatching has too many lines which was confusing me on green objects
  whihc are at the back of the whole volume.

                                      311
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • I needed more cues to deduce the depth. Parallax was not always enough.

 • The easier the task seems to me, the better is my percieved performance


G.3     General Comments
 • The display kept dropping in and out hence it made counting harder as I would
   have to recount again.

 • Some of the effects are easy to count

 • Thank you

 • Very cool tech, good work

 • the outline one is the best makes everyting easier. Would prefer to increase
   the transparency of the cross hatching line for the red objects and reduce the
   transparency of the green or blue cross hatching, would be better. Also would
   be better to remove the billboard functions for the cross hatching, I would
   prefer the objects are staying not follow my eyes to rotate, so i can see the
   objects from different views.

 • I think FPS performance is a factor as with the heavier effects (i.e. crosshatch-
   ing) the object would jump and jitter a little more, which when combined with
   the view dependent effect was a bit confusing.

 • I never really had a good strategy to do this task, for most of the task I was
   just trying to mentally keep track of what objects I had already counted. I
   would start from a random object and then go to the next closest object etc
   and keep track that way. Early on in the study I tried to go in order from top
   to bottom or furthest to closest but this strategy wasn’t working working well
   for me

 • Not that I can think of right now.

 • The cross harching one is really lagging and make me feel sick.

 • Mainly just crosshatching looks like it could be great as it helps detect overlaps.
   But it’s very noisy with crosshatching the red blob (I assume it is anyway).
   That or there is a lot of other blobs inside the red blob and it just gets too
   cluttered with crosshatching for me.

 • counting outloud and pointing made it easier for me to count, cause when i
   move my head, the headset moves along which made me dizzy, pointing keeps
   me focused and eases the dizziness

                                        312
  • Other than "No effect", the visulization was not stable when I move my head
    quickly. It may affect the ease of use (it may also get people dizzy) but I
    personally tried to ignore it.

  • I was able to se clearly thgough the green volumes with the No effect condition,
    which helped me spotting the background objects


G.4      Validation for Answers Given in the Post-
         Study Questionaire
During the post hoc comparison, participants were asked a series of questions stating
why they selected some answers over others. In terms of what was easy vs what
what they believed they did better with.


G.4.1     What Made The Visualization Easy to use?
  • Because the distance and colour are obvius

  • For some reason the dots immediately made sense and helped a lot with depth
    perception.

  • Stippling method does not much affect to original image and easy to recognize
    which one is closer.

  • the distances between the lines are easier for me to judge, i used that as the
    refences to make the decision.

  • the white dot

  • It has clear references to compare the depth of them.

  • Stippling is quite discreet but yet does it a good job (IMO) at indicating the
    depth. Halo also seemed to help for some reason (might be placebo) but the
    reason why I preffered it over the others is because it’s just simple to read and
    appealing.

  • Had points where its easy to tell the depth

  • Stippling and cross hatching let me tell the shape of the blue thing very quickly,
    but stippling was more consistent when I moved my head.

  • colour contrast as a depth indicator



                                        313
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • I think the stippling and outline effects were the easiest to use as they were
   quick to understand and didn’t experience artefacting when moving my head.

 • Stippling and CH allowed for a better understand of front and back

 • the visualizations were consistent which made it easier for me to use

 • The glittering surface made me easier to perceive depth information.

 • The outline/halo effect was clearer to differentiate the depth of different objects

 • The dots and lines are helpful.

 • The Halo/Outline can quickly help detect the distance by its quantities

 • Hatching allowed for an indication when the blue blobs were close together so
   I found it easier

 • with stippling it was very easy to identify the closest blue object due to color
   density of white dots and size of the dots in both the objects. I was also able
   to identify the shape of both of the blue objects due to different orientation of
   white dots.

 • The dots in stippling helped me to better understand the closest object easier
   than the other three visualizations

 • the objects visualization made it more understanding when judging distances

 • No effect was easy to see but hard to understand the 3D object.

 • Seems more vivid and 3D

 • I was primarily focusing on the blurryness of the blue object, the stipling and
   cros hatching helped to emphasize on the object paralax


G.4.2    What Made These visualization Difficult to use?
 • The distance is difficult to identify

 • Because there are no reference points. trying to figure out depth of semitrans-
   parent objects is always hard.

 • If visual effect is too much used, it might cause mental load and difficult to
   percept depth since occlusion of outer tissue’s visualization effect.

 • In many cases, i have to move around to be able to see which object is closer.


                                        314
• there is no outline and hard to check the depth of each object

• It does not have clear references to compare the depth

• Cross Hatching, if it’s supposed to work like I think it’s supposed to, is in
  theory a great idea. However it is very hard to distinguish between small
  differences in depth, on top of being very distracting because it changes based
  on the viewer’s location. No effect was definitely the hardest because appart
  from the decrease in saturation depending on the depth and the perspective,
  there is nothing to indicate depth.

• Covering the objects made it difficult to compare depth. I think if there is too
  much going on then it makes it harder not easier.

• If I couldn’t tell early on which shape was closer, no effect and halo (in part)
  did not help much when I moved my head.

• obfuscation of depth from additional components, ie, crosshatching

• The cross hatching was the most confusing simply because of the way it changes
  based on head position, wheras a static wireframe would have been easier.

• None cause an optical allusion that made the red and green parts of the vis
  move, Halo provided not depth understanding

• because i have no understanding as to how to differenciate the closeness of
  blob with these visualizations, so i had to get used to it in the beginning

• I don’t have much visual information so I had to back and forth to decide.

• No effect was the only least preferred visualization as it was harder to identify
  the object’s distances

• It’s difficult to understand the depth because the boundaries are blurred.

• There is little highlighted marks so that it might require users to come up with
  other strategy to identify the distance like the volume/brightness/blurriness
  etc.

• Outline did not improve how close they were and it felt like the green blob was
  harder to determine how close it was to me. No effect itself I could focus a
  lot on it though but it was difficult to determine which was closest when they
  were closer together.




                                      315
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • No Effect was a bit difficult to use because I felt like both the images are dull
   and blur. so with that it was a bit hard to identify which one is close even
   after head movement.

 • there isn’t too much visualization or effect to support my task

 • was hard to tell distances and see overlapping objects

 • For Cross Hatching and Stippling, it was a bit difficult to see but made it
   easier to recognize the bkue objects as 3D objects.

 • Too flat

 • I had little to no depth information


G.4.3    Why Do You Think You Performed Better With These
         visualization?
 • Becsue I can see the distance and all the colour of the object

 • Cross hatching does help a lot when you take a bit of time to move your head
   to the sides. i guess this was my best score.

 • Easy to understand purpose of visualization.

 • not sure, based on my guess

 • the white dot can help to check the object’s depth

 • It has clear references, the shape of the objects are clear.

 • Same reason as before

 • The halo did help a bit but it was still a bit harder than no effect.

 • Stippling was less annoying than cross hatching so I felt more confident with
   my answer quicker.

 • practice, high colour contrast for comparison

 • The stippling effect felt like it enhanced my ability to understand the 3D depth
   of the obejcts and wasn’t too noisy. I’m not sure if I actually did well with no
   effect, but it was easier to focus on just the colour of the object with no other
   white visualisation effect.

 • See above


                                       316
 • No effect had the least "distractions" although the other visualizations were to
   assist me in making a decision, i also didnt have to move much for no effect
   while i had to keep moving my head position for the others which made it
   dizzy for me

 • I think I relatively easily decide the answer in the stippling condition.

 • Personally, I felt more confident in judging the depth of the objects

 • I think that it’s easy to understand.

 • The Outlines/Halo condition ease the most consideration workload when mak-
   ing selections

 • Hatching I could tell the close together ones easier. No effect I could focus
   on the green blob size so I think I got a majority of the easier ones correct
   without second guessing. But was hard to tell the close together ones

 • It was much eaier to identify the shape and size of the blue objects with
   stippling effect. I was also able to identify the distance from me with the help
   of color density. I did not have to move much I was able to identify by sitting
   still. some of them required me movement because some of them looked kinda
   similar (90

 • There are many attributes from this visualization that helped me with the
   prediction "color, size, the dots"

 • was easier to tell objects apart

 • I used light reflections to recognize the objects but even then a bit of white
   visualization was necessary to see it as 3D object. But too much visualization
   made it hard to see.

 • I am more confident because it is easier to tell the distance.

 • same as previous question


G.4.4   Why Do You Think You Performed Worse With These
        visualization?
 • I coudnt see the distance of the object

 • For the ’no effect’ condition I was mostly guessing. So I imagine this was by
   worst score.


                                       317
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • I just had to rely on brightness and contrast of tissue, it might not indicate
   its depth.

 • the objects are not easy to see it clearly for the distance

 • it’s not clear

 • Very hard to compare the depth of the objects. Doesn’t have reference in the
   environment

 • Same reason as before

 • Too much going on (as well as covering the objects)

 • I think I felt like I was guessing too much with no effect.

 • obfuscation of depth and difficulty comparing colour

 • The cross hatching was a close second/third but I think due to the move-
   ment I never felt fullly confident with it. However it did help the most with
   understanding the shape of the objects. Finally I think the outline was the
   worst because even though it was very clear to understand, the outline made
   it harder to focus on the colour and felt like it exaggerated depth rather than
   make it feel clearer.

 • See above

 • i found it rather distracting and i had to think more to determine, and staring
   at the strippling and crosshatching made me a bit dizzy and had to keep
   reminding myself "which is closer"

 • It was most difficult when I had to choose the answer in the no effect condition.

 • The no effect was essentially harder to use as there was no way to identify the
   depth of the objects

 • There were times when I was a little unsure.

 • the cross Hatching hightlighted the edge but hinder the judgement of distance

 • Outline just felt difficult to determine in 3d space, made it more 2d style so I
   feel I got a lot incorrect.

 • I found no effect the worst because I was not able to focus on the blue object.
   it was blurry most of the time. I had to move to much to identify the real
   closest blue objects but I may have failed on some of them.


                                       318
  • There are too much distractions in the cross hatching "or wireframe"

  • was hard to tell ojects apart and gauge distances

  • Having no virtualization made it harder to see.

  • Had a few guess.

  • same as previous question


G.5      General Comments
The following comments were gathered from users as a method for them to provide
general feedback about the study or to say whatever they wanted to say.

  • If I can face to a white wall of the background will be better

  • Thank you!

  • One idea is allocate artificial spot light and shadow.

  • I’m not sure if you added it or not, but i used the transparency of the color to
    help to judge the locations.

  • The cross hatching one is bit of lagging.

  • Interesting study!

  • I think if cross hatch was more consistent when I moved my head, I might
    have liked it more.

  • fun study

  • I think my answers here are accurate for a sitting experience wher one cannot
    move around the visualisation to observe it. If I was able to use more signfi-
    cant movement to help judge depth, then the outline effect would have been
    preferable I think.

  • The allusion caused by the none condition was interesting, it look although
    the green and red areas were slowly rotating.

  • none

  • What if the objects in each study were the same size? Would that change the
    participants choices in idenifying the depth of each object?



                                       319
   G. FEEDBACK FROM CHAPTER 6 EXAMINING THE LIMITS
OF DEPTH PERCEPTION

 • Probably can add more guidance before the day so that paticipants may have
   more clear ideas on this research.

 • the eye can get fatigue especially in the final condition due to the long usage
   " 1hour" of focus therefore I’m concerned if my performance could be negatively
   affected in the last condition. In addition, the reflection from the HoloLens
   2 made me easy to see the researcher therefore causing distruptions at some
   point.

 • Overall, my eyes felt bit strained starting for a long time but otherwise, it was
   fine.




                                       320
Appendix H

Statements of Authorship

This section contains the Statement of Authorship clarifying the contributions of
the authors of the published works found in this thesis.




                                      321
,
,
,
                                                                                    Statement of Authorship


     Title of Paper


                                                  Published

                                                  Accepted for Publication
     Publication Status
                                                  Submitted for Publication

                                                  Unpublished and Unsubmitted work written in manuscript style




     Publication Details
     Relevant reference details




     PRINCIPAL AUTHOR

     Name of Principal Author
     (Candidate)



     Contribution to the Paper
     Brief description of your work in this
     publication




     Overall Percentage (%)

                                              This paper reports on original research I conducted during the period of my Higher
     Certification                            Degree by Research candidature and is not subject to any obligations or contractual
                                              agreements with a third party that would constrain its inclusion in this thesis. I am
                                              the primary author of this paper.

     Signature
                                                                                           Date
     Signatures will be redacted by the
     Library at the time of publication

     CO-AUTHOR CONTRIBUTIONS


     By signing the Statement of Authorship, each author certifies that:
         I.
        II.    permission is granted for the candidate in include the publication in the thesis; and
       III.    the sum of all co-


     Name of Co-Author 1

                                                                                                         The University of South Australia
                                                                                                                       www.unisa.edu.au
                                                                                                           CRICOS Provider No. 00121B
    Last updated: May 2023


,
    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    Name of Co-Author 2


    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 3

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 4

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 5

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 6

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 7

    % Contribution to the Paper



                                                The University of South Australia
                                                              www.unisa.edu.au
                                                  CRICOS Provider No. 00121B




,
    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 8

    % Contribution to the Paper


    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 9

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 10

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication




                                                The University of South Australia
                                                              www.unisa.edu.au
                                                  CRICOS Provider No. 00121B




,
,
,
,
                                                                                    Statement of Authorship


     Title of Paper


                                                  Published

                                                  Accepted for Publication
     Publication Status
                                                  Submitted for Publication

                                                  Unpublished and Unsubmitted work written in manuscript style




     Publication Details
     Relevant reference details




     PRINCIPAL AUTHOR

     Name of Principal Author
     (Candidate)



     Contribution to the Paper
     Brief description of your work in this
     publication




     Overall Percentage (%)

                                              This paper reports on original research I conducted during the period of my Higher
     Certification                            Degree by Research candidature and is not subject to any obligations or contractual
                                              agreements with a third party that would constrain its inclusion in this thesis. I am
                                              the primary author of this paper.

     Signature
                                                                                           Date
     Signatures will be redacted by the
     Library at the time of publication

     CO-AUTHOR CONTRIBUTIONS


     By signing the Statement of Authorship, each author certifies that:
         I.
        II.    permission is granted for the candidate in include the publication in the thesis; and
       III.    the sum of all co-


     Name of Co-Author 1

                                                                                                         The University of South Australia
                                                                                                                       www.unisa.edu.au
                                                                                                           CRICOS Provider No. 00121B
    Last updated: May 2023


,
    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    Name of Co-Author 2


    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 3

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 4

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 5

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 6

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 7

    % Contribution to the Paper



                                                The University of South Australia
                                                              www.unisa.edu.au
                                                  CRICOS Provider No. 00121B




,
    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 8

    % Contribution to the Paper


    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 9

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 10

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication




                                                The University of South Australia
                                                              www.unisa.edu.au
                                                  CRICOS Provider No. 00121B




,
,
,
,
,
,
,
                                                                                    Statement of Authorship


     Title of Paper


                                                  Published

                                                  Accepted for Publication
     Publication Status
                                                  Submitted for Publication

                                                  Unpublished and Unsubmitted work written in manuscript style




     Publication Details
     Relevant reference details




     PRINCIPAL AUTHOR

     Name of Principal Author
     (Candidate)



     Contribution to the Paper
     Brief description of your work in this
     publication




     Overall Percentage (%)

                                              This paper reports on original research I conducted during the period of my Higher
     Certification                            Degree by Research candidature and is not subject to any obligations or contractual
                                              agreements with a third party that would constrain its inclusion in this thesis. I am
                                              the primary author of this paper.

     Signature
                                                                                           Date
     Signatures will be redacted by the
     Library at the time of publication

     CO-AUTHOR CONTRIBUTIONS


     By signing the Statement of Authorship, each author certifies that:
         I.
        II.    permission is granted for the candidate in include the publication in the thesis; and
       III.    the sum of all co-


     Name of Co-Author 1

                                                                                                         The University of South Australia
                                                                                                                       www.unisa.edu.au
                                                                                                           CRICOS Provider No. 00121B
    Last updated: May 2023


,
    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    Name of Co-Author 2


    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 3

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 4

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 5

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 6

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 7

    % Contribution to the Paper



                                                The University of South Australia
                                                              www.unisa.edu.au
                                                  CRICOS Provider No. 00121B




,
    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication

    \




    Name of Co-Author 8

    % Contribution to the Paper


    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 9

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication


    Name of Co-Author 10

    % Contribution to the Paper

    Signature
    Signatures will be redacted by the   Date
    Library at the time of publication




                                                The University of South Australia
                                                              www.unisa.edu.au
                                                  CRICOS Provider No. 00121B




,
,
,
,
H. STATEMENTS OF AUTHORSHIP




                    346
