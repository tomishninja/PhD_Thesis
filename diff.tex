%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%DIF LATEXDIFF DIFFERENCE FILE
%DIF DEL D:\Documents\PhDThesisChanges\ThomasClarkeThesisOriginal/main.tex   Sat Aug 30 23:03:40 2025
%DIF ADD D:\Documents\PhDThesisChanges\PhDThesis\main.tex                    Sun Nov 16 12:57:15 2025
% NOTE: THIS IS A ROUGH TEMPLATE THAT WILL MOST LIKELY NEED CHANGES TO
% CONFORM TO YOUR INDIVIDUAL SITUATION
% THERE IS NO GUARANTEE OF THE ACCURACY OR COMPLETENESS OF THE TEMPLATE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt,a4paper,twoside,openright,final,titlepage]{report}
\usepackage[utf8]{inputenc}
\usepackage{layout}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{cite}
%DIF 15a15
\usepackage{fixme} %DIF > 
%DIF -------
%\usepackage{todonotes}

\setlength{\headheight}{15pt}

%\usepackage[square,sort,numbers,authoryear]{natbib}
%\usepackage{chapterbib}
%\setlength\bibhang{.3in}

% spacing info
\usepackage{setspace}
\usepackage{svg}
\onehalfspacing

\usepackage{fancyhdr}
\usepackage[T1]{fontenc}     % Better handling of accented characters
\usepackage{lmodern}         % Latin Modern fonts (an enhanced version of Computer Modern)

\usepackage[labelfont=bf]{caption}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
% For electronic submission use:
\usepackage[a4paper,width=150mm,top=25mm,bottom=25mm]{geometry}
% For printing use:
%\usepackage[inner=4cm,outer=2cm,top=2cm=bottom=2cm]{geometry}
\usepackage{tocbibind}
%DIF 40c41
%DIF < \usepackage[acronym,toc]{glossaries-extra}
%DIF -------
\usepackage[automake,acronym,toc]{glossaries-extra} %DIF > 
%DIF -------

%
% packages Added By Tom
%
\usepackage{comment}
\usepackage[section]{placeins} % used for the appendix section to keep tables in the correct section
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{array}
\usepackage[hidelinks]{hyperref}[hyphens]
\usepackage[percent]{overpic}
\usepackage{xcolor}
\usepackage{pdfpages}

% to fix table of contents
\newcommand{\nocontentsline}[3]{}
\let\origcontentsline\addcontentsline
\newcommand\stoptoc{\let\addcontentsline\nocontentsline}
\newcommand\resumetoc{\let\addcontentsline\origcontentsline}

%
%
%

% reference packages need to be last as they read other packages
\usepackage{cleveref}
\usepackage{chngcntr}

% for pusdo code
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath,amsfonts,amssymb,amsthm}

%
% Commands Created By Tom
%
\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1pt
    \futurelet \reserved@a \@xhline
}

\newcolumntype{"}{!{\vrule width 1pt}}
\makeatother
%
%
%

\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{\thechapter. #1 }}{}}
\renewcommand{\sectionmark}[1]{}
\counterwithin{algorithm}{section} % Number algorithms by section
\crefname{algorithm}{Algorithm}{Algorithms}

\fancyhf{}
\fancyhead[RO]{\bfseries\rightmark}
\fancyhead[LE]{\bfseries\leftmark}
\fancyfoot[C]{\thepage}
\pagestyle{fancy}

\usepackage[english]{babel}
\addto\extrasenglish{
  \def\subsectionautorefname{Subsection}
}
\addto\extrasenglish{
  \def\sectionautorefname{Section}
}
\addto\extrasenglish{
  \def\chapterautorefname{Chapter}
}

\newcommand\frontmatter{\pagenumbering{roman}}
\newcommand\mainmatter{\cleardoublepage\pagenumbering{arabic}}

\newenvironment{chapabstract}{%
	\begin{center}%
		\singlespacing
		\bfseries ABSTRACT
	\end{center}
    }%


\makeglossaries
    
%DIF PREAMBLE EXTENSION ADDED BY LATEXDIFF
%DIF UNDERLINE PREAMBLE %DIF PREAMBLE
\RequirePackage[normalem]{ulem} %DIF PREAMBLE
\RequirePackage{color}\definecolor{RED}{rgb}{1,0,0}\definecolor{BLUE}{rgb}{0,0,1} %DIF PREAMBLE
\providecommand{\DIFaddtex}[1]{{\protect\color{blue}\uwave{#1}}} %DIF PREAMBLE
\providecommand{\DIFdeltex}[1]{{\protect\color{red}\sout{#1}}} %DIF PREAMBLE
%DIF SAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddbegin}{} %DIF PREAMBLE
\providecommand{\DIFaddend}{} %DIF PREAMBLE
\providecommand{\DIFdelbegin}{} %DIF PREAMBLE
\providecommand{\DIFdelend}{} %DIF PREAMBLE
\providecommand{\DIFmodbegin}{} %DIF PREAMBLE
\providecommand{\DIFmodend}{} %DIF PREAMBLE
%DIF FLOATSAFE PREAMBLE %DIF PREAMBLE
\providecommand{\DIFaddFL}[1]{\DIFadd{#1}} %DIF PREAMBLE
\providecommand{\DIFdelFL}[1]{\DIFdel{#1}} %DIF PREAMBLE
\providecommand{\DIFaddbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFaddendFL}{} %DIF PREAMBLE
\providecommand{\DIFdelbeginFL}{} %DIF PREAMBLE
\providecommand{\DIFdelendFL}{} %DIF PREAMBLE
%DIF HYPERREF PREAMBLE %DIF PREAMBLE
\providecommand{\DIFadd}[1]{\texorpdfstring{\DIFaddtex{#1}}{#1}} %DIF PREAMBLE
\providecommand{\DIFdel}[1]{\texorpdfstring{\DIFdeltex{#1}}{}} %DIF PREAMBLE
\newcommand{\DIFscaledelfig}{0.5}
%DIF HIGHLIGHTGRAPHICS PREAMBLE %DIF PREAMBLE
\RequirePackage{settobox} %DIF PREAMBLE
\RequirePackage{letltxmacro} %DIF PREAMBLE
\newsavebox{\DIFdelgraphicsbox} %DIF PREAMBLE
\newlength{\DIFdelgraphicswidth} %DIF PREAMBLE
\newlength{\DIFdelgraphicsheight} %DIF PREAMBLE
% store original definition of \includegraphics %DIF PREAMBLE
\LetLtxMacro{\DIFOincludegraphics}{\includegraphics} %DIF PREAMBLE
\newcommand{\DIFaddincludegraphics}[2][]{{\color{blue}\fbox{\DIFOincludegraphics[#1]{#2}}}} %DIF PREAMBLE
\newcommand{\DIFdelincludegraphics}[2][]{% %DIF PREAMBLE
\sbox{\DIFdelgraphicsbox}{\DIFOincludegraphics[#1]{#2}}% %DIF PREAMBLE
\settoboxwidth{\DIFdelgraphicswidth}{\DIFdelgraphicsbox} %DIF PREAMBLE
\settoboxtotalheight{\DIFdelgraphicsheight}{\DIFdelgraphicsbox} %DIF PREAMBLE
\scalebox{\DIFscaledelfig}{% %DIF PREAMBLE
\parbox[b]{\DIFdelgraphicswidth}{\usebox{\DIFdelgraphicsbox}\\[-\baselineskip] \rule{\DIFdelgraphicswidth}{0em}}\llap{\resizebox{\DIFdelgraphicswidth}{\DIFdelgraphicsheight}{% %DIF PREAMBLE
\setlength{\unitlength}{\DIFdelgraphicswidth}% %DIF PREAMBLE
\begin{picture}(1,1)% %DIF PREAMBLE
\thicklines\linethickness{2pt} %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\framebox(1,1){}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,0){\line( 1,1){1}}}% %DIF PREAMBLE
{\color[rgb]{1,0,0}\put(0,1){\line(1,-1){1}}}% %DIF PREAMBLE
\end{picture}% %DIF PREAMBLE
}\hspace*{3pt}}} %DIF PREAMBLE
} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbegin}{\DIFaddbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddend}{\DIFaddend} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbegin}{\DIFdelbegin} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelend}{\DIFdelend} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbegin}{\DIFOaddbegin \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbegin}{\DIFOdelbegin \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelend}{\DIFOaddend \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddbeginFL}{\DIFaddbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOaddendFL}{\DIFaddendFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelbeginFL}{\DIFdelbeginFL} %DIF PREAMBLE
\LetLtxMacro{\DIFOdelendFL}{\DIFdelendFL} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddbeginFL}{\DIFOaddbeginFL \let\includegraphics\DIFaddincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFaddendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelbeginFL}{\DIFOdelbeginFL \let\includegraphics\DIFdelincludegraphics} %DIF PREAMBLE
\DeclareRobustCommand{\DIFdelendFL}{\DIFOaddendFL \let\includegraphics\DIFOincludegraphics} %DIF PREAMBLE
%DIF AMSMATHULEM PREAMBLE %DIF PREAMBLE
\makeatletter %DIF PREAMBLE
\let\sout@orig\sout %DIF PREAMBLE
\renewcommand{\sout}[1]{\ifmmode\text{\sout@orig{\ensuremath{#1}}}\else\sout@orig{#1}\fi} %DIF PREAMBLE
\makeatother %DIF PREAMBLE
%DIF COLORLISTINGS PREAMBLE %DIF PREAMBLE
\RequirePackage{listings} %DIF PREAMBLE
\RequirePackage{color} %DIF PREAMBLE
\lstdefinelanguage{DIFcode}{ %DIF PREAMBLE
%DIF DIFCODE_UNDERLINE %DIF PREAMBLE
  moredelim=[il][\color{red}\sout]{\%DIF\ <\ }, %DIF PREAMBLE
  moredelim=[il][\color{blue}\uwave]{\%DIF\ >\ } %DIF PREAMBLE
} %DIF PREAMBLE
\lstdefinestyle{DIFverbatimstyle}{ %DIF PREAMBLE
	language=DIFcode, %DIF PREAMBLE
	basicstyle=\ttfamily, %DIF PREAMBLE
	columns=fullflexible, %DIF PREAMBLE
	keepspaces=true %DIF PREAMBLE
} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim}{\lstset{style=DIFverbatimstyle}}{} %DIF PREAMBLE
\lstnewenvironment{DIFverbatim*}{\lstset{style=DIFverbatimstyle,showspaces=true}}{} %DIF PREAMBLE
\lstset{extendedchars=\true,inputencoding=utf8}

%DIF END PREAMBLE EXTENSION ADDED BY LATEXDIFF

\begin{document}

\newglossaryentry{SpaitalAwareness}
{
    name=Spatial awareness,
    plural=Spatial awarenesses,
    description={The ability to perceive and understand the spatial relationships between objects in the environment, as well as the position and movement of one's own body in relation to those objects. It is a fundamental aspect of human perception and is essential for many activities, such as navigation, manipulation of objects, and interaction with others.}
}

\newglossaryentry{Depth Perception}
{
    name=Depth Perception,
    plural=Depth Perceptions,
    description={The ability to perceive and judge the spatial relationships between objects in three dimensions. An essential aspect of human vision, allowing us to navigate and interact with the physical environment.}
}

\newglossaryentry{X-ray Visualization}
{
    name=X-ray Visualization,
    plural=X-ray \DIFdelbegin \DIFdel{Visions}\DIFdelend \DIFaddbegin \DIFadd{Visualizations}\DIFaddend ,
    description={A \DIFdelbegin \DIFdel{graphical effect that gives the user the impression of seeing through real-world objects. }\DIFdelend \DIFaddbegin \DIFadd{rendering or graphical technique used in computer graphics, medical imaging, and augmented reality to simulate the effect of looking through solid objects. Instead of physically penetrating materials as in real X-rays, this method selectively hides, fades, or highlights parts of a model or environment to reveal obscured internal structures or layers for analysis, learning, or interaction.}\DIFaddend }
    %DIF > description={A graphical effect that gives the user the impression of seeing through real-world objects.}
}

\newglossaryentry{X-ray Vision}
{
    name=X-ray Vision,
    description={\DIFdelbegin \DIFdel{The ability }\DIFdelend \DIFaddbegin \DIFadd{A perceptual or technological capability, either fictional or simulated, that allows an observer }\DIFaddend to see through \DIFdelbegin \DIFdel{solid objects and view the object’s internal }\DIFdelend \DIFaddbegin \DIFadd{opaque objects and reveal their hidden interior }\DIFaddend structures or the environment beyond \DIFdelbegin \DIFdel{it. }\DIFdelend \DIFaddbegin \DIFadd{them. In practice, X-ray vision is achieved through imaging technologies or visual effects that mimic this ability, providing insights not accessible through direct surface observation.}\DIFaddend }
    %DIF > description={The ability to see through solid objects and view the object’s internal structures or the environment beyond it.}
}

%DIF >  \newglossaryentry{xrayvisualization}{
%DIF >      name={X-ray Visualization},
%DIF >      plural={X-ray Visualizations},
%DIF >      description={A rendering or graphical technique used in computer graphics, medical imaging, and augmented reality to simulate the effect of looking through solid objects. Instead of physically penetrating materials as in real X-rays, this method selectively hides, fades, or highlights parts of a model or environment to reveal obscured internal structures or layers for analysis, learning, or interaction.}
%DIF >  }
\DIFaddbegin 

%DIF >  \newglossaryentry{xrayvision}{
%DIF >      name={X-ray Vision},
%DIF >      description={A perceptual or technological capability, either fictional or simulated, that allows an observer to see through opaque objects and reveal their hidden interior structures or the environment beyond them. In practice, X-ray vision is achieved through imaging technologies or visual effects that mimic this ability, providing insights not accessible through direct surface observation.}
%DIF >  }

\DIFaddend \newglossaryentry{AugmentedRealityX-rayVision}
{
    name=Augmented Reality enabled X-ray Vision,
    plural=Augmented Reality enabled X-ray Visions,
    description={The ability to overlay virtual content on top of the real-world environment, allowing users to see virtual objects positioned behind real-world obstacles.}
}

\newglossaryentry{microsaccades}
{
    name=Microsaccades,
    text=microsaccades,
    description={Microsaccades are tiny, involuntary eye movements that occur even when we try to maintain fixation on a point, serving important functions in visual perception and attention by preventing sensory adaptation and aiding in gathering additional visual information from the environment.}
}

\newglossaryentry{ar_g}{
    name= {Augmented Reality},
    plural= {Augmented Reality},
    description={A technology that overlays digital information and virtual objects onto the real world, enhancing the user's perception and interaction with their environment through a smartphone or smart glasses device. By blending the physical and digital realms, AR provides immersive experiences and practical applications across various industries, ranging from entertainment and gaming to education and industrial design.}
}

\newglossaryentry{ar}{
    type=\acronymtype, 
    name = {AR\glsadd{ar_g}},
    description={Augmented Reality\glsadd{ar_g}}, 
    plural={AR},
    first={Augmented Reality (AR)\glsadd{ar_g}}, 
    see=[Glossary:]{ar_g}
}

\newglossaryentry{vr_g}{
name= {Virtual Reality},
plural={Virtual Reality},
description={A technology that creates a simulated and immersive digital environment, typically experienced through a head-mounted display and motion-tracking devices. It transports users to a computer-generated world, allowing them to interact with realistic or fantastical scenarios, offering a range of applications from entertainment and gaming to training and therapy.}
}

\newglossaryentry{vr}{
type=\acronymtype,
name = {VR},
plural = {VR},
description = {Virtual Reality},
first={Virtual Reality (VR)\glsadd{vr_g}},
see=[Glossary:]{vr_g}
}

\newglossaryentry{ost_g}{
name= {Ocular See Through},
plural = {Ocular See Through},
description={A technology that provides a transparent display, allowing users to view the real world while overlaying digital information or virtual objects onto their field of vision. By enabling the simultaneous perception of the physical and digital environments, OST enhances user experiences and finds applications in various fields, including gaming, navigation, and industrial training.}
}

\newglossaryentry{ost}{
type=\acronymtype,
name = {OST},
plural = {OST},
description={Ocular See Through},
first={Ocular See Through (OST)\glsadd{ost_g}},
see=[Glossary:]{ost_g}
}

\newglossaryentry{vst_g}{
name= {Video See Through},
plural= {Video See Through},
description={A technology that utilizes a video feed from cameras to overlay digital information or virtual objects onto the real world, allowing users to see their surroundings through a display device. By merging live video footage with computer-generated graphics, VST enhances user perception and enables interactive experiences in fields such as gaming, medical imaging, and remote assistance.}
}

\newglossaryentry{vst}{
type=\acronymtype,
name = {VST},
plural = {VST},
description={Video See Through},
first={Video See Through (VST)\glsadd{vst_g}},
see=[Glossary:]{vst_g}
}

\newglossaryentry{dvr_g}{
name= {Direct Volume Rendering},
plural={Direct Volume Rendering},
description={A technique in computer graphics and visualization that allows for the direct generation of images from volumetric data, such as medical scans or scientific simulations. DVR enables the exploration and visualization of complex three-dimensional structures and phenomena by applying various rendering algorithms and shading techniques.}
}

\newglossaryentry{dvr}{
type=\acronymtype,
name = {DVR},
plural = {DVR},
description={Direct Volume Rendering},
first={Direct Volume Rendering (DVR)\glsadd{dvr_g}},
see=[Glossary:]{dvr_g}
}

\newglossaryentry{lmm_g}{
name= {Linear Mixed Effects Models},
description={A statistical modeling approach that incorporates both fixed effects and random effects to analyze data with hierarchical or clustered structures. LMMs are used to account for within-group dependencies and capture individual variations, making them suitable for studying longitudinal or repeated measures data in various fields, including social sciences, biology, and psychology.}
}

\newglossaryentry{lmm}{
type=\acronymtype,
name = {LMM},
plural = {LMMs},
description={Linear Mixed Effects Models},
first={Linear Mixed Effects Models (LMM)\glsadd{lmm_g}},
see=[Glossary:]{lmm_g}
}

\newglossaryentry{hsd}{
    type=\acronymtype,
    name = {HSD},
    plural = {HSD},
    description={Honestly Significant Difference},
    first={Honestly Significant Difference (HSD)}
}

\newglossaryentry{hmd_g}{
name= {Head Mounted Device},
description={Technological devices that are worn on the head and typically feature a display screen, enabling users to experience immersive visual and auditory content. Head Mounted Devices, such as virtual reality headsets or augmented reality glasses, provide a user-centric interface and are widely used for gaming, virtual simulations, training, and entertainment purposes.}
}

\newglossaryentry{hmd}{
type=\acronymtype,
name = {HMD},
plural = {HMDs},
description={Head Mounted Device},
first={Head Mounted Device (HMD)\glsadd{hmd_g}},
firstplural={Head-Mounted Displays (HMDs)\glsadd{hmd_g}}
see=[Glossary:]{hmd_g}
}

\newglossaryentry{ct_g}{
    name= {Computed Tomography},
    plural= {Computed Tomographies},
    description={A medical imaging technique that uses X-ray technology to create detailed cross-sectional images of the body. Computed Tomography (CT) scans provide valuable diagnostic information by capturing multiple X-ray images from different angles and combining them to generate a three-dimensional representation. CT scans are commonly used to diagnose various conditions and guide medical procedures.}
}

\newglossaryentry{ct}{
    type=\acronymtype,
    name = {CT},
    plural = {CTs},
    description={Computed Tomography},
    first={Computed Tomography (CT)\glsadd{ct_g}},
    firstplural={Computed Tomographies (CTs)\glsadd{ct_g}},
    see=[Glossary:]{ct_g}
}

\newglossaryentry{mri_g}{
    name= {Magnetic Resonance Imaging},
    plural={Magnetic Resonance Imagings},
    description={A medical imaging technique that uses strong magnetic fields and radio waves to create detailed images of the internal structures of the body. Magnetic Resonance Imaging (MRI) provides high-resolution images that help diagnose various conditions and assess the health of organs and tissues. MRI scans are widely used in medicine for their non-invasive nature and ability to visualize soft tissues with excellent contrast.}
}

\newglossaryentry{mri}{
    type=\acronymtype,
    name = {MRI},
    plural = {MRIs},
    description={Magnetic Resonance Imaging},
    first={Magnetic Resonance Imaging (MRI)\glsadd{mri_g}},
    firstplural={Magnetic Resonance Imagings (MRIs)\glsadd{mri_g}},
    see=[Glossary:]{mri_g}
}

\newglossaryentry{sus_g}{
    name= {System Usability Scale},
    plural={System Usability Scales},
    description={A widely used questionnaire-based method for assessing the usability of a system or product. The System Usability Scale (SUS) consists of a set of standardized statements and Likert scale responses that provide quantitative measures of perceived usability, effectiveness, and user satisfaction. SUS scores are commonly used in user experience research to evaluate and compare the usability of different systems or product designs.}
}

\newglossaryentry{sus}{
    type=\acronymtype,
    name = {SUS},
    plural = {SUS},
    description={System Usability Scale},
    first={System Usability Scale (SUS)\glsadd{sus_g}},
    firstplural={System Usability Scales (SUS)\glsadd{sus_g}},
    see=[Glossary:]{sus_g}
}

\newglossaryentry{sar_g}{
    name= {Spatial Augmented Reality},
    plural={Spatial Augmented Realities},
    description={A technology that enhances the physical environment by projecting digital content onto real-world surfaces, such as walls, floors, or objects. Spatial Augmented Reality (SAR) combines projection mapping techniques with computer vision and spatial tracking to create interactive and immersive experiences. SAR finds applications in areas such as art installations, architectural visualization, and interactive displays.}
}

\newglossaryentry{sar}{
    type=\acronymtype,
    name = {SAR},
    plural = {SAR},
    description={Spatial Augmented Reality},
    first={Spatial Augmented Reality (SAR)\glsadd{sar_g}},
    firstplural={Spatial Augmented Realities (SAR)\glsadd{sar_g}},
    see=[Glossary:]{sar_g}
}

\newglossaryentry{imu_g}{
    name= {Inertial Measurement Unit},
    plural={Inertial Measurement Units},
    description={A sensor module that combines multiple inertial sensors, such as accelerometers, gyroscopes, and magnetometers, to measure and track an object's motion and orientation in real time. Inertial Measurement Units (IMUs) are commonly used in applications that require motion sensing, such as robotics, virtual reality, and motion capture systems. IMUs provide essential data for estimating objects' position, velocity, and orientation changes.}
}

\newglossaryentry{imu}{
    type=\acronymtype,
    name = {IMU},
    plural = {IMUs},
    description={Inertial Measurement Unit},
    first={Inertial Measurement Unit (IMU)\glsadd{imu_g}},
    firstplural={Inertial Measurement Units (IMUs)\glsadd{imu_g}},
    see=[Glossary:]{imu_g}
}

\newglossaryentry{dicom_g}{
    name= {Digital Imaging and Communications in Medicine},
    plural={Digital Imaging and Communications in Medicines},
    description={A standard for managing, storing, and transmitting medical images and related data. DICOM (Digital Imaging and Communications in Medicine) enables interoperability between different medical imaging devices and systems, ensuring compatibility and consistent communication of medical information. It is widely used in healthcare settings to store and exchange medical images, such as X-rays, CT scans, and MRIs.}
}

\newglossaryentry{dicom}{
    type=\acronymtype,
    name = {DICOM},
    plural = {DICOM},
    description={Digital Imaging and Communications in Medicine},
    first={Digital Imaging and Communications in Medicine (DICOM)\glsadd{dicom_g}},
    firstplural={Digital Imaging and Communications in Medicines (DICOM)\glsadd{dicom_g}},
    see=[Glossary:]{dicom_g}
}

\newglossaryentry{pc_g}{
    name= {Personal Computer},
    plural={Personal Computers},
    description={A type of computer designed for individual use, typically consisting of a central processing unit (CPU), memory, storage, and input/output devices. Personal computers (PCs) are widely used for various purposes, including work, communication, entertainment, and personal productivity. They offer a flexible and customizable computing platform for users to perform tasks, run software applications, and access the internet.}
}

\newglossaryentry{pc}{
    type=\acronymtype,
    name = {PC},
    plural = {PCs},
    description={Personal Computer},
    first={Personal Computer (PC)\glsadd{pc_g}},
    firstplural={Personal Computers (PCs)\glsadd{pc_g}},
    see=[Glossary:]{pc_g}
}

\newglossaryentry{hu_g}{
    name= {Hounsfield Units},
    plural={Hounsfield Units},
    description={A unit of measurement used in computed tomography (CT) imaging to quantify the radiodensity of tissues. Hounsfield Units (HU) assign a numerical value to tissue attenuation, where higher values represent denser or more radio-opaque structures and lower values indicate less dense or more radio-lucent structures. HU values help in differentiating and characterizing various tissues and abnormalities in medical imaging.}
}

\newglossaryentry{hu}{
    type=\acronymtype,
    name = {HU},
    plural = {HU},
    description={Hounsfield Units},
    first={Hounsfield Units (HU)\glsadd{hu_g}},
    firstplural={Hounsfield Units (HU)\glsadd{hu_g}},
    see=[Glossary:]{hu_g}
}

\newglossaryentry{mr_g}{
    name= {Mixed Reality},
    plural={Mixed Realities},
    description={A technology that merges elements of both the physical and virtual worlds, creating a seamless and interactive environment. Mixed Reality combines aspects of both Augmented Reality (AR) and Virtual Reality (VR), allowing users to interact with digital objects while maintaining awareness of the real world. This technology finds applications in fields such as gaming, architecture, training, and collaborative workspaces.}
}

\newglossaryentry{mr}{
    type=\acronymtype,
    name = {MR},
    plural = {MR},
    description={Mixed Reality},
    first={Mixed Reality (MR)\glsadd{mr_g}},
    firstplural={Mixed Realities (MR)\glsadd{mr_g}},
    see=[Glossary:]{mr_g}
}

\newglossaryentry{caves_g}{
    name= {Cave Automatic Virtual Environment},
    plural={Cave Automatic Virtual Environments},
    description={Cave Automatic Virtual Environment, or CAVE, is an immersive virtual reality environment that utilizes multiple large displays and surround sound to create a fully immersive experience. Users typically stand within a room-sized cube, and the walls, floor, and ceiling are used as screens to display virtual content. A CAVE provides a highly immersive and interactive virtual reality experience, enabling users to navigate and interact with digital environments naturally and intuitively. It finds applications in fields such as scientific visualization, architectural design, and medical research.}
}



\newglossaryentry{cave}{
    type=\acronymtype, 
    name = {CAVE},
    plural = {CAVEs},
    description={Cave Automatic Virtual Environment}, 
    first={Cave Automatic Virtual Environment (CAVE)\glsadd{caves_g}}, 
    firstplural={Cave Automatic Virtual Environments (CAVEs)\glsadd{caves_g}},
    see=[Glossary:]{caves_g}
}

\newglossaryentry{ultrasound}{
    name= {Ultrasound},
    plural={Ultrasounds},
    text= {ultrasound},
    description={A medical imaging technique that uses high-frequency sound waves to visualize internal structures of the body. Ultrasound imaging, also known as sonography, produces real-time images by emitting sound waves and capturing their reflections off tissues and organs. It is commonly used for non-invasive imaging of various body parts, such as the abdomen, heart, and developing fetus, and plays a crucial role in diagnosing medical conditions.}
}

\newglossaryentry{xray}{
    name= {X-ray},
    plural={X-rays},
    description={A form of electromagnetic radiation commonly used in medical imaging to visualize the body's internal structures. X-ray imaging involves passing X-ray photons through the body, with denser tissues absorbing more photons, resulting in varying levels of brightness in the final image. X-rays are widely utilized for diagnosing bone fractures, detecting abnormalities, and examining structures like the chest, teeth, and bones.}
}

\newglossaryentry{gantry}{
    name= {Gantry},
    plural={Gantries},
    text= {gantry},
    description={The circular or cylindrical structure of a CT scanner that houses the X-ray tube and detector array. The gantry rotates around the patient during the scanning process, capturing X-ray images from various angles. It plays a crucial role in producing cross-sectional images of the body, which are then used to reconstruct detailed 3D images. The design of the gantry allows for precise positioning and accurate imaging during CT scans.}
}

\newglossaryentry{isosurface}{
    name= {Iso-surface},
    plural={Iso-surfaces},
    text= {iso-surface},
    description={A three-dimensional representation of a specific value or range within a volumetric dataset, often used in scientific visualization. An iso-surface is created by identifying voxels (3D pixels) within the dataset that match a certain threshold value and then rendering the surface that connects these voxels. Iso-surfaces are useful for visualizing structures and boundaries within complex datasets, such as medical imaging scans or simulations in various scientific fields.}
}

\newglossaryentry{pathology}{
    name= {Pathology},
    plural={Pathologies},
    text= {pathology},
    description={The medical specialty that involves the study and diagnosis of diseases by examining tissue samples, cells, and bodily fluids. Pathologists analyze the structural and functional changes in tissues and organs to understand the nature of diseases and their underlying causes. Pathology plays a crucial role in disease diagnosis, treatment planning, and research, contributing to advancements in medical knowledge and patient care.}
}

\newglossaryentry{pathological}{
    name= {Pathological},
    plural={Pathologicals},
    text= {pathological},
    description={Relating to the study of diseases or disorders in living organisms. Pathological conditions involve abnormal changes in structure or function that can lead to health issues. Pathological studies aim to understand diseases' causes, mechanisms, and effects, often involving laboratory analysis, imaging, and clinical observations.}
    see=[Glossary:]{pathology}
}

\newglossaryentry{ui_g}{
    name= {User Interface},
    plural={User Interfaces},
    description={The point of interaction between a user and a digital or mechanical device, system, or software application. User interfaces encompass visual, auditory, and tactile elements that allow users to interact with and control devices or software, making it easier for humans to operate and communicate with machines. Effective user interfaces are crucial for ensuring a positive user experience and optimizing usability.}
}

\newglossaryentry{ui}{
    type=\acronymtype, 
    name = {UI},
    plural = {UIs},
    description={User Interface}, 
    first={User Interface (UI)\glsadd{ui_g}}, 
    firstplural={User Interfaces (UIs)\glsadd{ui_g}},
    see=[Glossary:]{ui_g}
}

\newglossaryentry{gui_g}{
    name= {Graphical User Interface},
    plural={Graphical User Interfaces},
    description={A type of user interface that allows users to interact with a computer or software application through graphical elements such as icons, buttons, windows, and menus. Graphical User Interfaces make it easier for users to perform tasks and access functions by providing visual representations of actions and options. GUIs are commonly used in operating systems, software applications, and websites, enhancing user accessibility and usability.}
}

\newglossaryentry{gui}{
    type=\acronymtype, 
    name = {GUI},
    plural = {GUIs},
    description={Graphical User Interface}, 
    first={Graphical User Interface (GUI)\glsadd{gui_g}}, 
    firstplural={Graphical User Interfaces (GUIs)\glsadd{gui_g}},
    see=[Glossary:]{gui_g}
}

\newglossaryentry{ai_g}{
    name= {Artificial Intelligence},
    plural={Artificial Intelligences},
    description={A branch of computer science that focuses on creating intelligent machines capable of performing tasks that typically require human intelligence. Artificial Intelligence (AI) involves developing algorithms and models that enable computers to process data, learn from it, and make decisions or predictions. AI finds applications in a wide range of fields, including natural language processing, machine learning, robotics, and data analysis, revolutionizing industries and enhancing automation and problem-solving capabilities.}
}

\newglossaryentry{ai}{
    type=\acronymtype, 
    name = {AI},
    plural = {AI},
    description={Artificial Intelligence}, 
    first={Artificial Intelligence (AI)\glsadd{ai_g}}, 
    firstplural={Artificial Intelligences (AI)\glsadd{ai_g}},
    see=[Glossary:]{ai_g}
}

\newglossaryentry{rd_g}{
    name= {Research and Development},
    plural={Research and Developments},
    description={The process of systematically investigating, designing, and creating new products, technologies, or processes, as well as improving existing ones. Research and Development (R\&D) activities aim to enhance knowledge and capabilities, leading to innovation and advancements in various fields, including science, technology, and industry. R\&D plays a critical role in driving economic growth, competitiveness, and the development of cutting-edge solutions across sectors.}
}

\newglossaryentry{rd}{
    type=\acronymtype, 
    name = {R\&D},
    plural = {R\&D},
    description={Research and Development}, 
    first={Research and Development (R\&D)\glsadd{rd_g}}, 
    firstplural={Research and Developments (R\&D)\glsadd{rd_g}},
    see=[Glossary:]{rd_g}
}

\newglossaryentry{ctc_g}{
    name= {Circulating Tumor Cells},
    plural={Circulating Tumor Cells},
    description={Tumor cells that have detached from the primary tumor and entered the bloodstream or lymphatic system, allowing them to circulate throughout the body. Circulating Tumor Cells (CTCs) can provide valuable information about cancer progression, metastasis, and treatment effectiveness through a simple blood test. The detection and analysis of CTCs have implications for cancer diagnosis, treatment planning, and monitoring patient response to therapy.}
}

\newglossaryentry{ctc}{
    type=\acronymtype, 
    name = {CTC},
    plural = {CTCs},
    description={Circulating Tumor Cells}, 
    first={Circulating Tumor Cells (CTC)\glsadd{ctc_g}}, 
    firstplural={Circulating Tumor Cells (CTCs)\glsadd{ctc_g}},
    see=[Glossary:]{ctc_g}
}

\newglossaryentry{di_g}{
    name= {Digital Imaging},
    plural={Digital Imagings},
    description={The process of capturing, storing, and displaying visual information in digital format, allowing for the manipulation, analysis, and transmission of images using electronic devices and computer systems. Digital Imaging has transformed fields such as photography, medical imaging, and remote sensing, offering improved image quality, storage efficiency, and the ability to apply various image processing techniques. It plays a vital role in diverse applications, including healthcare, entertainment, and scientific research.}
}

\newglossaryentry{di}{
    type=\acronymtype, 
    name = {DI},
    plural = {DI},
    description={Digital Imaging}, 
    first={Digital Imaging (DI)\glsadd{di_g}}, 
    firstplural={Digital Imagings (DI)\glsadd{di_g}},
    see=[Glossary:]{di_g}
}

\newglossaryentry{shs_g}{
    name= {School of Health Sciences},
    plural={Schools of Health Sciences},
    description={An academic institution or department dedicated to the study and training of healthcare professionals in various disciplines, including medicine, nursing, pharmacy, and allied health fields. The School of Health Sciences (SHS) plays a crucial role in preparing students for careers in healthcare, research, and patient care, contributing to the advancement of medical knowledge and the improvement of healthcare services.}
}

\newglossaryentry{shs}{
    type=\acronymtype, 
    name = {SHS},
    plural = {SHS},
    description={School of Health Sciences}, 
    first={School of Health Sciences (SHS)\glsadd{shs_g}}, 
    firstplural={Schools of Health Sciences (SHS)\glsadd{shs_g}},
    see=[Glossary:]{shs_g}
}

\newglossaryentry{tesla_g}{
    name= {Tesla},
    plural={Teslas},
    description={A unit of measurement for the magnetic field strength used in Magnetic Resonance Imaging (MRI). The Tesla is the standard international unit, representing the strength of the magnetic field influencing the behavior of atomic nuclei during the imaging process. Higher Tesla values often contribute to improved image resolution and quality in MRI, impacting diagnostic capabilities in medical imaging.}
}

\newglossaryentry{tesla}{
    type=\acronymtype, 
    name = {T},
    plural = {T},
    description={Tesla}, 
    first={Tesla (T)\glsadd{tesla_g}}, 
    firstplural={Teslas (T)\glsadd{tesla_g}},
    see=[Glossary:]{tesla_g}
}

% \newglossaryentry{gauss_g}{
%     name= {Gauss},
%     description={A unit of measurement for the magnetic field strength, named after Carl Friedrich Gauss. In the context of magnetic fields, one Gauss is equal to one ten-thousandth of a Tesla (1 G = 10^-4 T). Gauss is commonly used in discussions related to magnetism and magnetic resonance, providing a scale for the intensity of magnetic fields.}
% }

% \newglossaryentry{gauss}{
%     type=\acronymtype, 
%     name = {G},
%     description={Gauss}, 
%     first={Gauss (G)\glsadd{gauss_g}}, 
%     see=[Glossary:]{gauss_g}
% }

\newglossaryentry{pathologist}{
    name= {Pathologist},
    plural={Pathologists},
    text= {pathologist},
    description={A medical professional who specializes in the study and diagnosis of diseases. Pathologists analyze tissues, cells, and bodily fluids to understand the nature and causes of illnesses. Their expertise is crucial in providing accurate diagnoses, guiding treatment decisions, and contributing to medical research.}
}

\newglossaryentry{gpr_g}{
    name= {Ground Penetrating Radar},
    plural={Ground Penetrating Radars},
    description={A geophysical method that uses radar pulses to image the subsurface. Ground Penetrating Radar (GPR) is employed for detecting and mapping features underground, such as utilities, archaeological artifacts, and geological structures. It plays a crucial role in various fields, including civil engineering, environmental assessment, and archaeological exploration.}
}

\newglossaryentry{gpr}{
    type=\acronymtype, 
    name = {GPR},
    plural = {GPR},
    description={Ground Penetrating Radar}, 
    first={Ground Penetrating Radar (GPR)\glsadd{gpr_g}}, 
    firstplural={Ground Penetrating Radars (GPR)\glsadd{gpr_g}},
    see=[Glossary:]{gpr_g}
}

\newglossaryentry{gpu_g}{
    name= {Graphics Processing Unit},
    plural={Graphics Processing Units},
    description={A specialized electronic circuit designed to accelerate the processing of images and videos for display on a computer screen. Graphics Processing Units (GPUs) are essential components in rendering realistic graphics for applications such as gaming, video editing, and complex simulations. Their parallel processing capabilities make them well-suited for handling large-scale graphical computations.}
}

\newglossaryentry{gpu}{
    type=\acronymtype, 
    name = {GPU},
    plural = {GPUs},
    description={Graphics Processing Unit}, 
    first={Graphics Processing Unit (GPU)\glsadd{gpu_g}}, 
    firstplural={Graphics Processing Units (GPUs)\glsadd{gpu_g}},
    see=[Glossary:]{gpu_g}
}

\newglossaryentry{fps_g}{
    name= {Frames Per Second},
    plural={Frames Per Second},
    description={A metric that measures the number of individual frames or images displayed in one second. In the context of digital displays, including augmented reality, a higher frame rate, measured in frames per second (FPS), contributes to smoother and more fluid visual experiences. Higher FPS is particularly important in applications such as gaming and video playback.}
}

\newglossaryentry{fps}{
    type=\acronymtype, 
    name = {FPS},
    plural = {FPS},
    description={Frames Per Second}, 
    first={Frames Per Second (FPS)\glsadd{fps_g}}, 
    firstplural={Frames Per Second (FPS)\glsadd{fps_g}},
    see=[Glossary:]{fps_g}
}

\newglossaryentry{sql_g}{
    name= {Structured Query Language},
    plural={Structured Query Languages},
    description={A domain-specific language used for managing and manipulating relational databases. Structured Query Language (SQL) provides a standardized way to interact with databases, enabling tasks such as data querying, insertion, updating, and deletion. SQL is widely employed in software development, data analysis, and database management systems.}
}

\newglossaryentry{sql}{
    type=\acronymtype, 
    name = {SQL},
    plural = {SQL},
    description={Structured Query Language}, 
    first={Structured Query Language (SQL)\glsadd{sql_g}}, 
    firstplural={Structured Query Languages (SQL)\glsadd{sql_g}},
    see=[Glossary:]{sql_g}
}

\newglossaryentry{gan_g}{
    name= {Generative Adversarial Nets},
    plural={Generative Adversarial Nets},
    description={A class of artificial intelligence algorithms introduced by Ian Goodfellow and his colleagues in 2014. Generative Adversarial Nets (GANs) consist of two neural networks, a generator, and a discriminator, which are trained simultaneously through adversarial training. GANs are widely used for generating realistic synthetic data, image-to-image translation, style transfer, and other tasks in the realm of artificial intelligence and machine learning.}
}

\newglossaryentry{gan}{
    type=\acronymtype, 
    name = {GAN},
    plural = {GANs},
    description={Generative Adversarial Nets}, 
    first={Generative Adversarial Nets (GAN)\glsadd{gan_g}}, 
    firstplural={Generative Adversarial Nets (GANs)\glsadd{gan_g}},
    see=[Glossary:]{gan_g}
}

\newglossaryentry{virt}{
    type=\acronymtype, 
    name = {VIRT},
    plural = {VIRTs},
    description={Volumetric Illustrative Rendering Techniques}, 
    first={Volumetric Illustrative Rendering Techniques (VIRT)}
}

\newglossaryentry{sdf_g}{
    name= {Sign Distance Field},
    plural={Sign Distance Fields},
    description={A mathematical representation used in computer graphics to define the distance from a point in space to the nearest surface of an object. Sign Distance Fields (SDFs) are commonly employed in rendering techniques such as ray marching and distance field rendering. They allow for efficient and accurate rendering of complex shapes and can be used in various applications, including augmented reality, computer-aided design, and video games.}
}

\newglossaryentry{sdf}{
    type=\acronymtype, 
    name = {SDF},
    plural = {SDFs},
    description={Sign Distance Field}, 
    first={Sign Distance Field (SDF)\glsadd{sdf_g}}, 
    firstplural={Sign Distance Fields (SDFs)\glsadd{sdf_g}},
    see=[Glossary:]{sdf_g}
}

\newglossaryentry{aabb_g}{
    name= {Axis Aligned Bounding Box},
    plural={Axis Aligned Bounding Boxes},
    description={A rectangular cuboid aligned with the coordinate axes, typically used in computer graphics and computational geometry to enclose a set of objects or to define the spatial extent of an entity. Axis Aligned Bounding Boxes (AABBs) are commonly employed in collision detection algorithms, rendering optimizations, and spatial partitioning techniques. They provide a simple and efficient way to approximate the geometry of complex shapes and facilitate various operations in virtual environments, including augmented reality applications.}
}

\newglossaryentry{aabb}{
    type=\acronymtype, 
    name = {AABB},
    plural = {AABBs},
    description={Axis Aligned Bounding Box}, 
    first={Axis Aligned Bounding Box (AABB)\glsadd{aabb_g}}, 
    firstplural={Axis Aligned Bounding Boxes (AABBs)\glsadd{aabb_g}},
    see=[Glossary:]{aabb_g}
}

\newglossaryentry{hci_g}{
    name= {Human-Computer Interaction},
    plural={Human-Computer Interactions},
    description={A field of study focusing on the design and use of computer technology, particularly the interactions between humans (the users) and computers. HCI is concerned with the ways humans interact with computers and design technologies that let humans interact with computers in novel ways. It involves the study of how people use technology, the design of user-friendly interfaces, and the development of new interaction techniques to improve the user experience.}
}

\newglossaryentry{hci}{
    type=\acronymtype, 
    name = {HCI},
    plural = {HCI},
    description={Human-Computer Interaction}, 
    first={Human-Computer Interaction (HCI)\glsadd{hci_g}}, 
    firstplural={Human-Computer Interactions (HCI)\glsadd{hci_g}},
    see=[Glossary:]{hci_g}
}

\newglossaryentry{jnd_g}{
    name= {Just Noticeable Difference},
    plural={Just Noticeable Differences},
    description={The smallest change in a stimulus that can be detected by an observer, typically defined as the threshold at which a difference becomes perceptible to a human sensory system. In the context of visual perception, the Just Noticeable Difference (JND) refers to the minimum change in brightness, color, or other visual attribute that can be perceived by the human eye. Understanding JND is crucial in various fields, including image processing, display technology, and user interface design, to ensure optimal user experiences and minimize perceptual errors.}
}

\newglossaryentry{jnd}{
    type=\acronymtype, 
    name = {JND},
    plural = {JND},
    description={Just Noticeable Difference}, 
    first={Just Noticeable Difference (JND)\glsadd{jnd_g}}, 
    firstplural={Just Noticeable Differences (JND)\glsadd{jnd_g}},
    see=[Glossary:]{jnd_g}
}

\newglossaryentry{pse_g}{
    name= {Point of Subject Equality},
    plural={Points of Subject Equality},
    description={In psychophysics, the Point of Subject Equality (PSE) refers to the stimulus intensity at which a subject perceives two stimuli as being equal in some specific aspect, such as brightness, loudness, or size. It is a key concept used to study perceptual phenomena and understand the relationship between physical stimuli and subjective perception. The determination of PSE plays a crucial role in various fields, including sensory research, human factors engineering, and user experience design.}
}

\newglossaryentry{slam_g}{
    name= {Simultaneous Localization and Mapping},
    plural={Simultaneous Localizations and Mappings},
    description={Simultaneous Localization and Mapping (SLAM) is a technique used in robotics and computer vision to construct a map of an unknown environment while simultaneously tracking the position of the observer within that environment. SLAM systems utilize sensor data, such as camera images or laser scans, to estimate the robot's trajectory and create a map of its surroundings. This technology is essential for autonomous navigation in various applications, including self-driving cars, drones, and mobile robots.}
}

\newglossaryentry{slam}{
    type=\acronymtype, 
    name = {SLAM},
    plural = {SLAM},
    description={Simultaneous Localization and Mapping}, 
    first={Simultaneous Localization and Mapping (SLAM)\glsadd{slam_g}}, 
    firstplural={Simultaneous Localizations and Mappings (SLAM)\glsadd{slam_g}},
    see=[Glossary:]{slam_g}
}

\newglossaryentry{pse}{
    type=\acronymtype, 
    name = {PSE},
    plural = {PSE},
    description={Point of Subject Equality}, 
    first={Point of Subject Equality (PSE)\glsadd{pse_g}}, 
    firstplural={Points of Subject Equality (PSE)\glsadd{pse_g}},
    see=[Glossary:]{pse_g}
}

\newglossaryentry{twofc_g}{
    name= {Two-alternative forced choice},
    plural={Two-alternative forced choices},
    description={Two-alternative forced choice (2AFC) is a psychophysical method used to measure the detectability or discriminability of a stimulus. In a 2AFC task, the observer is presented with two alternatives and must choose which one matches the target stimulus or is different from a reference stimulus. This method is commonly used in perceptual experiments to assess sensory thresholds and performance in various domains, such as vision, audition, and touch.}
}

\newglossaryentry{twofc}{
    type=\acronymtype, 
    name = {2AFC},
    plural = {2AFC},
    description={Two-alternative forced choice}, 
    first={Two-alternative forced choice (2AFC)\glsadd{twofc_g}}, 
    firstplural={Two-alternative forced choices (2AFC)\glsadd{twofc_g}},
    see=[Glossary:]{twofc_g}
}

\newglossaryentry{regression_analysis}{
    name= {Regression Analysis},
    plural={Regression Analyses},
    description={A statistical method used to examine the relationship between one dependent variable and one or more independent variables. The goal of regression analysis is to model the expected value of the dependent variable in terms of the independent variables, allowing for prediction and understanding of the underlying patterns. It is widely used in fields such as economics, biology, engineering, and social sciences.}
}


\newglossaryentry{classifier_g}{
    name= {Classifier},
    plural={Classifiers},
    description={A classifier is an algorithm in machine learning and statistics used to assign categories or labels to data points based on input features. It operates by learning patterns from labeled training data and then predicting the categories of new, unseen data. Classifiers are used in various applications such as image recognition, spam detection, medical diagnosis, and more.}
}

\newglossaryentry{non_euclidean_space}{
    name= {Non-Euclidean Space},
    plural={Non-Euclidean Spaces},
    description={A type of geometric space that is not based on the postulates of Euclidean geometry. In non-Euclidean space, the parallel postulate of Euclidean geometry does not hold, leading to the development of hyperbolic and elliptic geometries. These spaces have unique properties and are used in various fields, including physics, computer science, and cosmology, to model complex structures and phenomena.}
}

\newglossaryentry{perspective_corrected_projection}{
    name= {Perspective-Corrected Projection},
    plural={Perspective-Corrected Projections},
    description={A technique used in computer graphics to ensure that textures and objects are rendered accurately from the viewer's perspective. This method corrects distortions that can occur in standard projections, providing a more realistic and visually consistent representation of 3D objects on a 2D screen. Perspective-corrected projection is essential for applications such as virtual reality, gaming, and simulations to enhance the immersive experience and maintain visual fidelity.}
}

\newglossaryentry{exocentric_perception}{
    name= {Exocentric Perception},
    plural={Exocentric Perceptions},
    first={exocentric}, 
    description={A perspective in which an observer views the environment from an external viewpoint, as if looking at a scene from outside their own body. This contrasts with egocentric perception, where the observer's viewpoint is from within their own body. Exocentric perception is commonly used in virtual reality and other visualization technologies to provide a broader, more comprehensive understanding of spatial relationships and object interactions within an environment.}
}

\newglossaryentry{ordinal_perception}{
    name= {Ordinal Perception},
    plural={Ordinal Perceptions},
    first={ordinal},
    description={The ability to perceive and understand the relative order or ranking of objects or events without necessarily quantifying the differences between them. In visual perception, this often involves recognizing that one object is in front of another or that one event occurred before another. Ordinal perception is crucial for tasks that require an understanding of sequences and hierarchies.}
}

\newglossaryentry{bfs_g}{
    name= {Breadth-First Search},
    plural={Breadth-First Searches},
    description={An algorithm for traversing or searching tree or graph data structures. It starts at the root (or an arbitrary node) and explores all nodes at the present depth level before moving on to nodes at the next depth level. BFS is commonly used in shortest path problems in unweighted graphs, level-order traversal of a tree, and in scenarios where we need to explore all nodes at the same distance from the starting node.}
}

\newglossaryentry{bfs}{
    type=\acronymtype, 
    name = {BFS},
    plural = {BFS},
    description={Breadth-First Search}, 
    first={Breadth-First Search (BFS)\glsadd{bfs_g}}, 
    firstplural={Breadth-First Searches (BFS)\glsadd{bfs_g}},
    see=[Glossary:]{bfs_g}
}

\newglossaryentry{voxel_g}{
    name= {Voxel},
    plural={Voxels},
    text= {voxel},
    description={A volumetric pixel, or voxel, represents a value on a regular grid in three-dimensional space. Voxels are commonly used in 3D graphics, medical imaging, and scientific simulations to model spatial data, enabling efficient rendering and analysis of complex structures. Unlike traditional 2D pixels, voxels contain depth information, making them essential for volumetric rendering and spatial partitioning.}
}

\newcommand{\newglossaryentrywithacronym}[3]{
    %%% The glossary entry the acronym links to   
    \newglossaryentry{#1_gls}{
        name={#1},
        long={#2},
        description={#3}
    }

    % Acronym pointing to glossary
    \newglossaryentry{#1}{
        type=\acronymtype,
        name={#1},
        description={#2},
        first={#2 (#1)\glsadd{#1_gls}},
        see=[Glossary:]{#1_gls}
    }
}


%	\layout
	\begin{titlepage}
		\centering
		\vfill
		\vskip 1.75cm
		{\bfseries\Large
        %VIRTs: Volumetric Illustrative Rendering Techniques to support X-Ray Vision on Optical See Through Mixed Reality to impove Perception

        %VIRTs: Volumetric Illustrative Rendering Techniques to support the perception of X-Ray Vizulizsed  with Voxel Data on Optical See Through Mixed Reality

        %VIRTs: Volumetric Illustrative Rendering Techniques to support X-Ray Vision on Optical See Through Mixed to Optimize Perception of Voxel Data

        VIRTs: Volumetric Illustrative Rendering Techniques to Improve X-Ray Vision Perception on Optical See Through Mixed Reality

				%Visualizing Spatial Voxel Data with X-ray Visualizations on Optical See-Through Mixed Reality \\
                }
		\vskip 1.5cm

		\LARGE{Thomas John Clarke}\\
        \textit{\small BSoftwEng(Hons)}\\
        \LARGE

		\vskip 2.25cm	
            \large
		\textit{A dissertation presented for the degree of\\DOCTOR OF PHILOSOPHY}\\
		\vskip .05cm
		\includegraphics[width=.40\linewidth]{unisaLogo.jpg}
		\vskip .05cm
		Australian Research Centre for Interactive\\and Virtual Environments\\
		\vskip .25cm
		  March 2025\\
		\vskip 1.5cm
		\textbf{Supervisors:} \\
			A. Prof. Ross Smith (Coordinating)\\
			A. Prof. Wolfgang Mayer\\
			Dr. Joanne Zucco\\
	\end{titlepage}
	%\maketitle
	\frontmatter
%	\linenumbers
	\clearpage
	\thispagestyle{empty}
	\phantom{a}
	\vfill
	\vfill


        \Huge
	\begin{abstract}
        \normalsize

    
            % Introduction - What issue is this thesis trying to 
            Seeing through objects is a desirable trait not just for superheroes trying to escape from a burning building or locate a villain but for any professional who needs to consider what they cannot yet see. 
            This can be related to a construction worker who needs to know what is going on the floor above them or a surgeon who needs to know where an object is within the human body. 


            \gls{ar} X-ray vision can extend \DIFdelbegin \DIFdel{people'}\DIFdelend \DIFaddbegin \DIFadd{a user’}\DIFaddend s sight into concealed spaces and \DIFdelbegin \DIFdel{areas beyond virtual content within }\DIFdelend \DIFaddbegin \DIFadd{through virtual objects to reveal hidden areas in }\DIFaddend the real world.
            However, this functionality hasn't been well explored using \DIFdelbegin \DIFdel{ocular }\DIFdelend Optical See-Through (OST) Augmented Reality (AR). 
            OST AR devices allow users to perceive both the real world and digital overlays, enhancing their value for context switching between tasks that require virtual and real-world information. 
            % add display limitations of the equipment
            The ability to see the real world and the digital overlay simultaneously makes occlusion difficult, which is further impacted by the limited color \DIFdelbegin \DIFdel{gambit }\DIFdelend \DIFaddbegin \DIFadd{gamut }\DIFaddend of current OST AR displays, impacting colors like darker colors and blacks the worse. 
            This \DIFdelbegin \DIFdel{is all because the OST AR display utilizes a projector, which projects light onto a transparent display.
            Dim colors utilize less lightto be displayed.
            %DIF <  Also talk about transparency
            %DIF <  These limitations impact depth perception capability when merging physical and virtual worlds. 
            }\DIFdelend \DIFaddbegin \DIFadd{occurs because OST AR displays use optical combiners that direct light from a microdisplay into the user’s eyes while allowing real-world light to pass through.
            Less vibrant colors on a OST AR devices emit or reflect less light, making them appear dimmer on an optical see-through display.
            }\DIFaddend The physical and computational constraints of current OST AR devices pose further challenges for realizing X-ray vision for volumetric information. These challenges raise significant questions about designing volumetric X-ray vision in OST AR while minimizing depth misinterpretation and enhancing alignment accuracy.

            To address these challenges, this dissertation aims to develop an effective method of X-ray vision on OST AR devices for displaying volumetric visualizations. 
            It also examines how different visualization styles influence human depth perception in scenarios demanding precise spatial understanding. This research's findings establish a foundation for designing future X-ray vision visualizations and systems aimed at assisting humans in stressful and dangerous tasks where precise spatial comprehension is essential.
            %DIF < This research evaluates the effectiveness of newly developed visualizations (VIRTs) and examines their impact on perception and depth of understanding through controlled studies.
%DIF > This research evaluates the effectiveness of newly developed visualizations (VIRTs) and examines their impact on perception and depth of understanding through controlled studies.s

            %DIF <  be mind ful of what is in this
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  Background: So I have removed this because it wasn't great, and talking about the work of others in this context isn't great
            %DIF <  X-ray vision topics from many fields must be relied on to create this version. 
            %DIF <  Studies on human perception have been going on for over 2000 years, but X-ray vision is a comparatively newer but well-researched subject.
            %DIF <  The lessons previously learned from X-ray vision are crucial for developing various X-ray vision elements.
            %DIF <  The previous ways that \gls{dvr} was investigated in Human-Computer Interaction studies.
%DIFDELCMD < 

%DIFDELCMD <             %%%
\DIFdelend % Edit of Wolfgangs, talking about chapter 3 and 4
            Discerning objects and understanding their shape is a key concern for X-ray visualizations, and the way objects are visualized can profoundly affect this ability. 
            X-ray visualizations investigated in prior research were extended to use an \DIFdelbegin \DIFdel{Ocular }\DIFdelend \DIFaddbegin \DIFadd{Optical }\DIFaddend See Through (OST) Augmented Reality (AR) device. A study that examines the accuracy of depth perception and object placement in a mobile AR X-ray vision scenario between edge-based, saliency-based, and occlusion-based X-ray vision methods was investigated.
            Demonstrating that occlusions provide significant cues for depth perception but may hinder accuracy in placement tasks

            Inspiring the visualizations developed in this dissertation, I coined a new term called Volumetric \DIFdelbegin \DIFdel{Illusrative }\DIFdelend \DIFaddbegin \DIFadd{Illustrative }\DIFaddend Rendering Techniques (VIRTs), which captures a new category of volumetric techniques for OST AR, which adapt illustrative techniques that convey the shape of objects in 2D illustrations.
            The Hatching, Stippling, and Halo VIRTs are the first x-ray visualizations specifically designed for OST AR devices and used independently with direct volume rendering.
            This thesis demonstrates that VIRTs can effectively enhance spatial understanding in X-ray vision on OST AR devices and showcases the efficient algorithms applied in a system that achieves these effects.
            % below was taken from else where but is being placed back now.
            %DIF < Study 1 examines the accuracy of depth perception and object placement in a mobile AR X-ray vision scenario, demonstrating that occlusions provide significant cues for depth perception but may hinder accuracy in placement tasks. 

            %DIF <  % X-ray Vision
            %DIF <  Before creating a new form of X-ray vision, it is essential to understand how they function on \gls{ost} \gls{ar}. 
            %DIF <  Most X-ray visualizations have yet to be tested on \gls{ost} \gls{ar}.
            %DIF <  A system that was able to represent effects designed for \gls{vst} \gls{ar} devices on \gls{ost} \gls{ar} \glspl{hmd}.
            %DIF <  Using this system with a placement study enabled a comparison of the advantages of different implementations of X-ray vision to gain an understanding of how X-ray visualization functions on \gls{ost} \gls{ar} \glspl{hmd}.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF < 
            %DIF <  Comment block, this comes down to a difference between Ross and Wolfgang... 
            %DIF <  basically I know ross would like each chapter to have a paragraph, 
            %DIF <  but I agree with Wolfgang that seems a bit off topic for the goals of this type of document. 
            %DIF <  I'll keep this here as is and mark the end of it, but this would read better
            %DIF < 
            %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  % Direct Volume Rendering and VIRTs
            %DIF <  \gls{dvr} presents a 3D image shown on a 2D plane to communicate volumetric data, similar to how a photograph.
            %DIF <  This makes it difficult to position this data over the real world, as flat planes tend to have different properties.
            %DIF <  This thesis talks about the methods required to make \gls{dvr} work on \gls{dvr} and proposes the use of \glspl{virt} as X-ray visualizations.
            %DIF <  Illustrative techniques have been used in illustrations and 2D displays to communicate the shape of \gls{dvr}.
            %DIF <  Not only this, but they can also be used as a form of occlusion to showcase depth perception in illustrations. This led us to explore these techniques for X-ray visualizations. 
            %DIF <  %This dissertation presents three new X-ray visualizations designed to be utilized with \gls{dvr} in the form of \glspl{virt}.  
            %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  % Randomly Generating Volumes
            %DIF <  Volumetric data is challenging to find in large quantities and expensive to come across. 
            %DIF <  Making it almost impossible to conduct a controlled study using this technique. 
            %DIF <  To ensure this, we developed a type of volume that could be stored in a very small amount of data and a highly modular system to generate it.
            %DIF <  Making it possible for us to evaluate the \glspl{virt} in a highly modular fashion.  
            %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  % Perception Study
            %DIF <  Perception is a concern when designing visualizations to aid any part of performing any stressful high-precision task.
            %DIF <  The random volumes are utilized to perform one of the first controlled studies using direct volume rendering in general.
            %DIF <  This study evaluates \glspl{virt}'s effectiveness in conveying data and examines how various illustrative techniques impact perception.
            %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  % Depth Perception Study
            %DIF <  Depth perception of an X-rayable object is important to aid the precise interaction of the virtual world. 
            %DIF <  By utilizing volumetric Noisy Spheres that were able to perform the first psycho-physical experiment analyzing the perceived depth of \gls{dvr} objects when they are viewed through an \gls{ost} \gls{ar} device and compared the impacts of different \glspl{virt}.
            %DIF <  To determine the impacts of depth perception on \gls{dvr} and on each individual \gls{virt}, allowing us to determine the exact thresholds that \gls{dvr} is acceptable on \gls{ost} \gls{ar} \gls{dvr}.
%DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  This dissertation contributes novel, efficient visualizations of internal structures using Direct Volume Rendering techniques. Through controlled user studies, it examines their effectiveness and impact on perception and depth of understanding. It presents a sophisticated technical solution for efficient and effective volumetric rendering on resource-constrained OST AR devices to support these developments.
            %DIFDELCMD < 

%DIFDELCMD <             %%%
\DIFdelend % Study 2 and 3
            This dissertation conducts three controlled user studies focusing on depth perception, object recognition, and spatial accuracy in OST AR environments to evaluate the effectiveness of the visualizations. 
            Through controlled user studies, it examines their effectiveness and impact on object placement, perception, and depth of understanding of X-ray visualizations and VIRTs.
            %To evaluate the effectiveness of the visualizations, if the visualizations were developed in this dissertation, two controlled user studies were further done to test their impact on depth perception and object recognition in OST AR environments. 
            The first study examines the accuracy of depth perception and object placement in a mobile AR X-ray vision scenario, demonstrating that occlusions provide significant cues for depth perception but may hinder accuracy in placement tasks. 
            The perception study investigates spatial understanding in complex hierarchical scenarios, revealing that outline-based visualizations are the most effective for object identification among those examined in this thesis. 
            The depth perception study assesses the limits of depth perception using visualizations, and the results indicate that users can discern differences in the depth placement of irregular objects at sub-centimeter resolution.

            % Conclude the abstract
            This dissertation lays the foundation for future innovations in OST AR based X-ray visualizations. 
            Providing groundwork could enhance future X-ray applications, particularly in improving depth perception through adaptive rendering and advances in AR hardware. 
            These advancements will enhance applications in medical imaging, remote inspections, and augmented training, making complex spatial tasks more intuitive and precise.

	       
	%DIF <  Our goal is to take direct and unprocessed volume data and create X-ray visualizations that work using OSTAR devices. 
            %DIF <  This will make them useful for people who need to interact with the real world but still need to be aware of their surroundings.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD <             %%%
%DIF <  This thesis aims to determine an appropriate method of creating a volumetric \acrfull{ar} enabled \Glspl{X-ray Visualization} to enable \Glspl{X-ray Vision} within a professional situation.
            %DIF <  Occupations that require interaction with the real world, such as those found in either the construction, security, or medical fields.
            %DIF <  This can, however, be an issue because many of the current versions of \Glspl{X-ray Vision} have been designed for \acrfull{vst} \acrshort{ar} systems, which don't tend to showcase the real world to the user accurately.
            %DIF <  So we have focused this research on finding and learning how \Glspl{X-ray Vision} techniques work and how they can be applied in the real world before we start researching how volumetric X-ray vision should be when using an \acrfull{ost} \acrshort{ar} headset.
		%DIFDELCMD < 

%DIFDELCMD < 	%%%
\DIFdelend \end{abstract}
        \normalsize

	 \newpage \chapter*{Acknowledgments}

This was a long journey filled with emotion, drama, and friendship. 
I wholeheartedly regret taking on this PhD. At the same time, it did expose me to research, and I loved the work. This was an unimaginably difficult time of my life, which forced me to grow at an incredible rate and come face-to-face with challenges like abuse and poverty. 
%If you are reading this and thinking about doing a PhD, I would recommend against it wholeheartedly, but I have no reason for this perception of my experience.
Still, my time with the \DIFdelbegin \DIFdel{WCL }\DIFdelend \DIFaddbegin \DIFadd{Wearable Computer Lab }\DIFaddend was fun. Like-minded people surrounded me, and you all made me grow as a person immensely.
I already miss the freedom being surrounded by so much technology afforded me.
Despite the stress and drama that plagued me all this time.
Thanks to you, I have many positive memories and stories to tell.

First, I should thank my supervisors. I am sorry for all (well, most) of the arguments and heated debates. 
Most people would have left me once they realized that the research was not what they hoped for and that it was struggling to get out there. 
This was a hard PhD, there is no doubt about it, but I did not help it much. Still, I doubt that I should have done a PhD; I must have been quite a handful. 

Dr. Joanne Zucco, you always told me what I needed to hear, whether it was a hard truth or providing silence and listening. You are an amazingly gifted person, and I doubt I would have maintained my sanity without you.
Thank you for leaving your comfort zone to be the person I needed and staying on board the ship throughout this turbulent journey. 

Dr. Wolfgang Mayer, you always looked at things from the other angle; you found critics of my work that no one in my field, without you pushing me to do more and move further, I would not be where I am today.
I appreciate that you seem to be the person I believe you are to this day. It was an honor and privilege to have you as a supervisor.

Dr. Ross T. Smith, you must have gone through a lot for me, things that I probably never want to hear about. 
Thank you for being there when I got emotional and for listening to me complain about my reviewers every time.
Thank you for not abandoning me when I lost my temper because my research hit a dead end. 
Thank you for turning a blind eye when I was a broken human. 
Thank you for pouring so much work into me, even while you did not benefit much.
You are a great supervisor and a talented manager. I sincerely wish you the best for the rest of your career, and I hope things become more interesting for you. 

To the other people who helped me aid my research. Dr. Brandon Matthews, Thank you for being the intermediary between Ross and me and for talking me through some of the most technical conversations I may ever get to have. I would also like to extend a thank you to Dr. Adam Drogemuller. You helped me grow my understanding of data visualization experiences and helped me become a better data analyst. 

To everyone with whom I was lucky enough to experience my Ph.D., thank you for being there and for being good people (for the most part). 
Thank you for helping me when I needed it, letting me stir you up, and not telling me off for distracting you and having interesting problems. 
You all made my PHD worthwhile and eventful, and the stories we have made I will keep telling to people for the rest of my life.

I would also like to thank the staff of UNISA (now the University of Adelaide) for making me a stronger person by constantly presenting me with challenges that I had to overcome at some of the most difficult times. 
Regardless of my health, be it from cancer, depression, or heart conditions, you maintained your dedicated focus on this task.
Your efforts took me from a good manager to an excellent one as I learned how to fix the issues you manufactured in a subtle manner. 
After working with you all, I do not think there is any problem I can not handle. 

I would also like to thank Dr. Ameliorate and Bradly Ross Richards for providing me with your artwork for this thesis.
My friends (you know who you are) for putting up with me being broke, time-poor, and miserable for years on end (I am surprised you hang around).
I want to thank my employers over these years during this Ph.D who allowed me to keep my house, Dr. Michael Ulpen from Sabit University, and everyone One Forty One, especially my employer, Dr. Jan Rombouts, for allowing me to keep a roof over my head and understanding when I was being put in a difficult position.
And a special thanks to the team at Siemens Healthineers for teaching me the skills I needed to know for this Ph.D.

Finally, thank you to my family. I want to thank my mum (Kylie Dunstan) for housing me when I returned to the country during the COVID-19 pandemic and could not find a suitable place to rent. Also, my sister (Paige Wijeratne), along with my Mum, listened to me stress out and lose my hair over having no money and being harassed on all sides of my life. It must have been hard having to experience that. 
%I also want to thank my partner (Nguyễn Hà Châu) and her mother (Hà Thị Duy Trà) for feeding me and caring for me while I was busy writing my thesis and making sure that I left my computer at least once a month and not only ate food but ate healthily. This thesis would have harmed me more if not for your efforts. 
I also want to thank my partner (Nguyen Ha Chau) and her mother (Ha Thi Duy Tra) for feeding me and caring for me while I was busy writing my thesis and making sure that I left my computer at least once a month and not only ate food but ate healthily. This thesis would have harmed me more if not for your efforts. 

This research was supported by an Australian Government Research Training Program (RTP) Scholarship.

 \newpage 

    %\include{Declaration/author_declaration}

	 \newpage \chapter*{Thesis Declaration}
\vskip 0.8cm
\begin{flushleft}
I, Thomas Clarke, declare that:
\vskip 0.5cm 
Thesis presents work carried out by myself and does not incorporate without acknowledgment of any material previously submitted for a degree or diploma in any university; to the best of my knowledge, it does not contain any materials previously published or written by another person except where due reference is made in the text; and all substantive contributions by others to the work presented, including jointly authored publications, is clearly acknowledged.
%This thesis has been substantially accomplished during my enrolment in the degree.
\vskip 0.25cm
This thesis does not contain material that has been accepted for the award of any other degree or diploma in my name, in any university or other tertiary institution.
\vskip 0.25cm
No part of this work will, in the future, be used in a submission in my name, for any other degree or diploma in any university or other tertiary institution without the prior approval of The University of South Australia.
\vskip 0.25cm
This thesis does not contain any material previously published or written by another person, except where due reference has been made in the text. 
\vskip 0.25cm
The work(s) are not in any way a violation or infringement of any copyright, trademark, patent, or other rights whatsoever of any person.
\vskip 0.25cm
The research involving human data reported in this thesis was assessed and approved by The University of South Australia Human Research Ethics Committee. Approval \#: 
%[insert approval number(s)].
203273
% \vskip 0.25cm
% Written patient consent has been received and archived for the research involving patient data reported in this thesis.
% \vskip 0.25cm
% The research involving animal data reported in this thesis was assessed and approved by The University of South Australia Animal Ethics Committee. Approval \#: 
% %[insert approval number(s)].
% 203273
% \vskip 0.25cm
% The research involving animals reported in this thesis followed the University of South Australia and national standards for the care and use of laboratory animals.
% \vskip 0.25cm
% The following approvals were obtained prior to commencing the relevant work described in this thesis: 
% %[List approvals here].
% 203273
%\vskip 0.25cm
%Third-party editorial assistance was provided in preparation of the thesis by [insert name/company].
%\vskip 0.25cm
%The work described in this thesis was funded by [insert name of grant and grant identification numbers].
%\vskip 0.25cm
%Technical assistance was kindly provided by [insert name of assistant(s)] for [insert description of assistance] that is described in [insert location in thesis].
% \vskip 0.25cm
% This thesis contains published work and work prepared for publication, some of which have been co-authored. 
\vskip 0.25cm

    Signature:\\
    \vspace{0.5pt}
    \includegraphics[width=0.2\linewidth]{Declaration/signTransperent.png}
    \vskip 0.5cm
    Date: 7/04/2025
% \begin{minipage}[t]{0.15\textwidth}
%     \vspace{0pt}
%     Signature: 
%   \end{minipage}%
%   \begin{minipage}[t]{0.25\textwidth}
%     \vspace{0pt}
%     \includegraphics[width=1\linewidth]{Declaration/signTransperent.png}
%   \end{minipage}
% \vskip 0.5cm
% Date: 7/04/2025
 \end{flushleft} \newpage 
    \printglossary[type=\acronymtype,title=Acronyms]
    \printglossary[title=Glossary]
    %\include{ListOfPublications}

\stoptoc
    \setcounter{tocdepth}{2}
    \tableofcontents
\resumetoc

	\mainmatter
	\raggedbottom
    \glsresetall
    %\include{DemoFolder/Intro}
     \newpage 

%Hi Tom, just some quick notes on the structure we discussed yesterday (4th Dec):
%
%- Aim to tell a chronological story with the progression of how CT/MRI scans have been consumed by professionals
%
%- The story in your first paragraph does not need to change. Following this there is some cut/paste to re-structure and group similar topics i.e. 2D and 3D.
%
%
%- then move onto all 2D examples - films to present data, and mention how they can be placed around the walls in a room to navigate them
% - Introduce computer displays to share the information i.e. all 2D
% - Discuss 3D on traditional displays
% - Move to immersive systems such as MR / VR...
% - Highlight the strength and the posibilities of immersive systems and perhaps xray vision as an example.
% - Then highlight the challenges: Misalignment, accuracy, followed by stating that this problem is the focus on your thesis

\chapter{Introduction} \label{Chap:Introduction}
% Explain what medical data is

%Tom perhaps consider adding something like this to the opening paragraph to make it clear your reserach is motivated my medical, but is also generic in its investigation to support other application areas. 
This thesis investigates visualisations that aim to understand and improve perception when using \gls{ost} augmented reality.
The topic area has been motivated by medical visualisations due to \DIFdelbegin \DIFdel{their need for precision, although }\DIFdelend \DIFaddbegin \DIFadd{the need for data to be presented in a way that aligns intuitively with the user’s working area, ensuring that overlaid information is easy to interpret and supports natural understanding.
Although }\DIFaddend other application areas, such as manufacturing~\cite{Nguyen2016}, microbiological~\cite{Goodsell1989}, \DIFdelbegin \DIFdel{geolocial}\DIFdelend \DIFaddbegin \DIFadd{geological}\DIFaddend ~\cite{Mathiesen2012, Liu2017}, and meteorological tasks~\cite{HibbardL.1986}\DIFdelbegin \DIFdel{are also applicable.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{, may also benefit from the perceptual support of the AR visualizations of this research.
}\DIFaddend Since the late 1980s, medical visualizations have provided healthcare professionals with a detailed and intuitive means of understanding complex anatomical structures, physiological processes, and pathological conditions within the human body. By translating intricate medical data into visual formats, medical visualizations aid in diagnosis, treatment planning, and patient education.
Medical visualizations \DIFdelbegin \DIFdel{are }\DIFdelend \DIFaddbegin \DIFadd{can be }\DIFaddend derived from an extensive range of imaging techniques \DIFaddbegin \DIFadd{which can produce 3D datasets }\DIFaddend such as \gls{ct} scans, \gls{mri} scans, \gls{ultrasound}s, and \gls{xray}s as \gls{dicom} files.
These \DIFdelbegin \DIFdel{visual forms }\DIFdelend \DIFaddbegin \DIFadd{visualizations }\DIFaddend enable medical practitioners to assess and interpret the intricate spatial relationships between organs, tissues, and abnormalities accurately, facilitating more informed medical decision-making.
Moreover, these visualizations contribute to medical research by offering insights into disease progression, treatment effectiveness, and the development of innovative medical interventions.


Medical imaging technologies can utilize advanced photorealistic rendering derived from \gls{ct} and \gls{mri} scans, \gls{ai} methods for image understanding, and computational methods providing decision support to doctors. 
These technologies support medical professionals and help guide the planning and execution of procedures tailored to a patient's specific needs.
%\gls{ct} utilizes a series of X-ray beams that are reconstructed to produce a 3D image and are used to analyze bones, blood vessels, and tumors.
%\gls{mri} uses a strong magnetic field with a radio frequency to create a detailed image of the body by detecting the signals emitted as the corresponding hydrogen atoms realign. 
\gls{mri} images are typically used to view details with soft tissues but are also used to avoid the radiation caused by \gls{ct} scanners. 
\gls{ai}-based medical imaging is a newer technology used to view and provide ongoing analysis of tissue or indicate any immediate changes to the environment~\cite {Oren2020, Hosny2018}. 
%\gls{ai}-based medical imaging is created using sensor data that can be used to give new meaning to existing volumetric scans~\cite{Hosny2018, Bongratz2022}. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Chapter1/Images/pexels-shvetsa-MedicalFilms.png}
    \caption[Images of \gls{ct} and \gls{mri} films being utilized in a medical setting.]
    {
        Images of \gls{ct} and \gls{mri} films being utilized in a medical setting. 
        Anna Shvets took the photos shown here, which were licensed under a Creative Commons Attribuation licence~\footnotemark
    }
    \label{fig:MedicalFilms}
\end{figure}

% Jo Wants this to add limitations or current representations
%The display technology used to view tomographic data used in radiology and associated fields can impact the interpretation of the data.
Medical visualizations are commonly designed with 2D data in mind, as these tend to be viewed on desktop displays or tablets, or by more traditional display methods like placing films on light boxes and lit boards.  
2D visualizations require training in order to understand them, however experienced users can make more confident decisions with them due to understanding the \DIFdelbegin \DIFdel{layout }\DIFdelend \DIFaddbegin \DIFadd{structure }\DIFaddend of the human body~\cite{Jurgaitis2008}. 
Traditionally, \gls{mri} and \gls{ct} scans are printed onto transparent films and placed on adjustable light displays as shown in \autoref{fig:MedicalFilms}\DIFaddbegin \DIFadd{.}\DIFaddend ~\footnotetext{\url{https://www.pexels.com/cs-cz/foto/ruka-doktor-ukazovani-lekar-4226264/}}
\DIFdelbegin \DIFdel{.
}\DIFdelend These adjustable light displays allow different scan slices to be visible in the film. 
These films are not easy to interpret for people without proper training.


Display technology employed to view tomographic data in radiology and related fields can significantly impact data interpretation.
Most medical visualizations displayed on a desktop typically represent 2D slices through the body.
%Since most monitors are limited to two dimensions of displaying data this has been known to be the preferred method as it most accurately utilizes the capabilities of the devices and interfaces~\cite{Mandalika2018}. 
Desktop and tablet interfaces will normally display this data in three different axes—one each for depth, width, or breadth—with an additional display presenting a 3D model for spatial awareness~\cite{Mandalika2018, Mast2019}.
This limitation may lead to solutions that only consider the primary axis.
%This limits the methodologies surgeons have to problem-solve in this space, leading to solutions that only consider the primary axis. 
This would allow surgeons to perform surgeries with more flexibility than they would currently be comfortable with~\cite{Dicken2005}.
%Tradional displays also suffer from the ambient and direct lighting conditions found in radiology reading rooms, coupled with the radiologist’s positioning of the display, can result in errors due to poorly calibrated displays and reflections. 
3D visualizations in some situations have been observed to be easier to understand, like identifying the distance between the operating area and high-risk areas or different methods to access the infected area~\cite{Dicken2005, Rieder2009, Cheung2021}.
This indicates that surgical planning could be further optimized and enhanced if 3D visualizations were \DIFdelbegin \DIFdel{utilized properly. }\DIFdelend \DIFaddbegin \DIFadd{displayed in a technically correct manner which is also contextually appropriate rendering and presentation. on their given displays and in the representative display environment. %DIF > \fixme{Explain what properly means}
}\DIFaddend 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter1/Images/CTandMRIScan.png}
    \DIFdelbeginFL %DIFDELCMD < \caption[Two similar slices of different human heads.]{%
{%DIFAUXCMD
\DIFdelFL{Two similar slices of different human heads: on the left, one a Magnetic Resonance Imaging (MRI) scan}%DIFDELCMD < \footnotemark[1]%%%
\DIFdelFL{; and on the right a Computed Tomography (CT) scan}%DIFDELCMD < \footnotemark[2]%%%
\DIFdelFL{. Both these images are licensed under a Creative Commons Attribution license.}}
    %DIFAUXCMD
\DIFdelendFL \DIFaddbeginFL \caption[Two similar slices of different human heads.]
    {
        \DIFaddFL{Two similar slices of different human heads: on the left, one a Magnetic Resonance Imaging (MRI) scan}\footnotemark[1]\DIFaddFL{; and on the right a Computed Tomography (CT) scan}\footnotemark[2]\DIFaddFL{. Both these images are licensed under a Creative Commons Attribution license.
    }}
    \DIFaddendFL \label{fig:CTAndMRI}
\end{figure}

%DIF > \fixme{Write a short paragraph about what CT and MRI scans and how they can be better used. }
\DIFaddbegin 

\DIFaddend % How is medical data used
% Explain what types what is a type of immersive technologies are

% Current research is investigating the use of 2D images in spatial environments. There are several challenges that have been identified including different pixel density of the screen's real estate and a mismatch between the expectations created by the spatial layout~\cite{St.John2001, Mandalika2018}.

On desktop displays, 2D visualizations can present shapes more clearly than 3D visualizations~\cite{Abbey2021, Zhou2022}. 
However, stereoscopic 3D displays (\gls{ar} and \gls{vr}) have been shown to represent 3D medical data as effectively as, or even better than, 2D visualizations~\cite{McIntire2012}.
Surgeons can navigate data from any angle, leading to a more intuitive presentation and greater precision~\cite{Ahlberg2007, Zhang2016a, Akpan2019}. 
These findings suggest that stereoscopic immersive \glspl{hmd} allow users to perceive 3D data naturally and intuitively~\cite{Vetter2002, Merino2018, Cecotti2021}. 
Most research on medical \gls{ar} has focused on 2D visualizations~\cite{Asadi2024}, likely due to the challenges that still need to be addressed in using immersive \gls{ar} devices for medical applications.


\footnotetext[1]{\url{https://www.flickr.com/photos/reighleblanc/3854685038}}
\footnotetext[2]{\url{https://pixabay.com/photos/head-magnetic-resonance-imaging-mrt-254863/}}
% What accommodations need to be taken for medicine
The use cases for \gls{ar} in medicine are inspiring, but there is a range of limitations that need to be addressed before \gls{ar} becomes common practice~\cite{Jha2021, Beams2022}. 
Firstly, medical practitioners may not risk losing sight of the real world~\cite{Beams2022}. 
This can happen by placing any camera-based or occlusive display over the user's eyes. 
This makes a \gls{vst} \gls{ar} \DIFdelbegin %DIFDELCMD < \gls{hmd} %%%
\DIFdelend \DIFaddbegin \glspl{hmd} \DIFaddend solution difficult because these systems have the potential to partially blind the user if they are unplugged or disconnected~\cite{Beams2022}.
Systems that are designed for robotic surgery that utilize a video-based surgery tend to use a robotic arm with inbuilt measures for removing any apparatus from the patient where a well-trained expert can aid the situation~\cite{Rosen2011}, preventing any issues that can occur from minor distortions of the input images. 
\gls{ar} \glspl{hmd} currently show a distorted view of reality, which can negatively influence depth perception and mislead users when performing precise interactions depending on the real world environment~\cite{Bichlmeier2007, Sielhorst2006}.
%Both Bichlmeier et al.~\cite{Bichlmeier2007} and Sielhorst et al.~\cite{Sielhorst2006} even when using techniques designed to minimise this impact that some unrealism existed. 
%\gls{ost} \gls{ar} does not have the limitations of having the user view the real world superimposed by the virtual world, but however, still has the limitations in regards to how the virtual world is viewed as compared to the real world. 
\gls{ost} \gls{ar} avoids the limitation of superimposing the real world with the virtual world. However, because this technology simply overlays information over the screen and covers the user's eyes, it still faces limitations in how the virtual world is perceived in comparison to the real world.

% Problems regarding  limitations of augmented reality
\gls{ost} \gls{ar} devices misrepresent users' depth perception compared to real-world vision, even under ideal conditions. 
These devices are not designed to fully align the virtual world with the user's perception, offering a view of the real world that is 'good enough' for most applications. 
Some \gls{ar} displays, such as \gls{ost} \gls{ar}, and sensors, like infrared cameras, struggle under bright lighting conditions. 
This causes inconsistent sensor readings and may reduce display visibility due to the \gls{ost} \gls{ar}'s shaded lens used to reflect light as a display~\cite{Geng2013, Xiong2021}. 
This shaded lens struggles to render transparent objects and can mandate a limited color gamut~\cite{Geng2013, Xiong2021}\DIFaddbegin \DIFadd{.
}\DIFaddend This lack of visibility and distortions caused by the display diminish users' spatial awareness of the \gls{ar} graphics~\cite{Jamiy2019, Rosales2019, Al-Kalbani2019, Armbruster2008}. 
These issues are less pronounced with mobile \gls{ar} and \gls{vst} \gls{ar} displays~\cite{Krevelen2010, Martin-Gomez2021}. 

\gls{ost} \gls{ar} devices all misrepresent their users' real depth perception compared to their real-world vision, even under ideal circumstances.
\gls{ar} devices are not designed to fully align the virtual world with the user's vision; they provide a good enough world viewpoint into the real world.
Some \gls{ar} displays (like \gls{ost} \gls{ar} displays) and sensors (such as infrared cameras) do not work under bright lighting conditions as they dim the colors shown in the display and introduce inconsistent sensor readings. 
Users' spatial awareness is also lessened when viewing graphics using \gls{ar}~\cite{Jamiy2019, Rosales2019, Al-Kalbani2019, Armbruster2008}. 
\gls{ost} \gls{ar} require a shaded lens to properly reflect light back at the users~\cite{Geng2013, Xiong2021}. 
These issues do not exist as much with mobile AR~\cite{Krevelen2010}. 
\gls{ost} \gls{ar} displays also have difficulties when rendering \DIFdelbegin \DIFdel{transparent objects }\DIFdelend \DIFaddbegin \DIFadd{transparency }\DIFaddend due to their limited color \DIFdelbegin \DIFdel{gambit}\DIFdelend \DIFaddbegin \DIFadd{gamut and how it is used to inform brightness}\DIFaddend ~\cite{Geng2013, Xiong2021}.

This dissertation explores methods for visualizing spatial data to ensure AR-rendered graphics seamlessly integrate with physical-world objects, such as the human body. 
A key challenge for any AR system is accurately presenting objects located within or behind other objects~\cite{Bajura1992, Avery2009, Kalkofen2013, Parsons2021}.
To understand the data's location, humans typically need specific cues to perceive an object inside another, such as occlusion or a real-world metaphor like a hole. Without these cues, the data often appears to hover in front of the occluding object, giving the impression that it is positioned incorrectly.

% a 3D dataset or representation that can be visualized, often using techniques like volume rendering, which displays information throughout the 3D space
This research utilizes volume data as a medium, a 3D representation of space representing a set of samples from a given position relating to a given value in this area of the given voxel~\cite{Kaufman1999}.
volumes data allows this dissertation to move beyond traditional research by creating visualizations that utilize \gls{dvr}, allowing dynamic adjustments based on spatial and occlusion cues. 
This will enhance depth perception and improve integration with the physical environment. 
\gls{dvr} still allows for the same functionality as other graphical formats but provides new stereoscopic challenges that need to be overcome.
This dissertation also employs an off-the-shelf optical see-through (OST) AR device, the HoloLens 2~\footnote{\url{https://www.microsoft.com/en-au/hololens}}, which uses a lightly shaded lens and a waveform display to render images to ensure repeatability and consistency in testing these visualization techniques.

% % examples where controlling an ar device works well
% There are use cases of \gls{ar} where the aforementioned are not essential.
% Remotely controlling robotics using a Davinci system can be fine, since these systems are able to detect unsafe movements and have procedures in place~\cite{Rosen2011}. 
% However, when medical practitioners are using their hands or talking with patients, communicating these complex topics to their patients, they are required to see both the real world unobstructed and the virtual world where it is expected to be\cite{Marconi2017}.
% AR can provide spatial experiences that can be much more natural and intuitive than what is possible on desktop displays, for example, being able to move around and interact with objects in 3D space, but also provides the ability to see into 3D objects.

% % What is this thesis aiming to do
% This thesis explores how well volume data can be rendered in full volumes within solid real-world objects using an \gls{ost} \gls{ar} device.
% Ensuring objects appear where they are supposed to be, even if they are inside another real-world object.
% %While also ensuring that the distortion to depth perception, that is, by trying to remove this effect, is minimal.
% However, this thesis is not looking at testing the hardware. Instead, the research here uses off-the-shelf hardware of a HoloLens2~\footnote{\url{https://www.microsoft.com/en-au/hololens}}.
% Utilizing a lightly shaded lens with a waveform display to create images.
% %Many medical visualizations require the use of transparency.

\section{Motivations} \label{sec:IntroMotivations}
% Introduce this section 
% - Describe how what currently existed
% - Talk about what you did in Germany

% Talk about common visualizations in the field
Augmenting the real world using wearable devices (esp. \glspl{hmd}) is a promising field that has received much attention recently. Augmenting can take many forms, but seeing through or inside objects is particularly interesting in several domains.
It is common for immersive medical visualizations to present volume data as a 3D model so that it can be viewed from any direction. 
\DIFdelbegin %DIFDELCMD < \gls{hmd}%%%
\DIFdel{s }\DIFdelend \DIFaddbegin \glspl{hmd} \DIFaddend allow for a natural way to view 3D data as users change their perspective by moving around the volume~\cite{Kasprzak2019, Pratt2018}.
\DIFdelbegin %DIFDELCMD < \gls{hmd}%%%
\DIFdel{s }\DIFdelend \DIFaddbegin \glspl{hmd} \DIFaddend can be integrated with current systems to extend current working practices like cardiography~\cite{Kasprzak2019} and \DIFdelbegin \DIFdel{Pathology}\DIFdelend \DIFaddbegin \DIFadd{pathology}\DIFaddend ~\cite{Hanna2018}. 

Hanna et al.~\cite{Hanna2018} looked into the potential of \gls{mr} within \gls{pathological} use cases, for example, enabling remote communication for autopsies for guidance and instruction using a 3D overlay and sharing scanned copies of specimens. 
\Glspl{pathologist}, like many medical professionals, are required to be able to keep a sterile environment. 
\gls{ar} \glspl{hmd}, like the HoloLens, utilize gesture commands that can keep an environment sterile and avoid with a keyboard and mouse which may contaminate the space.
This hands-free interaction is particularly beneficial in medical situations where sterility is paramount.

%They found that this type of work required the pathologist to communicate clearly to someone outside.
%This was generally done by touching something physical and infrared with the sterile nature of the work. 
%These communication protocols are the cause of several possible contamination issues. 
%By doing this, they gave people the ability to interact remotely within a completely sterile environment, allowing them to view information via the HoloLens display and use gestures without touching while keeping a sterilized environment.
%Allowing the pathologist to utilize these controls using gesture controls allowed the \gls{pathologist} to communicate to people outside of the sterile environment while seeing similar screens that did not affect their environment. 
%Users were able to annotate 3D objects and have other practitioners see the same scene as they are currently seeing, and they could mark up the virtual objects in it. 

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Chapter1/Images/HannaEtAlAutopsyHoloLensExample.PNG}
    \caption[A image of Hanna et al.'s~\cite{Hanna2018} sterile system for \glspl{pathologist}]{A image of Hanna et al.'s~\cite{Hanna2018} sterile system for \glspl{pathologist}. Used with permission from the College of American Pathologists}
    \label{fig:HannaEtAlAutopsyHoloLensExample}
\end{figure}

A wide range of medical visualizations fall outside the scope of volumetric data
but benefit from 3D medical visualizations by using \gls{gui} in conjunction with \gls{ar}. Endoscopies use a long flexible tube with a camera and light at one end to examine the inside of the body. Immersive approaches have created visualizations that hover above the body to provide medical practitioners with an indication of where inside of the body they are looking~\cite{Garcia-Vazquez2020}. Other visualizations have focused on guiding the practitioner with a Graphical User Interface (GUI) so they can either instruct people on what to expect in a given situation, like explaining to a nurse what tools she might be expected to use next~\cite{Unger2019}, or they could be used to try to communicate the angle and direction an incision should be performed using a 2D GUI projected on the area around the patient~\cite{Mewes2018}.
These applications could benefit from better in situ visualizations at a single point, providing a more natural interface and a more intuitive sense of data. 

% visualizations that aim to overlay information over a body
Research has explored overlaying a visualization for instructing a practitioner on using a syringe~\cite{Agten2018, Li2019a}.
Overlaying a \gls{mri} or \gls{ct} visualization has also been attempted on several occasions where it was found to to ensure that the medical practitioner can focus on what they are doing while observing the patient spontaneously and also the practitioner with annotations and instructions~\cite{Pratt2018, Si2018, Pratt2018, Blum2012}.
Visualizations can also educate the internal anatomy to less experienced practitioners or novices~\cite{Bajura1992}.

\subsection{Siemens Internship: Holographic Overlay System}

\begin{figure}
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=0.8\textwidth]{Chapter1/Images/HolographicOverlaySystemWithAnotations.png}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=0.8\textwidth]{Chapter1/Images/HolographicOverlaySystem.png}
    \DIFaddendFL \caption{The Holographic Overlay system in use with a CT scanner.
    %DIF > \fixme{Miss aligned is incorrectly spelled}
    }
    \label{fig:HolographicOverlaySystem}
\end{figure}

% High-level overview and summary
Enabling medical practitioners to more intuitively understand their patients and helping to ease the education of medical data are intuitive benefits to companies specializing in creating machines to produce this data. 
%These benefits of overlaying began the foundation of an early internship I did at Siemens Forchheim, Germany, to understand what roadblocks still existed between using scan data overlaid over the patient's bodies. 
The benefits of data overlay formed the basis of an early internship I undertook at Siemens in Forchheim, Germany, where I explored the challenges still present in using scan data overlaid on patients' bodies.
At the commencement of this Ph.D. I was supported to travel to Siemens Forchheim, Bavaria, Germany, as an intern in the School of Health Sciences Digital Imaging \gls{ct} Research and Development Circulating Tumor Cells \gls{ai} department
%\gls{shs} \gls{di} \gls{ct} \gls{rd} \gls{ctc} \gls{ai} department. 
I was asked to create a system capable of overlaying the volumetric data from a CT scanner and calibrating it to the bed of a \gls{ct} scanner so it could be overlaid on the patient.
This allows radiologists to view the patient data as it is overlaid onto the patient and communicate with the current status of the \gls{ct} machine to track the patient.
%Moving the visualization as the \gls{ct} bed would also lift the patient up and down.

I was tasked with bringing the Booij et al.~\cite{Booij2019} research, which focused on a 2D interface that used a Kinect to collect a patient's anatomical data to aid radiologists in placing electrodes in the correct position on the patient's body utilizing a 3D reconstruction of the patient's body. 
The correct placement of electrodes on a patient’s body can be a difficult procedure to learn as it requires knowledge of where certain organs exist.
It is possible to determine where to place an electrode by evaluating the shape of the patient's body. 
An issue with the Booij et al.~\cite{Booij2019} system is that a 2D representation of a human is not always an accurate guide for varying body types since organs, muscles, and other body parts may be misplaced causing significantly different results~\cite{Kania2014,Hadjiantoni2021}.
By using a 3D \gls{hmd}, \gls{ar} display (HoloLens1), a 3D overlay could be made to instruct any operator of the \gls{ct} machine on how to use the system. 

After the CT scan had been completed, the system would replace the electrode guidance functionality with a visualization slice on a Sagittal Plane of the radiological data. 
This was replaced in the 3D system with an iso-surface or a \gls{dvr} visualization that was to be superimposed over the patient's body to better guide and inform the operator of the quality of the \gls{ct} scan.
This was aimed at lowering the required learning for \gls{ct} operators. 

% What was the system that was built
The gantry bed would need to be tracked, and its position known relative to the patient and the \gls{gantry} to provide the radiologist with instructions on where to place electrodes~\cite{Booij2019}. 
This allowed the volume to be visualized wherever the patient was, and it also provided flexibility for the operator to apply the electrodes. 
The final results of this system can be seen in \autoref{fig:HolographicOverlaySystem}\DIFaddbegin \DIFadd{.
}\DIFaddend The technical details of this system are described in more detail in \autoref{App:HolographicOverlaySystemDetails}.

Porting \gls{dvr} to a \gls{hmd} \gls{ar} display revealed more \DIFdelbegin \DIFdel{issues than expected due to its }\DIFdelend \DIFaddbegin \DIFadd{challenges than anticipated, primarily due to the }\DIFaddend high processing cost \DIFdelbegin \DIFdel{, and the visualization appeared slightly distorted compared to the environment around.
The visualization was rendered }\DIFdelend \DIFaddbegin \DIFadd{and the resulting visualization appearing slightly distorted in comparison to the surrounding environment.
This distortion was likely caused by the limited depth cues provided by }\gls{ost} \gls{ar} \DIFadd{displays, which made it difficult for users to determine the correct spatial position of virtual objects when physical objects were already present }\DIFaddend in the same \DIFdelbegin \DIFdel{manner as }%DIFDELCMD < \autoref{fig:SiemensVolumeRendering}%%%
\DIFdel{, but due to the stereoscopic properties of the }%DIFDELCMD < \gls{hmd} %%%
\DIFdelend \DIFaddbegin \DIFadd{location~\mbox{%DIFAUXCMD
\cite{Petri2018}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{One issue that was noted with the system was that the visualization was not dynamic and was only able to show a subset of a single range of the data before requiring to reload the information.
What was desired was the ability to view all of the data in a dynamic manner such as }\gls{dvr}\DIFadd{.  
For medical imaging, this capability is critical because important diagnostic features may not be confined to a single slice or range but distributed throughout the entire scan volume.
Being able to see the whole volume at once allows users to explore the data more intuitively and avoid missing relevant anatomical details or pathologies.
Particularly those with less specialized radiology training.
}

\DIFadd{A prototype visualization of this can be seen in }\autoref{fig:SiemensVolumeRendering} \DIFadd{which presents a cube rendered with }\gls{dvr}\DIFadd{, showing the same inflated abdomen dataset as in }\autoref{fig:HolographicOverlaySystem}\DIFadd{, developed at Siemens Healthineers.
This visualization has several issues when viewed through an }\DIFaddend \gls{ar} display \DIFdelbegin \DIFdel{3D view, it was clear that the user was looking at a cube . 
This }\DIFdelend \DIFaddbegin \DIFadd{(Microsoft HoloLens), the visualization initially appeared to be in the correct spatial position. However, the stereoscopic properties of the }\gls{hmd} \DIFadd{made it evident that the visualization was rendered as a cube rather than a volumetric structure.
As a result, this }\DIFaddend method was ultimately \DIFdelbegin \DIFdel{removed }\DIFdelend \DIFaddbegin \DIFadd{excluded }\DIFaddend from the final version of the product.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Chapter1/Images/VolumeRendering.png}
    \DIFdelbeginFL %DIFDELCMD < \caption[The same data shown in \autoref{fig:HolographicOverlaySystem} but using \gls{dvr}.]{%
{%DIFAUXCMD
\DIFdelFL{The same data shown in }%DIFDELCMD < \autoref{fig:HolographicOverlaySystem} %%%
\DIFdelFL{but using }%DIFDELCMD < \gls{dvr} %%%
\DIFdelFL{which was developed at Siemens Healthineers}}
    %DIFAUXCMD
\DIFdelendFL \DIFaddbeginFL \caption[This is a image of a cube rendered to show the same data shown in \autoref{fig:HolographicOverlaySystem} but using \gls{dvr}.]
    {
        \DIFaddFL{This is a image of a cube rendered to show the same data of an inflated abdomen shown in }\autoref{fig:HolographicOverlaySystem} \DIFaddFL{but using }\gls{dvr} \DIFaddFL{which was developed at Siemens Healthineers.
    }}
    \DIFaddendFL \label{fig:SiemensVolumeRendering}
\end{figure}

% What was gained from this? 
\autoref{fig:HolographicOverlaySystem} depicts a challenge with the Holographic overlay system.
All of the visualizations that were supposed to be inside of the patient appeared smaller and outside of them. 
With enough time, users learned to orient themselves around this issue. 
This presentation of the data via \gls{ar} \gls{hmd} in this format seemed to cause more confusion about anatomy than simply displaying it on an external monitor or sitting above the patient.
%To repair this visual mismatch of visual data between the real and the augmented world, we propose to utilize X-ray vision. 

Overall, my Internship at Siemens highlighted that the current AR technologies were limited, including:
\begin{enumerate}
    \item When placing virtual objects behind or within real-world objects, there is a \DIFaddbegin \DIFadd{visual }\DIFaddend mismatch that occurs, causing the virtual object to appear misaligned. \DIFaddbegin \DIFadd{This issue is due to the difficulty in judging depth due to the lack of depth cues~\mbox{%DIFAUXCMD
\cite{Petri2018}}\hskip0pt%DIFAUXCMD
.
    }\DIFaddend \item Sub-centimetre precision poses a general challenge for visualization and perception using \gls{mr} devices. The effect of \gls{ost} \gls{ar} visualizations on the user's depth perception requires further investigation.
    \item There currently exists no standard method to present frequently changing volumetric data inside an object.
    %DIF > \item Medical information is often displayed as a 3D space with continuously changing properties, not just a set of discrete objects. Current polygonal methods don't render such methods of data as a complete and whole manner.
    \item Medical information is often \DIFdelbegin \DIFdel{a volume with continuously changing properties, not just a set of discrete objects.
    Current polygonal methods don't render such volumetric data well}\DIFdelend \DIFaddbegin \DIFadd{represented as volumetric data with continuously varying properties, rather than as discrete surface objects.
    Allowing for more direct contextual customization of the visualizations in real time paired with the ability to see the entire volume at once not just the given surfaces of a set range.
    Conventional polygonal rendering methods struggle to represent this type of data in a complete and integrated way}\DIFaddend .
\end{enumerate}



% % Talk about what was the current state of X-ray vision
% X-ray vision has been shown to repair this issue using virtual holes~\cite{Bajura1992}. 
% This solution still is not perfect. 
% While X-ray vision focused on medical situations, many of the solutions were designed to accommodate \gls{vst} display technology rather than \gls{ost} \glspl{hmd}, which would not have been practical in a medical situation~\cite{Wang2017a}.
% They all seemed to revolve around restricting the users' view of the data, making tasks like determining the \gls {ct} scan quality more challenging than needed.
% Another challenge was that most of the widely used solutions seemed to use image data or polygonal surface data. 
% I found that pre-processed rendering was too slow to allow for acceptable flexibility and was be accurate enough for the viewer. 


% % Why volume Rendering is so essential. 
% This dissertation is aimed at developing an understanding of how X-ray vision could be utilized and implemented into a system that utilized \gls{dvr}.
% \gls{dvr} is more flexible and allows for a clearer and more accurate view of the medical data, it can also be visulized at the same rate that the \gls{ct} scanner can produce images. 
%DIF <  This would require not only fixing the technical issues that were found trying to implement \gls{dvr} into the system but also looking for a method of integrating x-ray visulizations into them while trying to overcome their inherent flaws for medical use. 
%DIF >  This would require not only fixing the technical issues that were found trying to implement \gls{dvr} into the system but also looking for a method of integrating x-ray visualizations into them while trying to overcome their inherent flaws for medical use. 
% %This inspired this research into how X-ray vision and X-ray visualizations work and how we can translate and validate them for medical purposes.
% Further, this research would require exploring how X-ray vision affects depth perception and also examining how it affects human behavior.
% %Then, studies will be performed to learn precisely how these effects work.
% %This study would need to evaluate a user's ability to utilize the visualization and investigate elements like participants' perceived cognitive load, how difficult they found the different effects to use, and how they behaved(speed and movement) regarding the various effects.



\section{Research Goal} \label{sec:IntroReserachGoal}
% % high-level view of all the challenges that need to be overcome.
% Today, medical practices are not employing Mixed Reality devices.
% This is due to a plethora of challenges.
% There are organizational issues that will prevent X-ray vision techniques adopted into the medical profession.
% These include funding, the possibility of the visualizations being distracting for the users, and challenges of how usable it really is usability, and an overall lack of training in using the tools~\cite{Jha2021}, while other issues stem from end-user resistance, and insurance matters~\cite{DaliliSaleh2022}. 
% Many of the issues preventing \gls{ar} from being adopted by my medicine will be solved over time through advances in other fields. 
% %Many of the issues regarding the usability \gls{ar} enabled x-ray vision will be resolved with time as technology becomes more ubiquitous. For example, UX design becomes more integrated in the design of \gls{ar} experiances~\cite{Jha2021}.
% Addionally technical issues such calibration~\cite{Xiong2021, Zhan2020}, limited color gambit~\cite{Xiong2021, Diaz-Barrancas2020}, heavy weight~\cite{Beams2022}, and comfort~\cite{Beams2022} can all be addressed by improving the hardware.

% talk on a high level about what this thesis aims to overcome
This research seeks to investigate the use of \gls{dvr} within physical objects when using \gls{ost} \gls{ar} \gls{hmd}s. 
The holographic overlay system displayed this data out of place.
\DIFdelbegin %DIFDELCMD < \autoref{fig:HolographicOverlaySystem} %%%
\DIFdelend \DIFaddbegin \autoref{fig:DiagramX-rayVision} \DIFaddend shows that the visualization is \DIFdelbegin \DIFdel{technically sitting in the correct spot }\DIFdelend \DIFaddbegin \DIFadd{positioned correctly }\DIFaddend compared to the camera. However, because it is superimposed, it appears much larger and closer to the viewer. 
This was identified as a challenge to be addressed before systems become practical.
%Calibration would still be an issue, but since user perception seems to be the biggest issue, this would be our first step in ensuring that users perceive virtual objects to be in the locations where they are placed.
%DIF < Solutions for this are not well understood from at this point, most solutions for X-ray vision are designed to work with \gls{vst} \gls{ar} which is not well designed for high stress situations.
%DIF > Solutions for this are not well understood at this point; most solutions for X-ray vision are designed to work with \gls{vst} \gls{ar}, which is not well designed for high stress situations.

X-ray vision has been shown to repair this issue using virtual holes~\cite{Bajura1992}. 
However, this solution is not perfect. 
X-ray vision focused on medical situations, and many of the solutions were designed to accommodate \gls{vst} display technology rather than \gls{ost} \glspl{hmd}, which is not practical in a medical situation~\cite{Wang2017a}.
The solutions appeared to restrict the users' view of the data, making tasks like determining the \gls {ct} scan quality more challenging than needed.
The visualizations also utilized static geometry to generate the X-ray visualization while preventing the required level of flexibility.
%Another challenge was that most of the widely used solutions seemed to use image data or polygonal surface data. 
%I found that pre-processed rendering was too slow to allow for acceptable flexibility and was be accurate enough for the viewer. 

%DIF >  \begin{figure}
%DIF >      \centering
%DIF >      \includegraphics[width=1\linewidth]{Chapter1//Images/X-rayVisionIllustration.jpg}
%DIF >      \caption{
%DIF >          An image comparing Superman's fictional X-ray Vision, which uses no real depth cues, with depth established cues found in Augmented Reality Enabled X-ray Vision. 
%DIF >          This image was illustrated by Brandly Richards for use in this thesis and is available under Creative Commons.
%DIF >      }
%DIF >      \label{fig:placeholder}
%DIF >  \end{figure}
\DIFaddbegin 


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{Chapter1/Images/DiagramX-rayVisionMoreDetail.png}
    \caption{
        \DIFaddFL{An example of where it is difficult to interpret depth due to the absence of depth cues. The circular object is a virtual object displayed against the wall in the next room. 
        }\textbf{\DIFaddFL{Left}}\DIFaddFL{) shows the room with no X-ray vision; }\textbf{\DIFaddFL{center}}\DIFaddFL{) shows the same room with X-ray vision enabled. However, the lack of depth cues makes it impossible to determine where the circular object in the next room is located. 
        }\textbf{\DIFaddFL{Right}}\DIFaddFL{) shows the same rooms as on the left, displayed using an isometric perspective to illustrate where the items are actually located.
    }}
    \label{fig:DiagramX-rayVision}
\end{figure}

\DIFaddend % is the motivation for investigating how DVR techniques can be developed for OST AR, particularly for X-ray vision.
The high-stress occupations found in medicine demand a level of predictability from the tools that they use. 
The influence on X-ray vision's ability to hinder perception has previously been tested~\cite{Santos2015}, but how it obscures spatial understanding is still unknown. 
%DIF < To start this investigation it is imporant to learn how previously developed X-ray vision
%DIF > To start this investigation, it is imporant to learn how previously developed X-ray vision
There are also many unknowns about the ecological effects of X-ray vision, as most research in this area has mainly focused on perceptual tasks. 

% To address this, an experiment was conducted using these visualizations to ascertain the limits of human perception on \gls{ost} \gls{ar} by utilizing previously known X-ray visualizations.
% Adapting four different X-ray vision techniques that were designed to work on other devices to work on \gls{ost} \gls{ar} devices and by having participants perform a placement task, we were able to test out not only how these effects affected their depth perception but many other aspects of X-ray visualizations. 

% is the motivation for investigating visual cues that may aid in understanding spatial arrangement and relative depth perception.
\gls{dvr} itself has not been utilized on \gls{ost} \gls{ar} devices, despite having many applications in the medical domain alone. 
This, in turn, indicates there are a plethora of unknown consequences that may arise from its utility. 
% is the motivation for investigating the effect of visual cues on depth perception and the accuracy that can be achieved.
While creating X-ray visualizations that are designed to suit the \gls{ost} \gls{ar} display, it is important to note that these visualizations need to both suit the tasks they are required for and simultaneously function regardless of the use case. 
Ensuring these visualizations should suit the purposes where viewing \gls{dvr} information within the object itself is relevant, regardless if they are referring to manufacturing~\cite{Kanodia2005}, biological~\cite{Guo2012} or surgical tasks~\cite{Bajura1992}. 
These visualizations should be designed to allow users to view surfaces of any shape. 
X-ray visualizations should improve depth perception while allowing users to gain insight into the volume quickly.


% \begin{comment}
% \glspl{virt}
% \end{comment}

% A version of \gls{dvr} was developed that was designed to work within the limitations of \gls{dvr} was developed alongside three Volumetric Illustrative Rendering Techniques (VIRTs), which drew inspiration from 2D artistic effects to provide x-ray vision for \gls{dvr} objects viewed from an \gls{ost} \gls{ar}. 
% The impact that \glspl{virt} had on a person's ability to obtain information from the visualization as well as testing the effect of depth perception on the \glspl{virt}.


% % Pivit over to Volume rendering.
% Another large issue with X-ray vision in this field is that most \gls{X-ray Vision} techniques are designed to deal with solid and predictable surfaces like walls.
% Very few medical applications require their practitioners to look through walls. 
% People have paired X-ray vision with medical visualizations. However, they all focused on showcasing preprocessed data~\cite{Blum2012, Blum2012a, Kalkofen2007, Kalia2019}
% Walls are very different from human bodies; medical practitioners need to look through human bodies, which are much more curved and abstract than walls.
% \gls{mri} and \gls{ct} scans also don't tend to have a clearly defined surface. 
% Most items tend to get rapidly harder before they are dense enough to be solid. 
% To get around this, a method of performing X-ray vision can be flexible to work with abstract and curved surfaces that may not have clearly defined surfaces.


% % Talk about calibration issues
% Calibration of virtual objects anchored onto patients in medical science requires high precision (within 1mm)~\cite{Franz2014}.
% \gls{ar} struggles are not yet capable of this even in ideal settings with customized equipment~\cite{Swan2015}.
% Complications in the surgical environments caused by metallic medical equipment and distortions~\cite{Birkfellner1998} that can be triggered by user interactions~\cite{Ding2022, Soares2021} make this challenge much more difficult.
% The range of different \gls{ost} \gls{ar} \gls{hmd} displays also causes a prominent issue in the differences between depth perception evaluations range between 0.2cm and 2 cm~\cite{Swan2015, Rosales2019}.
% This distortion is even worse when you try to visualize an object behind a real-world object in AR~\cite{Bajura1992, Vishton1995}.

% % Talk about X-ray vision
% The field of X-ray vision has been looking at methods for placing virtual content beyond where it can easily be viewed~\cite{Bajura1992, Avery2008, Sandor2010}.
% Generally, when an object is rendered behind another object, it appears smaller but is perceived as being in front of the other object.
% When users move around the virtual space, the object will appear to move around or seem out of alignment.
% These issues, however, can be fixed when using X-ray vision~\cite{Bajura1992}. 

% % What is left to explore in X-ray vision
% The field of X-ray vision is very old in the field of Augmented Reality, having been researched for over 30 years~\cite{Bajura1992}.
% Many fields in this have been studied, ranging from depth perception to general usability~\cite{Bajura1992, Blum2012, Furmanski2002, Sandor2010, Otsuki2016, Tsuda2005, Zollmann2014}. 
% There are three main approaches to creating \gls{ar} enabled \gls{X-ray Vision} that can be used individually or together. 
% The most common is to render a texture over the surface that creates a degree of occlusion that the user can relate to~\cite{Kalkofen2007, Sandor2010, Otsuki2015}. 
% \gls{ar} X-ray Vision commonly archives highlights areas of interest in the real world and can manifest as geometric patterns calibrated to the physical world. 
% Another alternative is to create a visualization that conveys to the user the distance through objects such as a tunnel~\cite{Avery2009}. 
% The final option used is to show objects (virtual and physical) hidden in the physical world by rendering them as virtual objects~\cite{Bajura1992, Santos2015, Lerotic2007}.
% Unfortunately, there are still too many unknown factors to bring X-ray vision to the forefront.
% The field of X-ray vision does not have many ecological relevance user studies, meaning while depth is a well-explored area, the effects of spatial awareness especially have only seen little exploration~\cite{Li2016}, while precise spatial measures from X-ray vision have not been tested at all. 

% % Pivit over to Talk about issues regarding the headsets that have been worn
% Another issue is that \gls{X-ray Vision} research has tended to use \gls{vst} \gls{ar} devices, which, as mentioned at the start of this session, is poorly suited to medical purposes or other stressful occupations.
% This thesis aims to create the best quality X-ray vision possible for stressful occupations, allowing for \gls{ar} to stretch beyond our physical limitations of sight.
% To create the best quality \gls{X-ray Vision} effects, it is not only trying to build upon existing research but also using the lessons learned from this research to the field in question. 
% This thesis aims to find a method of X-ray vision for direct volumetric rendering using \gls{ost} \gls{ar} \gls{hmd} in a method that can be utilized in a stressful situation like surgery or medical diagnosis. 

\DIFdelbegin \DIFdel{Motivating }\DIFdelend \DIFaddbegin \DIFadd{The following factors motivate }\DIFaddend this research to focus on investigating:
\begin{itemize}
    \item How DVR techniques can be developed for OST AR, particularly for X-ray vision;
    \item If visual cues may aid in understanding spatial arrangement and relative depth perception;
    \item Observing the effects of visual cues and the accuracy that can be achieved on an \gls{ost} \gls{ar} \gls{hmd} display.
\end{itemize}

% % Since beginning this research several 
% This dissertation looks to find a form of X-ray vision that works on an \gls{ost} \gls{ar} device and can be utilized with \gls{dvr}.
% Leading to the creation of new \glspl{X-ray Visualization} for \gls{dvr} medical, microscopy, and any other application and verifying their effectiveness to use to test how they would function in real-world scenarios. 
% %This field of medical visualizations using \gls{ar} is becoming more topical as this field is moving on. 
% Since the beginning of this research, The field of x-ray vision for \gls{ost} \gls{ar} devices has flourished, showing that this is an incredibly relevant topic for today~\cite{MartinGomez2021, Fischer2020a, Gruenefeld2020, Guo2023, Fischer2023, Phillips2021}.
% This shows a growing demand for this research moving forward and further reinforcing my findings. 
% %My findings were backed up by the prior literature, which I used as a base for my research.
% %The initial phase involves reviewing previous work and then researching how well they interact within an ecologically relevant scenario. 

% \subsection{Direct Volume Rendering}
% % Why is this needed
% The holographic overlay system (Mentioned in \autoref{sec:IntroMotivations}) taught me how difficult to render in real-time based on when trying to visualize a large range of data. 
% Volume rendering requires no pre-processing allowing users to change the parameters they view in real-time and how they interact with it~\cite{Zhou2022}.
% This is very useful when working with medical data as it exists as a volume, and every \gls{voxel_g} can refer to a large range of densities dictated by the given \gls{hu_g}. 

% % A high-level overview of what DVR is
% \gls{dvr} uses simplified light physics algorithms to simulate how people could look through an object while retaining some semblance of reality.
% It does this by directly reading the volume data from memory and tracing the direction of light from the user's perspective.

% % 



% \subsection{Goals}
% % Why is X-ray vision important
% Users who navigate a complex object require adequate depth information regarding the position of objects to decide which actions to take.
% % Talking about limitations to superimposition
% The importance of accurate relative spatial perception can be seen in the work by Sielhorst et al.\cite{Sielhorst2006} and Pratt et al.\cite{Pratt2018}, who explored AR for 3D surgical guidance without using partial occlusion to aid in-depth perception. 
% %These works highlighted this technology's potential and current limitations in the field.
% Pratt et~al.~\cite{Pratt2018} mentioned the difficulties related to calibrating medical images superimposed on the patient while achieving correct spatial perception in the practitioner's view.
% Sielhorst et al.~\cite{Sielhorst2006} found that simulating a virtual window into the physical object was better than superimposing information on the patient's skin. 
% X-ray vision techniques were later integrated into this work by Bichlmeier et al.~\cite{Bichlmeier2007}.

% \textbf{Forming an understanding of what works and does not from previously used X-ray vision techniques}
% -
% To make this a reality we built a system that was capable of carrying over some highly published X-ray vision effects chosen via a systematic literature review \autoref{sec:X-rayBackGround}. We researched all of the X-ray vision effects from a VST AR headset over to an OST AR headset. 

% \textbf{Creating visualizations for Volumetric Data sets can be used for x-ray vision}
% -
% We take the lessons learned from this experiment and transition them to creating new visualizations for direct volume rendering.
% We then run two studies based on looking at these types of visualizations one to determine how well people can comprehend the data they are looking at and another one to test their improvements in depth perception. 

\section{Research Questions}
To address the research goals listed in \autoref{sec:IntroReserachGoal} focusing on creating \gls{X-ray Vision} methods for \gls{dvr}. 
This thesis will explore the following questions:

\begin{enumerate}[label=R.\arabic*]
    %DIF > \item Is there a difference in accuracy when placing a virtual object inside of a real world object when different X-ray visualization effects are used to describe the interaction of moving the virual object inside of the real object? (Chapter 3);
    \item \DIFdelbegin \DIFdel{Is there a difference in accuracy when placing a virtual object when different }\DIFdelend \DIFaddbegin \DIFadd{How do }\DIFaddend X-ray visualization effects \DIFdelbegin \DIFdel{are used}\DIFdelend \DIFaddbegin \DIFadd{influence how accurately users can perceive the spatial alignment of a virtual object embedded within a real-world object}\DIFaddend ? (Chapter 3);
    %\item How can Volumetric Illustrative Effects be adopted to become \gls{ost} \gls{ar} \gls{X-ray Vision} effects. (Chapter 4);
    \item Can \glspl{virt} aid a person’s comprehension of a volume when determining individual objects using direct volume rendering?  (Chapter 6);
    \item What is the minimum difference in depth that participants can reliably distinguish between volumetric objects, independent of any given \gls{virt}? (Chapter 7);
\end{enumerate}

\section{Contributions}
This thesis contributes the following:
\begin{enumerate}[label=C.\arabic*]
    \item A systematic literature review of \gls{X-ray Vision} describing the work to date. Presents an overview of the studies performed and an analysis of the use cases. (presented in Chapter 2, published in P.3);
    \item An algorithm that allows VST AR x-ray visualizations to be displayed on an OST AR device. (described in Chapter 3, published in P.4);
    \item Adapting X-ray techniques from \gls{vst} \gls{ar} to \gls{ost} \gls{ar}. (Presented in Chapter 4, published in P.6);
    \item A study comparing \gls{X-ray Vision} effects that had only previously been used on \gls{vst} \gls{ar} to be viewed on OST AR device. Showing that geometry-based \gls{X-ray Vision} techniques were better suited to OST AR devices (presented in Chapter 3, published in P.4);
    \item implementation of Volumetric \gls{X-ray Vision} utilizing Volumetric Illustrative Rendering \DIFdelbegin \DIFdel{Technqiues }\DIFdelend \DIFaddbegin \DIFadd{Techniques }\DIFaddend (VIRTs). (described in Chapter 4, published in P.6);
    \item The Random Volume Generation System. A tool to randomly generate volumes that allow for controlled user studies in \gls{X-ray Vision}. (presented in Chapter 5, published in P.5); 
    \item A perception-based counting study to determine how well users could see the three VIRTs created. Findings show that different VIRTs could negatively and positively impact a user's ability to perceive accurately and determine what elements were in a volume. (presented in Chapter 6);
    \item A study to determine the different perceivable depth thresholds that the Hatching Stippling and Halo VIRTs can have between each other and a baseline condition by utilizing a 2FCA psychophysical study design. (presented in chapter 7);
\end{enumerate}

\section{List Of Publications} \label{section:Publications}
\begin{enumerate}[label=P.\arabic*]
    \item \textbf{Clarke, T. J.} (2021). Depth Perception using X-Ray Visualizations. 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), 483–486. https://doi.org/10.1109/ISMAR-Adjunct54149.2021.00114
    \newline
    Contributed To: 
    \begin{itemize}
        \item Chapter 1;
    \end{itemize}

    \item Smith, R. T., \textbf{Clarke, T. J.}, Mayer, W., Cunningham, A., Matthews, B., \& Zucco, J. E. (2020). Mixed Reality Interaction and Presentation Techniques for Medical Visualisations. In P. M. Rea (Ed.), Biomedical Visualisation: Volume 8 (pp. 123–139). Springer International Publishing. https://doi.org/10.1007/$978-3-030-47483-6_7$
    \\
    Contributed To: 
    \begin{itemize}
        \item Chapter 1;
        \item Chapter 2;
    \end{itemize}

    \item \textbf{Clarke, T. J.}, Gwilt, I., Zucco, J., Mayer, W., and Smith, R T. (2024) Superpowers in the Metaverse: Augmented Reality Enabled X-Ray Vision in Immersive Environments. In Geroimenko V., Augmented and Virtual Reality in the Metaverse.
    \newline
    Contributed To: 
    \begin{itemize}
        \item Chapter 2;
    \end{itemize}

    \item \textbf{Clarke, T. J.}, Mayer, W., Zucco, J. E., Matthews, B. J., \& Smith, R. T. (2022). Adapting VST AR X-Ray Vision Techniques to OST AR. Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2022, 495–500. https://doi.org/10.1109/ISMAR-Adjunct57072.2022.00104
    \newline
    Contributed To: 
    \begin{itemize}
        \item Chapter 3;
    \end{itemize}

    \item \textbf{T. J. Clarke}, W. Mayer, J. E. Zucco and R. T. Smith, "Generating Pseudo Random Volumes for Volumetric Research," 2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), Sydney, Australia, 2023, pp. 266-270, doi: 10.1109/ISMAR-Adjunct60411.2023.00061.
    \newline
    Contributed To: 
    \begin{itemize}
        \item Chapter 4;
    \end{itemize}

    \item \textbf{T. J. Clarke}, W. Mayer, J. E. Zucco, A. Drogemuller and R. T. Smith, "Volumetric X-ray Vision Using Illustrative Visual Effects," 2023 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct), Sydney, Australia, 2023, pp. 769-771, doi: 10.1109/ISMAR-Adjunct60411.2023.00168.
    \newline
    Contributed To: 
    \begin{itemize}
        \item Chapter 5;
    \end{itemize}

    % \item \textbf{T. J. Clarke}, B. J. Mattews, A. Drogemuller, W. Mayer, J. Zucco, R. T. Smith, "VIRTs: Volumetric Illustrative Rendering Techniques to Improve Individual Object Identification and Depth Understanding Using Ocular See Though Devices", ACM Transactions on Applied Perception.
    % \newline
    % Contributed To: 
    % \begin{itemize}
    %     \item Chapter 4;
    %     \item Chapter 6;
    %     \item Chapter 7;
    % \end{itemize}

\end{enumerate}

\section{Dissertation Structure}
Following this chapter is the literature review (Chapter 2), which will focus on all the related research done on \gls{mr} with a focus on medical applications, followed by a look into the research done on \gls{hci} regarding volumetric data and finished with a systematic literature review on \gls{X-ray Vision} covering all of the previous work done over the past 31 years in this field. 

% The results from the literature review were used to inform the next chapter's study design (in Chapter 3), which looks at a variety of popular \gls{X-ray Vision} effects that are designed to utilize a range of different \gls{ar} \gls{hmd}s by creating a system that allows \gls{vst} \gls{ar} techniques to be used on \gls{ost} \gls{ar} devices and then performed a spatial estimation task with them.

The results from the literature review were used to inform the study in Chapter 3, which looks at a variety of popular \gls{X-ray Vision} effects that are designed to utilize a range of different \gls{ar} \gls{hmd}s. 
To achieve this, a system was developed that allows \gls{vst} \gls{ar} techniques to be used on \gls{ost} \gls{ar} devices. 
This system was then used to perform a spatial estimation task, enabling an in-depth evaluation of how these effects function across different augmented reality hardware configurations.

Chapter 4 investigates the technical work involved in volume rendering and details the complexities involved in creating \gls{dvr} techniques for X-ray vision, concluding in the creation of the Volumetric Illustrative Rendering Techniques (VIRTs). 
This was followed by a modular method to create random volumes and details, the Random Volume Generation system. The Random Volume Generation system allows controlled studies to be conducted with volumes to access the VIRTs (described in Chapter 5).
The combination of the Random Volume Generation system and the VIRTs is then utilized in Chapter 6 to run a perception-based user study designed to determine how well a user can count regions within a volume. 
The chapter (Chapter 7) then took the VIRTs and tested the degree of depth thresholds users could reliably distinguish depth by utilizing a 2FCA psychophysical study.
The final chapter draws together the results and findings of this dissertation (Chapter 8).

 \newpage 
    %\include{Chapter1/IntroductionBrief}
    \glsresetall
     \newpage \chapter{Background} \label{chap:Background}
% Introduce this section - 1 paragraph
%This section focuses on the history of the \gls{mr} leading to a definition of \gls{ar}.
This section begins by exploring the history of human vision research, tracing its development to our current understanding of human perception and its integration with \gls{mr} hardware.
We then look at the research behind how depth perception and perception can be considered with \gls{mr} \glspl{hmd}.
Followed by a systematic \DIFdelbegin \DIFdel{styled }\DIFdelend literature review of \gls{ar} enabled \gls{X-ray Vision}.
The discussion then shifts to volume rendering, emphasizing human-centered research using \gls{dvr}, and concludes with an analysis of illustrative effects and their applications within \gls{dvr}.

%DIF > \section{Perception and How it Relates to Augmented Reality (AR) and Virtual Reality (VR)}
\DIFaddbegin 

\DIFaddend \section{\DIFaddbegin \DIFadd{Human }\DIFaddend Perception and \DIFdelbegin \DIFdel{How it Relates to Augmented Reality (AR) and Virtual Reality (VR)}\DIFdelend \DIFaddbegin \DIFadd{Depth Perception}\DIFaddend }
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Chapter2/Images/CuttingDepthGraph.png}
    \caption[This graph of depth cues and distance provides guidelines for depth perception in relation to the distance and key perception parameters.]{
    This graph of depth cues and distance provides guidelines for depth perception in relation to the distance and key perception parameters. Used with permission from Cutting and Vishton~\cite{Vishton1995}.
    }
    \label{fig:CuttingDepthGraph}
\end{figure}

% a very fast history of perception science
Understanding the mechanisms of how perception works has been a goal for humans long before computing or \gls{mr} devices.
Perception has been studied since Democritus conceived that sight was formed from small indivisible particles (460-371 BC). 
Since then, we have learned the anatomical structures of eyes and that sight is processed in the mind rather than the eye (1011 - 1021).
During the 18th century, we started to gain a more modern view of how people see light (based on the reflection of light off of other objects), and we began to learn about how we observe beauty, aesthetics, and apparent deceptions. 
Newton's particle theory in \DIFdelbegin \DIFdel{Opticks }\DIFdelend \DIFaddbegin \DIFadd{Optics }\DIFaddend (1704) proposed that light is composed of small particles that travel in straight lines. These particles change direction and speed when they hit a reflective surface, dispersing into different colors. 
This started a revolutionary shift in our perception of light. Later findings disproved the belief in pure white light despite Goethe's defense of Aristotle's theory (1810).
This understanding of vision enabled technologies like motion images (developed in 1932), leading to the first motion picture projector (the Phantoscope) in 1895. 

% psycho physics has some importance for your thesis
In 1889, Gustav Fechner~\cite{alma9911190913502466} coined psycho-physical, and we developed a metric for quantitatively determining changes in people's perception.
Their study involved seeing at what point a user could no longer determine if more or fewer dots were in a pair of images. 
All pairs of images had ten more or fewer dots than the other one. 
This study showed that the more dots placed on a page, the harder it is for someone to determine the difference, creating the foundation of psychophysics analytics~\cite{alma9911190913502466}. 
From this point, qualitative experiments on perception began to run, and it became possible to understand precisely how human perception functioned.
Leading to our current understanding of topics like depth perception.

\subsection{Depth Perception Fundamentals}
Throughout the late 19\textsuperscript{th} century and the 20\textsuperscript{th} century, psychologists began to study depth perception and came up with many factors to describe it.
Most of these findings believed that depth perception was created by utilizing accommodation, convergence, motion perspective, binocular disparities, height of the visual field, aerial perspective, occlusion, and the relative size and density of objects~\cite{Vishton1995}.
In 1995, Cutting and Vishton took over a century's worth of research and concluded that the human ability to determine what parameters of depth perception are required to be effective.
Notably, not all depth cues are equal, and many are situational.
\autoref{fig:CuttingDepthGraph} shows a breakdown of how these depth cues can be contrasted based on how far away they are from each other. The obvious difference between the depth of two objects is shown using the virtual axis, while the horizontal axis shows the distance or the depth away from the viewer's vision they are effective. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ThesisDepthPerceptionImage.png}
    \caption[Several images showing various cases of Monoscopic forms of depth perception]{Depth cues and placements:
    Several images showing various cases of monoscopic forms of depth perception
    a) Shows an example of Occlusion trees that are occluded behind the main tree;
    b) An example of relative size where the larger trees a tree is the further in the foreground it seems to be;
    c) The field's height where the trees are located higher up in the image will seem farther away from the user. 
    }
    \label{fig:ThesisDepthPerceptionImage}
\end{figure}

% Talking about the different planes of depth perception 
Different forms of depth perception can be described by their utility in other spaces (Illustrated in \autoref{fig:CuttingDepthGraph}). 
Personal space refers to anywhere within 2m of the viewer, giving the viewer the required depth perception to interact with objects.
The action space relates to any distance between 2 and 25m where depth perception functions by using the optical relationship between different objects.
The vista space works by utilizing the changes of color in the horizon~\cite{Vishton1995}. 

% a paragraph talking about how the occlusion cue functions
Occlusion is considered to have the most influence over any depth perception technique~\cite{Vishton1995} and is effective as long as both the occluder and the occluded are in sight. 
The user can tell what object is closer to them~\cite{Vishton1995}.
%When one object hides another object, it tells the viewer that the occluded object is behind the occluding object, allowing users~\cite{Vishton1995}.
However, occlusion itself only reveals the order of these functions~\cite{Vishton1995}.
Occlusion power comes from it being such an obvious depth cue~\cite{Boring1942}, requiring only contrast, the opacity of objects, and the assumption that the object is not changing its shape without the viewer knowing about it. 

Relative size and density are based on the user's current knowledge of the world and only noticed a deduction in accuracy at distances past 5km away from the viewer~\cite{Bajura1992}. 
Relative size refers to someone's ability to determine depth based on how small it looks to them. 
Relative density refers to how densely these objects seem to be clustered together. 
\autoref{fig:RelativeDensityAndAerialPerspectiveExample} illustrates how together, these techniques discern the depth of field away from an object. 
If the users are familiar with an object, they can determine how far away it is. However, this is much more powerful when there is more than one object in the distance.
This depth cue, unlike occlusion, can be used to gain a more granular idea of depth. 

When looking off into the distance, objects may appear visibly lower the further away they are from the viewer.
This is the Height in the Visual Field shown in \autoref{fig:RelativeDensityAndAerialPerspectiveExample}.
This cue relies on the objects touching the ground, so things like airplanes are no good. 
This cue tends to work very well when an object is within several meters of a user (depending on its scale) and still works well up to about 1,000m away.

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/RelativeDensityAndAerialPerspectiveExample.png}
    \caption[Aerial Perspective, Relative Size, and Relative Density: An Image of a mountain view in Bavaria, Germany, with indicators to explain how monoscopic depth perception functions]{
        Aerial Perspective, Relative Size, and Relative Density: An Image of a mountain view in Bavaria, Germany. Several circles whose depths indicate how forests are viewed at various depths are shown. A line showing the gradual effects of the aerial perspective is also shown, indicating how it can be utilized to determine depth.
        \DIFaddbeginFL \DIFaddFL{The background image is licensed under a Creative Commons Attribution Universal 1.0 International license. 
        }\footnotemark
    \DIFaddendFL }
    \label{fig:RelativeDensityAndAerialPerspectiveExample}
\end{figure}

Aerial Perspective Refers to our ability to look through transparent objects. Generally, this refers to one's ability to look through the water in the air, making mountains look blue, and is also applicable underwater and when viewing gas-like elements~\cite{Vishton1995}.
Transparent objects can be pretty rare, so most of these elements will naturally be seen from a distance in \autoref{fig:RelativeDensityAndAerialPerspectiveExample}. 
This, however, may not be the need to be the case when using computer graphics~\cite{Vishton1995}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/Frenchetal.png}
    \caption[Depth Cues and Motion: Schematic illustration of motion components arising from observer translation and scene-relative object motion.]{Depth Cues and Motion: Schematic illustration of motion components arising from observer translation and scene-relative object motion~\cite{French2022}. (a) An observer fixates on a traffic light while moving to the right, as a car independently moves left. (b–d) Depiction of the car's image motion components related to self-motion and object motion. (d) accounts for image inversion by the eye's lens. (b) If the car is stationary, it shows a leftward image motion due to the observer's movement. (c) If the car moves left while the observer moves right, the car's image motion also includes an object motion component. (d) The net image motion of the car, vret, is to the right. This figure is licensed under a Creative Commons Attribution license and was produced by French and Deangelis~\cite{French2022}.}
    \label{fig:FrenchEtAl}
\end{figure}

\DIFaddbegin \footnotetext{\url{https://pxhere.com/en/photo/965104}}
\DIFaddend Motion parallax is considered the depth cue relating to how we perceive motion. 
This seems foundational to how we perceive depth~\cite{Lee1980}.
As seen in \autoref{fig:FrenchEtAl}, the motion perspective normally relies on the user viewer focusing on an object that is moving concerning them~\cite{French2022}. 
This could be looking at a ball they are about to catch or a house in the distance while a user passes it in a car or some other form of transport~\cite{French2022}.
Motion is an effective depth cue within 15 meters, but as \autoref{fig:CuttingDepthGraph} illustrates, the effectiveness lessens when the objects are further away from the viewer and declines when an object is within 2m of a viewer as they can't fully perceive the object~\cite{Shojiro1991}. 

People determine the depth perception of objects based on how their eyes are focused, which can also be a useful depth cue~\cite{Zannoli2016}.
\autoref{fig:AccomodationAndConvergenceInRealAndVirtualWorlds} illustrates how changing the accommodation and convergence allows us to focus on a given object.
Humans can change the disposition between their eyes~\cite{Berkeley1948}. 
Accommodation relates to the ability to change the shape for focus. 
Of the eye to see an object clearly and at different distances. 
Whereas convergence relates to the ability to turn the eyes to focus on nearby objects inwardly. 
These eye movement behaviours have a limited range, \autoref{fig:CuttingDepthGraph} shows at closer distances to us, this depth cue works the most effectively together, but it also shows as the eyes move farther apart, objects positioned further in front of or behind the focal point become increasingly blurred~\cite{Bajura1992}.

% needs a conclusion paragraph to this section
\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{Chapter2/Images/AccomodationAndConvergenceInRealAndVirtualWorlds.png}
    \caption[A depiction of how convergence and accommodation work in the real world using Mixed Reality and OST AR.]{Convergence and Accommodation:
        A depiction of how convergence and accommodation work in the real world using Mixed Reality and OST AR. It consists of 4 diagrams showing how accommodation and convergence work together to better perception. The \DIFaddbeginFL \DIFaddFL{blured ducks indicate a point where a duck would not be in focus to the viewer based on the position sitting in this position due to the effect of convergence and accommodation. The }\DIFaddendFL frames below show examples of the resulting perceived images of the objects in each diagram. Each of the four diagrams showcases a human eye (the circular object) viewing some ducks. The point where the eyes meet is their vergence or the point of convergence (depicted by the line). The cone represents the accommodation, which focuses on the physical distance the viewer is from the display. This image was inspired by work by Rosedaler. This is licensed under a Creative Commons Attribution licence~\footnotemark.
    }
    \label{fig:AccomodationAndConvergenceInRealAndVirtualWorlds}
\end{figure}

\DIFdelbegin \DIFdel{~}\footnotetext{%DIFDELCMD < \url{https://commons.wikimedia.org/wiki/User:Rosedaler}%%%
}
%DIFAUXCMD
\DIFdelend Since both eyes have a different view of the real world, the image each eyes perceive is inherently different.
This enables humans with several different abilities that are processed in the brain: 
\begin{itemize}
    \item \textbf{Stereopsis :} Refers to our human ability to assemble a 3D image from two 2D images from each eye. Giving us the ability to see the world in 3D. 
    \item \textbf{Diplopia :} Also called double vision. This is what occurs with binocular disparity, which can't be completely fixed by Stereopsis, leaving the viewer with two images that are not correctly aligned.
\end{itemize}
These cues give viewers a clear indication of how the world is around them and are referred to in combination as binocular disparities. 
Binocular disparities are more effective when viewed closer to the viewer as the further they are away from each eye, the similar position they are in each eye~\cite{Vishton1995}. 

Other cues can give a viewer a better sense of depth. 
For example, it is possible to directly tell the viewer how far things are by presenting them with a neat grid texture. Explaining to a user exactly how big a world is can obviously present a high level of depth perception. 
Objects with a high amount of contrast can also be clearer to see; however, this can be seen as improving the relief size and density.
Finally, humans may process depth perception in ways we have yet to understand fully; it is quite possible there is an aspect to living on earth, like gravity, that may even have an effect on our sense of depth perception~\cite{Vishton1995, Watson1992}.

\DIFaddbegin \footnotetext{\url{https://commons.wikimedia.org/wiki/User:Rosedaler}}

\subsection{\DIFadd{Research into the Depth Perception of Color}}
\DIFadd{This dissertation utilizes several colorful objects in its evaluations and creates two studies that directly utilize someone's ability to discern color from a specified area. 
This particular section highlights the papers of note that researched the impact of colors on depth perception.
By understanding this research, the changes in depth perception that various colors can provide were mitigated across this dissertation. 
}

\DIFadd{Ping et al.~\mbox{%DIFAUXCMD
\cite{Ping2020a} }\hskip0pt%DIFAUXCMD
highlight the impact that colors can have on depth perception.
When talking about medical visualization in general, it is very common to have several transparent layers in a single visualization.
Some research has found that people tend to perceive different colors as being closer or further away than others~\mbox{%DIFAUXCMD
\cite{Aoi2020, Ping2020a}}\hskip0pt%DIFAUXCMD
.
This section of the thesis discusses the research that has been factored into considerations regarding how color influences depth perception. 
}

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ARToXrayVision.png}
    \caption[Three images of the Stanford bunny sitting behind a wall, each using a different \gls{X-ray Vision} effect.]{\DIFaddFL{Three images of the Stanford bunny sitting behind a wall, each using a different }\gls{X-ray Vision} \DIFaddFL{effect. To the left, a simple depth cue by placing the bunny behind a column. In the center, a virtual grid is placed over the physical wall to explain to the viewer that the bunny is behind the wall. On the right is highlighting the edge of the bricks with }\gls{ar} \DIFaddFL{using edge detection to indicate that the bunny is behind the wall.}}
    \label{fig:ARToXrayVision}
\end{figure}

\DIFadd{Aio and Li~\mbox{%DIFAUXCMD
\cite{Aoi2020} }\hskip0pt%DIFAUXCMD
wanted to test how the luminance and Contrast affected the depth perception of transparent plans when viewed on a computer monitor.
The goal was to determine what methods could be utilized to make it more obvious which object was behind the other. 
They had two conditions to achieve this. They utilized luminance contact, testing the difference between changing the gradient between light and dark to dark to light.
They utilized four planes of different sizes to distinguish depth perception. 
This study was conducted using an autoscopic display, which allowed for the presence and absence of motion parallax and no binocular parallax, giving the participants extra depth cues.  
Throughout this study, Aoi and Li~\mbox{%DIFAUXCMD
\cite{Aoi2020} }\hskip0pt%DIFAUXCMD
noticed participants underestimated depth perception but it could be improved by using either (or both) binocular parallax and motion parallax. 
They also noted that occluding darker or lighter colors did not make much difference.
Aio and Li~\mbox{%DIFAUXCMD
\cite{Aoi2020} }\hskip0pt%DIFAUXCMD
next utilized the information from their prior study to test this data on medical data and less difference between the choices of different colors. 
}

\section{\DIFadd{Illustrative Rendering Techniques}}
\DIFadd{Cutting and Vishton~\mbox{%DIFAUXCMD
\cite{Vishton1995} }\hskip0pt%DIFAUXCMD
claim there are several real-world elements to create depth perception, which then need to be adapted for virtual displays~\mbox{%DIFAUXCMD
\cite{Cutting1997}}\hskip0pt%DIFAUXCMD
.
However, artists have been able to establish depth perception even when illustrating non-realistic environments by using "Just Enough Reality"~\mbox{%DIFAUXCMD
\cite{Siegel2000} }\hskip0pt%DIFAUXCMD
to determine depth accurately.
}

\DIFadd{The latter part of this thesis looks at using }\glspl{virt} \DIFadd{as a method of }\gls{X-ray Vision}\DIFadd{. 
Building on these perceptual foundations, the challenge in computer graphics—and particularly in advanced visualization domains such as X-ray or mixed reality displays—is not just to replicate the cues found in natural vision, but to enhance and adapt them for clarity and insight. 
While perceptual mechanisms like stereopsis and texture gradients provide a basis for spatial understanding, there are scenarios where simply mimicking the real world is insufficient. 
Here, artistic illustrative effects become invaluable: by deliberately emphasizing, abstracting, or revealing underlying structures, these techniques enable viewers to "see" information that might otherwise remain obscured. 
In this way, the migration from perception-driven rendering toward illustrative approaches is not only a technical evolution, but also a creative one, leveraging artistic conventions to extend the capabilities of visual communication in graphics.
}


\DIFadd{This section is going to look at how these effects have previously been used, what their impact has been on computer science, and what their utility has been when using }\gls{mr} \DIFadd{devices.
While this thesis only looks into a subsection of illustrative techniques, limiting itself to either Hatching, Stippling, or Halos, the actual definition of this is broader~\mbox{%DIFAUXCMD
\cite{Lawonn2018}}\hskip0pt%DIFAUXCMD
.
Illustrative techniques can also include the scope of non-photorealistic rendering, like using cell-shading and the deformation of video footage to look like it was produced by a pencil or paintbrush~\mbox{%DIFAUXCMD
\cite{Lawonn2018}}\hskip0pt%DIFAUXCMD
. 
This section will also examine some user studies that have investigated this effectiveness~\mbox{%DIFAUXCMD
\cite{Lawonn2018}}\hskip0pt%DIFAUXCMD
. 
}

\DIFadd{The largest examples of illustrative effects being used can be seen in scientific textbooks like Gray's anatomy~\mbox{%DIFAUXCMD
\cite{gray1877anatomy}}\hskip0pt%DIFAUXCMD
.
This textbook utilized hatching's ability to communicate texture and depth using a black-and-white image.
These images were collected over years of diagramming the human body by directing unclaimed bodies from workhouses and mortuaries. Due to their clarity and accuracy, they are still widely used today. 
}

\DIFadd{Early work in the field by Interrante et al.~\mbox{%DIFAUXCMD
\cite{Interrante1995, Interrante1997, Interrante1997a} }\hskip0pt%DIFAUXCMD
looked at how illustrative effects can aid the perception of transparent objects.
Transparent objects make it difficult to understand the exact surface of the shape that a transparent object is formed as. 
This was done by pre-computing textures that used transparent and opaque regions.
The first work was done by creating several different textures, including multiple methods that depict valleys and ridges, grids, and curvature information~\mbox{%DIFAUXCMD
\cite{Interrante1995}}\hskip0pt%DIFAUXCMD
. 
}

\DIFadd{To improve the on their prior work~\mbox{%DIFAUXCMD
\cite{Interrante1995}}\hskip0pt%DIFAUXCMD
, Interrante~\mbox{%DIFAUXCMD
\cite{Interrante1997a} }\hskip0pt%DIFAUXCMD
created a visualization that utilized valleys, ridges, and curvature to explain the objects' flow. 
This texture was calculated by drawing lines around the parts of the mesh with the highest curvature and having them move toward the ridges and valleys of the shapes. 
This dissertation extends this research by creating a texture that could be viewed from all sides, requiring less computation when the object is viewed from different angles. 
}

\DIFadd{The textures mentioned in Interrante et al.'s~\mbox{%DIFAUXCMD
\cite{Interrante1997} }\hskip0pt%DIFAUXCMD
work were later tested with a tipping and a grid-based pattern on each. 
A user study was done to determine if the direction of the lines or opacity affected users' ability to determine the shape of the surface.
At the same time, the participant viewed the graphics on a stereoscopic display. 
This study tested whether participants could accurately determine the closest surface of one noisy sphere to another inside of it. 
The analysis did show that texturing the object improved depth perception compared to a base line condition of having no texturing effect, but there was no significant difference if participants could determine which shell was the closer to themselves. 
}

\subsubsection{\DIFadd{Hatching}}

\DIFadd{The hatching which was developed in Interrante et al.~\mbox{%DIFAUXCMD
\cite{Interrante1997} }\hskip0pt%DIFAUXCMD
was later extended by Hertzmann and Zorin~\mbox{%DIFAUXCMD
\cite{Hertzmann2000}}\hskip0pt%DIFAUXCMD
.
Hertzmann and Zorin~\mbox{%DIFAUXCMD
\cite{Hertzmann2000} }\hskip0pt%DIFAUXCMD
developed an algorithm that could translate hatching over to smooth surfaces by using a piecewise smooth subdivision to reconstruct a smooth surface from the mesh to compute the necessary qualities.
This allowed for a surface-based rendering technique that worked much like a shadow but also thinned the lines to the point of being invisible when they were in the direct view of the camera. 
They then used a combination of noise generation and denoising functions to create human errors that would be seen in a work of art. 
}

\DIFadd{The system that Hertzmann and Zorin~\mbox{%DIFAUXCMD
\cite{Hertzmann2000} }\hskip0pt%DIFAUXCMD
created was not designed for real-time interactions.
This means that even simple actions like rotating around the model are not possible. 
Praun et al.~\mbox{%DIFAUXCMD
\cite{Praun2001} }\hskip0pt%DIFAUXCMD
pre-generated a tonal art map based on different levels and then used these tonal art maps to determine the direction of the hashes before drawing them on the objects themselves. 
This system allowed for a wide variety of different configurations.
}

\DIFadd{This method of hatching was then furthered by Pelt et al.~\mbox{%DIFAUXCMD
\cite{Pelt2008} }\hskip0pt%DIFAUXCMD
and applied to an iso-surface representing }\gls{ct} \DIFadd{data.
Their algorithm was modified to consist of just a geometry shader rather than a fragment shader, removing the need for preprocessing the hatching. 
Rather, this system is able to compute the curvature of the iso-surface and an appropriate direction for the hashing in real time while providing a relatively fast frame rate, which would then create a textured striped pattern over a 3D object.
}

\DIFadd{Another system was created by Lawonn et al.\mbox{%DIFAUXCMD
\cite{Lawonn2013}}\hskip0pt%DIFAUXCMD
, which could run at even faster rates than Pelt et al.'s~\mbox{%DIFAUXCMD
\cite{Pelt2008} }\hskip0pt%DIFAUXCMD
work as long as it receives extensive pre-processing.
It first identifies key regions: contours, defined by surface normal and view vector perpendicularity, and feature regions, identified by maxima and minima in the mean curvature field.
Then, the direction of the lines is calculated directly from the direction of the curvature.
}

\DIFadd{Hatching effects are also closely tied to brush-like effects, as they require the system to understand brush direction and stroke size. 
Gerl and Isenberg~\mbox{%DIFAUXCMD
\cite{Gerl2012} }\hskip0pt%DIFAUXCMD
then furthered the possible interactions of hatching and painterly effects. 
This technique preprocessed a }\gls{classifier_g} \DIFadd{to segment areas of the 3D mesh, then it used a }\gls{regression_analysis} \DIFadd{to choose the most appropriate direction of the stroke directions.
To aid the AI methods, users were also given several interactions that allowed them to reconfigure the angle and direction of the effect~\mbox{%DIFAUXCMD
\cite{Gerl2012}}\hskip0pt%DIFAUXCMD
. 
}

\DIFadd{Lawonn et al.~\mbox{%DIFAUXCMD
\cite{Lawonn2017} }\hskip0pt%DIFAUXCMD
furthered this technique and paired it with a visualization of a cylinder, making the illustrative effects inside of it more apparent than the effects outside. 
The cylinder gave a similar impression to }\gls{X-ray Vision}\DIFadd{, where the illustrative effects in the cylinder were clear and easy to see, while the effects outside the cylinder were duller. 
The hatching was modified to work on a set of vessels, and the caps of all the vessels and each location where the vessels split were identified so the vessels could be rendered differently.
}

\DIFadd{Lawonn et al.~\mbox{%DIFAUXCMD
\cite{Lawonn2017} }\hskip0pt%DIFAUXCMD
then ran a study comparing their version of hatching to the same cylinder from their previous study~\mbox{%DIFAUXCMD
\cite{Lawonn2017} }\hskip0pt%DIFAUXCMD
with pseudo-chroma depth rendering and Phong shading.
Users were asked to define the model's depth tips of two vessels. 
pseudo-chroma depth rendering used the chroma colors to indicate depth, with more saturated colors being closer to the viewer and less saturated colors being further away whereas Phong shading used light and dark to indicate depth.
This study showed that while participants performed faster with the pseudo-chroma depth performed, they were more accurate at assessing the distances and felt more confident in their answers using the hatching condition. 
This shows that hatching may allow for better depth perception. 
}

\subsubsection{\DIFadd{Stippling}}
\DIFadd{Lu et al.~\mbox{%DIFAUXCMD
\cite{Lu2002} }\hskip0pt%DIFAUXCMD
furthered the stippling techniques shown by Interrante et al.~\mbox{%DIFAUXCMD
\cite{Interrante1997}}\hskip0pt%DIFAUXCMD
. 
By looking at the curvature of the model, finding localized curvature of 3D models, and spacing out the dots between the various pixels on the screen. This effect created a realistic stippling effect for 2D images.
}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/PasterAndStrotthote.png}
    \caption[Examples detailing the stippling algorithm created by Pastor and Strotthote~\cite{Pastor2004}.]{\DIFaddFL{Examples detailing the stippling algorithm created by Pastor and Strotthote~\mbox{%DIFAUXCMD
\cite{Pastor2004}}\hskip0pt%DIFAUXCMD
. The top of this image shows how the stippling subdivision is implemented using a graph function. The bottom image shows an example of how this stippling appears when it is applied to the target object (The bones representing a human hand). A small amount of stipple can be seen on the left of the image where the stipple was evenly and sparsely placed, but there is more depth perception in the center and right images where stippling is more frequent and varied. 
    Used with permission from IEEE \textcopyright{} 2004.}}
    \label{fig:PasterAndStrotthote}
\end{figure}

\DIFadd{An issue with drawing the dots for stippling was that Lu et al.'s~\mbox{%DIFAUXCMD
\cite{Lu2002, Lu20} }\hskip0pt%DIFAUXCMD
found that the distance between the dots requires to be randomly placed using a noise-based function rather than just at random. 
It was important space stippling randomly but evenly distributed. 
One solution for this was created by Pastor and Strothotte~\mbox{%DIFAUXCMD
\cite{Pastor2004}}\hskip0pt%DIFAUXCMD
.
Their version of stippling created a 3D Voronoi pattern over the 3D model, then a graph would be created linking the starting point of all of the dots which shared a boundary. 
This allowed for a seamless decline in the number of }\glspl{voxel_g} \DIFadd{shown as they would be separated into groups based on the parent-child relationship seen in the upper part of }\autoref{fig:PasterAndStrotthote}\DIFadd{. 
This enabled the even stippling thresholds seen in the lower part of }\autoref{fig:PasterAndStrotthote} \DIFadd{creating a sense of depth and shape of the objects it was applied to.
}

\DIFadd{Another way of creating even stippling while allowing for different angles is to utilize a geometry shader to further subdivide the mesh. 
This technique was initially proposed by Meruvia and Pastor\mbox{%DIFAUXCMD
\cite{MeruviaPastor2002}}\hskip0pt%DIFAUXCMD
.
By doing this, you can subdivide each polygon to give each polygon a set number of dots within it and evenly distribute the dots inside of each polygon.
This system operates under the assumption that areas with more polygons will require a higher density of dots, whereas flat areas will not~\mbox{%DIFAUXCMD
\cite{MeruviaPastor2002}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{This method of stippling was later extended by Ma et al.~\mbox{%DIFAUXCMD
\cite{Ma2018} }\hskip0pt%DIFAUXCMD
to allow for the stippling to be applied to a 3D model in real time that utilized pre-computation to speed up the process rather than a more complex shader.
%DIF > Ma et al.~\cite{Ma2018} later created a system to motivate further parts that belonged to be able to move but also by utan utilizing a similar pre-computation to Paster and Strotthote\cite{Pastor2004}.
The dot was placed using blue noise inside the Voronoi, adjusted in size to varying levels, and adjusted in tones based on where they appeared in parallel on the GPU.
This type of stippling allowed for the effect to be placed realistically onto 3D models even as they were changing their shapes. 
}

\subsubsection{\DIFadd{Halo}}
\DIFadd{Outlines~\mbox{%DIFAUXCMD
\cite{Bui2015}}\hskip0pt%DIFAUXCMD
, Boundary Enhancements~\mbox{%DIFAUXCMD
\cite{Svakhine2003}}\hskip0pt%DIFAUXCMD
, feature lines~\mbox{%DIFAUXCMD
\cite{Lum2002, Lawonn2015}}\hskip0pt%DIFAUXCMD
, and Halos~\mbox{%DIFAUXCMD
\cite{Ozgur2017} }\hskip0pt%DIFAUXCMD
go by many other names, but they all relate to outlining either individual objects or highlighting areas of very high curvature from the perspective of the viewer. 
With traditional rendering, this tends to be done by viewing the distance between various pixels on the depth map~\mbox{%DIFAUXCMD
\cite{Ozgur2017, Lawonn2017} }\hskip0pt%DIFAUXCMD
or by calculating the local curvature of the surrounding fragments~\mbox{%DIFAUXCMD
\cite{Bui2015, Svakhine2003}}\hskip0pt%DIFAUXCMD
. However, this can be very different when working with }\gls{dvr} \DIFadd{in part to the lack of a defined surface making any surface based calculations challenging.
}

\subsection{\DIFadd{Volumetric Illustrative Rendering Techniques}}
\DIFadd{In more recent years, many papers have been striving to take volumetric data and present it as illustrative images, with the belief that these images will be easier to communicate and understand in a 2D format.
}


%DIF >  Talking about how methods to create these effects for to volumes work:
\DIFadd{Initially Interrante et al.'s~\mbox{%DIFAUXCMD
\cite{Interrante1997} }\hskip0pt%DIFAUXCMD
proposed two methods to convert their illustrative techniques to }\gls{dvr}\DIFadd{:
}\begin{itemize}
    \item \textbf{\DIFadd{Scan-Conversion Method: }}\DIFadd{Converts texture slabs into a grayscale volume. This method is efficient for generating multiple views but compromises stroke crispness due to volume data resolution limitations~\mbox{%DIFAUXCMD
\cite{Interrante1997}}\hskip0pt%DIFAUXCMD
.
    }\item \textbf{\DIFadd{Geometric Definition Method: }}\DIFadd{Directly applies geometrical definitions of strokes during ray casting. This method maintains fine detail but is computationally expensive, requiring repeated intersection tests for each view~\mbox{%DIFAUXCMD
\cite{Interrante1997}}\hskip0pt%DIFAUXCMD
.
}\end{itemize}
\DIFadd{Since this thesis is focused on real-time volume rendering for immersive }\gls{mr} \DIFadd{devices, a new volume was required to be generated each frame. This section will have more of a focus on papers that utilize the Geometric Definition Method. Which has since been heaver extended by other to now utilize techniques that are available on more modern }\glspl{gpu} \DIFadd{like fragment shaders.
}

\DIFadd{Rheingans et al.~\mbox{%DIFAUXCMD
\cite{Rheingans2001} }\hskip0pt%DIFAUXCMD
developed a method that was able to separately compute the expected color and the transparency.
This allowed the lighting to react to certain elements and allow different textures within the volume to have a different appearance even if the Hounsfield unit was similar at that }\gls{voxel_g}\DIFadd{. 
Rheingans et al.~\mbox{%DIFAUXCMD
\cite{Rheingans2001} }\hskip0pt%DIFAUXCMD
}\gls{dvr} \DIFadd{algorithm was able to present an accurate texture representation of the various surfaces of the volume and allowed for objects to be presented without regard to density or realism.
}

\DIFadd{Lu et al.\mbox{%DIFAUXCMD
\cite{Lu20} }\hskip0pt%DIFAUXCMD
created a system that was able to apply stippling to a complete volume. 
This system used algorithms like ray marching similar to }\gls{dvr} \DIFadd{to perform this calculation.
Visualization focused on rendering the various surfaces to make their curvatures clear but also took into account the amount of density it would have required to reach a given surface. 
Surfaces that were facing the camera were faded out, and the lighting algorithm mentioned in Lu et al.~\mbox{%DIFAUXCMD
\cite{Lu20} }\hskip0pt%DIFAUXCMD
allowed for shading to become an option to be utilized by the final visualization.
}

\DIFadd{Work done by Bruckner et al.~\mbox{%DIFAUXCMD
\cite{Bruckner2007, Bruckner2006} }\hskip0pt%DIFAUXCMD
proposed that one method to get around some unwanted details that are an issue with }\gls{dvr} \DIFadd{would be to utilize non-photo-realistic rendering.
This style of rendering utilized on the surfaces of different objects and rendered them based on their curvature~\mbox{%DIFAUXCMD
\cite{Bruckner2006}}\hskip0pt%DIFAUXCMD
. 
From this, they developed a simple cartoon-like shading algorithm~\mbox{%DIFAUXCMD
\cite{Bruckner2006}}\hskip0pt%DIFAUXCMD
, Stippling~\mbox{%DIFAUXCMD
\cite{Bruckner2006}}\hskip0pt%DIFAUXCMD
, and Halos~\mbox{%DIFAUXCMD
\cite{Bruckner2007}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{When volume rendering is utilized in microscopy, it can be difficult to tell the difference between the different boundaries and the elements being visualized. 
Guo et al.~\mbox{%DIFAUXCMD
\cite{Guo2012} }\hskip0pt%DIFAUXCMD
used a halo visualization to separate the different molecules that can be viewed, as well as two new techniques for promoting contrast in regions of the volume called  Phase Contrast Volume Rendering (PCVR) and Difference Interference Contrast Volume Rendering (DICVR).
PCVR enhances the contrast of almost visible parts of the volume, allowing for a higher contrast.
DICVR tries to extend PCVR further by using interference contrast based on microscopy principles.
They simplified the equation required to create these effects each time by performing multiple rendering passes. 
This work by Bruckner et al.~\mbox{%DIFAUXCMD
\cite{Bruckner2007, Bruckner2006} }\hskip0pt%DIFAUXCMD
aimed at providing a more simplistic method for creating }\gls{dvr} \DIFadd{and extending its functionality in biology.
}

\section{\DIFadd{Direct Volume Rendering (DVR)}}
\DIFadd{The other focus of this thesis is volume rendering. 
Volume rendering relates to the visualization of a volume of data, which is typically created using a }\gls{ct} \DIFadd{or }\gls{mri} \DIFadd{machine. However, it can also be used for other scientific visualizations, such as fluid simulations, meteorological data, and geological data. 
Preprocessed Volume Rendering will generally present surfaces of the volumes using polygonal structures utilizing iso-surfaces~\mbox{%DIFAUXCMD
\cite{Lorensen:1987:MCA}}\hskip0pt%DIFAUXCMD
, but DVR does not require explicit surfaces to be precisely stated. }\gls{dvr} \DIFadd{can present the data within the natural format as a 3D image~\mbox{%DIFAUXCMD
\cite{Drebin1988}}\hskip0pt%DIFAUXCMD
.
This allows the system to dynamically represent the volumes from within the system.
However, it does highlight the need for techniques like }\gls{ct} \DIFadd{and }\gls{mri} \DIFadd{to be directly aligned with the physical data source. An }\gls{X-ray Vision} \DIFadd{technique needs to be developed to meet this criterion. 
This section highlights the prior work in this area that influenced the research in this dissertation. 
}

\subsection{\DIFadd{Visualizing Volumetric Data}}
\DIFadd{Generally, medical volumetric data is viewed using 2D slices of a human body.
After years of training, medical practitioners can be very precise when using these slices, but they are not intuitive to use or to learn how to use~\mbox{%DIFAUXCMD
\cite{Cheung2021}}\hskip0pt%DIFAUXCMD
. 
Like other forms of 3D data visualizations, converting this data into a 3D version makes it more intuitive to read and interact with~\mbox{%DIFAUXCMD
\cite{Jurgaitis2008}}\hskip0pt%DIFAUXCMD
. 
This process involves reconstructing 3D models from volumetric data, enabling more intuitive visualization and interaction compared to traditional 2D slice-based approaches.
%DIF > As noted earlier, in \autoref{sec:ApplicationsForStereoscopicDisplays} immersive \gls{mr} displays are better suited for helping their users see 3D data. 
}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Chapter2/Images/Cubrillies.png}
    \caption[A example of Cuberilles.]{\DIFaddFL{A example of Cuberilles. The left side shows the armadillo in its original form. On the right, the same model is rendered using Cuberilles.}}
    \label{fig:Cubrillies}
\end{figure}

%DIF >  numbers in citations are for links to the Marching Cubes paper
\DIFadd{Early methods to visualize volumes would focus on calculating the surface contours and calculating the exterior surface to match~\mbox{%DIFAUXCMD
\cite{Keppel1975}}\hskip0pt%DIFAUXCMD
, which was problematic as it created ambiguity when there were irregularities on the slice data such as those caused by noise~\mbox{%DIFAUXCMD
\cite{Fuchs1977}}\hskip0pt%DIFAUXCMD
, requiring user intervention to overcome~\mbox{%DIFAUXCMD
\cite{Christiansen1978}}\hskip0pt%DIFAUXCMD
.
Herman et al.~\mbox{%DIFAUXCMD
\cite{Herman1981} }\hskip0pt%DIFAUXCMD
tried creating a more automated surface by creating Cuberilles, which functioned similarly to Minecraft blocks~}\autoref{fig:Cubrillies}\DIFadd{. This was useful as it allowed for varying resolution~\mbox{%DIFAUXCMD
\cite{MEAGHER1982129}}\hskip0pt%DIFAUXCMD
.
The continuation of this work was marching cubes~\mbox{%DIFAUXCMD
\cite{Lorensen:1987:MCA}}\hskip0pt%DIFAUXCMD
. Unlike the rough surface afforded by utilizing Cuberilles, this algorithm made a smooth surface.
Marching Cubes utilized the fact that any six }\glspl{voxel_g} \DIFadd{neighboring }\glspl{voxel_g} \DIFadd{could be paired into 14 different symmetrical orientations if they were either inside or outside of the threshold.
This made it possible to create a 3D model of a }\gls{ct} \DIFadd{or }\gls{mri} \DIFadd{scan with a manageable polygon count that looked similar to the real volume.
}

%DIF >  Wünsche~\cite{Wunsche2003} created a system that allows users to view medical 3D data on desktop devices, utilizing bicubic Hermite interpolation with linear interpolation in the radial direction to generate an iso-surface for most models, while the ventricles are created by tracing \gls{mri} contours.
%DIF >  This system was then paired with a flexible UI, allowing for consistent color representation between models and the ability to incorporate mathematical models to segment the volume. A plane representing the axial and coronal planes also displays the segmented data around the 3D object in 2D.
%DIF >  A follow-up system to this was later created, which focused on allowing \gls{dvr} to be utilized in a similar format, providing tools to aid users in working with histograms of the volume data while still being able to interact with it in the same math-based manner~\cite{Liu2010}.

\DIFadd{Wünsche~\mbox{%DIFAUXCMD
\cite{Wunsche2003} }\hskip0pt%DIFAUXCMD
introduced a visualization toolkit designed for the exploration of complex biomedical data, with a particular focus on curvilinear finite element data sets. Unlike conventional volume visualization approaches that assume regularly gridded data, curvilinear finite element models define geometry in material space, where grid lines are curved when mapped into world coordinates. The system derives iso-surfaces in material space and then renders them in world space, enabling accurate visualization of organs modelled  using FE techniques, such as the left ventricle of the heart. The toolkit provides several novel features: a modular design for comparing multiple models simultaneously, a generalized field structure allowing the creation and manipulation of scalar, vector, and tensor fields, and boolean filters for segmentation and icon placement. Additional innovations include global color map controls for consistent interpretation across models, and flexible element, plane, and point selection mechanisms. This framework allowed researchers to integrate and explore biomedical data ranging from scalar tissue properties to tensor fields derived from MRI, supporting both quantitative analysis and interactive visualization.
}

\DIFadd{Liu et al.~\mbox{%DIFAUXCMD
\cite{Liu2010} }\hskip0pt%DIFAUXCMD
extended this line of work by introducing a novel interface for }\gls{dvr} \DIFadd{aimed at making transfer function design more intuitive and accessible, particularly for non-expert users. Traditional DVR requires carefully crafted transfer functions to map volume data values to color and opacity, a task that can be challenging without specialized visualization knowledge. To address this, Liu and colleagues proposed a spreadsheet-style, constructive visual component-based interface that follows a “programming-by-example” paradigm. Their system automatically analyzes the Douglas–Peucker algorithm~\mbox{%DIFAUXCMD
\cite{Douglas–Peucker1973} }\hskip0pt%DIFAUXCMD
histograms of the volume data using to detect meaningful structures, from which it generates “unit transfer functions” representing simple, recognizable features. Users can then combine, refine, and merge these units interactively to build more complex transfer functions. Preliminary evaluations demonstrated that even novice users were able to produce meaningful visualizations significantly faster and with less guidance than when using traditional transfer function editors, highlighting the potential of example-based interfaces for democratizing DVR in biomedical applications.
}

\DIFadd{Iso-surfaces are still used widely today as they provide the most efficient means of displaying a shell; however, tasks like diagnostic exploration and interactive tasks are better enabled by }\gls{dvr}\DIFadd{~\mbox{%DIFAUXCMD
\cite{Meissner2000}}\hskip0pt%DIFAUXCMD
.
Farrell~\mbox{%DIFAUXCMD
\cite{Farrell1983} }\hskip0pt%DIFAUXCMD
found a method of using ray casting to create a surface method showcasing one of the first attempts at }\gls{dvr}\DIFadd{.
However, many of the concepts for this would later be formed by }\gls{dvr}\DIFadd{, which was initially developed in 1988 by Drebin et al.~\mbox{%DIFAUXCMD
\cite{Drebin1988}}\hskip0pt%DIFAUXCMD
, who designed this type of visualization as a fix for the all-or-nothing approach that is possible when using an iso-surface. 
Drebin et al.'s\mbox{%DIFAUXCMD
\cite{Drebin1988} }\hskip0pt%DIFAUXCMD
approach allowed for a realistic, transparent representation of the volume collected from a }\gls{mri} \DIFadd{or gls}{\DIFadd{ct}} \DIFadd{scanner~\mbox{%DIFAUXCMD
\cite{Ney1990, Kaufman2000}}\hskip0pt%DIFAUXCMD
.
}\gls{dvr} \DIFadd{functionality was further refined by Engel et al.~\mbox{%DIFAUXCMD
\cite{Engel2001} }\hskip0pt%DIFAUXCMD
to work with modern equipment. 
Allowing all of the models to be viewed with minimal issues. 
}\gls{dvr} \DIFadd{rendering was initially only designed for }\gls{ct} \DIFadd{and }\gls{mri} \DIFadd{data~\mbox{%DIFAUXCMD
\cite{Ney1990, Kaufman2000}}\hskip0pt%DIFAUXCMD
, but it was later utilized in other fields.
}

\subsection{\DIFadd{Use Cases for Direct Volume Rendering}}

\DIFadd{As with many fields, }\gls{dvr} \DIFadd{can be utilized for education.
MacDougall et al.~\mbox{%DIFAUXCMD
\cite{MacDougall2016} }\hskip0pt%DIFAUXCMD
provide an example using a large 3D display wall of molecules for chemistry research and education.
These models used the traditional ball on a stick model and }\gls{dvr}\DIFadd{, which better represented depth and provided a more realistic or cloudy model of the quantum world. 
MacDougall et al.~\mbox{%DIFAUXCMD
\cite{MacDougall2016} }\hskip0pt%DIFAUXCMD
found that elements of this system could help create new drugs for the future and proposed use cases that would encourage students to become more hands-on.
}

\DIFadd{Hibbard~\mbox{%DIFAUXCMD
\cite{HibbardL.1986} }\hskip0pt%DIFAUXCMD
talked about how the data from 2D plots is easier to view in 3D when using volume rendering. 
Hibbard~\mbox{%DIFAUXCMD
\cite{HibbardL.1986} }\hskip0pt%DIFAUXCMD
then also conferred this data could be used to allow this data to be manipulated by using a time-variant, allowing phenomena like the wind to be simple to examine.
These techniques were then extended by Riley et al. ~\mbox{%DIFAUXCMD
\cite{Riley2003} }\hskip0pt%DIFAUXCMD
to allow for realistic visualizations of cloud maps.
This was impossible using iso-surfaces, which tended to require a form of lighting that clouds did not utilize~\mbox{%DIFAUXCMD
\cite }\hskip0pt%DIFAUXCMD
}{\DIFadd{Riley2003}}\DIFadd{.
~}\gls{dvr} \DIFadd{allows climate scientists to explore the internal patterns of the effect of time and space on weather phenomena~\mbox{%DIFAUXCMD
\cite{Wang2018}}\hskip0pt%DIFAUXCMD
.
}

\gls{dvr} \DIFadd{is also used while testing the quality of materials to visulize them. These materials can constis of metals alloys~\mbox{%DIFAUXCMD
\cite{Okuyan2014}}\hskip0pt%DIFAUXCMD
, minerals~\mbox{%DIFAUXCMD
\cite{Okuyan2014} }\hskip0pt%DIFAUXCMD
concrete~\mbox{%DIFAUXCMD
\cite{Okuyan2014}}\hskip0pt%DIFAUXCMD
, resins~\mbox{%DIFAUXCMD
\cite{Nguyen2016}}\hskip0pt%DIFAUXCMD
, and combinations of different materials used to create a single one~\mbox{%DIFAUXCMD
\cite{Groger2022, Okuyan2014}}\hskip0pt%DIFAUXCMD
. 
Material science requires understanding materials' internal structures formed under different circumstances~\mbox{%DIFAUXCMD
\cite{Groger2022}}\hskip0pt%DIFAUXCMD
.
This could be done in the way of running CT scans of being dented or folded, allowing for structural analysis of how different conditions can affect different materials~\mbox{%DIFAUXCMD
\cite{Groger2022}}\hskip0pt%DIFAUXCMD
.
This can also be applied to the creation of different materials, like sponge-like materials that need to move in certain ways or quantum materials that will arrange atoms, creating some highly precise and gas-like materials~\mbox{%DIFAUXCMD
\cite{Okuyan2014, Grottel2012}}\hskip0pt%DIFAUXCMD
.
Volume rendering can also be used to show how conducive liquids like resins are moving through non-conductive ones~\mbox{%DIFAUXCMD
\cite{Nguyen2016}}\hskip0pt%DIFAUXCMD
, allowing for real-time testing of how to develop products using these materials and presenting communication methods with end users~\mbox{%DIFAUXCMD
\cite{Nguyen2016}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{Molecular Sciences also use volume rendering to visualize the output gathered from electronic microscopes~\mbox{%DIFAUXCMD
\cite{Nguyen2022, Goodsell1989}}\hskip0pt%DIFAUXCMD
.
This allows the user to view the contents of a sample collected in 3D based on the different densities, much like }\gls{ct} \DIFadd{and }\gls{mri} \DIFadd{data.
Nguyen et al.'s~\mbox{%DIFAUXCMD
\cite{Nguyen2022} }\hskip0pt%DIFAUXCMD
DiffTEM system adds to this by allowing denoising that utilizes many images of the data collected from different orientations before the data is rendered. 
}

\DIFadd{Geologists can utilize }\gls{dvr} \DIFadd{to represent the values of radar data~\mbox{%DIFAUXCMD
\cite{Baker2007}}\hskip0pt%DIFAUXCMD
.
They tend to do this by using ground-penetrating radar~\mbox{%DIFAUXCMD
\cite{Baker2007}}\hskip0pt%DIFAUXCMD
.
Unlike the previous examples of use cases for }\gls{dvr}\DIFadd{, radar has many blank areas; Zehner~\mbox{%DIFAUXCMD
\cite{Zehner2021} }\hskip0pt%DIFAUXCMD
states that }\gls{dvr} \DIFadd{can be used to both present a more full view of the area but to also better represent the level of uncertainty that can be viewed from this viewpoint.
}

\subsection{\DIFadd{Human Computer Interaction (HCI) Experiments Using Direct Volume Rendering (DVR)}}
%DIF >  Start writing up the separate sections here
\DIFadd{Understanding how people visualize or interact with }\gls{dvr} \DIFadd{is important to this research.
While being able to visualize various data using DVR is one thing, the user experience is more important than the act of being able to display the content because the content rendered by the }\gls{dvr} \DIFadd{needs to be a more pleasant experience than the alternative situations for it to have utility.
The following section looks at how various studies over time have evaluated systems using }\gls{dvr}\DIFadd{.
}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Chapter2/Images/Hui1993.png}
    \caption[Examples of  Hui et al.'s cursors on 3D planes.]{\DIFaddFL{Examples of  Hui et al.'s~\mbox{%DIFAUXCMD
\cite{Hui1993} }\hskip0pt%DIFAUXCMD
cursors on 3D planes. Right) Nail on plane, where the cursor has the ability to rotate around the volume; Left) Venetain blind, a non-flat plane, which allows the user to navigate easier on all dimensions using the cursor. Used with permission from IEEE \textcopyright{} 1993.}}
    \label{fig:Hui1993}
\end{figure}

\DIFadd{One of the first instances of human interaction being a concept using }\gls{dvr} \DIFadd{is the work by Hui et al.~\mbox{%DIFAUXCMD
\cite{Hui1993} }\hskip0pt%DIFAUXCMD
on a cursor for these interactions.
This cursor is displayed in }\autoref{fig:Hui1993} \DIFadd{and works similarly to how a mouse works on a 2D plane, but it had the ability to be rotated on another plane using another 1D input, like the scroll wheel on a mouse, to rotate the plane the mouse cursor was sitting on.
To inform the user of the depth of the plane was highlighted on the outside of the volume, and a variation blind effect would be used to prevent the cursor from moving too much while still being able to get everywhere~\mbox{%DIFAUXCMD
\cite{Hui1993}}\hskip0pt%DIFAUXCMD
. 
}

\DIFadd{Kersten et al.~\mbox{%DIFAUXCMD
\cite{Kersten2006} }\hskip0pt%DIFAUXCMD
performed one of the first studies ever to be done using }\gls{dvr} \DIFadd{using a 3D display.
This study was focused on how transparency affected depth perception when using }\gls{dvr} \DIFadd{on a stereoscopic display.
The type of transparency used was commonly associated with direct volume rendering. 
The study design would slowly rotate a cylinder filled with Perlin noise~\mbox{%DIFAUXCMD
\cite{Perlin1989} }\hskip0pt%DIFAUXCMD
and then rotate it slowly in one direction.
To tell what direction a cylinder was rotating, users would have to understand the approximate position of elements in the noise.
Participants of this study had to do this when using a mono and stereoscopic display and between various amounts levels of opacity. 
Kersten et al.'s~\mbox{%DIFAUXCMD
\cite{Kersten2006} }\hskip0pt%DIFAUXCMD
findings showed that }\gls{dvr} \DIFadd{is much more effective on stereoscopic displays.
This shows that the real use case for these techniques may be within the use of stereoscopic devices. 
}

\DIFadd{Several years later, Kersten-Oertel et al.~\mbox{%DIFAUXCMD
\cite{Kersten-Oertel2014} }\hskip0pt%DIFAUXCMD
looked at methods to tell depth within a sparse volume rather than an occluded one.
This was in the form of a set of Cerebral vascular volumes. 
Five different depth cues and a baseline were utilized throughout this study: Edge Detection (Halos), Pseudo-Chromadepth, Fog, Kinetic depth, and Stereo Vision.
Kersten-Oertel et al.~\mbox{%DIFAUXCMD
\cite{Kersten-Oertel2014} }\hskip0pt%DIFAUXCMD
tested a combination of experts and novices separately on two different studies.
All of the studies utilized the same procedure, where two vessels were highlighted, and the participants guessed which one was closer to the participant.
One of these studies utilized each depth cue individually, while the other focused on their different combinations.
Individual Chromadepth, Fog, and Stereo were shown to be much more beneficial than the other cues when shown individually. 
While chroma depth and stereo seem to have showcased the most substantial values for the combination, Kersten-Oertel et al.~\mbox{%DIFAUXCMD
\cite{Kersten-Oertel2014} }\hskip0pt%DIFAUXCMD
study again shows why Stereo-vision of }\gls{dvr} \DIFadd{is such a strong depth cue.
}

\DIFadd{Another user study looking into the transparency created by using }\gls{dvr} \DIFadd{was done by Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012}}\hskip0pt%DIFAUXCMD
. 
This system used two layers of volume rendering: an outer transparent layer and an inner occluded layer.
By having occluded surfaces, Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012} }\hskip0pt%DIFAUXCMD
could provide lighting to parts of the volume that were occluded by rendering the image throughout multiple passes.
%DIF > From this, they then ran a user study that investigated using the user's preferences to compare their shadowed-enabled volume rendering to a version that didn't have any shadows.
}

\DIFadd{To evaluate the effectiveness of placing shadows in }\gls{dvr} \DIFadd{Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012} }\hskip0pt%DIFAUXCMD
ran a series of studies using a computer monitor running at approximately 20fps, each consisting of less than 20 participants.
The first one was designed to test the users' preferences. This was done by having the participants view the same object side by side. Participants were asked to say whether the shadowed-enabled or non-shadowed versions told them more about the volume. 
The next study looked at shape perception, which had participants arrange two images of similar body parts from two different datasets.
This shape perception study showed that raycasted shadows were not essential for shape perception, but instead, the }\gls{ui} \DIFadd{was because participants were able to orientate their depth perception by with the ability to rotate the object.
The next study looked at relative depth perception. One user chose a point on the screen that was closest to themselves, and they found that showing shadows significantly increased depth perception.
They note here that it was uncommon for participants to answer this incorrectly. 
The final experiment had users determine how far into a volume the point was by having them estimate the location of an artifact inside of the volume. 
This task found that depth perception was not affected by the distance or the presence of shadows inside of the volume; rather, it may hinder the absolute depth perception. 
Overall, Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012} }\hskip0pt%DIFAUXCMD
show that shadows can help detail information in a Volume, but they don't necessarily improve perception if there are other depth cues present like motion.
}

\DIFadd{Three studies to use }\gls{dvr} \DIFadd{with }\gls{mr} \DIFadd{were done by Laha et al.~\mbox{%DIFAUXCMD
\cite{Laha2013, Laha2012, Laha2012a, Laha2014} }\hskip0pt%DIFAUXCMD
who looked at studies which were focused on determining the immersion of different }\gls{mr} \DIFadd{devices displaying volumetric information.
To best determine the amount of immersion when using }\gls{mr} \DIFadd{devices, they enabled and disabled head tracking and limited the area that was displayed to the user to $360^{\circ}$, $270^{\circ}$, $180^{\circ}$, $90^{\circ}$. 
These conditions allowed them to determine the required or appropriate level of immersion that was preferable for each task. 
All of their studies utilized a selection of open-source real-world data to do their studies and utilized a range of different tasks suited for each dataset on each one.
The first study was focused on testing if there was any noticeable benefit to }\gls{mr} \DIFadd{when needing to utilize a }\gls{cave} \DIFadd{using the restricted motion tracking~\mbox{%DIFAUXCMD
\cite{Laha2012}}\hskip0pt%DIFAUXCMD
. 
This study which was focused on restricted head motion caused them to notice that any combination of the two conditions was able to grant better results.
Laha et al.'s~\mbox{%DIFAUXCMD
\cite{Laha2013} }\hskip0pt%DIFAUXCMD
next study utilized the NVisor SX111 a (}\gls{vst} \gls{ar} \DIFadd{display) under similar circumstances. 
This study found that the performance of their participants was improved by providing the most immersion possible~\mbox{%DIFAUXCMD
\cite{Laha2013}}\hskip0pt%DIFAUXCMD
.
The final study ran by Laha et al.~\mbox{%DIFAUXCMD
\cite{Laha2014} }\hskip0pt%DIFAUXCMD
looked at what happened when iso-surfaces were used as the data set, requiring them to change the tasks required and also had them reintroduce stereo vision back to the conditions. 
This study observed that the combination of all of the results was the most suitable for the tasks required to interact with volumetric data visualizations. 
}

\DIFadd{Afterward, Laha et al.~\mbox{%DIFAUXCMD
\cite{Laha2016} }\hskip0pt%DIFAUXCMD
created a taxonomy of the different types of tasks that are possible when using }\gls{mr}\DIFadd{, with the goal of remove the domain dependency that exists with empirical studies.
This was done by consulting 167 people using a questionnaire regarding how this taxonomy should be shaped. 
This was done with the aim to allow basic types of interactions to be considered similar to other types of interactions for different fields. 
These different types were classified as:
}\begin{itemize}
    \item \textbf{\DIFadd{Searching:}} \DIFadd{Searching for the presence or absence of an object, or counting the amount of a given object.
    }\item \textbf{\DIFadd{Pattern Recognition:}} \DIFadd{Looking for trends like what side of the data set are there more blood vessels or repetition and asking the participant how many times certain items appear.
    }\item \textbf{\DIFadd{Spatial Understanding:}} \DIFadd{This section is for tasks that require the participant to understand the orientation or position of a feature in the dataset. 
    %DIF > Spatial understanding can be split into three subcategories:
    %DIF >  \begin{itemize}
    %DIF >      \item \textit{Absolute} Participants Locate a feature in the dataset that is the highest, lowest, furthest, etc., from the participant viewpoint or in the dataset.
    %DIF >      \item \textit{Reative:} Participants Judge if one feature in the dataset is in front, behind, higher, or lower than another object.
    %DIF >      \item \textit{Intersection:} Participants are asked if two objects are intersecting with each other. 
    %DIF >  \end{itemize}
    }\item \textbf{\DIFadd{Quantitative Estimation:}} \DIFadd{These focus on tasks that have the users estimate the properties of a feature of the dataset. 
    }\item \textbf{\DIFadd{Shape Description:}} \DIFadd{Requires the user to describe a shape they are viewing. 
}\end{itemize}
\DIFadd{They also note two possible viewing styles: Egocentric and exocentric. They also note how the different dimensionality of the data should be considered.
Overall, this study paper has informed the design process of the later studies. 
}

%DIF >  sketch-based interactions
\DIFadd{Sketch-based interactions allow for an easy interaction with volumes.
Shen et al.'s~\mbox{%DIFAUXCMD
\cite{Shen2014} }\hskip0pt%DIFAUXCMD
split the various use cases this technique could be used into seven different categories: selection, cutting, segmentation, matching, coloring, augmentation, and illustration. 
}

\DIFadd{An early version of volumetric hatching used in this dissertation was inspired by research from Feng et al.~\mbox{%DIFAUXCMD
\cite{Feng20152548}}\hskip0pt%DIFAUXCMD
, which helped establish fundamental approaches to illustrative rendering.
Feng et al.~\mbox{%DIFAUXCMD
\cite{Feng20152548} }\hskip0pt%DIFAUXCMD
created a system that projected a grid over the area of the volumetric area.
This was chosen to help highlight objects and clearly present the depth to different areas when faced with users who do not understand the depth of objects within the volume.
This system worked by projecting a 2D grid on the first available }\gls{voxel_g} \DIFadd{in the ray marching algorithm when it was in the range of the grid texture. 
They note in their findings that chroma color could be utilized to highlight depth perception.
}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Grosset.png}
    \caption[
            The 6 datasets used in Grosset et al.'s~\cite{Grosset2013} research.
        ]
        {
            \DIFaddFL{The 6 datasets used in Grosset et al.'s~\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
research: (a) aneurysm, (b) backpack, (c) bonsai, (d) flame, (e) Richtmyer-Meshkov instability \& (f) thorax. Used with permission from IEEE \textcopyright{} 2013.
        }}
    \label{fig:Grosset}
\end{figure}

\DIFadd{Almost all interactions with }\gls{dvr} \DIFadd{require good depth perception, even when they are using a 2D display. 
Grosset et al.~\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
created a user study that aimed to test if the depth of field could be utilized with }\gls{dvr}\DIFadd{. 
This study looked at replicating a person's ability to focus on an element based on how deep it was in the volume that they were focused on.
This was tested using a Dynamic and a Static experiment. 
The static experiment asked participants to judged which of two circled features in still images was closer in depth, testing whether Depth of Field and projection type affected speed and accuracy.
The Dynamic Experiment required participants to watch short videos where the focal plane swept through the scene and then judged which circled feature was closer, testing whether dynamic focusing improved depth accuracy.
The Dynamic Experiment allowed the user to control the depth of field by tracking their eyes. 
The other experiment by Grosset et al.~\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
set the focal points at the most optimal place.
While there was not much difference between the different conditions, both studies provided similar answers.
Whether this technique was static or dynamic didn't make much difference between various conditions. 
However, they did notice a difference between the different sets of data shown in }\autoref{fig:Grosset}\DIFadd{.
Each different dataset which was visualized had a different amount of depth perception, with the bonsai and thorax datasets being the easiest to determine depth, while the flame and Richtmyer-Meshkov instability datasets were the hardest to determine depth.
This was likely because each dataset they utilized presented a vastly different type of visualization, which utilized different depth cues to various levels~\mbox{%DIFAUXCMD
\cite{Grosset2013}}\hskip0pt%DIFAUXCMD
.
}

\DIFadd{Roberts et al.~\mbox{%DIFAUXCMD
\cite{Roberts2016} }\hskip0pt%DIFAUXCMD
investigated the impact of different types of reconstruction filters for }\gls{dvr} \DIFadd{graphics using a questionnaire.
%DIF > The different reconstruction filters they used as the conditions for their study were: b-spline, Trilinear, CatMull-Rom, Int B-spline, and Welch.
These reconstruction filters included B-spline, Trilinear, CatMull-Rom, Int B-spline, and Welch filters.
Participants were asked to view several open-source datasets and rate their depth quality, layout, sharpness, and jaggedness. 
This paper did not have many major findings. B-spline was preferable for the tasks they suggested but not to a significant amount. 
What was interesting about this research was that the datasets seemed to have more of an impact than the conditions themselves did, much like Grosset et al.'s~\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
study.
}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Englund2018.jpg}
    \caption[Conditions used in Englund et al.'s~\cite{Englund2016, Englund2018} studies.]{\DIFaddFL{Conditions used in Englund et al.'s~\mbox{%DIFAUXCMD
\cite{Englund2016, Englund2018} }\hskip0pt%DIFAUXCMD
studies.
    (a) Direct Volume Rendering (DVR), (b) Depth of Field, (c) Depth Darkening, (d) Volumetric Halos, (e) Volume Illustration, and (f) Volumetric Line Drawings. Used with permission from John Wiley and Sons \textcopyright{} 2016}}
    \label{fig:Englund2018}
\end{figure}

\DIFadd{To get around the issue of not being able to create a sated by Roberts et al.~\mbox{%DIFAUXCMD
\cite{Roberts2016} }\hskip0pt%DIFAUXCMD
and Grosset et al.'s~\mbox{%DIFAUXCMD
\cite{Grosset2013}}\hskip0pt%DIFAUXCMD
, Englund et al.~\mbox{%DIFAUXCMD
\cite{Englund2016, Englund2018} }\hskip0pt%DIFAUXCMD
created 15 volumetric images in the shape of a cube and took photos of each of the sides these volumes, which allowed them to take 90 images of different volumes, which they then utilized in a questionnaire.
%DIF > A sample of these volumes can be seen in ~\autoref{fig:Englund2018}.
They then took these images with six different }\gls{dvr} \DIFadd{visualization techniques seen in }\autoref{fig:Englund2018}
\DIFadd{~}\footnote{\DIFadd{The descriptions used in Englund et al.'s~\mbox{%DIFAUXCMD
\cite{Englund2016, Englund2018} }\hskip0pt%DIFAUXCMD
may be misleading when compared to other research. So the names they have used are in parenthesis.}} \DIFadd{: Depth Darkening (Depth Darkening),
Depth of Field (Depth of Field), 
Subtractive Color Blending (Volumetric Halos), 
Additive Color Blending (DVR), 
White Outlines(Volumetric Lines), 
Toon Shading (Depth Darkening), 
Black Outlines(Volume Illustration).
This questionnaire had participants perform }\gls{twofc} \DIFadd{questionnaire for multiple depth perception studies if participants could visually orient the images in the same positions and how attractive the different conditions they presented were~\mbox{%DIFAUXCMD
\cite{Englund2016}}\hskip0pt%DIFAUXCMD
.
This study found that depth darkening and Halo's gave the best sense of depth perception out of all the conditions, and the occlusion cue-in seems to be the most effective way to determine depth perception based on it. Orienting the shapes was easiest to determine using the toon shading, as that method highlighted the shapes of the foreground objects the best. Most users preferred depth darkening~\mbox{%DIFAUXCMD
\cite{Englund2016}}\hskip0pt%DIFAUXCMD
. 
}

\DIFadd{The Englund et al.'s~\mbox{%DIFAUXCMD
\cite{Englund2018} }\hskip0pt%DIFAUXCMD
study design and conditions were later consulted on by three experts. 
This clearly explains both the Subtractive Color Blending and Black Outlines. 
One main issue an expert noted was that as these sources of data only represented a single image, they didn't accurately represent the interaction that people would have with volume data, as this was expected to be utilized to properly view the data, like being able to rotate the volume. 
The same expert noted that the Toon shading hid aspects of the volume due to its high opacity.
One expert noted that the outlines would be better suited if they could react to the surfaces's outline.
Some of the most important expert findings from Englund et al.'s research~\mbox{%DIFAUXCMD
\cite{Englund2018} }\hskip0pt%DIFAUXCMD
was that by highlighting the edges of the objects, depth perception was most improved by communicating to the user where the front and back of the objects were. 
These factors likely were likely why the black outlines performed as well as they did.
}


\DIFaddend \section{An introduction to Augmented Reality (AR) and Virtual Reality (VR)}
% Millgrims AR
Augmented Reality or \gls{ar} refers to the ability to bring virtual objects into the real world \cite{Milgram1994}.
\gls{ar} technologies are part of Milgram et al.'s~\cite{Milgram1994} Mixed Reality Continuum (seen in \autoref{fig:MillgirmsMixedReality}), which represents the spectrum of technology that can augment the visual world. 
The more of the world that is the viewer's perception is moved towards only being able to sense the virtual world \DIFdelbegin \DIFdel{, the more virtual reality is in place }%DIFDELCMD < \gls{vr}%%%
\DIFdelend \DIFaddbegin \DIFadd{the more further along the scale it becomes until a user is fully immersed in a virtual world}\DIFaddend . 
%\gls{ar} refers to the ability to bring virtual objects into the real world by overlaying them on someone's vision.
\gls{mr} talks about the spectrum between the real and virtual worlds, not including them. 
\DIFdelbegin \DIFdel{This is for }\DIFdelend \DIFaddbegin \DIFadd{Resulting in }\gls{mr} \DIFadd{to include }\DIFaddend many different types of displays, given the types of interactions ranging from \gls{hmd}s, \gls{cave}s, Handheld Devices, \gls{sar} displays, and even some large monitors can be used as mixed reality devices. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Chapter1/MillgirmsMixedReality.pdf}
    \caption{A simple view of \DIFdelbeginFL \DIFdelFL{Milgrims }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Milgram }\DIFaddendFL et al.'s Mixed Reality Continuum~\cite{Milgram1994}.}
    \label{fig:MillgirmsMixedReality}
\end{figure}

\subsection{Mixed Reality (MR) Hardware}
% Why is this section needed
This section covers the relevant hardware for this dissertation, focusing on devices that provide an \gls{mr} experience to gain an understanding of the experiences that are relevant and that combine real-world interactions with computer graphics.
%Medical, \gls{X-ray Vision} applications and Depth Perception studies have been developed for a large plethora of devices that can be found all across Milgram's~\cite{Milgram1994} Mixed Reality Continuum.
These devices have been designed for different applications and, as such, have different strengths and weaknesses across Milgram 's~\cite{Milgram1994} Mixed Reality Continuum.
This section focuses on the advantages and disadvantages of the hardware used in the relevant research for this dissertation. 

% Talking about the first AR and VR paper
%In 1968 AR
The first \gls{ar} system (seen in \autoref{fig:TheSwordOfDamaclies}) was created by Ivan Sutherland in 1968 "The Sword of Damocles"~\cite{Sutherland1968}. 
This modified a person's sight by adding computer graphics to it. 
By tracking where the user moved their head every 10ms, the system would update the images that the system presented to them within 3D space.
The position of the user's eyes in each lens would determine the direction of the 3D object, which was tracked by a collection of exterior sensors placed around the space users could walk around~\cite{Sutherland1968}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/TheSwordOfDamaclies.PNG}
    \caption[Ivan Sutherland's~\cite{Sutherland1968} "The Sword of Damocles" one of the first head-mounted \gls{ar} systems.]{Ivan Sutherland's~\cite{Sutherland1968} "The Sword of Damocles" one of the first head-mounted \gls{ar} systems. This image is a reprint published by~\cite{Baus2014} licensed under a Creative Commons Attribution licence. Permission of the original image is granted to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee, provided that copies are not made or distributed for profit or commercial advantage.}
    \label{fig:TheSwordOfDamaclies}
\end{figure}

% the implications of Sutherlands system. 
Sutherland's~\cite{Sutherland1968} Sword of \DIFdelbegin \DIFdel{Damaclies }\DIFdelend \DIFaddbegin \DIFadd{Damocles }\DIFaddend became the progenitor of all modern \gls{ar} and \gls{vr} \glspl{hmd}. 
In the following decades, researchers developed methods to interact with computers using hand gestures~\cite{Pizer1986} to interact with 3D desktop displays and found cheaper and more ergonomic ways to present this information~\cite{Wang1990}.
Research was done to shrink the size of these displays into a more portable size~\cite{Mann1997}.

% What is augmented Reality Used for
\gls{ar} can be found in many different industries~\cite{Park2021}.
Entertainment and gaming use \gls{ar} to create an experience that is based on navigation like Ingress or Pokemon Go~\footnote{\url{https://pokemongolive.com/}}, also first-person shooters, puzzle games, and board games~\cite{Park2021, Marto2022}.
Industrial maintenance uses \gls{ar} to instruct people on the maintenance of complex machinery~\cite{Kim2018}, 
Tour Guides use \gls{ar} to guide users around cities and buildings and to explain the contextual information to the end user ~\cite{Kim2018}.
\gls{ar} is also being used to extend existing telecommunications technologies, allowing volumetric communications to be possible between different groups of people~\cite{Kim2018}.

% An overview of smartphone technology
Modern smartphones and tablets provide a suitable platform for presenting video see-through mixed reality experiences that use the smartphone's camera to capture the physical world, which is augmented with digital content. The wide availability of smartphones has also made them a popular choice for \gls{mr} experiences. 
One advantage of this approach is the display's brightness, which often outperforms head-worn displays that can appear washed out compared to head-worn displays.  
Mobile devices don't provide an immersive experience; smartphones need to be held by the user, and they can allow for an \gls{ar} experience or require a stand during the operation. A use case that has been gaining popularity is for training or education applications where 3D content is presented as an adjunct to a paper-based medium.

% An overview of SAR-based systems
Projector-based mixed reality systems, also known as \gls{sar}, employ projectors to alter the appearance of physical objects that can be highly organic in shape instead of a typical projector screen. This form of augmented reality has received less attention compared to head-worn and hand-held solutions. However, projected information provides some unique features that have the potential for several applications. An advantage of projected information overlaid on physical objects is that multiple observers can be perceived as the same projection simultaneously and from multiple points of view. For example, visualizations of internal anatomical details directly projected onto the human body~\cite{Hoang2017}.

% Explaining caves in some detail
\DIFdelbegin %DIFDELCMD < \gls{cave}%%%
\DIFdel{s }\DIFdelend \DIFaddbegin \glspl{cave} \DIFaddend environments are another instance of \gls{sar}. Caves are three-dimensional cubicles that use several projected images to simulate an immersive 3D environment without users needing to wear a headset. As such, caves can provide a collaborative \gls{ar} experience for one or more medical professionals who wish to explore vast amounts of information collaboratively. For example, medical data can be visualized in the form of three-dimensional models that can be viewed collaboratively in a cave environment for diagnosis~\cite{Knodel2018}. \gls{dicom} files were processed into 3D models using an algorithm that converts voxels to polygons that are refined into visualizations to be viewed using 3D glasses in a collaborative environment.

% Talking about what are Head-mounted displays
\DIFdelbegin %DIFDELCMD < \gls{hmd}%%%
\DIFdel{s }\DIFdelend \DIFaddbegin \glspl{hmd} \DIFaddend rely on placing the display of the headset as close to the user's line of sight as possible. 
They allow users to move around the virtual content and interact with it naturally through head movement. This offers a means of presenting mixed or virtual reality content suitable for medical visualizations, training, step-by-step guidance, and many other applications~\cite{Park2021, Ding2022}. 
Since Ivan Sutherland's \gls{hmd} in 1968~\cite{Sutherland1968}, it has been iterated on since, especially in the last several years since VR headsets have drastically lowered in price and become a consumer commodity~\cite{Xie2021, Ding2022}. 

% What is virtual reality
\gls{vr} \glspl{hmd} are available in many different forms. They are accompanied by supporting technologies such as position tracking, front-facing cameras, eye tracking, EEG sensors, and hand controllers to deliver compelling virtual experiences~\cite{Lee2008}. 
Desktop systems such as the Oculus Rift S and HTC Vive Pro offer high-quality display systems that, coupled with a high-end PC, present immersive 3D environments in real time. \DIFdelbegin \DIFdel{Desktop systems such as the Oculus Rift S and HTC Vive Pro offer high-quality display systems that, coupled with a high-end PC, present immersive 3D environments in real time. }\DIFdelend They also incorporate tracking systems that enable hand-held controllers to support interactions in virtual worlds. Standalone hardware options such as the Oculus Quest and Vive Focus have also become available, removing the need to be tethered to a PC and allowing for wide-area virtual environments to be configured. One of the advantages of head-worn displays is that they leave the operators’ hands free to perform other tasks, unlike hand-held technologies, such as smartphones, which require the user to hold the device during operation.

% Introduction into ocular see through 
\textbf{\gls{ar} \glspl{hmd}} tend to come in two distinct types: 
\gls{vst}, and \gls{ost} \gls{hmd}s. 
\gls{vst} \gls{hmd}s use exterior sensors (usually a camera) and an occlusive headset (typically a VR headset)~\cite{Zhan2020, Geng2013, Xiong2021}. 
While \gls{ost} \gls{ar} use a range of different transparent displays.

% Augmented Reality
There have been ongoing improvements in these devices' quality, ergonomic design, resolution, and capability. Early consumer-level displays, such as the Sony Glasstron (1996), provided an augmented reality display solution connecting to a desktop computer. 
This has since been taken to utilizing a Mobile platform with the Microsoft HoloLens (2016).
The HoloLens allowed the user to move around untethered and interact with the virtual environment. 
The system was able to react to the real-world environment, and it allowed for precise (~1.5 cm) spatial tracking.

% An explanation of Augmented Reality HMD's
\gls{ost} \gls{ar} \gls{hmd}s like the Microsoft HoloLens~\cite{microsofthololens} can also be used to display \gls{ar} content within the user's field of sight.
Objects should be locked into place in the real world, allowing users to move around the 3D virtual content while being able to view the real-world content. 
The HoloLens uses environmental information to locate the real world and brings virtual objects into the user's field of view. 

\subsection{Designing Applications for Stereoscopic Displays} \label{sec:ApplicationsForStereoscopicDisplays}
\gls{ar} devices require different types of interfaces based on the required context and functionality~\cite{Karambakhsh2019, Kim2022}.
This section presents design considerations that are important to understand when creating a display that accommodates \gls{X-ray Vision} in a medical setting, which is a key use case for this thesis. 

In their review by Akpan and Shanker~\cite{Akpan2019}, they investigated 3D data visualizations displayed on \gls{vr} headsets compared to 2D displays. 
This review presented studies that tested 3D visualizations identifying key performance elements:
\begin{itemize}
    \item Data \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{can be }\DIFaddend easier to analyze with visualizations and promotes more experimentation with data sets \DIFaddbegin \DIFadd{in the correct situations}\DIFaddend . 
    \textit{This can be seen in Cordil et al.'s~\cite{Cordeil2017} work where users are encouraged to move around the different axes of graphs to create new ones, encouraging data analysis on a different level. Allowing for a rapid visual analysis with clear results};
    \item Aiding users in understanding the data at a faster rate. \textit{Tanagho et al.~\cite{Tanagho2012} has found that when guiding people using 2D and 3D interfaces for performing a Laparoscopic surgery, participants were faster and more accurate when using a 3D interface compared to a 2D one};
    \item Verifying data using a 3D format \DIFdelbegin \DIFdel{helps }\DIFdelend \DIFaddbegin \DIFadd{can help }\DIFaddend users to more rapidly verify models. \textit{Majber et al.~\cite{Mujber2004} found this to be the presenting simulation data to stakeholders for factory designs and layouts. The Interface and data visualized by the \gls{vr} device were understandable even to people who were not knowledgeable about the products or factories};
    \item The overall presentation of 3D data \DIFdelbegin \DIFdel{is able to take on much }\DIFdelend \DIFaddbegin \DIFadd{tends to be able interpreted }\DIFaddend more intuitively than 2D models such as graphs or slices. \textit{Qu et al.~\cite{Qu2010} found this when they were simulating the growth of an eggplant, where they found the simulations were made much easier to understand when the user was presented with a 3D graphic of an approximation of the predicted growth of the plant rather than 2D graphs}.
\end{itemize}
Similar findings can also be found in McIntire et al.'s ~\cite{McIntire2012} literature review on the utility of stereoscopic displays compared to conventional monoscopic displays.
McIntire et al.'s ~\cite{McIntire2012} found that stereoscopic display either improved or made no difference in spatial awareness, spatial understanding, classifying objects, spatial manipulation, navigation, and educational activities. 
There is evidence that 3D information made sense when viewed via a stereoscopic display.


Guo et al.\cite{Guo2019} noted that even if a \gls{vst} \gls{ar} \gls{hmd} could be perfectly calibrated to the real world, it still made surgeons nervous to really on an external device connected to the \gls{hmd} via a wired or wireless connection, because it could lose its calibration making the system inaccurate, or it could turn off and occlude the surgeon's vision altogether.
Andrews et al.~\cite{Andrews2021} note in their literature review that due to the HoloLens's gesture controls \DIFdelbegin \DIFdel{, it can operate in a sterile environment}\DIFdelend \DIFaddbegin \DIFadd{allow for operations within sterile environments}\DIFaddend . 
Making the headset functional in a clinical or surgical setting. 
When Rodrigues et al.~\cite{Rodrigues2017} asked two surgeons how best to create an \gls{mr} sweet of tools. 
Rodrigues et al.~\cite{Rodrigues2017} spent several months talking with two surgeons to choose the best hardware. 
Both opted to use the HoloLens over all other hardware choices.
Pratt et al.~\cite{Pratt2018} utilized the HoloLens to perform several surgeries where the patients \gls{ct} and \gls{mri} data was laid over the top of the patients. 
It is for these reasons that the research in this dissertation chose to use an \gls{ost} \gls{ar} \gls{hmd} (a HoloLens2) for this research.

%DIF <  An explanation of Augmented Reality
\DIFdelbegin \section{\DIFdel{Perception and Depth Perception tasks in Augmented Reality (AR) and Virtual Reality (VR) Head Mounted Displays}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
%DIF <  This section is talking about the issues of depth perception in oST AR
%DIF <  The ability to modify our visual perception of the world allowed for an almost limitless amount of possibilities of changes that are possible to augment people's vision. 
%DIF <  However, this required us to learn about the limitations of this perception beyond what we could see naturally. 
%DIF <  The most major form of this was to gain a concrete understanding of how depth worked when displaying a 2D image. 
%DIF <  The visual impacts that have been caused by these devices were caused by the camera lenses don't see the world the same way as people do, so the displays would need to appear~\cite{Cutting1997}.
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Depth perception }\DIFdelend \DIFaddbegin \subsection{\DIFadd{Volume Rendering In Mixed Reality}}
\DIFadd{Graphical capabilities }\DIFaddend on \gls{mr} \glspl{hmd} \DIFdelbegin \DIFdel{has been a goal for over 25 years~\mbox{%DIFAUXCMD
\cite{Cutting1997}}\hskip0pt%DIFAUXCMD
.
%DIF < \gls{mr} \glspl{hmd} tend to be quite limited in methods that are possible to use to measure how a participant can measure depth in the real world, which has prompted 
Several common methods of testing depth perception in the real world. These include 
}%DIFDELCMD < \begin{itemize}
\begin{itemize}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Blind Walking or Blind reaching:}} %DIFAUXCMD
\DIFdel{Asking a participant to place their hands or to walk to a location where a virtual artifact was previously~\mbox{%DIFAUXCMD
\cite{Jamiy2019, Jamiy2019b}}\hskip0pt%DIFAUXCMD
;
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Verbal Reporting:}} %DIFAUXCMD
\DIFdel{Requesting the participant tell you how far away the virtual object is from them~\mbox{%DIFAUXCMD
\cite{Jamiy2019, Jamiy2019b}}\hskip0pt%DIFAUXCMD
;
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Matching Protocols: }} %DIFAUXCMD
\DIFdel{Placing a virtual object relive to where the virtual object is (or was)~\mbox{%DIFAUXCMD
\cite{Jamiy2019, Jamiy2019b}}\hskip0pt%DIFAUXCMD
;
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{%DIFDELCMD < \gls{twofc_g}%%%
\DIFdel{:}} %DIFAUXCMD
\DIFdel{Giving the participant where there is one correct and wrong answer on a set of conditions that will get closer to being equal to determine at what point can participants no longer determine proper depth perception~\mbox{%DIFAUXCMD
\cite{Otsuki2017, Fan1996}}\hskip0pt%DIFAUXCMD
.
}
\end{itemize}%DIFAUXCMD
%DIFDELCMD < \end{itemize}
%DIFDELCMD < %%%
\DIFdel{Using these types of studies 
%DIF <  Start talking about the early work trying to understand depth in AR and VR
early work in this field focused on testing how seeing graphics rendered using virtual reality headsets may have changed. 
It also began understanding how real-world factors impact the depth perception of the environment~\mbox{%DIFAUXCMD
\cite }\hskip0pt%DIFAUXCMD
}%DIFDELCMD < {%%%
\DIFdel{Ellis1998}%DIFDELCMD < }%%%
\DIFdel{.
Ellis et al.~\mbox{%DIFAUXCMD
\cite{Ellis1998} }\hskip0pt%DIFAUXCMD
between monocular vision, binocular vision, and a stereoscopic display utilizing a rotating display where participants were asked to judge the depths they saw different virtual objects. 
To do this, they had participants place and verbally estimate positions in which real or virtual objects were away from them.
Overall, this finding found a high amount of work stating that both binocular and stereoscopic displays provided excellent depth perception when compared to monocular displays. 
They also noted that when virtual content was displayed behind a real-world object, it created the same mismatch, about 6cm closer to the viewer/participant than it was displayed. 
Most depth estimations were within 2cm of the actual position~\mbox{%DIFAUXCMD
\cite{Ellis1998}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{McCandless et al.~\mbox{%DIFAUXCMD
\cite{McCanless2000} }\hskip0pt%DIFAUXCMD
followed up this study by adding time by adding both motion and a time delay on }%DIFDELCMD < \glspl{hmd}%%%
\DIFdel{.
People moving their heads causes motion parallax, which allows for a sense of depth between objects. 
McCandless et al.~\mbox{%DIFAUXCMD
\cite{McCanless2000} }\hskip0pt%DIFAUXCMD
control study found that when the virtual object was moved over a meter away, movements caused a noticeable drop in depth perception, which was proven to be understated compared to their main study.
This demonstrated that the worse the experience was, the more users moved around and noted 
 an increase in the large time delay between their head movements and the interaction time.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Rolland et al.~\mbox{%DIFAUXCMD
\cite{Rolland2002} }\hskip0pt%DIFAUXCMD
looked into the perception of different shapes (cubes, octahedrons, and cylinders) when viewed from a }%DIFDELCMD < \gls{hmd}%%%
\DIFdel{.
They did this by presenting them in several different sizes and testing if participants could determine what objects were closer to them using a }%DIFDELCMD < \gls{twofc} %%%
\DIFdel{study design.
This study design only found a little change between different study designs.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Later on, Mather and Smith.~\mbox{%DIFAUXCMD
\cite{Mather2004} }\hskip0pt%DIFAUXCMD
investigated a method to determine if using multiple depth cues could improve depth perception.
The depth cues investigated in this experiment were Contrast, Blur, and Interpolation. 
All possible different combinations of these conditions were used. 
This experiment was done using a computer monitor. Participants saw many different textures displayed on four planes partially overlapping each other, each with a different depth cue that could be used as an aid. 
Participants would click on all the different textures from nearest to furthest to determine where an item should be placed. 
This experiment showed that participants could most easily tell where objects were with all three cues, but they struggled with the other cues, especially interpolation. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{While a lot of work happening at this time was focused on the far-plane, Wither et al.~\mbox{%DIFAUXCMD
\cite{Wither2005} }\hskip0pt%DIFAUXCMD
looked at methods that make virtual objects appear further away rather than just smaller.
This was a common monoscopic }%DIFDELCMD < \gls{hmd} %%%
\DIFdel{of the time, as they lacked basic depth cues. 
Their study utilized flat planes as showdowns in the virtual world, Giving users an on-screen map to view items with and coloring the markers so they appeared in the correct positions.  
They would have users view objects that were 38, 55, and 65 meters away, and users would have to guess their location. 
This was done using a group of objects and using individual objects. 
This study showed that shadows and size were the best indicators while changing the color was the worst depth cue Wither et al.\mbox{%DIFAUXCMD
\cite{Wither2005} }\hskip0pt%DIFAUXCMD
tested. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Armbrüster et al.~\mbox{%DIFAUXCMD
\cite{Armbruster2008} }\hskip0pt%DIFAUXCMD
worked on determining what elements the virtual reality headset displayed that affected the participants' depth perception. 
This included: 
}%DIFDELCMD < \begin{itemize}
\begin{itemize}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\DIFdel{The virtual environment would change between three different environments: one had no graphics shown as part of the world, one showed the world as a meadow, and the final one was a large but enclosed gray room. 
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{participants were asked to guess a total of ten different distances. Four of these were in the near-field space, and six of these were in the action space.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{They would also toggle a ruler on the ground that would showcase the distance from themselves in meters.
}
\end{itemize}%DIFAUXCMD
%DIFDELCMD < \end{itemize}
%DIFDELCMD < %%%
\DIFdel{They also had two tasks, one where participants would either need to be able to see spheres located at all of the different distances, or they could only see a single sphere at a time.
This research did not provide any clear indication of whether any of these conditions were able to be observed, but they did note that participants underestimated the distances of the objects they were viewing and that users had a better sense of depth when objects were closer to them.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Another study that examined the virtual environment's effect on participants was performed by Murgia and Sharkey~\mbox{%DIFAUXCMD
\cite{Murgia2009}}\hskip0pt%DIFAUXCMD
.
This study looked solely into how people perceive depth within the virtual space. 
To do this, they created a life-sized virtual environment using a }%DIFDELCMD < \gls{cave}%%%
\DIFdel{, designed to work with stereoscopic glasses and react to users moving within the virtual environment. 
They tested a range of conditions, including two levels of graphical quality, one where the environment was bland and one where 1-meter objects would appear. 
They introduced real-world reference objects to help the participants understand the correct depth at which their virtual counterparts would be located. 
This study found that the clearer the virtual environment was at displaying depth, the easier the user could determine depth within it. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Depth Perception User Studies on Ocular See Through (OST) Augmented Reality (AR) displays}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \DIFadd{require more simultaneous processes than on traditional displays as they as they require the rendering of multiple displays which need to be redrawn from a slightly different angle each frame, eliminating current hardware acceleration approaches~\mbox{%DIFAUXCMD
\cite{Geng2013}}\hskip0pt%DIFAUXCMD
.
This is made much harder as many modern }\DIFaddend \gls{ost} \gls{ar} \DIFdelbegin \DIFdel{displays allow users to view the real world while graphics are overlaid on the real world, which makes them useful for a range of stress-inducing professions.
This creates a different dynamic for depth perception as the whole environment is the real world. 
This can lead to increased stress levels when performing precise tasks, making depth perception a critical element of any augmented reality system using }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{displays. 
}\DIFdelend \DIFaddbegin \DIFadd{headsets utilize mobile hardware even to produce the most simple 3D visualization~\mbox{%DIFAUXCMD
\cite{Zari2023}}\hskip0pt%DIFAUXCMD
, resulting in only a few papers on this topic.
Authors have, however, found ways of getting this technique to function on }\gls{mr} \glspl{hmd} \DIFadd{by utilizing external devices and optimizing rendering through specialized GPU pipeline~\mbox{%DIFAUXCMD
\cite{Mejias2025,Gao2025}}\hskip0pt%DIFAUXCMD
.
This has made possible research that spans gamification, medical assistance, collaboration, and high-fidelity graphics interaction, resulting in some interesting features.
}\DIFaddend 

%DIF >  extend this part Ross note
\DIFaddbegin 

\DIFaddend \begin{figure}
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width= \textwidth]{Chapter2/Images/ImagesFromOtherWorks/Swan2007ExperimentalSetup.png}
%DIFDELCMD <     \caption[The \gls{ost} \gls{ar} display was utilized for Swan et al.'s~\cite{Swan2007} study.]{
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width = \columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Cecottietal.png}
    \caption[The virtual scene created by Cecotti et al.]{\DIFaddendFL The \DIFdelbeginFL %DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdelFL{display was utilized for Swan }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{virtual scene created by Cecotti }\DIFaddendFL et al.\DIFdelbeginFL \DIFdelFL{'s}\DIFdelendFL ~\DIFdelbeginFL \DIFdelFL{\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
study.
    These images show how }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\mbox{%DIFAUXCMD
\cite{Cecotti2021} }\hskip0pt%DIFAUXCMD
(a) }\DIFaddendFL the \DIFdelbeginFL %DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd} %%%
\DIFdelFL{was mounted for people to view}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{3D data, (b) the sagittal (y-z), (c) coronal (x-y), (d): transaxial (x-z) planes}\DIFaddendFL . Used with permission from IEEE \DIFdelbeginFL \DIFdelFL{\textcopyright 2007.
    }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\textcopyright{} 2021.}\DIFaddendFL }
    \DIFdelbeginFL %DIFDELCMD < \label{fig:Swan2007ExperimentalSetup}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:Cecottietal}
\DIFaddendFL \end{figure}

\DIFdelbegin \DIFdel{Swan }\DIFdelend \DIFaddbegin \DIFadd{Gamification is often used to increase student retention in education. 
One system created by Cecotti }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
performed the first depth perception study using an }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{devices.
Participants were asked to either verbally report or blindly walk to a point displayed using the headset while viewing real-world objects with and without a real-world headset. 
The world was rendered virtually, or only the marker was rendered. 
The headset was mounted on an apparatus shown in }%DIFDELCMD < \autoref{fig:Swan2007ExperimentalSetup} %%%
\DIFdel{that was not able to move.
Forcing users to move to the position of the stimuli that they saw.
The users were asked to turn as the examiners removed the real-world object}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Cecotti2021} }\hskip0pt%DIFAUXCMD
allowed students to learn how to read medical images using 3D spaces as well as 2D ones. 
The scene shown in }\autoref{fig:Cecottietal} \DIFadd{shows the information presented to the students and how it allows them to map individual planes to a 3D object, allowing them to learn how various organs are seen using 2D planes.
This is currently integrated into their institution's anatomy curriculum}\DIFaddend .
\DIFdelbegin \DIFdel{This study found that the }%DIFDELCMD < \gls{ar} %%%
\DIFdel{conditions were more accurate than the }%DIFDELCMD < \gls{vr} %%%
\DIFdel{conditions.
They also found that blind walking is performed more accurately than verbally guessing where an object is positioned.
}\DIFdelend 

\DIFdelbegin \DIFdel{Later on from this, Jones }\DIFdelend \DIFaddbegin \DIFadd{Pelanis }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Jones2011, Jones2008} }\hskip0pt%DIFAUXCMD
ran }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Pelanis2020} }\hskip0pt%DIFAUXCMD
asked }\DIFaddend a series of \DIFdelbegin \DIFdel{studies that looked into several different methods of depth perception was ran on these studies. 
Their first study ran a similar process to Swan et al.'s~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
compare }%DIFDELCMD < \gls{vst} \gls{ar} %%%
\DIFdel{to }\DIFdelend \DIFaddbegin \DIFadd{surgeons to compare how using a 3D rendering of }\gls{ct} \DIFadd{data compared to a more traditional 2D representation of the same }\gls{ct} \DIFadd{data when using an }\DIFaddend \gls{ost} \gls{ar} \DIFdelbegin \DIFdel{. 
However, this time, the }\DIFdelend \gls{hmd}\DIFdelbegin \DIFdel{was mounted to their head instead of being fixed, allowing for depth cues involving stereopsis~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011}}\hskip0pt%DIFAUXCMD
.
These results showed that the }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{conditions did better than the prior ones. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{One issue that was possible with the previous three experiments was that users may have been estimating the location based on aspects in the environment~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011}}\hskip0pt%DIFAUXCMD
.
To get around this, Jones et al.~\mbox{%DIFAUXCMD
\cite{Jones2011} }\hskip0pt%DIFAUXCMD
ran another study where they did not just occlude the area behind the }%DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd}%%%
\DIFdel{, but they occluded the participant's complete vision.
This indicated that users were using the real-world environment to assist their depth perception. 
These results were further confirmed when users only had their vision partially occluded.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The prior work by Swan et al. 
~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
and Jones et al.~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011} }\hskip0pt%DIFAUXCMD
explained that when in the action space }%DIFDELCMD < \gls{ar} %%%
\DIFdel{depth perception is more limited when compared to }%DIFDELCMD < \gls{vr}%%%
\DIFdel{.
However, this still leaves two main questions that had to be answered before }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{devices could see widespread use: the first being how accurate can }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{devices and how are tasks performed in the near field affect these techniques.
The latter was investigated by Singh et al.~\mbox{%DIFAUXCMD
\cite{Singh2011} }\hskip0pt%DIFAUXCMD
who created a study design based on blindly reaching where they found that off-the-shelf hardware could present depth accurate to 2cm to 4cm to the point~\mbox{%DIFAUXCMD
\cite{Singh2011, Singh2012a}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < \begin{figure}
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/swan2015.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[Swan et al.'s~\cite{Swan2015} study design.]{%
{%DIFAUXCMD
\DIFdelFL{Swan et al.'s~\mbox{%DIFAUXCMD
\cite{Swan2015} }\hskip0pt%DIFAUXCMD
study design, detailing the apparatus used for the two different types of experiments they ran utilizing reaching tasks. Used with permission from IEEE \textcopyright{} 2015.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:Swan2015}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Previous studies by Jones et al.~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011} }\hskip0pt%DIFAUXCMD
Swan et al.~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
utilized a Sony Glassatron. This was changed for the nVisor ST60 }%DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd} %%%
\DIFdel{seen in }%DIFDELCMD < \autoref{fig:Swan2015}%%%
\DIFdel{~\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
This research consisted of three separate studies: one of these was done having participants move another virtual object to the exact location as a physical one by moving a slider underneath the desk (shown in }%DIFDELCMD < \autoref{fig:Swan2015} %%%
\DIFdel{(a)); the other two had participants placing a virtual object in the same position as the one they were looking at (shown in }%DIFDELCMD < \autoref{fig:Swan2015} %%%
\DIFdel{(b)).
One of these placement studies was used to determine whether corrective feedback from graphical elements could aid participants.
All of these studies showed that there was approximately a 1cm difference between placing virtual objects using an }%DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd} %%%
\DIFdel{than in real life when in this task environment.
These experiments with the newer headsets showed these results were accurate between 1cm }\DIFdelend \DIFaddbegin \DIFadd{.
The }\gls{ct} \DIFadd{data consisted of images of 150 different legions. 
The medical practitioners correctly accessed 89 of the 150 legions using both formats, }\DIFaddend and \DIFdelbegin \DIFdel{2cm.
Showing there was that displays utilized on different headsets would impact the accuracy possible for different }\DIFdelend \DIFaddbegin \DIFadd{there was no real difference in the accuracy of the visualizations.
There was a difference between the time required, where MRI scans required an average time of 23.5 sections, while the }\DIFaddend \gls{ost} \gls{ar} \DIFdelbegin %DIFDELCMD < \glspl{hmd}%%%
\DIFdel{.
Between underestimation and overestimation, it seems limited as the average placement of each virtual object was less than 3cm, as people guessed higher or lower at an even rate. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Many years after Swan et al.'s~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
initial research, Medeiros et al.~\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
ran a slightly different experiment that also tried to learn what the impact was between depth perception between }%DIFDELCMD < \gls{vst} %%%
\DIFdel{and an }%DIFDELCMD < \gls{ost} \gls{ar} \glspl{hmd}%%%
\DIFdel{.
The infrastructure to create }%DIFDELCMD < \gls{vst} %%%
\DIFdel{and }%DIFDELCMD < \gls{ost} \gls{ar} \glspl{hmd} %%%
\DIFdel{was, at this point, quite different, and they began to have different pros and cons. 
The Occlus }%DIFDELCMD < \gls{vr} %%%
\DIFdelend \gls{hmd} \DIFdelbegin \DIFdel{had recently been publicly released, which used a Fresnel lens~\mbox{%DIFAUXCMD
\cite{Cheng2022}}\hskip0pt%DIFAUXCMD
. 
Meanwhile, the Meta lens utilized a distorted reflection of a screen's projection.
Medeiros et al.'s~\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
study design asked users to draw a line between two spheres in the virtual space.
The findings from Medeiros }\DIFdelend \DIFaddbegin \DIFadd{only took practitioners an average of 6 sections to diagnose.
Pelanis }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
study showed that the }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{headset was less accurate in drawing lines between two points.
Participant comments indicated this was due to the difference in view windows between the Occlus and the Meta }%DIFDELCMD < \glspl{hmd}%%%
\DIFdel{.
Participants using the Meta headset struggled to keep both objects in view at once, while when they were wearing the occlus, participants seemed to have no issues. 
This study highlighted the importance of a field of view when comparing activities between two headsets, as a smaller field of view could impact the ease with which a given participant can react to certain stimuli. 
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Pelanis2020} }\hskip0pt%DIFAUXCMD
highlights that there is still an advantage to using 3D graphics even when a person is highly trained. 
}\DIFaddend 

\DIFdelbegin %DIFDELCMD < \begin{figure}
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Singh2018.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[\gls{ost} \gls{ar} \gls{hmd} which was utilized in Singh et al.~\cite{Singh2018} experiments.]{%
{%DIFAUXCMD
%DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd} %%%
\DIFdelFL{which was utilized in Singh et al.~\mbox{%DIFAUXCMD
\cite{Singh2018} }\hskip0pt%DIFAUXCMD
experiments. 
    (a) Details the utility of each lens and how they distort the user's view.
    (b) Details how these participants see through this lens, allowing for the }%DIFDELCMD < \gls{hmd} %%%
\DIFdelFL{to function.
    (c) The real-life implementation of the custom }%DIFDELCMD < \gls{hmd}%%%
\DIFdelFL{. Used with permission from IEEE \textcopyright{} 2018.
    %DIF < \textbf{Note: May need permission for this, Technically it should be creative comments but it is not clear.}
    }} 
    %DIFAUXCMD
%DIFDELCMD < \label{fig:Singh2018}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The work that was started by Swan }\DIFdelend \DIFaddbegin \DIFadd{Cinematic Rendering is generally a costly method of viewing Volumetric data. It cannot be run in real time and requires a very powerful computer. 
Stadlinger }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
was later finalized by Singh et al.~\mbox{%DIFAUXCMD
\cite{Singh2018} }\hskip0pt%DIFAUXCMD
who developed the }%DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd} %%%
\DIFdel{detailed in }%DIFDELCMD < \autoref{fig:Singh2018} %%%
\DIFdel{which was able to react to five different focal cues allowing for highly accurate visualizations. 
Due to its large weight, this headset was placed on a desk whose height could be adjusted to match the participants'. 
The near field depth perception of this headset was tested by a series of matching the graphical cue-like tasks similar to what was done prior in Swan et al.'s~\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
The first study tested the same conditions as Swan et al.'s previous study~\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
This type of }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{device was shown to be able to improve depth perception to about 1 cm accuracy in the near field, regardless of the user 's age.
The second study evaluated how people of different ages performed the task, which showed that people's depth perception of }%DIFDELCMD < \gls{ar} %%%
\DIFdel{seemed to function fine regardless of age.
The final study changed the brightness of the graphical content, which showed no difference~\mbox{%DIFAUXCMD
\cite{Singh2018}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Hua and Swan~\mbox{%DIFAUXCMD
\cite{Hua2014} }\hskip0pt%DIFAUXCMD
utilized the same apparatus and ran a study that asked users to tell the depth of an occluded object.
This was done by allowing for the option of a temporary occlusion barrier. 
They found this headset was able to reduce the errors from a previous 4cm~\mbox{%DIFAUXCMD
\cite{Edwards2004} }\hskip0pt%DIFAUXCMD
on }%DIFDELCMD < \gls{vst} \gls{ar} \glspl{hmd}%%%
\DIFdel{.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Whitlock et al.~\mbox{%DIFAUXCMD
\cite{Whitlock2018} }\hskip0pt%DIFAUXCMD
later looked at the difference between using controller gestures and voice commands to aid the placement of objects using a HoloLens. 
Participants were asked to complete a series of precision tasks, including selecting, rotating, and translating virtual objects placed at different distances from each type of control interface.
Whitlock }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{stadlinger2021cinematic, Stadlinger2019} }\hskip0pt%DIFAUXCMD
utilized a workaround for this method where they took many images of the volume and rendered them on 2D planes for the user to see.
This allowed this technique to be viewed using a lowered-powered machine. 
This made the processing so efficient that Steffen }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Whitlock2018} }\hskip0pt%DIFAUXCMD
found that participants perceived embodied freehand gestures as the most efficient and usable interaction compared to device-mediated interactions. 
Voice interactions demonstrated robustness across distances, while embodied gestures and handheld remotes became slower and less accurate when the distance was increased. 
These findings emphasize the importance of selecting appropriate interaction modalities based on distance when designing the studies in this thesis}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Steffen2022} }\hskip0pt%DIFAUXCMD
could display cinematic rendering on the }\gls{vr} \DIFadd{headset}\DIFaddend .

\DIFdelbegin %DIFDELCMD < \begin{figure}[tb]
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/AlKalbani2019.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[Several images from Al-Kalbani et al.'s~\cite{Al-Kalbani2019} study set up. ]{%
{%DIFAUXCMD
\DIFdelFL{Several images from Al-Kalbani et al.'s~\mbox{%DIFAUXCMD
\cite{Al-Kalbani2019} }\hskip0pt%DIFAUXCMD
study set up. 
    (a) displays grasp measurements required for Grasp Aperture (GAp) and grasp middle point (gmp)
    (b - m) images of participants completing the task, (b - g) without shadows, (h-m) with shadows.
    (n) the interaction space in reference to the Kinect. Used with permission from IEEE \textcopyright{} 2019}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:AlKalbani2019}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Many previous papers showcased how the depth perception of computer graphics can be realized when they are placed in the real world. However, very few of these focused on what happens if these graphical objects react to the real world, such as having them leave a shadow over the ground.
Using }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{devices is difficult as they cannot display darker shades of colors.
Al-Kalbani }\DIFdelend \DIFaddbegin \DIFadd{Steffen }\DIFaddend et al.\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{Al-Kalbani2019} }\hskip0pt%DIFAUXCMD
ran a study that looked at the accuracy of participants trying to grab hold of virtual 3D shapes using }\DIFdelend \DIFaddbegin \DIFadd{'s~\mbox{%DIFAUXCMD
\cite{Steffen2022} }\hskip0pt%DIFAUXCMD
research had ten trained medical investigators look at ten patient cases using both a desktop display and }\gls{vr} \gls{hmd}\DIFadd{. 
When these users were asked to assess the volume details, they were more accurate when using }\DIFaddend a \DIFdelbegin \DIFdel{HoloLens2 as shown in }%DIFDELCMD < \autoref{fig:AlKalbani2019} %%%
\DIFdel{(b - m). 
To do this the placement of their hands was situated around the virtual cube as shown in }%DIFDELCMD < \autoref{fig:AlKalbani2019} %%%
\DIFdel{(a) using a remote sensor (a Kinect 2~}\footnote{%DIFDELCMD < \url{https://en.wikipedia.org/wiki/Kinect}%%%
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{) placed just over 2 meters away as shown in }%DIFDELCMD < \autoref{fig:AlKalbani2019} %%%
\DIFdel{(n). 
The results from this study showed that participants tended to overestimate the area they had to grasp.
Drop shadows were appreciated when they were visible to the participants and increased the amount of time required to grasp the object, but they did not improve the participant's accuracy.
Users also underestimated the depth required by 2cm. 
}\DIFdelend \DIFaddbegin \DIFadd{desktop. 
They also noted that desktop displays offered better depth perception when viewing cinematic rendering.
}\DIFaddend 

\DIFdelbegin \DIFdel{It is common in }%DIFDELCMD < \gls{ar} %%%
\DIFdel{experiments to display objects as if they are hovering in reality; however, since this itself is not a natural appearance of the virtual object, it may be misleading the end user. 
This is even more relevant when using }%DIFDELCMD < \gls{ost} \gls{ar} \glspl{hmd} %%%
\DIFdel{as they show virtual objects superimposed over the participants' vision. 
Rosales et al.~\mbox{%DIFAUXCMD
\cite{Rosales2019} }\hskip0pt%DIFAUXCMD
set out to test this hypothesis by testing if a participant's depth perception was more accurate by having participants blindly walk to a position where they believed they saw a virtual cube that was either hovering or on the ground.
Whether an object was on or off the ground, participants tended to underestimate the depth they needed to move towards, but targets off the ground they judged to be closer to them. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Adams }\DIFdelend \DIFaddbegin \DIFadd{Heinrich }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Adams2022} }\hskip0pt%DIFAUXCMD
also ran a study looking at how depth perception could be influenced by having the graphics respond to the real world by investigating depth perception with virtual objects placed on and off the ground. 
This study design utilized a 3 x 2 x 2 x 2 design looking at three distances in the action space (3m, 4.5m, and 6m), the presence of shadows, causing the virtual object to hover off the ground, and if it was being viewed by a }%DIFDELCMD < \gls{vst} %%%
\DIFdel{or an }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{display.
The results from Adams et al.~\mbox{%DIFAUXCMD
\cite{Adams2022} }\hskip0pt%DIFAUXCMD
showed an underestimation of }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Heinrich2021} }\hskip0pt%DIFAUXCMD
performed a series of tests looking at }\DIFaddend the \DIFdelbegin \DIFdel{values of over 17\%. 
There was a small increase in accuracy regarding the presence of shadows, showing a significant improvement of 2\%.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Similar to Swan et al.~\mbox{%DIFAUXCMD
\cite{Swan2007}}\hskip0pt%DIFAUXCMD
, Medious et al.\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
and Adams et al.~\mbox{%DIFAUXCMD
\cite{Adams2022}}\hskip0pt%DIFAUXCMD
, Ping et al.~\mbox{%DIFAUXCMD
\cite{Ping2020} }\hskip0pt%DIFAUXCMD
aimed to determine what the limits of depth perception were possible to observe between }\DIFdelend \DIFaddbegin \DIFadd{difference between depth perception between }\DIFaddend \gls{ar} and \gls{vr} \DIFdelbegin \DIFdel{and if do different depth cues have a different impact than others.
This study used three different depth cues (points, lines, and boxes) as conditions and four different illumination ranges. 
Participants were asked to use these depth cues to align differently sized objects while they were between 1.75m and 7.35 m away from the shapes they were being asked to align.
Participants would control one of the shapes using a keyboard input, allowing them to move the virtual objects forward or backward.
Ping et al.'s~\mbox{%DIFAUXCMD
\cite{Ping2020} }\hskip0pt%DIFAUXCMD
results showed that there was little difference in the depth cues that improved depth perception between the }%DIFDELCMD < \gls{ost} %%%
\DIFdel{and }%DIFDELCMD < \gls{vst} %%%
\DIFdelend \DIFaddbegin \DIFadd{when looking at volumetric vascular structures rendered as an iso-surface.
They then utilized three different depth cues: Phong Shading, pseudo-chroma depth shading, and surrounding the volume with a Fog.
Users would be presented with a set of spheres located at the ends of all of the blood vessels and asked if they could sort them from nearest to farthest.
Using Pseudo-Chromadepth shading or Fog techniques clearly improved desktops, as these showed the z-axis. 
However, VR users rarely make any errors. 
This was because they could freely use an array of other depth perception cues, such as Convergence, Binocular Disparities, Motion Perspective, and even, in the case of Fog and Aerial Perspective. In contrast, the desktop was limited to Occlusion, Real Size, and Density, which any display could also use.
However, this does show that just using a }\DIFaddend \gls{ar} \DIFdelbegin %DIFDELCMD < \glspl{hmd}%%%
\DIFdel{, but they did note that illumination of the objects made a larger effect. 
This was likely because if the objects were the same shape regardless of their size, they would all look alike. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{This finding caused Ping et al.~\mbox{%DIFAUXCMD
\cite{Ping2020a} }\hskip0pt%DIFAUXCMD
to focus on another study focused on how different shaders could influence depth perception .
This study had a similar physical setup as their previous study but only used a holoLens~\mbox{%DIFAUXCMD
\cite{Ping2020}}\hskip0pt%DIFAUXCMD
.
The conditions were split into a 3 x 3 study setup using three different shaders: Half-Lambert, Blinn-Phong, and Cook Torrance, each colored as blue, yellow, or green, and displayed at either 25cm, 30cm, 35cm, or 40cm.
This study showed that participants seemed to be more accurate with the brighter-colored virtual objects (yellow or green). Participants struggled to place larger objects, and both the shaders that had more pronounced specular highlights were found to aid depth perception the most. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Research into the Depth Perception of Color}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{This dissertation utilizes several colorful objects in its evaluations and creates two studies that directly utilize someone's ability to discern color from a specified area.
This particular section highlights the papers of note that researched the impact of colors on depth perception.
By understanding this research, the changes in depth perception that various colors can provide were mitigated across this dissertation. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Ping et al.~\mbox{%DIFAUXCMD
\cite{Ping2020a} }\hskip0pt%DIFAUXCMD
highlight the impact that colors can have on depth perception.
When talking about medical visualization in general, it is very common to have several transparent layers in a single visualization.
This section of the paper discusses the research that has been factored into considerations regarding how color influences depth perception }\DIFdelend \DIFaddbegin \DIFadd{device will improve the depth perception of a task using volume data, but it also indicated that the need limits required for testing medical data needed to become much more precise than before to enable the type of accuracy required by the medical field}\DIFaddend .

\DIFdelbegin %DIFDELCMD < \begin{figure}[bt]
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ARToXrayVision.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[Three images of the Stanford bunny sitting behind a wall, each using a different \gls{X-ray Vision} effect.]{%
{%DIFAUXCMD
\DIFdelFL{Three images of the Stanford bunny sitting behind a wall, each using a different }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdelFL{effect. To the left, a simple depth cue by placing the bunny behind a column. In the center, a virtual grid is placed over the physical wall to explain to the viewer that the bunny is behind the wall. On the right is highlighting the edge of the bricks with }%DIFDELCMD < \gls{ar} %%%
\DIFdelFL{using edge detection to indicate that the bunny is behind the wall.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:ARToXrayVision}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Aio and Li~\mbox{%DIFAUXCMD
\cite{Aoi2020} }\hskip0pt%DIFAUXCMD
wanted to test how the luminance and Contrast affected the depth perception of transparent plans when viewed on a computer monitor.
The goal was to determine what methods could be utilized to make it more obvious which object was behind the other. 
They had two conditions to achieve this. They utilized luminance contact, testing the difference between changing the gradient between light and dark to dark to light.
They utilized four planes of different sizes to distinguish depth perception. 
This study was conducted using an autoscopic display, which allowed for the presence and absence of motion parallax and no binocular parallax, giving the participants extra depth cues.  
Throughout this study, Aoi and Li~\mbox{%DIFAUXCMD
\cite{Aoi2020} }\hskip0pt%DIFAUXCMD
noticed participants underestimated depth perception but could be improved by using either (or both) binocular parallax. 
They also noted that occluding darker or lighter colors did not make much difference.
Aio and Li~\mbox{%DIFAUXCMD
\cite{Aoi2020} }\hskip0pt%DIFAUXCMD
next utilized the information from their prior study to test this data on medical data and less difference between the choices of different colors. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \section{Augmented Reality Enabled X-ray Vision} \label{sec:BackXRayVision}
One important aspect of \gls{ar} enabled \gls{xray} vision relates to the field of research looking at aligning the depth mismatch when virtual objects are placed inside of physical objects in \gls{ar}. 
As is shown in \autoref{fig:ARToXrayVision} since \gls{ar} superimposes the virtual objects over the real world it is difficult to make them look like they are behind an occlusive surface. 
It is to this end that many \gls{X-ray Vision} effects have been developed for different devices to solve these issues. 

The goal of this research is to determine methods where \gls{X-ray Vision} effects can work with \gls{dvr} on \gls{ost} \gls{ar} \glspl{hmd}. 
To ensure that this research in this dissertation has a solid basis for the lessons learned by the previous works, a systematic literature review was conducted. \DIFdelbegin \DIFdel{An overview is provided in \mbox{%DIFAUXCMD
\Cref{app:XRayLitReviewMethodology}}\hskip0pt%DIFAUXCMD
. }\DIFdelend %DIF > An overview is provided in \Cref{app:XRayLitReviewMethodology}.
\DIFaddbegin \DIFadd{This literature review was inspired by the PRIMA 2020 SRC (Page et al. 2021) protocols checklist. It utilized five databases: 
ACM Digital Library~}\footnote{\url{https://dl.acm.org/}}\DIFadd{, 
IEEE Xplore~}\footnote{\url{https://ieeexplore.ieee.org/Xplore/home.jsp}}\DIFadd{, 
Pub Med~}\footnote{\url{https://pubmed.ncbi.nlm.nih.gov/}}\DIFadd{, 
Web of Science~}\footnote{\url{https://www.webofscience.com/wos/}}\DIFadd{, 
and Scopus~}\footnote{\url{https://www.scopus.com/}} 
\DIFadd{using the search terms:
}\textit{\DIFadd{“("X-ray vision" OR "X-ray visualization" OR ("occlusion" AND "perception") OR (visualization AND x-ray) OR "see-through vision" OR "ghosted views" OR "augmented reality x-ray") AND ("AR" OR "Augmented Reality" OR "Mixed Reality")”}}
\DIFadd{These terms were filtered by the paper’s Abstract, Title, and Keywords. A backwards and forwards snowball was conducted where each papers citations and references where each checked for overlaps on to the existing papers selected for the study which then produced either caused the final search to be adjusted (to the one mentioned earlier). Each time the search criteria was changed the search would begin again. If the papers were not located in the databases listed (grey literature) would be added if they passed all criteria and where papers cited by more than five papers were also considered for entry into this database (2 papers found and accepted). For a paper to be accepted, it needed to meet the eligibility criteria outlined below as determined by the single reviewer. The process and results of this review as depicted in }\autoref{fig:X-LItReviewMethodology}
\DIFaddend 

\DIFaddbegin \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Chapter2/Images/LItReviewMethodology.pdf}
    \caption{\DIFaddFL{The protocol utilized for this literature review with the matching results. }}
    \label{fig:X-LItReviewMethodology}
\end{figure}

\subsubsection{\DIFadd{Eligibility Criteria}}
\DIFadd{All papers in this review needed to meet the following criteria:
}\begin{itemize}
    \item \DIFadd{X-ray vision (XRV) needed to be explored in the research, placing virtual objects behind real-world objects.
    }\item \DIFadd{Mixed Reality Technologies other than VR must have been used.
    }\item \DIFadd{Papers that simply superimposed data over the real world and papers that did not focus on making the content appear on the other side of or within the real-world object were not included.
    }\item \DIFadd{Studies included in multiple papers were only recorded once all papers that highlighted the same research had been included in this review, as the content between both articles provides different amounts of information.
    }\item \DIFadd{If the focus on XRV was light compared to other study elements and two or more reviewers agreed on it, then the work would not be included in favor of more focused research. 
}\end{itemize}

\DIFadd{These criteria ensured that the X-ray visualization was more than superimposing a medical image onto a human subject. Any paper that did not utilize }\gls{ar} \DIFadd{was removed. This meant that some documents that utilized Virtual Reality technologies to test }\gls{ar} \DIFadd{in a completely virtual environment were not included.
}


\DIFaddend \subsection{Observations from Systematic Literature Review}

\begin{figure}[bt]
    \centering
    \includegraphics[width=\textwidth]{Chapter2/Images/XRayResearchDoneOverTime.png}
    \caption[A bar graph representing the number of papers that were published between 1990 and 2023 focusing on \gls{X-ray Vision}.]{A bar graph representing the number of papers that were published between 1990 and 2023 focusing on \gls{X-ray Vision}, each categorized by the type of research undertaken.}
    \label{fig:XRayResearchDoneOverTime}
\end{figure}

This section presents the complete findings of all the \gls{X-ray Vision} papers that were found as of October 30, 2023. It discusses all the devices on which \gls{X-ray Vision} techniques have been used, along with the techniques for \gls{X-ray Vision}, their functional mechanism, and the research focus. Following this, I looked at how each user study was conducted and the parameters utilized for the various experiments. 

Of all the selected papers that were longer than two pages, 27 Technical papers were found (3 of which also included a case study), and another 29 papers were found that presented a user study. Three demonstrations have been showcased at conferences. Ten short papers have highlighted several methods of research, including a proposal, many of which detail pilot studies for some of the 29 user studies. Every task that required its own set of results was considered a study, and if no results were published for a given study, it was not included in our analysis. 

\DIFdelbegin \DIFdel{Research }\DIFdelend \DIFaddbegin \DIFadd{In recent years, peer-reviewed research }\DIFaddend proposals have become \DIFdelbegin \DIFdel{more frequent than technical papersin recent times. Papers prior }\DIFdelend \DIFaddbegin \DIFadd{increasingly common compared to technical papers. Prior }\DIFaddend to 2010\DIFdelbegin \DIFdel{tended to be largely technical . Over the last decadeor more, we have learned more about how }\DIFdelend \DIFaddbegin \DIFadd{, the literature was predominantly technical in nature. This shift may reflect an evolving trend in the field, where research is expected to address a broader range of topics or where purely technical contributions are less frequently regarded as sufficient on their own. Importantly, research proposals continue to play a valuable role by introducing new ideas and approaches, guiding future investigations, and helping to shape the direction of the field. Over the past decade, our understanding of }\DIFaddend \gls{X-ray Vision} \DIFdelbegin \DIFdel{works by focusing on }\DIFdelend \DIFaddbegin \DIFadd{has advanced through a combination of }\DIFaddend user studies and research proposals \DIFdelbegin \DIFdel{. This is detailed }\DIFdelend \DIFaddbegin \DIFadd{that explore and refine emerging techniques. As shown }\DIFaddend in \autoref{fig:XRayResearchDoneOverTime}, \DIFdelbegin \DIFdel{which illustrates the steady maturation of the field , moving away from novel techniques to understanding their effects and how they can be improved}\DIFdelend \DIFaddbegin \DIFadd{the field has steadily matured, with a growing emphasis on evaluating the impact of these techniques and improving their effectiveness}\DIFaddend .

%DIF >  Peer reviewed research proposals have become more frequent than technical papers in recent times. Papers prior to 2010 tended to be largely technical. This may indicate a new trend in the field where papers need to cover more topics now than previously or the technical reseach in this space may no longer be seen as a signficant contribution. Over the last decade or more, we have learned more about how \gls{X-ray Vision} works by focusing on user studies and research proposals detailing new techniques that might be used. This is detailed in \autoref{fig:XRayResearchDoneOverTime}, which illustrates the steady maturation of the field, moving away from novel techniques to understanding their effects and how they can be improved. 
\DIFaddbegin 

\DIFaddend \subsection{Research Exploring X-ray Vision}

Creating an effective \gls{X-ray Vision} solution for a task requires careful design, construction, and system validation to discover an appropriate balance among the possible solutions to address the challenges related to \gls{ar} and \gls{X-ray Vision} technologies. The literature contains research studies investigating depth perception, spatial awareness, interaction techniques, and usability of \gls{X-ray Vision} systems. \autoref{fig:LitReviewTypesOfResearchVsDevices} shows the prevalence of the different topics in the surveyed literature.

The effectiveness of visualizations and interactions for a given task may depend on the distance of the physical and virtual object(s) from the user. The human eye’s ability to perceive depth and fine detail may vary as a function of distance, and a human’s reach to manipulate physical (and sometimes virtual) objects is constrained by human anatomy. Perception and interaction have been studied in several settings, ranging from within arms’ reach distance to scenarios where distant objects are interrogated and manipulated. Depth perception \gls{ar} and \gls{vr} tend to look at methods of improving the user’s sense of space in an environment. Research into the depth perception of \gls{X-ray Vision} focuses on how to influence the impact on depth perception when looking through an object.

Depth perception and perception, more broadly, have received considerable attention in the research community since accurate depth perception is vital for \gls{X-ray Vision} user experience. 
A key focus of research has explored depth perception in \gls{X-ray Vision} as related to fixing the depth/distance offset caused by trying to place a virtual object on the other side of a real-world object \DIFaddbegin \DIFadd{in reference to the user's view point}\DIFaddend ~\cite{Furmanski2002, Ghasemi2018, Sandor2010}. This makes the work in this field done with depth perception different from other forms of research on depth perception. Some works are looking into methods to enhance the depth perception of visualization~\cite{Martin-Gomez2021, Fischer2023}. The major finding that seems prevalent in this field is that partial occlusion is an effective tool to improve depth perception.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/LitReviewTypesOfResearchVsDevices.png}
    \caption{A plot showing the studies that have been run on \gls{X-ray Vision} by device type.}
    \label{fig:LitReviewTypesOfResearchVsDevices}
\end{figure}

Spatial awareness concerns a user’s understanding of and ability to operate effectively in a physical environment augmented through \gls{X-ray Vision}. Findings in this field generally look at the impacts of using these visualizations on aspects like the general placement of an object or attempt to navigate around a foreign environment~\cite{Heinrich2021, Li2016}. \gls{X-ray Vision} can orient people using exterior landmarks across large buildings~\cite{Dey2012}.

Effective mechanisms to interact with and control an \gls{X-ray Vision} visualization are essential to the effectiveness and usability of an \gls{X-ray Vision} system. Making usability the second largest topic of research in \gls{X-ray Vision}. Creating interaction mechanisms that cater to the combined characteristics of physical and virtual elements in \gls{X-ray Vision} can be challenging. Interaction mechanisms, including gestures, button and menu commands, and eye tracking, have been proposed by Wang et al.~\cite{Wang2022, Wang2023} and \DIFaddbegin \DIFadd{Liao et al.}\DIFaddend ~\cite{Liao2023}. 


\subsection{X-ray Vision Techniques}
% Needs to be changed
%Various approaches have been employed to create \gls{X-ray Vision} techniques. 
Partial occlusion is a very depth perception cue, while other techniques rely on other depth cues to allow the user to look through real-world objects such as stereopsis.
There are many variations of these effects; very similar effects have been categorized together in this section and will be discussed as a collection of work.

There are two main categories for \gls{X-ray Vision} effects: ones that typically utilize visualization to present occlusion techniques and others that utilize other cues.
There has been much more research within the visualization category, so this has been split into three different sections: Hole in the World-Based \glspl{X-ray Visualization}, Real-World Overlays, and Computer Vision Techniques.
Splitting the visualizations into these groups allows for a more explicit discussion of how they function in their current form.

\begin{figure}[bt]
    \centering
    \includegraphics[width = \columnwidth]{Chapter2/Images/MiscXRayVision.png}
    \caption[Examples of \gls{X-ray Vision} techniques that don't function using traditional techniques but rather label or indicate where objects should be located.]{
    Examples of \gls{X-ray Vision} techniques that don't function using traditional techniques but rather label or indicate where objects should be located.
    a \& b ) Cote and Mercier's~\cite{Cote2018} underground schematics visualized on the ground;
    c ) Eren et al.'s~\cite{Eren2013} labels designed to tell the user if a objects is above or below the ground;
    d ) Muthalif et al.'s~\cite{Muthalif2022} showcases the labels indicating how far below the ground an object is.
    Permission was granted for images a \textcopyright{} 2018, b \textcopyright{} 2018, and c \textcopyright{} 2022 from IEEE. Figure (d) is licensed under a Creative Commons Attribution licence.
    }
    \label{fig:MiscXRayVision}
\end{figure}

Graphs in this section will note an "other" \gls{X-ray Vision} category. 
These effects are used as baselines for various experiments but do provide an understanding that an object is supposed to be rendered underground. 
These effects include things like presenting a written definition of the location of an object displayed like a sign~\cite{Eren2013}.
Other options are a line between the user and the object~\cite{Gruenefeld2020} or a line between the ground and the object~\cite{Muthalif2022}.
Options that were not included but can give a similar form of information could be considered overlaying the ground with the schematics~\cite{Cote2018}. 
These solutions were not included as even though they gave a clear indication of where the user was, they were all complicated for the user, adding steps beyond looking at a 3D object and determining its location. These solutions make great base conditions as they are flawed and complicated, but they are robust. 

\subsubsection{\gls{X-ray Vision} Effects}
\gls{X-ray Vision} can also be achieved by utilizing other depth cues, such as accommodation convergence, relative scale, and density. 

\paragraph{Auxiliary Augmentation} utilizes the relationship between two objects, employing the depth cues of relative size and density and the effect of motion. 
Using another occlusive \gls{X-ray Vision} technique on one object provides enough information to inform the user of the position of the other object~\cite{Kyto2013}.
Auxiliary Augmentation effect should be possible on a 2D display as it uses relive scale and density as its primary depth cues. Still, it is also made more potent on a stereoscopic display like a \gls{hmd}. 
%By using a stereoscopic display, it should also be possible to utilize depth cues like binocular disparities. 

\paragraph{\DIFdelbegin \DIFdel{Vengeance}\DIFdelend \DIFaddbegin \DIFadd{Vergence}\DIFaddend } 
\gls{X-ray Vision} is designed to have people change and move their focus away from the foreground to focus on the background image. 
This method simulates binocular distortion. By tracking a user's eyes, it is possible to tell when they are trying to look past an object. 
This can advise what should and should not be kept in view whilst creating and placing objects approximately where the users expect them to be. 
By taking advantage of that, it is possible to blur virtual objects when a user is not looking at them to present a viewing experience similar to real life using Accommodation and Convergence\DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Kitajima2015, Dunn2018}}\hskip0pt%DIFAUXCMD
}\DIFaddend .

This effect has not seen widespread use \DIFdelbegin \DIFdel{. 
}\DIFdelend \DIFaddbegin \DIFadd{only being utilized by Kitajima et al.~\mbox{%DIFAUXCMD
\cite{Kitajima2015} }\hskip0pt%DIFAUXCMD
who manually created a depth based AR system where the plans were moved so a user could only focus on one to determine if they could regain tell if what was in front or behind.
This study did not find any significant difference in the depth perception of the objects, but they did note that the users found it easier to focus on one object at a time. 
}\DIFaddend One possible reason is that Accommodation and Convergence are difficult to track, and off-the-shelf \gls{ar} solutions do not provide this capability. 
%This has led to a relatively low amount of research that has taken use of it. 
It makes it a difficult version of \gls{X-ray Vision} to use fully~\cite{Kitajima2015}. 
%This effect seems more effective when used in conjunction with other \gls{X-ray Visualization}.

\subsubsection{Transparency Based X-ray Visualizations}
One form of \gls{X-ray Vision} used is the ability to adjust the transparency of a part of an image (as seen in \autoref{fig:ComputerVisionEffectsAndTransperency}). 
By configuring the transparency according to the current view, a user can be given the impression that the object is either in the foreground or that the transparency is low enough to feel as if they are looking inside the object.
This form of \gls{X-ray Vision} is called Alpha Blending. 

Three papers~\cite{Kameda2004, Ohta2010, Tsuda2005} have explored Alpha Blending and its possible applications when adapting it to security cameras, allowing for one line of sight to combine the information of several security cameras at once. Rather than having each security camera link to a single screen, their \gls{X-ray Vision} method makes the foreground object more transparent, presenting the content behind them. Early observations of this technique presented better information and insight when looking through buildings. Tsuda et al.~\cite{Tsuda2005} found that this technique works best when paired with another visualization technique. 
The inverse of this has also been used as a visualization technique by making the virtual object less visible when compared to the real world~\cite{Aaskov2019}.

Making the virtual world more transparent can help the user focus on the real world using the \gls{ar} system as a guide. 
This method is commonly used in medicine where users want to use \gls{X-ray Visualization} as a guide while still being able to concentrate on their work since they are not \DIFdelbegin \DIFdel{district}\DIFdelend \DIFaddbegin \DIFadd{distinct}\DIFaddend ~\cite{DePaolis2019, Erat2013, Pauly2012}.
However, when used from a stereoscopic viewpoint, these transparent effects don't seem to address the depth perception mismatch as they don't provide a clear depth cue to the user. 

\subsubsection{``Hole in the World'' X-ray Visualizations} \label{sec:HoleInTheWorld}
Hole-based \glspl{X-ray Visualization} focus on creating a sense of partial occlusion by allowing a user to look through an object. \autoref{fig:VirtualHoleAndVirtualWindowExamples} depicts the three ways this can has been achieved. 
These methods may use partial occlusion by simply having a user look through a 2D window (\autoref{fig:VirtualHoleAndVirtualWindowExamples} virtual window), while others use tunnelling~\cite{Avery2009} or cutaways~\cite{DePaolis2019, Feiner1992}. 
The most popular of these techniques is creating an open box for people to look into, allowing a sense of how far away the virtual object is placed in a real-life object. 
This provides the impression that the virtual object is inside a real-life object. 
This can be seen in \autoref{fig:VirtualHoleAndVirtualWindowExamples} where the armadillos seem to be sitting inside boxes inside the wall rather than superimposed in front of the wall. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/VirtualHoleAndVirtualWindowExamples.png}
    \caption[Armadillos sitting behind a corkboard in \gls{ar} with the same orientation using hole-in-the-worlds effects to function.]{Armadillos sitting behind a corkboard in \gls{ar} with the same orientation using hole-in-the-worlds effects to function. On the left are virtual holes, and on the right are virtual windows. }
    \label{fig:VirtualHoleAndVirtualWindowExamples}
\end{figure}

Although hole-in-the-world visualizations work quite effectively utilizing any \gls{ar} display~\cite{Bajura1992, DePaolis2019, Feiner1992, Avery2009, Muthalif2022, Habert2015}, they have a few limitations. Firstly, the visualization needs to be in a static position. 
It cannot rotate or change size relative to the user’s motions, so the hole should not react to the user's position. 
The user could choose to make the hole larger or smaller without breaking the effect. There is a limited field of view into the hole. 
Although the limited view creates the \gls{X-ray Vision} effect, it restricts the user's view of the data. 
One advantage of this \gls{X-ray Vision} method is that it can be viewed using any display.

\paragraph{Virtual Box/Hole:}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\columnwidth]{Chapter2/Images/BajuraetalX-ray.png}
    \caption[A image of Bajura et al.'s~\cite{Bajura1992} \gls{X-ray Vision} technique.]{A image of Bajura et al.'s~\cite{Bajura1992} \gls{X-ray Vision} technique. Used with permission from ACM \textcopyright{} 1992. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee, provided that copies are not made or distributed for profit or commercial advantage}
    \label{fig:BajuraetalX-ray}
\end{figure}

The Virtual hole \gls{X-ray Visualization} was the first type of \gls{X-ray Vision} to be implemented and was designed by Bajura et al.~\cite{Bajura1992} (seen in \autoref{fig:BajuraetalX-ray}). 
Who aimed to visualize a baby fetus inside of its mother's womb.
They accomplished this by presenting a box for the baby to be seen in, which would allow for some level of occlusion.
It is still one of the most used \glspl{X-ray Visualization} and is still being researched in modern studies due to their effectiveness and versatility~\cite{Blum2012, DePaolis2019, Phillips2021}. 
Virtual holes have been found to provide a relationship between virtual objects and the real world. 
%Kytö et al.~\cite{Kyto2013} found that users could better determine distance if they had a partially occluded object in the scene for users to use as a reference.

Research has also observed that introducing some particle occlusion alone may aid depth perception~\cite{Kyto2013}.
It has been observed by Kytö et al.~\cite{Kyto2013} that users could better determine distance if they had a partially occluded object in the scene for users to use as a reference or even using other virtual objects as a reference.
This was demonstrated in a study that partially occluded the edges of other objects and found that as long as some of the \gls{X-ray Vision} effects were covering part of the virtual scene, then it was possible to make smaller objects behind it appear to be a given depth behind as there was another object sitting in the front\cite{Kyto2013}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/AvvedutoEtAl.png}
    \DIFdelbeginFL %DIFDELCMD < \caption[This image contains the \gls{X-ray Vision} study environment for Avveduto et al.~\cite{Avveduto2017}.]{%%%
\DIFdelendFL \DIFaddbeginFL \caption[This image contains the \gls{X-ray Vision} study environment from Avveduto et al.~\cite{Avveduto2017}.]{\DIFaddendFL This image contains the \gls{X-ray Vision} study environment \DIFdelbeginFL \DIFdelFL{for }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{from }\DIFaddendFL Avveduto et al.~\cite{Avveduto2017}. 
    a) shows a study setup that was utilized throughout this study, with the virtual objects present.
    b) shows the no \gls{X-ray Vision} condition used for this experiment from the participant's view.
    c) shows the \gls{X-ray Vision} conditions used for this experiment from the participant's view. 
    Used with Permission from Avveduto et al.~\cite{Avveduto2017}.
    }
    \label{fig:AvvedutoEtAl}
\end{figure}

\gls{sar} \gls{X-ray Vision} techniques papers utilize virtual holes as they work well with \gls{perspective_corrected_projection}~\cite{Avveduto2017, Heinrich2019, Heinrich2022}. 
The initial research in this space looks at what is required for a \gls{sar} based \gls{X-ray Vision} system. 
This was done by Avveduto et al.~\cite{Avveduto2017}, who asked participants to perform a mock biopsy like the one shown in \autoref{fig:AvvedutoEtAl}. 
Participants were then asked to attempt this procedure using either an image of a leathery surface or an image of buttons\DIFdelbegin \DIFdel{was presented and compared using a }\DIFdelend \DIFaddbegin \DIFadd{, which provided two different levels of contrast as backdrops for the }\DIFaddend virtual hole.
A virtual hole to a virtual hole with an occlusion mask against a baseline.
The authors found that using the occlusion mask and the virtual hole provided accurate results, with participants only being off by approximately 2.5cm (sd = 0.5), whereas in tasks where the participants used no \gls{X-ray Vision} 3cm (sd = 0.5). 


Heinrich et al.~\cite{Heinrich2019} later used virtual holes with \gls{sar} using \gls{perspective_corrected_projection} tested methods to illustrate depth perception with and without stereo vision. 
They compared using the conditions shown in \autoref{fig:Heinrich2019} (Phong Shading, Virtual Mirror, Depth-Encoding Silhouettes, Pseudo-Chromadepth, and Supporting Lines). 
This task asked participants to correctly identify the \DIFaddbegin \DIFadd{depth }\DIFaddend order of the cubes that can be seen in \autoref{fig:Heinrich2019}. 
This research found that the stereoscopic representation of SAR improved all conditions regarding time required and perceived difficulty. 
Pseudo-Chromadepth and Support Lines were the most effective \glspl{X-ray Visualization}, whereas Phong and the Virtual Mirror conditions were the most challenging. 

\paragraph{Cutaways and Tunnelling:}

\begin{figure}[!b]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Heinrich2019.jpg}
    \caption[The conditions used in Heinrich et al.'s~\cite{Heinrich2019} investigation in to \gls{sar} based virtual holes.]{The conditions used in Heinrich et al.'s~\cite{Heinrich2019} investigation in to \gls{sar} based virtual holes. a) Phong Shading. b) Virtual Mirror. c) Depth-Encoding Silhouettes. d) Pseudo-Chromadepth. e) Supporting Lines. Used with permission from IEEE \textcopyright{} 2019.}
    \label{fig:Heinrich2019}
\end{figure}

Cutaways and Tunnelling present a hole through an object, revealing an object on the other side of the object~\cite{Avery2008, Avery2009, Feiner1992}. 
Both these techniques can be seen in \autoref{fig:JustTunneling}. They appear as a box with no back. 
The point of these visualizations is to give the sense of going through a real-world object.
These visualizations focus more on indicating where the data is in the real world rather than giving the user a better sense of depth when looking through a particular piece of data~\cite{Avery2009}.


Avery et al.~\cite{Avery2009} observed a slight offset to the perception of depth when tunneling is used on its own~\cite{Avery2009}. 
This can be improved by combining this technique with either a real-world overlay~\cite{Lerotic2007} or any Computer Vision-Enabled Technique~\cite{Avery2009} on either the front or back to help ground the visualization.
This allowed Avery et al.~\cite{Avery2009} and Lerotic et al.~\cite{Lerotic2007} to give their users the perspective of looking through a wall, whereas~\cite{Erat2018} used a smaller form of this to look through an individual wall. This smaller attempt would be considered a cutaway\DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Feiner1992}}\hskip0pt%DIFAUXCMD
}\DIFaddend . 

Cutaways don't appear much in \gls{ar} literature \DIFaddbegin \DIFadd{however some notable works exist~\mbox{%DIFAUXCMD
\cite{Zollmann2012}}\hskip0pt%DIFAUXCMD
}\DIFaddend . The smaller size of the effect means they may require another visualization to repair the depth offset. However, no research has been done to test whether this could be an issue.
Research from Erat et al.~\cite{Erat2018} did find that cutaways make for a very natural way to look through a wall to interact with something like a drone on the other side when compared to looking at the drone from a first-person's viewpoint, but little else is understood about using cutaways in \gls{ar} for \gls{X-ray Vision}.
When testing this use case, they found that technological limitations still held users back from utilizing \gls{X-ray Vision} to render objects underneath the ground~\cite{Erat2018}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/JustTunneling.png}
    \caption[A image of the tunneling x-ray visualization used to look through multiple walls.]{A image of the tunneling x-ray visualization used to look through multiple walls. The first arrow looks into a storeroom, whereas the second arrow looks out to the outside. White arrows showcase the depth of the tunnel.}
    \label{fig:JustTunneling}
\end{figure}

\paragraph{Virtual Windows:} 
Virtual Windows shows the user a perspective of a 2D hole in the wall. 
These are sometimes presented as a crack in the wall\DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Feiner1992} }\hskip0pt%DIFAUXCMD
}\DIFaddend or will have a slight frame, but what separates this visualization from cutaways is that they ignore the content that is not known and start the virtual space immediately after the real-world surface\DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Phillips2022}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
This can be seen in \autoref{fig:VirtualHoleAndVirtualWindowExamples} \DIFdelbegin \DIFdel{where the armadillos appear behind the wall in a separate 3D reality, but also where the real world seems paper thin. 
These windows can range from literal holes in the walls to }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend merging with the surface of a given material~\cite{Bichlmeier2007, Guo2022}. 

Guo et al.~\cite{Guo2022} utilized virtual holes to enable video games to take place in a \gls{non_euclidean_space}. 
\DIFdelbegin \DIFdel{They }\DIFdelend \DIFaddbegin \DIFadd{The authors }\DIFaddend aimed to look at methods where \gls{X-ray Vision} could be made to be more interactive. 
Enabling people to play a game together regardless of their spatial requirements. 
Their experiment had them tag virtual objects that were to tag objects behind the physical walls, which showed to be almost as useful as giving the participants a virtual mini-map with the targets located on it.

\DIFaddbegin \DIFadd{Another form of virual windows has been titled "}\DIFaddend Contextual Anatomic Mimesis\DIFaddbegin \DIFadd{" by Bichlmeier et al.~\mbox{%DIFAUXCMD
\cite{Bichlmeier2007}}\hskip0pt%DIFAUXCMD
. This }\DIFaddend is one of these windows designed to work autonomously with medical applications. 
\DIFaddbegin \DIFadd{The technique that blends virtual anatomical structures with the appearance of real tissue, gradually transitioning from realistic skin to virtual content.
}\DIFaddend It gradually moves from having the appearance of skin to looking more like the virtual content while still presenting a slight overlay based on the roughness of the skin~\cite{Bichlmeier2007}. 
The effects of Contextual Anatomic Mimesis were furthered by Martin-Gomez et al.~\cite{Martin-Gomez2021}, who have shown a significant improvement when paired with a \DIFaddbegin \DIFadd{another }\DIFaddend second effect to help represent the depth of an object.
\DIFaddbegin \DIFadd{Noting that using more than 1 depth cue is beneficial.
}\DIFaddend These effects ranged from hatching to applying shading to show shadows to the effect where it was found that hatching outperformed the other \DIFdelbegin \DIFdel{results}\DIFdelend \DIFaddbegin \DIFadd{conditions (hole, ghosted mask, constant, shaded, black, chromatic, and bright)~\mbox{%DIFAUXCMD
\cite{Martin-Gomez2021}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
These results have led us to the understanding that illustrative effects may have a slight advantage when providing an understanding of the positioning of data for medical diagrams \DIFaddbegin \DIFadd{as they can not only provide a sense of depth perception but are also easy to understand for a general user}\DIFaddend . 

\subsubsection{Real-World Overlays} \label{sec:realWorldOverlay}
Another way of providing \gls{X-ray Vision} effects is to represent the physical environment using a virtual pattern such as a wireframe or a grid. 
This is typically done in two ways. One way is to place a pattern on the ground, allowing users to retain some knowledge of depth by using a constant geometric cue~\cite{Gruenefeld2020}. The other method utilizes overlaying the real-world object with a pattern of some sort. 
The second method is used when viewing virtual objects in stereoscopic displays, as it requires a level of geometric saliency to be perceptible. 
Binocular disparities are required for these types of visualizations to function~\cite{Otsuki2015}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/RealWorldOverlays.jpg}
    \caption[Wireframe, Random Dot, and Grid \gls{X-ray Visualization} over colorful patterned box with a virtual cube, sphere icosahedron, and a Stanford bunny rendered inside.]{\DIFaddbeginFL \DIFaddFL{This is a demonstration of real world overlay's and how they are utilized as forms of X-ray vision. the }\glspl{X-ray Visualization} \DIFaddendFL Wireframe, Random Dot, and Grid \DIFdelbeginFL %DIFDELCMD < \gls{X-ray Visualization} %%%
\DIFdelendFL \DIFaddbeginFL \DIFaddFL{and are placed }\DIFaddendFL over colorful patterned box with a virtual cube, sphere icosahedron, and a Stanford bunny rendered inside. The image was taken using the HoloLens' screen view. }
    \label{fig:RealWorldOverlays}
\end{figure}

Overlaying a real-world overlay as a guide over the ground does not require any specific device since as long as the overlay is able to interact with the real world environment. 
It will perform better, however, when it is used with a binocular \gls{hmd} as this can better represent the surface~\cite{Otsuki2016}.
While they do provide a good occlusive cue, the main role they serve is to help the user locate the position of an object with reference to a real world object. 
The additional cues provided from that point moving forward should take note of what is required from that point moving forward. 

\autoref{fig:RealWorldOverlays} illustrates forms real-world overlays can take, for example overlaying grids~\cite{Becher2021, Heinrich2022, Heinrich2019, Johnson2014}, Wireframes~\cite{Chen2020, Gruenefeld2020, Tsuda2005} and Random Dots~\cite{Ghasemi2018, Otsuki2015, Otsuki2016, Otsuki2017}. All of these effects are designed to utilize partial occlusion to highlight the shape of the object and to help the user understand the orientation of the wall. This gives the user a better comprehension of the distance between themselves and the objects they are looking inside of. 

\paragraph{Grids:}
Grids allow for a persistent barrier between a given surface and provide a guide on where a surface is in an augmented world~\cite{Johnson2014, Heinrich2022}. 
Johnson et al.~\cite{Johnson2014} developed a surgical system that utilized a grid-based visualization to allow for \gls{ct} data to be viewed at the correct position within the patient when using a wearing an \gls{ar} \gls{hmd}.
A grid was utilized to represent different types of flesh and a 3D object. 
They had surgical students utilize this system with laparoscopic video to evaluate it, where it was received with a positive reception \DIFaddbegin \DIFadd{from the subjective study which was conducted}\DIFaddend ~\cite{Johnson2014}. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Heinrich2022.png}
    \DIFdelbeginFL %DIFDELCMD < \caption[The conditions utaized for Heinrich et al.'s~\cite{Heinrich2022}  research.]{%%%
\DIFdelendFL \DIFaddbeginFL \caption[The conditions utilized for Heinrich et al.'s~\cite{Heinrich2022}  research.]{\DIFaddendFL The conditions \DIFdelbeginFL \DIFdelFL{utaized }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{utilized }\DIFaddendFL for Heinrich et al.'s~\cite{Heinrich2022} research. The first row shows how they were visualized using the HoloLens, while the second line shows the output from the \gls{sar} display. The columns represent the three visualizations they used in their study, which were then split into three, showing the difference between their visualizations for guiding the angle and depth required for the needle. Used with permission from IEEE \textcopyright{} 2022.}
    \label{fig:Heinrich2022}
\end{figure}

Grids also tend to be utilized to explain to the user where the background of the \gls{X-ray Visualization} can be seen. 
\autoref{fig:Heinrich2022} shows how Heinrich et al.~\cite{Heinrich2022} utilized a grid overlaying a virtual hole and tested how accurate a biopsy would be using this as visualizations between an \gls{ost} \gls{ar} and \gls{sar}. 
This required testing the accuracy of the initial insertion angle and the depth to which the user placed the device. 
Heinrich et al.~\cite{Heinrich2022} utilized two conditions without \gls{X-ray Visualization}. 
One was a target that showed and would change in color when the needle approached the target, and the other was a virtual needle for the users to match. 
%All the conditions are shown in \autoref{fig:Heinrich2022}.
\DIFdelbegin \DIFdel{This method showed that the }%DIFDELCMD < \gls{X-ray Visualization} %%%
\DIFdel{was less }\DIFdelend %DIF > This method showed that the \gls{X-ray Visualization} was less effective than the glyph visualization, as the latter gave better feedback than being able to see the \gls{X-ray Visualization}. 
\DIFaddbegin \DIFadd{This method demonstrated that the glyph visualization was more }\DIFaddend effective than the \DIFdelbegin \DIFdel{glyph visualization, as the latter gave better feedback than being able to see the }%DIFDELCMD < \gls{X-ray Visualization}%%%
\DIFdelend \DIFaddbegin \gls{X-ray Visualization}\DIFadd{, as it provided participants with a more intuitive }\gls{X-ray Vision} \DIFadd{experiance}\DIFaddend .
However, both of them performed better than showing the visualization of the desired entry of the needle~\cite{Heinrich2022}. 

\paragraph{Random Dot:}
Random dot is another way of expressing more occlusion than other geometric pattern effects typically employed~\cite{Ghasemi2018, Otsuki2015}. This effect provides a partial sense of occlusion by providing a semi-occlusive layer over real-world objects~\cite{Otsuki2015}. 
This effect is one of the least computationally expensive but requires an immersive environment with flat surfaces. 
Highlighting various pixels on the surface of objects provides a stereoscopic effect of occlusion for the user and should technically allow for a better sense of depth. 

Ghasemi et al.~\cite{Ghasemi2018} conducted two studies to evaluate Random Dot visualizations. A psycho-physical depth perception experiment, which consisted of a series of thin circles, was run to determine the effectiveness of this \gls{X-ray Visualization}. 
Participants were given six different distances that ranged from 5mm in front and behind the visualization/wall.
The results study found that this effect is most effective when around 50\% of the space is filled using a random dot.
The amount of dots and their size could be altered, but it was most effective when more small dots were used up close and fewer larger ones up close and more remote. 
This distance could be increased if objects are further away from the wall, allowing for more depth perception. 

\paragraph{Halo and the Silhouette:}

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=0.75\columnwidth]{Chapter2/Images/JustHalo.png}
%     \caption{A directly rendered volume of a hierarchical set of spheres utilizing \gls{X-ray Visualization} that uses a silhouette/Halo \gls{X-ray Visualization}.}
%     \label{fig:JustHalo}
% \end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Chapter2/Images/OzgurEtAl.png}
    \caption[Ozgur et al.'s~\cite{Ozgur2017} \gls{X-ray Vision} system using Halos.]{
    Ozgur et al.'s~\cite{Ozgur2017} \gls{X-ray Vision} system using Halos.
    The silhouettes are generated through orthographic projection onto the front and back planes, which are oriented based on the sight line passing through the tumor's center of mass. These planes are positioned at the points where the organ's surface intersects the sight line.
    Used with permission from IEEE \textcopyright{} 2017 and Ozgur et al.~\cite{Ozgur2017}
    }
    \label{fig:JustHalo}
\end{figure}

This effect goes by many names but is most commonly called Halo or Silhouette as it creates an outline of the virtual objects.
It will be referred to as the Halo visualization in this dissertation for simplicity.
\DIFaddbegin \DIFadd{The }\DIFaddend Halo \glspl{X-ray Visualization} \DIFdelbegin \DIFdel{effects present }\DIFdelend \DIFaddbegin \DIFadd{as stated in Ozgur et al.'s~\mbox{%DIFAUXCMD
\cite{Ozgur2017} }\hskip0pt%DIFAUXCMD
paper presents the effect as }\DIFaddend a bright light around the edges of the object of interest and use alpha blending to communicate depth~\cite{Ozgur2017}. 
\DIFdelbegin \DIFdel{In }%DIFDELCMD < \autoref{fig:JustHalo}%%%
\DIFdel{, it can be seen the halo's is set , allowing it }\DIFdelend %DIF > This is very similar to the silhouette effect found in medicine~\cite{Rheingans2001}, but since it is the only time this will be treated as if this is the termonlolgy for this effect in this thesis. 
\DIFaddbegin \DIFadd{This effect is very similar to what is referred to as the "silhouette" effect in medical visualization~\mbox{%DIFAUXCMD
\cite{Rheingans2001}}\hskip0pt%DIFAUXCMD
. 
However, terminology varies across fields, and in this thesis, I will consistently use the term "Halo visualization" to describe this effect for clarity. 
}\autoref{fig:JustHalo} \DIFadd{show how the the halo is set }\DIFaddend to leverage the depth perception cue of relative size.
%DIF > In \autoref{fig:JustHalo}, it can be seen the halo's is set, allowing it to leverage the depth perception cue of relative size. 
This \gls{X-ray Vision} effect is unique due to its lack of occlusion, making it effective for surgical situations~\cite{Ozgur2017}. 
It has been tested using a monoscopic display in a study designed for minimally invasive surgeries. It can also exploit a color-based effect, applying visual language to communicate depth to the user~\cite{Ozgur2017}.

\subsubsection{Computer Vision-Enabled Techniques} \label{sec:ComputerVisionEnabledTechnqiue}
In computer vision, several methods are used to highlight salient features in an image. This is normally done by highlighting various salient factors within an image. 
These techniques are commonly used on monoscopic \DIFaddbegin \gls{vst} \DIFaddend devices as they provide a good sense of where an object is positioned relative to another in 2D. 
\DIFaddbegin \DIFadd{This is separate from the Halo technique as these do not generally utilize the image of the real world while the halo technique utilizes the shape of the object that has been inserted. 
}\DIFaddend To this date, the results lack the performance of Computer Vision-Enabled Techniques in stereo, with most of the publications found in this space either being demos or research proposals~\cite{Rompapas2014, Phillips2021}.

\begin{figure}[!b]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ComputerVisionEffectsAndTransperency.jpg}
    \caption{\gls{X-ray Vision} showing alpha blending, Saliency, and edge-based Visualizations (Shown left to right) looking through a building.}
    \label{fig:ComputerVisionEffectsAndTransperency}
\end{figure}

\paragraph{Edge-Based:}
Edge-based \glspl{X-ray Visualization} (depicted in \autoref{fig:ComputerVisionEffectsAndTransperency}) identify areas in the user’s view that contrast highly between neighboring pixels and highlight them. 
This high contrast effect tends to be regarded by most humans as salient, making it a reliable method for determining salient artifacts~\cite{Avery2009}. 
Created initially by Kalkofen et al.~\cite{Kalkofen2007} to allow for better image-based \gls{X-ray Vision}.
This visualization creates a predictable method for highlighting areas on an image (given that the surface has a high level of contrast) and makes it a popular way of showcasing \gls{X-ray Vision} on a monoscopic \gls{ar} device~\cite{Avery2009, Chen2010, Kalkofen2007, Sandor2010}. 
By highlighting the \DIFaddbegin \DIFadd{visible }\DIFaddend edges of an object, it is possible to create the appearance that an object is behind the object either through partial occlusion~\cite{Avery2009, Kalkofen2007} or by cutting out elements of the X-rayed object~\cite{Dey2012, Dey2014}.

\paragraph{Saliency:}
Saliency highlights areas of the real world that are likely to be of interest and makes the areas that are unlikely to be less apparent. 
\autoref{fig:ComputerVisionEffectsAndTransperency} shows one method of using saliency as an \gls{X-ray Visualization}. 
The visualization technique can vary between implementations because it can be done algorithmically\DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Sandor2010} }\hskip0pt%DIFAUXCMD
}\DIFaddend or by training an AI model to detect the features a human is more or less likely to look at~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Cong2019, Sandor2010}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Cong2019}}\hskip0pt%DIFAUXCMD
}\DIFaddend . 
A user could naturally peer through objects using the saliency method, given that the camera could find some salient and non-salient areas of the image~\cite{Sandor2010}. 
As of the beginning of this thesis, Saliency has only been utilized with \gls{vst} \gls{ar}.

\paragraph{Ghosting:}
Ghosting utilizes the effects of visual saliency and focuses on allowing artistic approaches to partial occlusion~\cite{1238308, Zollmann2010}. Adaptive Ghosting is an extension that combines the salient effect of ghosting and edge-based \gls{X-ray Vision} effects. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=1\linewidth]{Chapter2/Images/KalkofenEtAlSystem.png}
    \caption[A description of the \gls{X-ray Vision} system used by Kalkofen et al.~\cite{Kalkofen2013}]{A description of the \gls{X-ray Vision} system used by Kalkofen et al.~\cite{Kalkofen2013}\DIFaddbeginFL \DIFaddFL{. This image shows the various components of the graphics pipeline required to create the adaptive ghosting effect.
    }\DIFaddendFL (a) The occluder and occluded elements are combined into a ghosted view using state-of-the-art transparency mapping based on the importance map of the occluder. However, this initial ghosted view lacks \DIFdelbeginFL \DIFdelFL{certain critical features}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{is difficult to integrate into the real world seamlessly as it is either used to cover a area or it is not}\DIFaddendFL . To address this, a new importance map is generated for the resulting ghosted view \DIFaddbeginFL \DIFaddFL{which looks at area's of high contrast and places the visualisation on to them as well as over the rest of any are's close to virtual elements that need to be seen within the physical objects}\DIFaddendFL .
    (b) This new importance map is then compared to the original importance map of the occluder. Features that were marked as important in the occluder but are no longer prominent in the ghosted view are identified for enhancement.
    (c) By applying these enhancements to the adaptive ghosted view, the previously obscured important structures become clearly visible again.
    Used with permission from IEEE \textcopyright{} 2013.
    }
    \label{fig:KalkofenEtAlSystem}
\end{figure}

Zollman et al.~\cite{Zollmann2014} tested the effectiveness of Edge-Based, Ghosting, and Randomly occluding underground pipes.
Participants were asked to identify the depth of the pipes, which were located underground, based on a series of Images they received.
This study found that ghosting was better equipped than edge-based \gls{X-ray Vision} at identifying depth underground and noted that it was easy to use, but it did observe the details of the shape. 

Adaptive Ghosting technique to the current environment makes it less varied when viewing it from different directions~\cite{Kalkofen2013}.
\autoref{fig:KalkofenEtAlSystem} presents Kalkofen et al.'s~\cite{Kalkofen2013} system for Adaptive Ghosting, which normalizes the contrast of the input image to allow this effect to maintain a similar effect even in different lighting. 
This system combats one major issue with computer vision-enabled \gls{X-ray Vision} techniques: their inconsistency depending on changes to the scene and lighting. 
Adaptive Ghosting can cause unexpected artifacts and misleading visualizations, but it is still possible to notice this effect even when using Adaptive Ghosting.

\subsubsection{Comparisons Of X-ray Vision Effects}
%DIF >  \begin{figure}[tb]
%DIF >      \centering
%DIF >      \caption{A heat map showing the frequency of pairwise comparison of different \gls{X-ray Vision} effects. The total number of studies for each effect is shown in the lower left to upper right diagonal.}
%DIF >      \label{fig:ComparisionsOFVisionInTheLiturature}
%DIF >  \end{figure}
\DIFaddbegin 

\DIFaddend \begin{figure}[tb]
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\columnwidth]{Chapter2/Images/ComparisionsOFVisionInThe Liturature.png}
%DIFDELCMD <     \caption{%%%
\DIFdelendFL %DIF > \includegraphics[width=\columnwidth]{Chapter2/Images/ComparisionsOFVisionInThe Liturature.png}
    \DIFaddbeginFL \includegraphics[width=\columnwidth]{Chapter2/Images/ComparisionsOFVisionInTheLiturature.pdf}
    \caption[A heat map illustrating how often different \gls{X-ray Vision} effects are compared to each other in the literature.]{
        \DIFaddendFL A heat map \DIFdelbeginFL \DIFdelFL{showing the frequency of pairwise comparison of }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{illustrating how often }\DIFaddendFL different \gls{X-ray Vision} effects \DIFaddbeginFL \DIFaddFL{are compared to each other in the literature}\DIFaddendFL . The \DIFaddbeginFL \DIFaddFL{diagonal cells (from lower left to upper right) show the }\DIFaddendFL total number of studies for each effect\DIFdelbeginFL \DIFdelFL{is shown in }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{, while }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{lower left to upper right diagonal}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{center row indicates studies that examined each effect individually without comparison}\DIFaddendFL .
\DIFaddbeginFL 

        %DIF >  A heat map showing the frequency of pairwise comparison of different \gls{X-ray Vision} effects found throughout the literature. The total number of studies for each effect is shown in the lower left to upper right diagonal. the center row is focused on the number of studies that did not compare the effect to another.
        \DIFaddendFL }
    \label{fig:ComparisionsOFVisionInTheLiturature}
\end{figure}

Given all these methods of \gls{X-ray Vision}, it is quite common for people to try to compare them to determine the positive and negative aspects. 
Studies in this area have investigated:
\begin{itemize}
    \item The amount of occlusion density required for visualizations to create the \gls{X-ray Vision} \DIFdelbegin \DIFdel{visulisations  }\DIFdelend \DIFaddbegin \DIFadd{visualisations }\DIFaddend and effective limits to the practicality of these effects~\cite{Santos2016};
    \item How different displays react to \glspl{X-ray Visualization} on Mobile Devices~\cite{Dey2012, Dey2014}.
    \item What is the impact of \gls{X-ray Vision} between different visualizations~\cite{Sandor2010, Zollmann2014, Martin-Gomez2021, Gruenefeld2020}.
\end{itemize}
These comparisons are detailed in \autoref{fig:ComparisionsOFVisionInTheLiturature}.

Partial occlusion is important to most \glspl{X-ray Visualization}. It shows how much occlusion is needed for a given technique and how much occlusion makes interactions impossible.
This was the question that Santos et al.~\cite{Santos2016} addressed for mobile \gls{ar} \gls{X-ray Vision} when using edge-based or saliency \gls{X-ray Vision} effects.
This study had participants look at a box wrapped in either wrapping paper or crumpled foil.
This study utilized two tasks: One looking at what thresholds were required to allow participants to see 2D objects using \gls{X-ray Vision} effects, and the other looking at the maximum allowable alpha values these effects could have~\cite{Santos2016}. 

Santos et al.'s~\cite{Santos2016} first tested how the user could see four objects placed inside the box and asked them to reveal the maximum decision cutoff (threshold).
The next task was to lower the alpha value of the visualization until participants could clearly see the object hidden behind it. 
Inside the box would be visualized an image of an object they were asked to see, which could either be a small or a large object about the box. 
Each second, the transparency of the visualization would decrease, asking them to press a button on the display to stop the visualization when they could identify the item.
The combination of Santos et al.'s~\cite{Santos2016} results indicates the clarity of the object was dependent on the size, color, and texture of the box rather than the visualization used. 
The study noted Saliency was slightly harder to see through but not to a significant level. 
This dissertation treats these results as a guideline and has based its use of transparency and contrast with \gls{X-ray Vision} \DIFdelbegin \DIFdel{based }\DIFdelend on these results. 

Sandor et al.~\cite{Sandor2010} Compared Edge-based visualizations to their newer Saliency visualization by having participants try to find a target in four scenes. 
This study found little difference between saliency and edge-based visualizations, but participants preferred the edge overlay. 
After this, both these conditions were tested using an online survey where participants would look at images presenting the information where saliency was seen to be better for providing \gls{X-ray Vision} for images. 

Dey et al.~\cite{Dey2012, Dey2014} compared \gls{X-ray Vision} on different-sized mobile devices to determine the display size could impact the visualizations required for Mobile devices.
Firstly, they developed an \gls{X-ray Visualization} called Melt, which showed a part of the foreground and a part of the background together.
Their first experiment tested the Edge-based technique against the Melt X-ray technique from far distances using a 7-inch display.
This study found that Melt was a more accurate in-depth estimation but took the participants longer to judge, which was likely used to overcome the drawbacks of the visualization~\cite{Dey2012, Dey2014}.

Dey et al.~\cite{Dey2014} ran a user study comparing Edge-based and Saliency using a Mobile \gls{ar} display. 
They created three levels of each \gls{X-ray Visualization}, where they were more or less sensitive to environmental concerns.
These results indicated that users struggled to use \gls{X-ray Visualization} when the edges were too thick for the edge-based variable, and saliency struggled in bright environments. 

All of the comparisons explained in this section utilized similar \gls{X-ray Vision} visualizations and most focused on how \glspl{X-ray Visualization} worked in different areas.
All these examples utilize mobile devices, so how these would affect \glspl{hmd} in general is a question that is yet to be answered. 
Also, most of these comparisons have utilized various forms of computer vision techniques (except for one use of the Melt visualization). 
Currently, there are many unknowns that exist in this space.


\subsection{Applications of X-ray Vision}
\DIFdelbegin \DIFdel{The application of X-ray vision focuses on specific domains where the visualization of internal structures and hidden information are important.  
X-ray vision technologies are prominently featured in the medical, construction, and maintenance domains. In addition, generic techniques may be applicable in multiple domains, as shown in the Generic category in }%DIFDELCMD < \autoref{fig:XRayVisionUseCaseDistribution} %%%
\DIFdel{and }%DIFDELCMD < \autoref{fig:LitReviewDevicesBeingUsed}%%%
\DIFdel{. Security and Navigation each presented several examples, while few examples of X-ray vision were encountered in the remaining application areas.
}\DIFdelend 

\begin{figure}[tb]
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\textwidth]{Chapter2/Images/XRayVisionUseCaseDistribution.png}
%DIFDELCMD <     \caption{%%%
\DIFdelFL{A bar chart representing research aimed at a given application }\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Eren2018.png}
    \caption[Eren et al.'s~\cite{Eren2013} different visualizations of underground pipe networks using \gls{X-ray Vision} techniques.]{\DIFaddFL{Eren et al.'s~\mbox{%DIFAUXCMD
\cite{Eren2013} }\hskip0pt%DIFAUXCMD
different visualizations }\DIFaddendFL of \DIFaddbeginFL \DIFaddFL{underground pipe networks using }\DIFaddendFL \gls{X-ray Vision} \DIFaddbeginFL \DIFaddFL{techniques}\DIFaddendFL . \DIFaddbeginFL \DIFaddFL{a) baseline, b) edge-based, c) virtual hole, and d) cross-sectional techniques. Used with permission from Springer Nature \textcopyright{} 2018.}\DIFaddendFL }
    \DIFdelbeginFL %DIFDELCMD < \label{fig:XRayVisionUseCaseDistribution}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:Eren2013}
\DIFaddendFL \end{figure}

\DIFdelbegin \DIFdel{All of the generic papers presented in }%DIFDELCMD < \autoref{fig:XRayVisionUseCaseDistribution} %%%
\DIFdel{and }%DIFDELCMD < \autoref{fig:LitReviewDevicesBeingUsed} %%%
\DIFdel{tend to present important information relating to fundamental aspects/tasks/challenges that can be applied to }\DIFdelend \DIFaddbegin \DIFadd{X-ray vision has been applied across }\DIFaddend a wide range of \DIFdelbegin \DIFdel{situations. These include navigating through one’s home or office, interacting with robots, determining the distance between an object and a user, or simply proving how to place items in a location. The findings of these papers are relevant to many fields. These findings span a diverse range of novel research with the development of visualizations primarily tailored for these purposes, offering foundational guidance for domain-specific papers~\mbox{%DIFAUXCMD
\cite{Kalkofen2007, Kitajima2015, Otsuki2015, Sandor2010}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\paragraph{\DIFdel{Medicine:}}
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
\DIFdel{In the medical research domain, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{(}%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{) integration supports in-situ diagnostic capabilities and surgical activities. }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{allows medical professionals to visualize virtual information related to internal structures , such as bones, organs, and tissues, in real-time overlays onto a patient's body during examinations or procedures~\mbox{%DIFAUXCMD
\cite{Bichlmeier2007, Blum2012, Erat2018, Habert2015, Lerotic2007}}\hskip0pt%DIFAUXCMD
. }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{can enhance diagnostic accuracy by providing an immediate, three-dimensional view }\DIFdelend \DIFaddbegin \DIFadd{domains where visualizing hidden or internal structures is valuable. Medical research has been a central focus, where overlays }\DIFaddend of anatomical structures \DIFdelbegin \DIFdel{, aiding in identifying abnormalities or subtle variations~\mbox{%DIFAUXCMD
\cite{Bichlmeier2007, Blum2012}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{During surgeries, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{could potentially enable surgeons to navigate complex anatomies, guiding them to target specific areas and confidently perform invasive procedures . 
The works encountered in the literature present technologies validated by case studies demonstrating how such solutions could be adopted in diagnostic and interventional aspects of healthcare~\mbox{%DIFAUXCMD
\cite{Bajura1992}}\hskip0pt%DIFAUXCMD
. 
However, further work is needed to mature the technology and align it with clinical requirements and regulations to enable the clinical use of such technologies on a routine basis~\mbox{%DIFAUXCMD
\cite{Pratt2018}}\hskip0pt%DIFAUXCMD
. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Talking about Seilhorst's experiment
\DIFdel{When asking surgeons for their expertise, Sielhorst et al.~\mbox{%DIFAUXCMD
\cite{Sielhorst2006} }\hskip0pt%DIFAUXCMD
and Bichlmier et al. ~\mbox{%DIFAUXCMD
\cite{Bichlmeier2007} }\hskip0pt%DIFAUXCMD
tried to visualize a set of data within the patient's body similar to Bajura et al. \mbox{%DIFAUXCMD
\cite{Bajura1992}}\hskip0pt%DIFAUXCMD
. 
The surgeons were shown seven different visualizations, all representing a spinal cord anchored in the correct position on a mannequin. 
These seven conditions included three different types of rendering of a spine: a 3D iso-surface, the spine rendered using }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{, and a wireframe model of the iso-surface, a transparent version of the iso-surface.
The four conditions utilized other forms of }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{and changed their visibility using various }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{techniques.
One of these produced a glass-like reflection on the mannequin's skin (acting like a real-world overlay) over the area where the spine was rendered, which was used with the iso-surface. 
Another one of these utilized a virtual hole that sat over the iso-surface and }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{version of the rendering.
The surgeons all noted that the sense of depth was not adequate, and the }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{rendering was too difficult to use as it could not render in }\DIFdelend \DIFaddbegin \DIFadd{can support diagnosis and guide surgical procedures by allowing clinicians to perceive organs, tissues, and bones in situ~\mbox{%DIFAUXCMD
\cite{Bichlmeier2007, Blum2012, Bajura1992}}\hskip0pt%DIFAUXCMD
. While demonstrations have shown promise, challenges remain around depth perception and }\DIFaddend real-time \DIFdelbegin \DIFdel{, but they did note that it was more accurate to place the object here}\DIFdelend \DIFaddbegin \DIFadd{performance, as highlighted in comparative evaluations of iso-surfaces, direct volume rendering, and alternative rendering effects}\DIFaddend ~\cite{Sielhorst2006, Bichlmeier2007}. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\paragraph{\DIFdel{Construction and Maintenance:}}
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
\DIFdel{In construction and maintenance, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{enables construction professionals to see through physical structures. This facilitates a comprehensive understanding of internal structure and integrity and identifies potential issues that may be otherwise hidden. Such capability can support building inspections, allowing real-time visualization of hidden structural elements, electrical wiring, and plumbing systems~\mbox{%DIFAUXCMD
\cite{Becher2021, Eren2018, Muthalif2022}}\hskip0pt%DIFAUXCMD
. Moreover, it can help pinpoint areas requiring attention within existing structures. By providing a transparent view into the inner workings of buildings, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{enhances construction quality control, accelerates maintenance procedures, and contributes to the safety of built environments~\mbox{%DIFAUXCMD
\cite{Feiner1995, Liu2018}}\hskip0pt%DIFAUXCMD
. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Construction and architecture have been a long-running goal of }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{Outside of medicine, construction and maintenance tasks also benefit, with researchers exploring techniques for seeing into walls or underground pipe networks. Early work by }\DIFaddend Feiner et al.~\cite{Feiner1995} \DIFdelbegin \DIFdel{wrote one of the first papers looking into }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{. It was focused on giving the user the impression of looking through walls by rendering them in }%DIFDELCMD < \gls{ar} %%%
\DIFdel{and gradually making them more occlusive as the user is looking through them.
}%DIFDELCMD < 

%DIFDELCMD < \begin{figure}[tb]
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Eren2018.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[Eren et al.'s~\cite{Eren2013} different visualizations of underground pipe networks using \gls{X-ray Vision} techniques.]{%
{%DIFAUXCMD
\DIFdelFL{Eren et al.'s~\mbox{%DIFAUXCMD
\cite{Eren2013} }\hskip0pt%DIFAUXCMD
different visualizations of underground pipe networks using }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdelFL{techniques. a) baseline, b) edge-based, c) virtual hole, and d) cross-sectional techniques. Used with permission from Springer Nature \textcopyright{} 2018.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:Eren2013}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Laying the visualization of underground pipes comes up in several papers~\mbox{%DIFAUXCMD
\cite{VanSon2018, Eren2018, Muthalif2022, Becher2021}}\hskip0pt%DIFAUXCMD
, and the techniques for dealing with this are either overlaying schematics over the real world~\mbox{%DIFAUXCMD
\cite{VanSon2018} }\hskip0pt%DIFAUXCMD
or rendering them underground using }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{focused on gradually revealing occluded structures, and more recent studies (e.g., }\DIFaddend Eren et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Eren2013} }\hskip0pt%DIFAUXCMD
originally proposed using }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{with underground pipes for mobile }%DIFDELCMD < \gls{ar} %%%
\DIFdel{applications with a series of }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{techniques }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Eren2013,Eren2018}}\hskip0pt%DIFAUXCMD
, }\DIFaddend shown in \autoref{fig:Eren2013}\DIFdelbegin \DIFdel{. 
Later, Eren et al.~\mbox{%DIFAUXCMD
\cite{Eren2018} }\hskip0pt%DIFAUXCMD
conducted a study to measure how accurately Virtual }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{could be judged. 
This study used a mobile device fixed at a height of 150 cm. The pipes were placed horizontally 2.5 meters from the participants, and different }%DIFDELCMD < \glspl{X-ray Visualization} %%%
\DIFdel{were used.
From this, the cross-section and the X-ray visualization were both considered to perform at a similar rate compared to the baseline. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Muthalif et al.~\mbox{%DIFAUXCMD
\cite{Muthalif2022} }\hskip0pt%DIFAUXCMD
explored over 4 }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{methods of looking through ground and walls using three different methods using a virtual box with some variants.
One of these variants had lines to help measure the depth into the ground the visualization was displaying. 
Another used interaction as a way of illustrating }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{by using a slider to allow the user to control how deep the virtual box is. 
Their final method showcased caused shadows to appear against the walls of the virtual box, allowing for accurate depth perception even when items did not collide with the edges of the visualization}\DIFdelend \DIFaddbegin \DIFadd{) have evaluated different visualization methods for accurately perceiving underground utilities}\DIFaddend .

\DIFdelbegin \paragraph{\DIFdel{Security:}}
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
\DIFdel{In the security industry, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{can augment situational awareness and reveal potential securitythreats. }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{has the potential to reveal persons or objects obscured by walls and other objects, support situational awareness by melding real-time surveillance information with the security personnel’s view of the immediate environment, and identify concealed items. Although use cases in the securitydomain may benefit from the ability afforded by }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{to see through and around physical objects, relatively few user studies of such applications have been found in the literature. A possible explanation may be that such applications are commercially sensitive and, hence, not published in the academic literature~\mbox{%DIFAUXCMD
\cite{Kameda2004, Livingston2005, Ohta2010, Phillips2020, Tsuda2005}}\hskip0pt%DIFAUXCMD
. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Monitoring security cameras can be difficult, as covering large, dense areas requires a security officer to concentrate on multiple screens at once.
This prompted Kameda et al.~\mbox{%DIFAUXCMD
\cite{Kameda2004}}\hskip0pt%DIFAUXCMD
, Tsuda et al.~\mbox{%DIFAUXCMD
\cite{Tsuda2005}}\hskip0pt%DIFAUXCMD
, and Ohta et al.~\mbox{%DIFAUXCMD
\cite{Ohta2010} }\hskip0pt%DIFAUXCMD
to create a system designed to look through buildings utilizing }%DIFDELCMD < \gls{perspective_corrected_projection}%%%
\DIFdel{.
The images are calibrated using location landmarks that can be found from both cameras~\mbox{%DIFAUXCMD
\cite{Kameda2004}}\hskip0pt%DIFAUXCMD
. 
The }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{was initially performed by making the foreground transparent. 
Later, several different }%DIFDELCMD < \glspl{X-ray Visualization} %%%
\DIFdel{were investigated using people who were experienced with the original security system.
Users of the system claimed that more occlusive }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{effects were more useful. However, they did work the best. 
This would have been most effectiveif the }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{could have been toggled on and off. }%DIFDELCMD < 

%DIFDELCMD < %%%
\paragraph{\DIFdel{Navigation and Tourism}}
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
\DIFdel{For spatial and location-based activities, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{has been proposed to support the user’s ability to see beyond their immediate physical environment, enabling users to see inside and through built structures . Such an extended view has been proposed to aid navigation tasks.  These include showing navigational cues on a road that can respond to depth cues (Yasuda and Ohama 2012), looking through buildings to understand what is on the other side to give an increased sense of space~\mbox{%DIFAUXCMD
\cite{Dey2011, Maia2016}}\hskip0pt%DIFAUXCMD
, and even navigational experiencescloser to “time travel” such as the sight of the historic appearance of modern cities in situ~\mbox{%DIFAUXCMD
\cite{Yamamoto2014}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Yamamoto et al. ~\mbox{%DIFAUXCMD
\cite{Yamamoto2014} }\hskip0pt%DIFAUXCMD
developed an }%DIFDELCMD < \gls{ar} %%%
\DIFdel{application that utilized }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{to view underground ruins for mobile devices. They created a cubic environment map with a 3D model of the ruin and panorama photos, then implemented a movable viewpoint to simulate a 3D effect with alpha blending. This approach improves user perception by allowing the viewpoint to move according to the device’s orientation, providing a parallax effect and making the objects appear more three-dimensional.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{As time moves forward, buildings rise and fall, and }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{is capable of explaining to users the layout cities used while seeing the appearance of the old buildings.
Maia et al.~\mbox{%DIFAUXCMD
\cite{Maia2016} }\hskip0pt%DIFAUXCMD
created a mobile }%DIFDELCMD < \gls{ar} %%%
\DIFdel{application that utilized }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{to illustrate the buildings that used to be and the previous layout of the city.
This system was evaluated using a SUS questionnaire with positive responses.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Dey et al.~\mbox{%DIFAUXCMD
\cite{Dey2011} }\hskip0pt%DIFAUXCMD
developed a mobile }%DIFDELCMD < \gls{ar} \gls{X-ray Vision} %%%
\DIFdel{system designed for pedestrian navigation and evaluated it by comparing it with two traditional navigation applications (North-up and View-up maps).
Participants walked a 900-meter route with three checkpoints using one of the navigation aids. The study found that the }%DIFDELCMD < \gls{ar} \gls{X-ray Vision} %%%
\DIFdel{system significantly reduced the number of context switches (shifts between looking at the phone and the environment) }\DIFdelend \DIFaddbegin \DIFadd{Beyond these domains, X-ray vision concepts have been investigated in security, navigation, and education. In security, transparency effects and perspective-corrected projections have been used to augment situational awareness, though evaluations suggest togglable, more occlusive effects may be most effective~\mbox{%DIFAUXCMD
\cite{Kameda2004, Tsuda2005, Ohta2010}}\hskip0pt%DIFAUXCMD
. Navigation and tourism applications demonstrate how seeing through structures or into the past can improve wayfinding and enrich cultural experiences, with evaluations showing reduced cognitive load }\DIFaddend compared to traditional maps\DIFdelbegin \DIFdel{. 
This suggests that an }%DIFDELCMD < \gls{ar} %%%
\DIFdel{enabled }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{system may serve to further enable navigation systems in the future. 
}%DIFDELCMD < 

%DIFDELCMD < \begin{figure}[tb]
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/DrGrordbortsInvaders.jpg}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[A promotional Image from Dr Grordborts Invaders made by Weta Workshop for the Magic Leap.]{%
{%DIFAUXCMD
\DIFdelFL{A promotional Image from Dr Grordborts Invaders made by Weta Workshop for the Magic Leap. Used with permission from Weta Workshop.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:DrGrordbortsInvaders}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\paragraph{\DIFdel{Education:}}
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
\DIFdel{Santos et al.\mbox{%DIFAUXCMD
\cite{Santos2015} }\hskip0pt%DIFAUXCMD
have produced some work looking into the effect that }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{has when educating school children but did not find any major findings regarding }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{itself. Instead, they discovered that }%DIFDELCMD < \gls{ar} %%%
\DIFdel{did make students more excited about learning~\mbox{%DIFAUXCMD
\cite{Santos2015}}\hskip0pt%DIFAUXCMD
. It is quite likely more work could be done, exploring avenues of using }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{in anatomical classes as there has been evidence that this may improve retention of information regarding human anatomy~\mbox{%DIFAUXCMD
\cite{Akpan2019}}\hskip0pt%DIFAUXCMD
}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Dey2011, Maia2016, Yamamoto2014}}\hskip0pt%DIFAUXCMD
. In education, while the effects of X-ray vision itself remain underexplored, augmented reality has been shown to increase engagement and may support long-term retention in domains like anatomy~\mbox{%DIFAUXCMD
\cite{Akpan2019, Santos2015}}\hskip0pt%DIFAUXCMD
}\DIFaddend .

\DIFdelbegin \paragraph{\DIFdel{Video Games:}}
%DIFAUXCMD
\addtocounter{paragraph}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{research in games and entertainment appears to be rare, according to the results shown in }%DIFDELCMD < \autoref{fig:XRayVisionUseCaseDistribution}%%%
\DIFdel{~\mbox{%DIFAUXCMD
\cite{Guo2023}}\hskip0pt%DIFAUXCMD
. 
%DIF < This is likely because there is a lot of overlap between these fields and generic topics.  
However, several augmented reality video games like }\DIFdelend \DIFaddbegin \DIFadd{Entertainment provides a final but growing space for X-ray vision. While formal research is limited~\mbox{%DIFAUXCMD
\cite{Guo2023}}\hskip0pt%DIFAUXCMD
, several commercial AR games such as }\DIFaddend Microsoft’s RoboRaid \DIFdelbegin \DIFdel{~}\footnote{%DIFDELCMD < \url{https://www.microsoft.com/en-us/p/roboraid/9nblggh5fv3j?activetab=pivot:overviewtab}%%%
} %DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{and DrGrordbort's Invaders ~}\footnote{%DIFDELCMD < \url{https://world.magicleap.com/en-us/details/com.magicleap.invaders}%%%
} %DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{(}\DIFdelend \DIFaddbegin \DIFadd{and Dr. Grordborts Invaders (shown in }\DIFaddend \autoref{fig:DrGrordbortsInvaders}) \DIFdelbegin \DIFdel{do exist in the space. Gaming strives to take }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{into new territories and finds different methods of interacting with the virtual world. This lends itself to aiding usability, but the field still has a lot to learn about how }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{can and should be used}\DIFdelend \DIFaddbegin \DIFadd{have experimented with the concept, using it to merge real and virtual environments in playful, interactive ways. These applications highlight both the versatility of X-ray vision across domains and the ongoing challenges of designing visualization techniques that are perceptually effective, intuitive, and computationally feasible}\DIFaddend .

\DIFaddbegin \begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/DrGrordbortsInvaders.jpg}
    \caption[A promotional Image from Dr Grordborts Invaders made by Weta Workshop for the Magic Leap.]{\DIFaddFL{A promotional Image from Dr Grordborts Invaders made by Weta Workshop for the Magic Leap. Used with permission from Weta Workshop.}}
    \label{fig:DrGrordbortsInvaders}
\end{figure}


\DIFaddend \subsection{Hardware For X-ray Vision}
\glspl{X-ray Visualization} can be split into two key components to enable \gls{X-ray Vision}: A display to view the visualization and sensors to collect the data for \gls{X-ray Vision}.
While many studies have utilized collected information, allowing them to require only devices to collect pre-rendered devices, others have utilized a display. 
%Even then, \gls{X-ray Vision}, the type of \gls{X-ray Vision} used, tends to be influenced by the display utilized.

\DIFaddbegin \glspl{X-ray Visualization} \DIFadd{rely on two essential aspects: the availability of data representing the internal or hidden structures, and the means to visualize this data effectively. While many studies utilize pre-existing or independently collected datasets, others focus on the development and evaluation of display technologies for presenting }\gls{X-ray Vision}\DIFadd{. It is important to note that data acquisition is typically a separate process from visualization, and in most cases, data is gathered without a specific visualization method in mind.
However, the choice of data acquisition can limit the types of visualizations that can be effectively employed. For instance, real-time data acquisition methods may restrict the complexity of visualizations due to processing constraints, while pre-collected datasets allow for more intricate and computationally intensive visualization techniques.
}

\DIFaddend \begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/DispalyModesUsedForXRayVis.png}
    \caption{A bar graph of the type of display used to visualize the various \gls{X-ray Vision} effects.}
    \label{fig:DispalyModesUsedForXRayVis}
\end{figure}

\subsubsection{Displays}
% \gls{X-ray Vision} is possible on any augmented reality device, ranging from phones to head-mounted displays, but can also extend to systems of static screens in the real world that react to gesture-like commands~\cite{Park2021}. 
% In this field, many different displays have been used to create different display technologies from varying perspectives and display types. 
\gls{ar} displays come in different variations, each with its own benefits.
These different types of displays can be generalized as \gls{ost} \gls{ar}, \gls{vst} \gls{ar}, and \gls{sar}.
\gls{ost} \gls{ar} works by projecting the image on a transparent reflective display, creating a transparent display which is generally used for \glspl{hmd}.
\gls{vst} \gls{ar} displays utilize one or more cameras and display them with graphics overlaid on them. 
\gls{sar} uses projectors to place computer graphics to overlaid onto reality.
\autoref{fig:DispalyModesUsedForXRayVis} shows there are \DIFdelbegin \DIFdel{biases }\DIFdelend \DIFaddbegin \DIFadd{preferences }\DIFaddend between different types of \gls{X-ray Vision} devices to an extent due to the individual benefits and \DIFdelbegin \DIFdel{disbenefits }\DIFdelend \DIFaddbegin \DIFadd{drawbacks }\DIFaddend caused by these displays \DIFaddbegin \DIFadd{which have been laid out in }\autoref{tab:XRayVisionDeviceComparison}\DIFaddend .
\autoref{fig:ARTypesAndVisUsage}, shows real-world overlays are much more common than computer vision techniques, but real-world visual techniques are less common on monoscopic displays. 



%DIF >  Define a compact list environment for tight bullet spacing
%DIF >  Define compact list environment with sublist support
\DIFaddbegin \newlist{titemize}{itemize}{3}
%DIF >  Level 1 (top level)
\setlist[titemize,1]{label=--, left=0pt, itemsep=1pt, topsep=2pt, parsep=0pt}
%DIF >  Level 2 (sublists)
\setlist[titemize,2]{label=$\ast$, left=1.5em, itemsep=1pt, topsep=1pt, parsep=0pt}
%DIF >  Level 3 (sub-sublists, optional)
\setlist[titemize,3]{label=$\circ$, left=3em, itemsep=1pt, topsep=1pt, parsep=0pt}


\begin{table}[ht]
    \small
    \centering
    \caption{\DIFaddFL{Advantages and Disadvantages of Devices Used for X-ray Vision}}
    \label{tab:XRayVisionDeviceComparison}

    %DIF > %%%%%%%%%%%%%%%%%%%%%
    %DIF >  Device 1
    %DIF > %%%%%%%%%%%%%%%%%%%%%
    \begin{tabular}{p{0.45\linewidth}|p{0.45\linewidth}}
        \hline
        \multicolumn{2}{c}{\textbf{\DIFaddFL{OST AR Head-Mounted Displays (Optical See-Through)}}} \\
        \hline
        \textbf{\DIFaddFL{Advantages}} & \textbf{\DIFaddFL{Disadvantages}} \\
        \hline
        \begin{titemize}
            \item \DIFaddFL{Direct view of real world with overlays
            }\item \DIFaddFL{Maintains natural depth cues and peripheral vision
            }\item \DIFaddFL{Good for tasks needing situational awareness (e.g., surgery)
            }\item \DIFaddFL{Hands-free operation
        }\end{titemize}
        &
        \begin{titemize}
            \item \DIFaddFL{Limited brightness and contrast
            }\item \DIFaddFL{Smaller field of view vs. VST
            }\item \DIFaddFL{Alignment/calibration issues
            }\item \DIFaddFL{Optical distortion of virtual content
            }\item \DIFaddFL{Lower graphical fidelity
        }\end{titemize} \\
    \end{tabular}
    \vspace{1em}

    %DIF > %%%%%%%%%%%%%%%%%%%%%
    %DIF >  Device 2
    %DIF > %%%%%%%%%%%%%%%%%%%%%
    \begin{tabular}{p{0.45\linewidth}|p{0.45\linewidth}}
        \hline
        \multicolumn{2}{c}{\textbf{\DIFaddFL{VST AR Head-Mounted Displays (Video See-Through)}}} \\
        \hline
        \textbf{\DIFaddFL{Advantages}} & \textbf{\DIFaddFL{Disadvantages}} \\
        \hline
        \begin{titemize}
            \item \DIFaddFL{High graphical fidelity and brightness
            }\item \DIFaddFL{Easier integration of computer vision
            }\item \DIFaddFL{Wider field of view possible
            }\item \DIFaddFL{Flexible virtual content manipulation
        }\end{titemize}
        &
        \begin{titemize}
            \item \DIFaddFL{Camera-mediated view
            }\begin{titemize}
                \item \DIFaddFL{Latency
                }\item \DIFaddFL{Distortion
            }\end{titemize}
            \item \DIFaddFL{Lower situational awareness
            }\item \DIFaddFL{Reduced situational awareness
            }\item \DIFaddFL{Can cause motion sickness
            }\item \DIFaddFL{Heavier, bulkier hardware
        }\end{titemize} \\
    \end{tabular}
    \vspace{1em}

    %DIF > %%%%%%%%%%%%%%%%%%%%%
    %DIF >  Device 3
    %DIF > %%%%%%%%%%%%%%%%%%%%%
    \begin{tabular}{p{0.45\linewidth}|p{0.45\linewidth}}
        \hline
        \multicolumn{2}{c}{\textbf{\DIFaddFL{Mobile Devices (Phones/Tablets)}}} \\
        \hline
        \textbf{\DIFaddFL{Advantages}} & \textbf{\DIFaddFL{Disadvantages}} \\
        \hline
        \begin{titemize}
            \item \DIFaddFL{Widely available, easy to use
            }\item \DIFaddFL{Bright displays
            }\item \DIFaddFL{Portable for quick tasks
            }\item \DIFaddFL{Stable, high-quality graphics
        }\end{titemize}
        &
        \begin{titemize}
            \item \DIFaddFL{Less immersive
            }\item \DIFaddFL{Must be handheld
            }\item \DIFaddFL{Smaller screens limit detail
            }\item \DIFaddFL{Hardware fragmentation
            }\item \DIFaddFL{Limited spatial interaction
        }\end{titemize} \\
    \end{tabular}
    \vspace{1em}

    %DIF > %%%%%%%%%%%%%%%%%%%%%
    %DIF >  Device 4
    %DIF > %%%%%%%%%%%%%%%%%%%%%
    \begin{tabular}{p{0.45\linewidth}|p{0.45\linewidth}}
        \hline
        \multicolumn{2}{c}{\textbf{\DIFaddFL{Spatial Augmented Reality (SAR) / Projectors}}} \\
        \hline
        \textbf{\DIFaddFL{Advantages}} & \textbf{\DIFaddFL{Disadvantages}} \\
        \hline
        \begin{titemize}
            \item \DIFaddFL{Projects info directly on objects
            }\item \DIFaddFL{Multiple users at once
            }\item \DIFaddFL{No wearables needed
            }\item \DIFaddFL{Limited multi-user suitability
        }\end{titemize}
        &
        \begin{titemize}
            \item \DIFaddFL{Needs precise calibration/tracking
            }\item \DIFaddFL{Works best in controlled environments
            }\item \DIFaddFL{Sensitive to lighting/surface
            }\item \DIFaddFL{Limited multi-user suitability
        }\end{titemize} \\
    \end{tabular}
    \vspace{1em}

    %DIF > %%%%%%%%%%%%%%%%%%%%%
    %DIF >  Device 5
    %DIF > %%%%%%%%%%%%%%%%%%%%%
    \begin{tabular}{p{0.45\linewidth}|p{0.45\linewidth}}
        \hline
        \multicolumn{2}{c}{\textbf{\DIFaddFL{Desktop/Fixed Screens}}} \\
        \hline
        \textbf{\DIFaddFL{Advantages}} & \textbf{\DIFaddFL{Disadvantages}} \\
        \hline
        \begin{titemize}
            \item \DIFaddFL{High computational power
            }\item \DIFaddFL{Stable, high-quality graphics
            }\item \DIFaddFL{Large display for detail
            }\item \DIFaddFL{Comfortable for long use
            }\item \DIFaddFL{Good for collaboration
            }\item \DIFaddFL{Easy integration with systems
        }\end{titemize}
        &
        \begin{titemize}
            \item \DIFaddFL{Not immersive
            }\item \DIFaddFL{Stationary, low spatial interaction
            }\item \DIFaddFL{Requires looking away from real world
            }\item \DIFaddFL{Limited field of view
            }\item \DIFaddFL{Not mobile-friendly
        }\end{titemize} \\
    \end{tabular}

\end{table}

\renewlist{titemize}{itemize}{1}
\setlist[titemize,1]{}


%DIF >  \begin{table}[ht]
%DIF >      \centering
%DIF >      \caption{A table describing the Advantages and Disadvantages of Devices Used for X-ray Vision}
%DIF >      \begin{tabular}{l|l}
%DIF >          \hline
%DIF >          \textbf{Device Type} & \textbf{Advantages} & \textbf{Disadvantages} \\
%DIF >          \hline
%DIF >          OST AR Head-Mounted Displays (Optical See-Through) &
%DIF >          \begin{itemize}
%DIF >              \item Allows direct view of the real world with virtual overlays
%DIF >              \item Maintains natural depth cues and peripheral vision
%DIF >              \item Suitable for tasks requiring high situational awareness (e.g., surgery)
%DIF >              \item Hands-free operation
%DIF >          \end{itemize} &
%DIF >          \begin{itemize}
%DIF >              \item Limited brightness and contrast of virtual content
%DIF >              \item Smaller field of view of virtual objects compared to VST
%DIF >              \item Alignment/calibration issues can affect accuracy
%DIF >              \item Distortion of virtual content due to optics
%DIF >              \item Lower graphical fidelity due to transparent displays
%DIF >          \end{itemize} \\
%DIF >          \hline
%DIF >          VST AR Head-Mounted Displays (Video See-Through) &
%DIF >          \begin{itemize}
%DIF >              \item High graphical fidelity and brightness
%DIF >              \item Easier to integrate advanced computer vision techniques
%DIF >              \item Can provide a wider field of view
%DIF >              \item More flexible for virtual content manipulation
%DIF >          \end{itemize} &
%DIF >          \begin{itemize}
%DIF >              \item Real-world view is mediated by cameras (can introduce latency and distortion)
%DIF >              \item Reduced situational awareness and peripheral vision
%DIF >              \item Potential for motion sickness or discomfort
%DIF >              \item Heavier and bulkier hardware
%DIF >          \end{itemize} \\
%DIF >          \hline
%DIF >          Mobile Devices (Phones/Tablets) &
%DIF >          \begin{itemize}
%DIF >              \item Widely available and easy to use
%DIF >              \item Bright displays, good visibility
%DIF >              \item Portable for quick tasks
%DIF >              \item Stable, high-quality graphics
%DIF >          \end{itemize} &
%DIF >          \begin{itemize}
%DIF >              \item Less immersive
%DIF >              \item Must be held by user
%DIF >              \item Smaller screens limit detail
%DIF >              \item Too many different hardware specifications
%DIF >              \item Limited spatial interaction
%DIF >          \end{itemize} \\
%DIF >          \hline
%DIF >          Spatial Augmented Reality (SAR) / Projectors &
%DIF >          \begin{itemize}
%DIF >              \item Overlays info directly onto physical objects
%DIF >              \item Multiple users can view simultaneously
%DIF >              \item No wearable device needed
%DIF >          \end{itemize} &
%DIF >          \begin{itemize}
%DIF >              \item Requires precise calibration/tracking
%DIF >              \item Best in controlled environments
%DIF >              \item Limited by lighting and surface properties
%DIF >              \item Not suitable for multiple users.
%DIF >          \end{itemize} \\
%DIF >          \hline
%DIF >          Desktop/Fixed Screens &
%DIF >          \begin{itemize}
%DIF >              \item Allows for high computational power
%DIF >              \item Stable, high-quality graphics
%DIF >              \item Large display area for detail
%DIF >              \item Comfortable for long-term use
%DIF >              \item Good for collaborative tasks
%DIF >              \item Easily integrated with existing systems
%DIF >          \end{itemize} &
%DIF >          \begin{itemize}
%DIF >              \item Not immersive
%DIF >              \item Stationary, limited spatial interaction
%DIF >              \item Requires user to look away from real world
%DIF >              \item Limited field of view for spatial tasks
%DIF >              \item Not suitable for mobile tasks
%DIF >          \end{itemize} \\
%DIF >          \hline
%DIF >      \end{tabular}
%DIF >      \label{tab:XRayVisionDeviceComparison}
%DIF >  \end{table}

\DIFaddend \gls{X-ray Vision} systems can be supported by display devices, including mobile phones, head-mounted displays, computer screens, and projectors~\cite{Kim2018}. Devices range from traditional computer screens and mobile personal devices, head-mounted displays to projectors. 
Most earlier devices only accommodate monocular vision, while recent head-mounted displays can support stereoscopic vision. \autoref{fig:LitRevewDeviceFrequencyPlot} shows that the choice of technology is often a result of technological advances and emerging products. It can be observed that mobile screen-based displays were most popular following the release of powerful mobile phones in 2007, underpinned by emerging computer vision techniques for tracking and the generation of visualizations. Similarly, stereoscopic head-mounted displays have gained popularity because of increased capabilities, technology maturity, and general availability in recent years.

Recently, research utilizing Microsoft HoloLens 1 and 2 has been widely used in \gls{X-ray Vision} research because of their advanced display tech and gesture recognition, making them a useful device for security and medical operations~\cite{Phillips2020, Martin-Gomez2021, Rodrigues2017, AlJanabi2020}. \glspl{hmd} shine in medical settings, offering a sterile work environment. Medical applications use screens for remote interactions, like controlling robotic arms in minimally invasive surgery~\cite{Kalia2019}. In contrast, the construction and maintenance industries employ diverse devices due to their varied environments~\cite{Muthalif2022, Eren2013, Eren2018, Becher2021}.

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ARTypesAndVisUsage.png}
    \caption{A bar graph displaying The types of \gls{ar} that was used to visualize the different types of \gls{X-ray Vision} effects.}
    \label{fig:ARTypesAndVisUsage}
\end{figure}


\DIFdelbegin %DIFDELCMD < \autoref{fig:LitRevewDeviceFrequencyPlot} %%%
\DIFdel{shows SAR techniques are a relatively novel addition to }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{, where only a few exemplars have been published. 
Both }%DIFDELCMD < \autoref{fig:DispalyModesUsedForXRayVis} %%%
\DIFdel{and }%DIFDELCMD < \autoref{fig:ARTypesAndVisUsage} %%%
\DIFdel{show the flexibility of }%DIFDELCMD < \gls{sar} %%%
\DIFdel{displays tend to be very flexible, allowing and possess both the ability to utilize Real real-world overlays, computer-enabled Enabled Techniques and visualization effects. 
In contrast to head-mounted or fixed display devices, }%DIFDELCMD < \gls{sar} %%%
\DIFdel{technologies can augment the physical environment in situ without obstructing the user’s field of view~\mbox{%DIFAUXCMD
\cite{Heinrich2019, Heinrich2019b}}\hskip0pt%DIFAUXCMD
. However, accurate calibration and tracking of the user’s vantage point are essential for high-precision }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{. Generally, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{SAR-based systems are uncommon but have been done in medical settings and settings where the user is relatively stationary and needs an unobscured vision~\mbox{%DIFAUXCMD
\cite{Heinrich2019}}\hskip0pt%DIFAUXCMD
.
}\DIFdelend %DIF > %% Toms notes I revised this section to remove some of the repetition and make it clearer.
%DIF >  Some froms of AR do not require wearable devices at all. Spatial Augmented Reality (\gls{sar}) uses projectors to overlay digital content onto physical objects and surfaces in the real world.
%DIF >  %Creating an immersive experience without the need for head-mounted displays or handheld devices.
%DIF >  \autoref{fig:LitRevewDeviceFrequencyPlot} shows SAR techniques are a relatively novel addition to \gls{X-ray Vision}, where only a few papers have been published however, they are one of the few area's of x-ray vision that has been adopted by industry Augmetrics~\footnote{\url{https://augmedics.com/}} has created a system that utilizes \gls{sar} to project medical insuctions onto a patient's body, allowing surgical staff to gain insight into the body without looking away from the patient.
%DIF >  Both \autoref{fig:DispalyModesUsedForXRayVis} and \autoref{fig:ARTypesAndVisUsage} show the flexibility of \gls{sar} displays tend to be very flexible, allowing and possess both the ability to utilize Real real-world overlays, computer enabled techniques and visualization effects. 
%DIF >  In contrast to head-mounted or fixed display devices, \gls{sar} technologies can augment the physical environment in situ without obstructing the user’s field of view~\cite{Heinrich2019, Heinrich2019b}. However, accurate calibration and tracking of the user’s vantage point are essential for high-precision \gls{X-ray Vision}. 
%DIF >  Generally, \gls{X-ray Vision} SAR-based systems are uncommon but have been done in medical settings and settings where the user is relatively stationary and needs an unobscured vision~\cite{Heinrich2019}.

\subsubsection{Comparison of Displays} \label{sec:ComparisionOfDisplays}
Comparing the effectiveness of different displays seems to be a relatively new area of study for \gls{ar} enabled \gls{X-ray Vision}.
Prior to starting this dissertation in 2019, our review found no studies that compared \glspl{X-ray Visualization} between different papers.
Since then, several studies have published their results on comparing \gls{vst} \gls{ar} devices against \gls{ost} \gls{ar} devices.

Gruenefeld et al.~\cite{Gruenefeld2020} performed depth perception studies using an OST headset utilizing \gls{X-ray Vision}, including Grid\DIFdelbegin \DIFdel{and }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend Cut-out effects. 
The grid effect, placed on the ground \DIFaddbegin \DIFadd{within the X-ray able space }\DIFaddend and a perpendicular wall \DIFaddbegin \DIFadd{facing the viewer}\DIFaddend , could be used to approximate a relationship between the wall and the object. In contrast, the cut-out visualization provided a hole in the wall, which the user could look through to see where the object was on the other side, against a baseline that pointed participants to the object with a red arrow and displayed a perpendicular line going from the far point of the arrow to the bottom of the visualization. 
Gruenefeld et al.'s~\cite{Gruenefeld2020} found that the weakest of the depth cues were the \DIFdelbegin \DIFdel{wireframe and the }\DIFdelend cut-out. This was because \DIFdelbegin \DIFdel{neither of these cues conveyed }\DIFdelend \DIFaddbegin \DIFadd{the cues did not convey a }\DIFaddend clear depth indications to the user~\cite{Gruenefeld2020}\DIFaddbegin \DIFadd{, because the grid effect provided a clear indication of depth, allowing the users to count the amount of grid squares between the object and the wall they had a better sense of depth}\DIFaddend .

Another set of studies published by Martin-Gomez et al.~\cite{MartinGomez2021} studied the difference between \DIFdelbegin \DIFdel{five }\DIFdelend \DIFaddbegin \DIFadd{four }\DIFaddend \glspl{X-ray Visualization} (None (Superimposition), virtual hole, ghosting, and Random Dot) on both \gls{vst} \gls{ar} and \gls{ost} \gls{ar} devices in the near field. 
They found that users better utilized \gls{X-ray Vision} on a \gls{vst} \gls{ar} headset than on an \gls{ost} \gls{ar} device.
This prompted another study investigating different rendering techniques for \gls{X-ray Vision} (shading, hatching, ghosting) and brightness levels, finding that bright, clear objects work best in \gls{ost} \gls{ar}.

Heinrich et al.'s~\cite{Heinrich2022} research comparing \gls{ost} \gls{ar} displays to \gls{sar} displays (previously mentioned in \autoref{sec:realWorldOverlay}) presented findings that both \gls{ost} \gls{ar} and \gls{sar} displays functioned in a similar fashion. 
%DIF > However, they did find that \gls{X-ray Vision} works better with \gls{ost} \gls{ar} displays while \gls{ui} elements (\autoref{fig:Heinrich2022} Glyph Vis) tend to function better when using projectors.
However, they \DIFdelbegin \DIFdel{did find }\DIFdelend \DIFaddbegin \DIFadd{found }\DIFaddend that \gls{X-ray Vision} \DIFdelbegin %DIFDELCMD < \autoref{fig:Heinrich2022} %%%
\DIFdel{See Though }\DIFdelend works better with \gls{ost} \gls{ar} displays\DIFaddbegin \DIFadd{, }\DIFaddend while \gls{ui} elements (\DIFaddbegin \DIFadd{see }\DIFaddend \autoref{fig:Heinrich2022}\DIFaddbegin \DIFadd{, }\DIFaddend Glyph Vis) tend to function better when using projectors.
These different conditions can be seen and compared in \autoref{fig:Heinrich2022}\DIFaddbegin \DIFadd{.
}\DIFaddend %The details of this research were previously mentioned in \label{sec:realWorldOverlay}.

\begin{figure}
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\columnwidth]{Chapter2/Images/LitRevewDeviceFrequencyPlot.png}
%DIFDELCMD <     \caption[A graph showing the prevalence of various \gls{ar} devices across the various use cases found in \gls{X-ray Vision}.]{%%%
\DIFdelFL{A graph showing the prevalence of various }%DIFDELCMD < \gls{ar} %%%
\DIFdelFL{devices }\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\columnwidth]{Chapter2/Images/LitRevewDeviceFrequencyPlotBar.pdf}
    \caption[Devices examined for X-ray vision across the literature over time.]{\DIFaddFL{Devices examined for X-ray vision }\DIFaddendFL across the \DIFdelbeginFL \DIFdelFL{various use cases found in }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdelendFL \DIFaddbeginFL \DIFaddFL{literature over time}\DIFaddendFL . \DIFdelbeginFL \DIFdelFL{Head-mounted displays used as display devices are grouped. Portable screens like phones and tablets}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Each point represents a publication}\DIFaddendFL , \DIFdelbeginFL \DIFdelFL{larger screens, }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{categorized by device type (vertical axis) }\DIFaddendFL and \DIFdelbeginFL \DIFdelFL{magic mirrors fall under the 'Screens' category}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{year of appearance (horizontal axis)}\DIFaddendFL . \DIFdelbeginFL \DIFdelFL{Additionally}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{The distribution highlights how research interest has shifted across different device categories}\DIFaddendFL , \DIFdelbeginFL \DIFdelFL{three user studies utilized Images }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{with increased attention to head-mounted displays }\DIFaddendFL and \DIFdelbeginFL %DIFDELCMD < \gls{sar}%%%
\DIFdelendFL \DIFaddbeginFL \DIFaddFL{mobile devices in more recent years}\DIFaddendFL .}
    \label{fig:LitRevewDeviceFrequencyPlot}
\end{figure}

\subsubsection{Sensors Utilized}

%An IoT metaverse is created by a collection of sensors located in various devices and around the user's environment, if not the entire world~\cite{Asif2023, } (Asif and Hassan 2003; Blanco-Novoa et al. 2020). Not only do these devices need to be calibrated to work together, but they also need to be able to visualize a world connected that is comprehendible to its users (Pereira et al. 2019). None of the literature used this as a whole, but several different techniques have been created to visualize \gls{X-ray Vision}. 

The information used in \gls{X-ray Vision} applications may possess different characteristics, such as the nature of the data, its temporal characteristics, and its realism. Three-dimensional models, point clouds, video feeds and photos, medical data, and depth maps are examples of the diversity of data visualized in \gls{X-ray Vision}. Moreover, one can distinguish static information, which remains unchanged for a task, from dynamic information, which may change during a task. Finally, the degree of realism of the data can vary. 

\autoref{fig:LitReviewDevicesBeingUsed} illustrates that static information prevails as the most common virtual element in \gls{X-ray Vision}, constituting 65\% of the 54 papers examined. Examples include 3D models, medical images, and building schematics, predominantly found in medical, construction, and maintenance domains. Virtual objects range from simple geometric shapes to intricate representations of real objects~\cite{Becher2021, Eren2013, Eren2018, Muthalif2022, Zollmann2014}.

%DIF >  Dynamic data, as depicted in \autoref{fig:LitReviewDevicesBeingUsed}, often comprises video feeds from static or mobile cameras. This technique, capturing information from multiple perspectives, enables users to perceive distant or hidden details while viewing the actual environment simultaneously. Security and robotic system control applications benefit from these methods~\cite{Erat2018, Phillips2021}. The real-time delivery of camera-captured information mirrors the real world, with non-dynamic data commonly used in construction to model underground pipes. This addresses the challenge of communicating the position of underground structures to end users~\cite{Becher2021, Eren2018, Eren2013, Muthalif2022, Zollmann2014}. Notably, \gls{X-ray Vision} experiments in construction consistently utilize simulated data, aiming to replicate real-world scenarios.
\DIFaddbegin 

\DIFaddend Dynamic data, as \DIFdelbegin \DIFdel{depicted }\DIFdelend \DIFaddbegin \DIFadd{shown }\DIFaddend in \autoref{fig:LitReviewDevicesBeingUsed}, \DIFdelbegin \DIFdel{often comprises }\DIFdelend \DIFaddbegin \DIFadd{typically consists of }\DIFaddend video feeds from static or mobile cameras. \DIFdelbegin \DIFdel{This technique, capturing information from multiple perspectives , enables }\DIFdelend \DIFaddbegin \DIFadd{By providing multiple perspectives in real time, these feeds allow }\DIFaddend users to perceive distant or hidden details while \DIFdelbegin \DIFdel{viewing }\DIFdelend \DIFaddbegin \DIFadd{still observing }\DIFaddend the actual environment\DIFdelbegin \DIFdel{simultaneously. Security }\DIFdelend \DIFaddbegin \DIFadd{, a capability that has proven valuable in domains such as security }\DIFaddend and robotic system control\DIFdelbegin \DIFdel{applications benefit from these methods}\DIFdelend ~\cite{Erat2018, Phillips2021}. \DIFdelbegin \DIFdel{The real-time delivery of camera-captured information mirrors the real world, with }\DIFdelend \DIFaddbegin \DIFadd{In contrast, }\DIFaddend non-dynamic data \DIFdelbegin \DIFdel{commonly used in constructionto model underground pipes . This addresses }\DIFdelend \DIFaddbegin \DIFadd{is often employed in construction, where pre-recorded or modelled information is used to represent underground pipes and other hidden infrastructure~\mbox{%DIFAUXCMD
\cite{Becher2021, Eren2018, Eren2013, Muthalif2022, Zollmann2014}}\hskip0pt%DIFAUXCMD
. Within this context, }\gls{X-ray Vision} \DIFadd{studies in construction have relied primarily on simulated data to approximate real-world conditions and to address }\DIFaddend the challenge of \DIFdelbegin \DIFdel{communicating the position }\DIFdelend \DIFaddbegin \DIFadd{clearly communicating the location }\DIFaddend of underground structures to end users\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{Becher2021, Eren2018, Eren2013, Muthalif2022, Zollmann2014}}\hskip0pt%DIFAUXCMD
.
Notably, }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{experiments in construction consistently utilize simulated data, aiming to replicate real-world scenarios.
}\DIFdelend \DIFaddbegin \DIFadd{.
}\DIFaddend 

Most of the research that utilizes live recordings in this review employed cameras or medical equipment (shown in \autoref{fig:LitReviewDevicesBeingUsed}). No studies were found that visualized radar and other sensor data in a 3D manner to create an \gls{X-ray Vision} effect—limiting the types of visualizations that people were viewing. Techniques like photogrammetry were not found either which would be able to recreate images from several views into a 3D scene~\cite{Nguyen2013, Pereira2019}. This \DIFdelbegin \DIFdel{leads us to believe that}%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{, to this point, mainly focuses on visualizing }\DIFdelend \DIFaddbegin \DIFadd{suggests that, so far, }\gls{X-ray Vision} \DIFadd{has primarily focused on presenting users with visualizations of }\DIFaddend the known world\DIFdelbegin \DIFdel{to the user rather than looking into having the system visualize a black box}\DIFdelend \DIFaddbegin \DIFadd{. In contrast, little work has explored systems that could reveal entirely unknown or hidden environments in real time, effectively giving users access to true X-ray vision}\DIFaddend .

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/LitReviewDevicesBeingUsed.png}
    \caption[A plot showing nine devices found in the literature.]{A plot showing nine devices found in the literature. Head-mounted displays used as display devices are grouped. Portable screens like phones and tablets, larger screens, and magic mirrors fall under the 'Screens' category. Additionally, three user studies utilized Images and \gls{sar}.}
    \label{fig:LitReviewDevicesBeingUsed}
\end{figure}


%DIF >  An explanation of Augmented Reality
\section{\DIFdelbegin \DIFdel{Illustrative Rendering Techniques}\DIFdelend \DIFaddbegin \DIFadd{Perception and Depth Perception tasks in Augmented Reality (AR) and Virtual Reality (VR) Head Mounted Displays}\DIFaddend }
\DIFdelbegin \DIFdel{Cutting and Vishton~\mbox{%DIFAUXCMD
\cite{Vishton1995} }\hskip0pt%DIFAUXCMD
claim there are several real-world elements to create depth perception , which then need to be adapted for virtual displays}\DIFdelend %DIF >  This section is talking about the issues of depth perception in oST AR
%DIF >  The ability to modify our visual perception of the world allowed for an almost limitless amount of possibilities of changes that are possible to augment people's vision. 
%DIF >  However, this required us to learn about the limitations of this perception beyond what we could see naturally. 
%DIF >  The most major form of this was to gain a concrete understanding of how depth worked when displaying a 2D image. 
%DIF >  The visual impacts that have been caused by these devices were caused by the camera lenses don't see the world the same way as people do, so the displays would need to appear~\cite{Cutting1997}.
\DIFaddbegin 

\DIFadd{Depth perception on }\gls{mr} \glspl{hmd} \DIFadd{has been a goal for over 25 years}\DIFaddend ~\cite{Cutting1997}.
\DIFdelbegin \DIFdel{However, artists have been able to establish depth perception even when illustrating non-realistic environments by using "Just Enough Reality"~\mbox{%DIFAUXCMD
\cite{Siegel2000} }\hskip0pt%DIFAUXCMD
to determine depth accurately. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The latter part of this thesis looks at using }%DIFDELCMD < \glspl{virt} %%%
\DIFdel{as a method of }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{. 
This section is going to look at how these effects have previously been used, what their impact has been on computer science, and what their utility has been when using }%DIFDELCMD < \gls{mr} %%%
\DIFdel{devices.
While this Thesis only looks into a subsection of illustrative techniques, limiting itself to either Hatching, Stippling, or Halos, the actual definition of this is broader~\mbox{%DIFAUXCMD
\cite{Lawonn2018}}\hskip0pt%DIFAUXCMD
.
Illustrative techniques can also include the scope of non-photorealistic rendering, like using cel-shading and the deformation of video footage to look like it was produced by a pencil or paintbrush~\mbox{%DIFAUXCMD
\cite{Lawonn2018}}\hskip0pt%DIFAUXCMD
. 
This section will also examine some user studies that have investigated this effectiveness~\mbox{%DIFAUXCMD
\cite{Lawonn2018}}\hskip0pt%DIFAUXCMD
. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The largest examples of illustrative effects being used can be seen in scientific textbooks like Gray's anatomy~\mbox{%DIFAUXCMD
\cite{gray1877anatomy}}\hskip0pt%DIFAUXCMD
.
This textbook utilized hatching's ability to communicate texture and depth using a black-and-white image.
These images were collected over years of diagramming the human body by directing unclaimed bodies from workhouses and mortuaries. Due to their clarity and accuracy, they are still widely used today. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend %DIF > \gls{mr} \glspl{hmd} tend to be quite limited in methods that are possible to use to measure how a participant can measure depth in the real world, which has prompted 
\DIFaddbegin \DIFadd{Several common methods of testing depth perception in the real world. These include 
}\begin{itemize}
    \item \textbf{\DIFadd{Blind Walking or Blind reaching:}} \DIFadd{Asking a participant to place their hands or to walk to a location where a virtual artifact was previously~\mbox{%DIFAUXCMD
\cite{Jamiy2019, Jamiy2019b}}\hskip0pt%DIFAUXCMD
;
    }\item \textbf{\DIFadd{Verbal Reporting:}} \DIFadd{Requesting the participant tell you how far away the virtual object is from them~\mbox{%DIFAUXCMD
\cite{Jamiy2019, Jamiy2019b}}\hskip0pt%DIFAUXCMD
;
    }\item \textbf{\DIFadd{Matching Protocols: }} \DIFadd{Placing a virtual object relive to where the virtual object is (or was)~\mbox{%DIFAUXCMD
\cite{Jamiy2019, Jamiy2019b}}\hskip0pt%DIFAUXCMD
;
    }\item \textbf{\gls{twofc_g}\DIFadd{:}} \DIFadd{Giving the participant where there is one correct and wrong answer on a set of conditions that will get closer to being equal to determine at what point can participants no longer determine proper depth perception~\mbox{%DIFAUXCMD
\cite{Otsuki2017, Fan1996}}\hskip0pt%DIFAUXCMD
.
}\end{itemize}
%DIF > Using these types of studies 
%DIF >  Start talking about the early work trying to understand depth in AR and VR
%DIF > early work in this field focused on testing how seeing graphics rendered using virtual reality headsets may have changed. 
%DIF > It also began understanding how real-world factors impact the depth perception of the environment~\cite {Ellis1998}.
\DIFaddend Early work in \DIFdelbegin \DIFdel{the field by Interrante et al.~\mbox{%DIFAUXCMD
\cite{Interrante1995, Interrante1997, Interrante1997a} }\hskip0pt%DIFAUXCMD
looked at how illustrative effects can aid the perception of transparent objects.
Transparent objects make it difficult to understand the exact surface of the shape that a transparent object is formed as.
This was done by pre-computing textures that used transparent and opaque regions.The first work was done by creating several different textures, including multiple methods that depict valleys and ridges, grids, and curvature information~\mbox{%DIFAUXCMD
\cite{Interrante1995}}\hskip0pt%DIFAUXCMD
. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To improve the following, Itterante~\mbox{%DIFAUXCMD
\cite{Interrante1997a} }\hskip0pt%DIFAUXCMD
created a visualization that utilized valleys, ridges, and curvature to explain the objects' flow. 
This texture was calculated by drawing lines around the parts of the mesh with the highest curvature and having themmove toward the ridges and valleys of the shapes.
We created a texture that could be viewed from all sides, requiring less computation when the object is viewed from different angles. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The textures mentioned in Interrante }\DIFdelend \DIFaddbegin \DIFadd{this field focused on testing how seeing graphics rendered using virtual reality headsets may have changed, and how real-world factors impact the depth perception of the environment~\mbox{%DIFAUXCMD
\cite{Ellis1998}}\hskip0pt%DIFAUXCMD
.
Ellis et al.~\mbox{%DIFAUXCMD
\cite{Ellis1998} }\hskip0pt%DIFAUXCMD
Evaluated the difference between monocular vision, binocular vision, and a stereoscopic display utilizing a rotating display where participants were asked to judge the depths they saw different virtual objects. 
To do this, they had participants place and verbally estimate positions in which real or virtual objects were away from them.
Overall, Ellis }\DIFaddend et al.'s~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Interrante1997} }\hskip0pt%DIFAUXCMD
work were later tested with a tipping and a grid-based pattern on each. 
A user study was done to determine if the direction of the effect or opacity affected users' ability to determine the shape of the surface.
At the same time, the participant viewed the graphics on a stereoscopic display. 
This study tested whether participants could accurately determine the closest surface of one noisy sphere to another inside of it }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Ellis1998} }\hskip0pt%DIFAUXCMD
study found a high amount of work stating that both binocular and stereoscopic displays provided excellent depth perception when compared to monocular displays. 
They also noted that when virtual content was displayed behind a real-world object, it created the same mismatch, about 6cm closer to the viewer/participant than it was displayed. 
Most depth estimations were within 2cm of the actual position~\mbox{%DIFAUXCMD
\cite{Ellis1998}}\hskip0pt%DIFAUXCMD
}\DIFaddend .
\DIFdelbegin \DIFdel{The analysis did show that texturing the object improved depth perception, but there was no significant difference if participants could determine the closer shell of an object. 
}\DIFdelend 

\DIFdelbegin \subsubsection{\DIFdel{Hatching}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The hatching which was developed in Interrante }\DIFdelend \DIFaddbegin \DIFadd{McCandless }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Interrante1997} }\hskip0pt%DIFAUXCMD
was later extended by Hertzmann and Zorin~\mbox{%DIFAUXCMD
\cite{Hertzmann2000}}\hskip0pt%DIFAUXCMD
.
Hertzmann and Zorin~\mbox{%DIFAUXCMD
\cite{Hertzmann2000} }\hskip0pt%DIFAUXCMD
developed an algorithm that could translate hatching over to smooth surfaces by using a piecewise smooth subdivision to reconstruct a smooth surface from the mesh to compute the necessary qualities.
This allowed for a surface-based rendering technique that worked much like a shadow but also thinned the lines to the point of being invisible when they were in the direct view of the camera. 
They then used a combination of noise generation and denoising functions to create human errors that would be seen in a work of art. 
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{McCanless2000} }\hskip0pt%DIFAUXCMD
followed up this study by adding both motion and a time delay on }\glspl{hmd}\DIFadd{.
People moving their heads causes motion parallax, which allows for a sense of depth between objects. 
McCandless et al.~\mbox{%DIFAUXCMD
\cite{McCanless2000} }\hskip0pt%DIFAUXCMD
control study found that when the virtual object was moved over a meter away, movements caused a noticeable drop in depth perception, which was shown to be understated compared to their motion parallax study.
This demonstrated that the worse the experience was, the more users moved around and noted 
 an increase in the large time delay between their head movements and the interaction time.
}\DIFaddend 

\DIFdelbegin \DIFdel{The system that Hertzmann and Zorin~\mbox{%DIFAUXCMD
\cite{Hertzmann2000} }\hskip0pt%DIFAUXCMD
created was not designed for real-time interactions.
This means that even simple actions like rotating around the model are not possible.
Praun }\DIFdelend \DIFaddbegin \DIFadd{Rolland }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Praun2001} }\hskip0pt%DIFAUXCMD
pre-generated a tonal art map based on different levels and then used these tonal art maps to determine the direction of the hashes before drawing them on the objects themselves.
This system allowed for a wide variety of different configurations}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Rolland2002} }\hskip0pt%DIFAUXCMD
looked into the perception of different shapes (cubes, octahedrons, and cylinders) when viewed from a }\gls{hmd}\DIFadd{.
They did this by presenting these shapes in several different sizes and testing if participants could determine what shapes were closer to them using a }\gls{twofc} \DIFadd{study design.
This study design only found a little change between different designed shapes}\DIFaddend .

\DIFdelbegin \DIFdel{This method of hatching was then furthered by Pelt et al.~\mbox{%DIFAUXCMD
\cite{Pelt2008} }\hskip0pt%DIFAUXCMD
and applied to an iso-surface representing }%DIFDELCMD < \gls{ct} %%%
\DIFdel{data.
Their algorithm was modified to consist of just a geometry shader rather than a fragment, removing the need for preprocessing the hatching. 
Rather, this system is able to compute the curvature of the iso-surface and an appropriate direction for the hashing in real time while providing a relatively fast frame, which would then create a textured stripe. 
}\DIFdelend \DIFaddbegin \DIFadd{Later on, Mather and Smith.~\mbox{%DIFAUXCMD
\cite{Mather2004} }\hskip0pt%DIFAUXCMD
investigated a method to determine if using multiple depth cues could improve depth perception.
The depth cues investigated in this experiment were Contrast, Blur, and Interpolation. 
All possible different combinations of these conditions were used. 
This experiment was done using a computer monitor. Participants saw many different textures displayed on four planes partially overlapping each other, each with a different depth cue that could be used as an aid. 
Participants would click on all the different textures from nearest to furthest to determine where an item should be placed. 
This experiment showed that participants could most easily tell where objects were with all three cues, but they struggled with the other cues, especially interpolation. 
}\DIFaddend 

\DIFdelbegin \DIFdel{Another system was created by Lawonn et al. \mbox{%DIFAUXCMD
\cite{Lawonn2013}}\hskip0pt%DIFAUXCMD
, which could run at even faster rates than Pelt et al.'s~\mbox{%DIFAUXCMD
\cite{Pelt2008} }\hskip0pt%DIFAUXCMD
work as long as it receives extensive pre-processing. 
It first identifies key regions: contours, defined by surface normal and view vector perpendicularity, and feature regions, identified by maxima and minima in the mean curvature field.
Then, the direction of the lines is calculated directly from the direction of the curvature.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Hatching effects are also closely tied to brush-like effects, as they require the system to understand brush direction and stroke size. 
Gerl and Isenberg~\mbox{%DIFAUXCMD
\cite{Gerl2012} }\hskip0pt%DIFAUXCMD
then furthered the possible interactions of hatching and painterly effects. 
This technique preprocessed a }%DIFDELCMD < \gls{classifier_g} %%%
\DIFdel{to segment areas of the 3D mesh, then it used a }%DIFDELCMD < \gls{regression_analysis} %%%
\DIFdel{to choose the most appropriate direction of the stroke directions.
To help aid the AI methods, users were also given several interactions that allowed them to reconfigure the angle and direction of the effect~\mbox{%DIFAUXCMD
\cite{Gerl2012}}\hskip0pt%DIFAUXCMD
. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Lawonn }\DIFdelend \DIFaddbegin \DIFadd{While a lot of work happening at this time was focused on the far-plane, Wither }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Lawonn2017} }\hskip0pt%DIFAUXCMD
furthered this technique and paired it with a visualization of a cylinder, making the illustrative effects inside of it more apparent than the effects outside.
The cylinder gave a similar impression to }%DIFDELCMD < \gls{X-ray Vision}%%%
\DIFdel{, where the illustrative effects in the cylinder were clear and easy to see, while the effects outside the cylinder were duller.  
The hatching was modified to work on a set of vessels, and the caps of all the vessels and each locationwhere the vessels split were identified so the vessels could be rendered differently. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Lawonn et al.~\mbox{%DIFAUXCMD
\cite{Lawonn2017} }\hskip0pt%DIFAUXCMD
then ran a study comparing their version of hatching to it with pseudo-chroma depth rendering and Phong shading. 
Users were asked to define the model's depth tips of two vessels. 
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Wither2005} }\hskip0pt%DIFAUXCMD
looked at methods that make virtual objects appear further away rather than just smaller.
A Sony Glassatron was used which was a common monoscopic }\gls{hmd} \DIFadd{of the time, but they lacked basic depth cues. 
Their study utilized flat planes as showdowns in the virtual world, giving users an on-screen map to view items with and coloring the markers so they appeared in the correct positions.  
They would have users view objects that were 38, 55, and 65 meters away, and users would have to guess their location. 
This was done using a group of objects and using individual objects. 
}\DIFaddend This study showed that \DIFdelbegin \DIFdel{while participants performed faster with the pseudo-chroma depth performed, they were more accurate at assessing the distances and felt more confident in their answers. 
Showing that hatching may allow for better depth perception. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsubsection{\DIFdel{Stippling}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdel{Lu }\DIFdelend \DIFaddbegin \DIFadd{shadows and size were the best indicators while changing the color was the worst depth cue Wither }\DIFaddend et al.\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{Lu2002} }\hskip0pt%DIFAUXCMD
furthered the stippling techniques shown by Interrante }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Wither2005} }\hskip0pt%DIFAUXCMD
tested. 
}

\DIFadd{Armbrüster }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Interrante1997}}\hskip0pt%DIFAUXCMD
. 
By looking at the curvature of the model, finding localized curvature of 3D models, and spacing out the dots between the various pixels on the screen. This effect created a realistic stippling effect for 2D images. 
    }%DIFDELCMD < 

%DIFDELCMD < \begin{figure}[tb]
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/PasterAndStrotthote.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[Examples detailing the stippling algorithm created by Pastor and Strotthote~\cite{Pastor2004}.]{%
{%DIFAUXCMD
\DIFdelFL{Examples detailing the stippling algorithm created by Pastor and Strotthote~\mbox{%DIFAUXCMD
\cite{Pastor2004}}\hskip0pt%DIFAUXCMD
. The top of this image shows how the stippling subdivision is implemented using a graph function. The bottom image shows an example of how this stippling appears when it is applied to the target object (The bones representing a human hand). Used with permission from IEEE \textcopyright{} 2004.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:PasterAndStrotthote}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{An issue with drawing the dots for stippling was that both Lu et al.'s~\mbox{%DIFAUXCMD
\cite{Lu2002} }\hskip0pt%DIFAUXCMD
found that the distance between the dots requires red based on a noise-based function. It was important spacestippling randomly but evenly distributed. 
One solution for this was created by Pastor and Strothotte~\mbox{%DIFAUXCMD
\cite{Pastor2004}}\hskip0pt%DIFAUXCMD
.
Their version of stippling created a 3D Voronoi pattern over the 3D model, then a graph would then be created linking the starting point of }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Armbruster2008} }\hskip0pt%DIFAUXCMD
worked on determining what elements the virtual reality headset displayed that affected the participants' depth perception. 
This included: 
}\begin{itemize}
    \item \DIFadd{The virtual environment would change between three different environments: one had no graphics shown as part of the world, one showed the world as a meadow, and the final one was a large but enclosed gray room. 
    }\item \DIFadd{participants were asked to guess a total of ten different distances. Four of these were in the near-field space, and six of these were in the action space.
    }\item \DIFadd{They would also toggle a ruler on the ground that would showcase the distance from themselves in meters.
}\end{itemize}
\DIFadd{They also had two tasks, one where participants would either need to be able to see spheres located at }\DIFaddend all of the \DIFdelbegin \DIFdel{dots which shared a boundary.
This allowed for a seamless decline in the number of }%DIFDELCMD < \glspl{voxel_g} %%%
\DIFdel{shown as they would be separated into groups based on the parent-child relationship seen in the upper part of }%DIFDELCMD < \autoref{fig:PasterAndStrotthote}%%%
\DIFdel{. 
This enabled the even stippling thresholds seen in the lower part of }%DIFDELCMD < \autoref{fig:PasterAndStrotthote}%%%
\DIFdelend \DIFaddbegin \DIFadd{different distances, or they could only see a single sphere at a time.
This research did not provide any clear indication of whether any of these conditions were able to be observed, but they did note that participants underestimated the distances of the objects they were viewing and that users had a better sense of depth when objects were closer to them}\DIFaddend .

Another \DIFdelbegin \DIFdel{way of creating even stippling while allowing for different angles is to utilize a geometry shader to further subdivide the mesh. 
This technique was initially proposed by Meruvia and Pastor\mbox{%DIFAUXCMD
\cite{MeruviaPastor2002}
}\hskip0pt%DIFAUXCMD
By doing this, you can subdivide each polygon to give each polygon a set number of dots within it and evenly distribute the dots inside of each polygon. 
This system operates under the assumption that areas with more polygons will require a higher density of dots, whereas flat areas will not~\mbox{%DIFAUXCMD
\cite{MeruviaPastor2002}}\hskip0pt%DIFAUXCMD
. 
}\DIFdelend \DIFaddbegin \DIFadd{study that examined the virtual environment's effect on participants was performed by Murgia and Sharkey~\mbox{%DIFAUXCMD
\cite{Murgia2009}}\hskip0pt%DIFAUXCMD
. 
This study looked solely into how people perceive depth within the virtual space. 
To do this, they created a life-sized virtual environment using a }\gls{cave}\DIFadd{, designed to work with stereoscopic glasses and react to users moving within the virtual environment. 
They tested a range of conditions, including two levels of graphical quality, one where the environment was bland and one where 1-meter objects would appear. 
They introduced real-world reference objects to help the participants understand the correct depth at which their virtual counterparts would be located. 
This study found that the clearer the virtual environment was at displaying depth, the easier the user could determine depth within it. 
}\DIFaddend 


\DIFdelbegin \DIFdel{Ma et al.~\mbox{%DIFAUXCMD
\cite{Ma2018} }\hskip0pt%DIFAUXCMD
later created a system to motivate further parts that belonged to be able to move but also by utan utilizing a similar pre-computation to Paster and Strotthote\mbox{%DIFAUXCMD
\cite{Pastor2004}}\hskip0pt%DIFAUXCMD
.
The dot was placed using blue noise inside the Voronoi, adjusted in size to varying levels , and adjusted in tones based on where they appeared in parallel on the GPU. 
This type of stippling allowed for the effect to be placed realistically onto 3D models even as they were changing their shapes. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsubsection{\DIFdel{Halo}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdel{Outlines~\mbox{%DIFAUXCMD
\cite{Bui2015}}\hskip0pt%DIFAUXCMD
, Boundry Enhancements~\mbox{%DIFAUXCMD
\cite{Svakhine2003}}\hskip0pt%DIFAUXCMD
, feature lines~\mbox{%DIFAUXCMD
\cite{Lum2002, Lawonn2015}}\hskip0pt%DIFAUXCMD
, and Halos~\mbox{%DIFAUXCMD
\cite{Ozgur2017} }\hskip0pt%DIFAUXCMD
go by many other names, but they all relate to outlining either individual objects or highlighting areas of very high curvature from the perspective of the viewer. 
With traditional rendering, this tends to be done by viewing the distance between various pixels on the depth map~\mbox{%DIFAUXCMD
\cite{Ozgur2017, Lawonn2017} }\hskip0pt%DIFAUXCMD
or by calculating the local curvature of the surrounding fragments~\mbox{%DIFAUXCMD
\cite{Bui2015, Svakhine2003}}\hskip0pt%DIFAUXCMD
. 
However, this can be very different when working with }%DIFDELCMD < \gls{dvr}%%%
\DIFdelend \DIFaddbegin \subsection{\DIFadd{Depth Perception User Studies on Ocular See Through (OST) Augmented Reality (AR) displays}}
\gls{ost} \gls{ar} \DIFadd{displays allow users to view the real world while graphics are overlaid on the real world, which makes them useful for a range of stress-inducing professions.
This creates a different dynamic for depth perception as the whole environment is the real world. 
This can lead to increased stress levels when performing precise tasks, making depth perception a critical element of any augmented reality system using }\gls{ost} \gls{ar} \DIFadd{displays}\DIFaddend . 


\DIFdelbegin \subsection{\DIFdel{Volumetric Illustrative Rendering Techniques}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{In more recent years, many papers have been striving to take volumetric data and present it as illustrative images, with the belief that these images will be easier to communicate and understand in a 2D format.
}\DIFdelend \DIFaddbegin \begin{figure}
    \centering
    \includegraphics[width= \textwidth]{Chapter2/Images/ImagesFromOtherWorks/Swan2007ExperimentalSetup.png}
    \caption[The \gls{ost} \gls{ar} display was utilized for Swan et al.'s~\cite{Swan2007} study.]{
    \DIFaddFL{The }\gls{ost} \gls{ar} \DIFaddFL{display was utilized for Swan et al.'s~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
study.
    These images show how the }\gls{ost} \gls{ar} \gls{hmd} \DIFaddFL{was mounted for people to view.
    Used with permission from IEEE \textcopyright 2007.
    }}
    \label{fig:Swan2007ExperimentalSetup}
\end{figure}
\DIFaddend 

%DIF <  Talking about how methods to create these effects for to volumes work:
\DIFdelbegin \DIFdel{Initially Interrante }\DIFdelend \DIFaddbegin \DIFadd{Swan }\DIFaddend et al.\DIFdelbegin \DIFdel{'s~\mbox{%DIFAUXCMD
\cite{Interrante1997} }\hskip0pt%DIFAUXCMD
proposed two methods to convert their illustrative techniques to }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{:
}%DIFDELCMD < \begin{itemize}
\begin{itemize}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Scan-Conversion Method: }}%DIFAUXCMD
\DIFdel{Converts texture slabs into a grayscale volume. This method is efficient for generating multiple views but compromises stroke crispness due to volume data resolution limitations~\mbox{%DIFAUXCMD
\cite{Interrante1997}}\hskip0pt%DIFAUXCMD
.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Geometric Definition Method: }}%DIFAUXCMD
\DIFdel{Directly applies geometrical definitions of strokes during ray casting. This method maintains fine detail but is computationally expensive, requiring repeated intersection tests for each view~\mbox{%DIFAUXCMD
\cite{Interrante1997}}\hskip0pt%DIFAUXCMD
. 
}
\end{itemize}%DIFAUXCMD
%DIFDELCMD < \end{itemize}
%DIFDELCMD < %%%
\DIFdel{Since this thesis is focused on real-time volume rendering for immersive }%DIFDELCMD < \gls{mr} %%%
\DIFdel{devices, a new volume was required to be generated each frame.
This section will have more of a focus on papers that utilize the Geometric Definition Method. 
}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
performed the first depth perception study using an }\gls{ost} \gls{ar} \DIFadd{devices.
Participants were asked to either verbally report or blindly walk to a point displayed using the headset while viewing real-world objects with and without a real-world headset. The world was rendered virtually, or only the marker was rendered. 
The headset was mounted on an apparatus shown in }\autoref{fig:Swan2007ExperimentalSetup} \DIFadd{that was not able to move.
Forcing users to move to the position of the stimuli that they saw.
The users were asked to turn as the examiners removed the real-world object.
This study found that the }\gls{ar} \DIFadd{conditions were more accurate than the }\gls{vr} \DIFadd{conditions.
They also found that blind walking is performed more accurately than verbally guessing where an object is positioned.
}\DIFaddend 

\DIFdelbegin \DIFdel{Rheingans et al.~\mbox{%DIFAUXCMD
\cite{Rheingans2001} }\hskip0pt%DIFAUXCMD
developed a method that was able to separately compute the expected color and the transparency.
This allowed the lighting to react to certain elements and allow different textures within the volume to have a different appearance even if the Hounsfueild unit was similar at that }%DIFDELCMD < \gls{voxel_g}%%%
\DIFdel{.
Rheingans }\DIFdelend \DIFaddbegin \DIFadd{Later on from this, Jones }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Rheingans2001} }\hskip0pt%DIFAUXCMD
}%DIFDELCMD < \gls{dvr} %%%
\DIFdel{algorithm was able to present an accurate texture representation of the various surfaces of the volume and allowed for objects to be presented without regard to density or realism. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Lu }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Jones2011, Jones2008} }\hskip0pt%DIFAUXCMD
ran a series of studies that looked into several different methods of depth perception. 
Their first study ran a similar process to Swan }\DIFaddend et al.\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Lu20} }\hskip0pt%DIFAUXCMD
created a system that was able to apply stipping to a complete volume. 
This system used algorithms like ray marching similar to }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{to perform this calculation.
Visualization focused on rendering the various surfaces to make their curvatures clear but also took into account the amount of density it would have required to reach a given surface.
Surfaces that were facing the camera were faded out, and lighting the algorithm allowed for lighting to become an option to be utilized}\DIFdelend \DIFaddbegin \DIFadd{'s~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
compare }\gls{vst} \gls{ar} \DIFadd{to }\gls{ost} \gls{ar}\DIFadd{. 
However, this time, the }\gls{hmd} \DIFadd{was mounted to their head instead of being fixed, allowing for depth cues involving stereopsis~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011}}\hskip0pt%DIFAUXCMD
.
These results showed that the }\gls{ost} \gls{ar} \DIFadd{conditions did better than the prior ones}\DIFaddend . 

\DIFdelbegin \DIFdel{Work done by Bruckner et al.~\mbox{%DIFAUXCMD
\cite{Bruckner2007, Bruckner2006} }\hskip0pt%DIFAUXCMD
proposed that one method to get around some unwanted details that are an issue with }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{would be to utilize non-photo-realistic rendering.
This style of rendering utilized focused on the surfaces of different objects and rendered them based on their curvature~\mbox{%DIFAUXCMD
\cite{Bruckner2006}}\hskip0pt%DIFAUXCMD
.
From this, they developed a simple cartoon-like shading algorithm~\mbox{%DIFAUXCMD
\cite{Bruckner2006}}\hskip0pt%DIFAUXCMD
, Stippling~\mbox{%DIFAUXCMD
\cite{Bruckner2006}}\hskip0pt%DIFAUXCMD
, and Halos~\mbox{%DIFAUXCMD
\cite{Bruckner2007}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{When volume rendering is utilized in microscopy, it can be difficult to tell the difference between the different boundaries and the elements being visualized. 
Guo }\DIFdelend \DIFaddbegin \DIFadd{One issue that was possible with the previous three experiments was that users may have been estimating the location based on aspects in the environment~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011}}\hskip0pt%DIFAUXCMD
.
To get around this, Jones }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Guo2012} }\hskip0pt%DIFAUXCMD
used a halo visualization to separate the different molecules that can be viewed, as well as two new techniques for promoting contrast in regions of the volume called  Phase Contrast Volume Rendering (PCVR) and Difference Interference Contrast Volume Rendering (DICVR).
PCVR enhances the contrast of almost visible parts of the volume, allowing for a higher contrast.
DICVR tries to extend PCVR further by using interference contrast based on microscopy principles.
They simplified the equation required to create these effects each time by performing multiple rendering passes. 
This work by Bruckner }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Jones2011} }\hskip0pt%DIFAUXCMD
ran another study where they did not just occlude the area behind the }\gls{ost} \gls{ar} \gls{hmd}\DIFadd{, but they occluded the participant's complete vision.
This indicated that users were using the real-world environment to assist their depth perception. 
These results were further confirmed when users only had their vision partially occluded.
}

\DIFadd{The prior work by Swan }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Bruckner2007, Bruckner2006} }\hskip0pt%DIFAUXCMD
aimed at providing a more simplistic method for creating }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{and extending its functionality in biology.}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
and Jones et al.~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011} }\hskip0pt%DIFAUXCMD
explained that when in the action space }\gls{ar} \DIFadd{depth perception is more limited when compared to }\gls{vr}\DIFadd{.
%DIF > However, this still leaves two main questions that had to be answered before \gls{ost} \gls{ar} devices could see widespread use: the first being how accurate are \gls{ost} \gls{ar} devices and how are tasks performed in the near field affect these techniques on \gls{ost} \gls{ar}.
However, two key questions remained before }\gls{ost} \gls{ar} \DIFadd{devices could see widespread use: 
first, how accurate are these devices; and second, how tasks performed in the near field influence the effectiveness of }\gls{ost} \gls{ar} \DIFadd{techniques.
The latter was investigated by Singh et al.~\mbox{%DIFAUXCMD
\cite{Singh2011} }\hskip0pt%DIFAUXCMD
who created a study design based on blindly reaching where they found that off-the-shelf hardware could present depth accurate to 2cm to 4cm to the point~\mbox{%DIFAUXCMD
\cite{Singh2011, Singh2012a}}\hskip0pt%DIFAUXCMD
.
}\DIFaddend 

\DIFdelbegin \section{\DIFdel{Direct Volume Rendering (DVR)}}
%DIFAUXCMD
\addtocounter{section}{-1}%DIFAUXCMD
\DIFdel{The other focus of this Thesis is Volume rendering. 
Volume rendering relates to the visualization of a volume of data, which is typically created using a }%DIFDELCMD < \gls{ct} %%%
\DIFdel{or }%DIFDELCMD < \gls{mri} %%%
\DIFdel{machine.
However, it can also be used for other scientific visualizations, such as fluid simulations, meteorological data, and geological data. 
Preprocessed Volume Rendering will generally present surfaces of the volumes using polygonal structures utilizing iso-surfaces~\mbox{%DIFAUXCMD
\cite{Lorensen:1987:MCA}}\hskip0pt%DIFAUXCMD
, but DVR does not require explicit surfaces to be precisely stated. }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{can present the data within the natural format as a 3D image~\mbox{%DIFAUXCMD
\cite{Drebin1988}}\hskip0pt%DIFAUXCMD
.
This allows the system to dynamically represent the volumes from within the system.
However, it does highlight the need for techniqueslike }%DIFDELCMD < \gls{ct} %%%
\DIFdel{and }%DIFDELCMD < \gls{mri} %%%
\DIFdel{to be directly aligned with the physical data source.
An }%DIFDELCMD < \gls{X-ray Vision} %%%
\DIFdel{technique needs to be developed to meet this criterion.This section highlights the prior work in this area that influenced the research in this dissertation. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Visulizing Volumetric Data}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{Generally, volumetric data is viewed using 2D slices of a human body.
After years of training, medical practitioners can be very precise when using these slices, but they are not intuitive to use or to learn how to use~\mbox{%DIFAUXCMD
\cite{Cheung2021}}\hskip0pt%DIFAUXCMD
.
Like other forms of 3D data visualizations, converting this data into a 3D version makes it more intuitive to read and interact with~\mbox{%DIFAUXCMD
\cite{Jurgaitis2008}}\hskip0pt%DIFAUXCMD
. 
%DIF < As noted earlier, in \autoref{sec:ApplicationsForStereoscopicDisplays} immersive \gls{mr} displays are better suited for helping their users see 3D data. 
This requires the conversion of these volumes into 3D objects that are normally used to create these 2D slices. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{figure}
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\linewidth]{Chapter2/Images/Cubrillies.png}
%DIFDELCMD <     \caption[A example of Cuberilles.]{%%%
\DIFdelFL{A example of Cuberilles}\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/swan2015.png}
    \caption[Swan et al.'s~\cite{Swan2015} study design.]{\DIFaddFL{Swan et al}\DIFaddendFL .\DIFdelbeginFL \DIFdelFL{The left side shows the armadillo in its original form. On the right}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{'s~\mbox{%DIFAUXCMD
\cite{Swan2015} }\hskip0pt%DIFAUXCMD
study design}\DIFaddendFL , \DIFaddbeginFL \DIFaddFL{detailing }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{same model is rendered using Cuberilles}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{apparatus used for the two different types of experiments they ran utilizing reaching tasks}\DIFaddendFL . \DIFaddbeginFL \DIFaddFL{Used with permission from IEEE \textcopyright{} 2015.}\DIFaddendFL }
    \DIFdelbeginFL %DIFDELCMD < \label{fig:Cubrillies}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:Swan2015}
\DIFaddendFL \end{figure}

%DIF <  numbers in citations are for links to the Marching Cubes paper
\DIFdelbegin \DIFdel{Early methods to visualize volumes would focus on calculating the surface contours and calculating the exterior surface to match~\mbox{%DIFAUXCMD
\cite{Keppel1975}}\hskip0pt%DIFAUXCMD
, which was problematic as it created ambiguity when there were irregularities on the slice data such as those caused by noise~\mbox{%DIFAUXCMD
\cite{Fuchs1977}}\hskip0pt%DIFAUXCMD
, requiring user intervention to overcome~\mbox{%DIFAUXCMD
\cite{Christiansen1978}}\hskip0pt%DIFAUXCMD
.Herman }\DIFdelend \DIFaddbegin \DIFadd{Previous studies by Jones et al.~\mbox{%DIFAUXCMD
\cite{Jones2008, Jones2011} }\hskip0pt%DIFAUXCMD
Swan }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Herman1981} }\hskip0pt%DIFAUXCMD
tried creating a more automated surface by creating Cuberilles, which functioned similarly to Minecraft blocks~}%DIFDELCMD < \autoref{fig:Cubrillies}%%%
\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
utilized a Sony Glassatron}\DIFaddend . This was \DIFdelbegin \DIFdel{useful as it allowed for varying resolution~\mbox{%DIFAUXCMD
\cite{MEAGHER1982129}}\hskip0pt%DIFAUXCMD
.
The continuation of this work was marching cubes~\mbox{%DIFAUXCMD
\cite{Lorensen:1987:MCA}}\hskip0pt%DIFAUXCMD
. Unlike the rough surface afforded by utilizing Cuberilles, this algorithm made a smooth surface.
Marching Cubes utilized the fact that any six }%DIFDELCMD < \glspl{voxel_g} %%%
\DIFdel{neighboring }%DIFDELCMD < \glspl{voxel_g} %%%
\DIFdel{could be paired into 14 different symmetrical orientations if they were either inside or outside of the threshold.
This made it possible to create a3D model of a }%DIFDELCMD < \gls{ct} %%%
\DIFdel{or }%DIFDELCMD < \gls{mri} %%%
\DIFdel{scan with a manageable polygon count that looked similar to the real volume.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Wunsche~\mbox{%DIFAUXCMD
\cite{Wunsche2003} }\hskip0pt%DIFAUXCMD
created a system that allows users to view medical 3D data on desktop devices, utilizing bicubic Hermite interpolation with linear interpolation in the radial direction to generate an iso-surface for most models, while the ventricles are created by tracing }%DIFDELCMD < \gls{mri} %%%
\DIFdel{contours.
This system was then paired with a flexible UI, allowing for consistent color representation between models and the ability to incorporate mathematical models to segment the volume.
A plane representing the axial and coronal planes also displays the segmented data around the 3D object in 2D.
A follow-up system to this was later created, which focused on allowing }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{to be utilized in a similar format, providing tools to aid users in working with histograms of the volume data while still being able to interact with it in the same math-based manner~\mbox{%DIFAUXCMD
\cite{Liu2010}}\hskip0pt%DIFAUXCMD
. }%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Iso-surfaces are still used widely today as they provide the most efficient means of displaying a shell; however, tasks like diagnostic expiration and interactive tasks are better enabled by }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{~\mbox{%DIFAUXCMD
\cite{Meissner2000}}\hskip0pt%DIFAUXCMD
.
Farrell~\mbox{%DIFAUXCMD
\cite{Farrell1983} }\hskip0pt%DIFAUXCMD
found a method of using ray casting to create a surface method showcasing one of the first attempts at }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{.
However, many of the concepts for this would later be formed by }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{, which was initially developed in 1988 by Drebin et al.~\mbox{%DIFAUXCMD
\cite{Drebin1988}}\hskip0pt%DIFAUXCMD
, who designed this type of visualization as a fix for }\DIFdelend \DIFaddbegin \DIFadd{changed for the nVisor ST60 }\gls{ost} \gls{ar} \gls{hmd} \DIFadd{seen in }\autoref{fig:Swan2015}\DIFadd{~\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
This research consisted of three separate studies: one of these was done having participants move another virtual object to the exact location as a physical one by moving a slider underneath the desk (shown in }\autoref{fig:Swan2015} \DIFadd{(a)); the other two had participants placing a virtual object in the same position as the one they were looking at (shown in }\autoref{fig:Swan2015} \DIFadd{(b)).
One of these placement studies was used to determine whether corrective feedback from graphical elements could aid participants.
All of these studies showed that there was approximately a 1cm difference between placing virtual objects using an }\gls{ost} \gls{ar} \gls{hmd} \DIFadd{than in real life when in this task environment.
%DIF > These experiments with the newer headsets showed these results were accurate between 1cm and 2cm.
%DIF > Showing there was that displays utilized on different headsets would impact the accuracy possible for different \gls{ost} \gls{ar} \glspl{hmd}.
These experiments with }\DIFaddend the \DIFdelbegin \DIFdel{all-or-nothing approach that is possible when using an iso-surface.
Drebin }\DIFdelend \DIFaddbegin \DIFadd{newer headsets showed that the results were accurate to within 1–2 cm. This demonstrates that the type of display technology used in different headsets can significantly impact the accuracy achievable with }\gls{ost} \gls{ar} \glspl{hmd}\DIFadd{.
The bais seen in this study's graphs shows was higher than this as it was common for people move the object too far away from them rather than closer to them when trying to match the position of physical objects~\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
%DIF > Between underestimation and overestimation, it seems limited as the average placement of each virtual object was less than 3cm, as people guessed higher or lower at an even rate. 
}

\DIFadd{Many years after Swan }\DIFaddend et al.'s\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Drebin1988} }\hskip0pt%DIFAUXCMD
approach allowed for a realistic, transparent representation of the volume collected from a }%DIFDELCMD < \gls{mri} %%%
\DIFdel{or gls}%DIFDELCMD < {%%%
\DIFdel{ct}%DIFDELCMD < } %%%
\DIFdel{scanner~\mbox{%DIFAUXCMD
\cite{Ney1990, Kaufman2000}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < \gls{dvr} %%%
\DIFdel{functionality was further refined by Engel }\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
initial research, Medeiros }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Engel2001} }\hskip0pt%DIFAUXCMD
to work with modern equipment. 
Allowing all of the models to be viewed with minimal issues. 
}%DIFDELCMD < \gls{dvr} %%%
\DIFdel{rendering was initially only designed for }%DIFDELCMD < \gls{ct} %%%
\DIFdel{and }%DIFDELCMD < \gls{mri} %%%
\DIFdel{data~\mbox{%DIFAUXCMD
\cite{Ney1990, Kaufman2000}}\hskip0pt%DIFAUXCMD
, but it was later utilized in other fields.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Use Cases for Direct Volume Rendering}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{As with many fields, }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{can be utilized for education.
MacDougall et al.~\mbox{%DIFAUXCMD
\cite{MacDougall2016} }\hskip0pt%DIFAUXCMD
provide an example using a large 3D display wall of molecules for chemistry research and education.
These models used the traditional ball on a stick model and }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{, which better represented depth and provided a more realistic or cloudy model of the quantum world. 
MacDougall et al.~\mbox{%DIFAUXCMD
\cite{MacDougall2016} }\hskip0pt%DIFAUXCMD
found that elements of this system could help create new drugs for the future and proposed use cases that would encourage students to become more hands-on. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Hibbard~\mbox{%DIFAUXCMD
\cite{HibbardL.1986} }\hskip0pt%DIFAUXCMD
talked about how the data from 2D plots is easier to view in 3D when using volume rendering. 
Hibbard~\mbox{%DIFAUXCMD
\cite{HibbardL.1986} }\hskip0pt%DIFAUXCMD
then also conferred this data could be used to allow this data to be manipulated by using a time-variant, allowing phenomena like the wind to be simple to examine.
These techniques were then extended by Riley et al. ~\mbox{%DIFAUXCMD
\cite{Riley2003} }\hskip0pt%DIFAUXCMD
to allow for realistic visualizations of cloud maps.
This was impossible using iso-surfaces, which tended to require a form of lighting that clouds did not utilize~\mbox{%DIFAUXCMD
\cite }\hskip0pt%DIFAUXCMD
}%DIFDELCMD < {%%%
\DIFdel{Riley2003}%DIFDELCMD < }%%%
\DIFdel{. 
~}%DIFDELCMD < \gls{dvr} %%%
\DIFdel{allows climate scientists to explore the internal patterns of the effect of time and space on weather phenomena~\mbox{%DIFAUXCMD
\cite{Wang2018}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < \gls{dvr} %%%
\DIFdel{is also used to test the quality of materials like metals alloys~\mbox{%DIFAUXCMD
\cite{Okuyan2014}}\hskip0pt%DIFAUXCMD
, minerals~\mbox{%DIFAUXCMD
\cite{Okuyan2014} }\hskip0pt%DIFAUXCMD
concrete~\mbox{%DIFAUXCMD
\cite{Okuyan2014}}\hskip0pt%DIFAUXCMD
, resins~\mbox{%DIFAUXCMD
\cite{Nguyen2016}}\hskip0pt%DIFAUXCMD
, and many combinations of materials~\mbox{%DIFAUXCMD
\cite{Groger2022, Okuyan2014}}\hskip0pt%DIFAUXCMD
. 
Material science requires understanding materials'internal structures formed under different circumstances~\mbox{%DIFAUXCMD
\cite{Groger2022}}\hskip0pt%DIFAUXCMD
.
This could be done in the way of running CT scans of being dented or folded, allowing for structural analysis of how different conditions can affect different materials~\mbox{%DIFAUXCMD
\cite{Groger2022}}\hskip0pt%DIFAUXCMD
.This can also be applied to the creation of different materials, like sponge-like materials that need to move in certain ways or quantum materials that will arrange atoms, creating some highly precise and gas-like materials~\mbox{%DIFAUXCMD
\cite{Okuyan2014, Grottel2012}}\hskip0pt%DIFAUXCMD
.
Volume rendering can also be used to show how conducive liquids like resins are moving through non-conductive ones~\mbox{%DIFAUXCMD
\cite{Nguyen2016}}\hskip0pt%DIFAUXCMD
, allowing for real-time testing of how to develop products using these materials and presenting communication methods with end users ~\mbox{%DIFAUXCMD
\cite{Nguyen2016}}\hskip0pt%DIFAUXCMD
.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Molecular Sciences also use volume rendering to visualize the output gathered from electronic microscopes~\mbox{%DIFAUXCMD
\cite{Nguyen2022, Goodsell1989}}\hskip0pt%DIFAUXCMD
.
This allows the user to view the contents of the contents of a sample collected in 3D based on the different densities, much like }%DIFDELCMD < \gls{ct} %%%
\DIFdel{and }%DIFDELCMD < \gls{mri} %%%
\DIFdel{data. 
Nguyen }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
ran a slightly different experiment that also tried to learn what the impact was between depth perception between }\gls{vst} \DIFadd{and an }\gls{ost} \gls{ar} \glspl{hmd}\DIFadd{.
The infrastructure to create }\gls{vst} \DIFadd{and }\gls{ost} \gls{ar} \glspl{hmd} \DIFadd{was, at this point, quite different, and they began to have different pros and cons. 
The Oculus }\gls{vr} \gls{hmd}\DIFadd{~}\footnote{\url{https://en.wikipedia.org/wiki/Reality_Labs}} \DIFadd{(now owned by Meta (previously Facebook)) had recently been publicly released, which used a Fresnel lens~\mbox{%DIFAUXCMD
\cite{Cheng2022}}\hskip0pt%DIFAUXCMD
. 
Meanwhile, the Meta lens (made by the company meta which is now insolvent insolvent) }\gls{ost} \gls{ar} \gls{hmd}\DIFadd{~}\footnote{\url{https://en.wikipedia.org/wiki/Meta_\% 28augmented_reality_company\%29}} \DIFadd{utilized a distorted reflection of a screen's projection.
Medeiros et al.'s~\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
study design asked users to draw a line between two spheres in the virtual space. 
The findings from Medeiros }\DIFaddend et al.\DIFdelbegin \DIFdel{'s~\mbox{%DIFAUXCMD
\cite{Nguyen2022} }\hskip0pt%DIFAUXCMD
DiffTEM system adds to this by allowing denoising that utilizes many images of the data collected from different orientations before the data is rendered.
}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Medeiros2016} }\hskip0pt%DIFAUXCMD
study showed that the }\gls{ost} \gls{ar} \DIFadd{headset was less accurate in drawing lines between two points.
Participant comments indicated this was due to the difference in view windows between the Oculus and the Meta }\glspl{hmd}\DIFadd{.
Participants using the Meta headset struggled to keep both objects in view at once, while when they were wearing the Oculus, participants seemed to have no issues. 
This study highlighted the importance of a field of view when comparing activities between two headsets, as a smaller field of view could impact the ease with which a given participant can react to certain stimuli. 
}\DIFaddend 

\DIFdelbegin \DIFdel{Geologists can utilize }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{to represent the values of radar data~\mbox{%DIFAUXCMD
\cite{Baker2007}}\hskip0pt%DIFAUXCMD
.
They tend to do this by using ground-penetrating radar~\mbox{%DIFAUXCMD
\cite{Baker2007}}\hskip0pt%DIFAUXCMD
.
Unlike the previous examples of use cases for }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{, radar has many blank areas; Zehner~\mbox{%DIFAUXCMD
\cite{Zehner2021} }\hskip0pt%DIFAUXCMD
states that }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{can be used to both present a more full view of the area but to also better represent the level of uncertainty that can be viewed from this viewpoint.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Human Computer Interaction (HCI) Experiments Using Direct Volume Rendering (DVR)}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
%DIF <  Start writing up the separate sections here
\DIFdel{Understanding how people visualize or interact with }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{is important to this research.
While being able to visualize various data using DVR is one thing, the user experience is more important than the act of being able to display the content because the content rendered by the }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{needs to be a more pleasant experience than the alternative situations for it to have utility. 
The following section looks at how various studies over time have evaluated systems using }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \begin{figure}
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\linewidth]{Chapter2/Images/Hui1993.png}
%DIFDELCMD <     \caption[Examples of  Hui et al.'s cursors on 3D planes.]{%%%
\DIFdelFL{Examples of  Hui }\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Singh2018_Modified.png}
    \caption[\gls{ost} \gls{ar} \gls{hmd} which was utilized in Singh et al.~\cite{Singh2018} experiments.]{
    \gls{ost} \gls{ar} \gls{hmd} \DIFaddFL{which was utilized in Singh }\DIFaddendFL et al.\DIFdelbeginFL \DIFdelFL{'s}\DIFdelendFL ~\DIFdelbeginFL \DIFdelFL{\mbox{%DIFAUXCMD
\cite{Hui1993} }\hskip0pt%DIFAUXCMD
cursors on 3D planes}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{\mbox{%DIFAUXCMD
\cite{Singh2018} }\hskip0pt%DIFAUXCMD
experiments}\DIFaddendFL . 
    \DIFdelbeginFL \DIFdelFL{Right}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{(a}\DIFaddendFL ) \DIFdelbeginFL \DIFdelFL{Nail on plane, where }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Details }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{cursor has }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{utility of each lens and how they distort }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{ability to rotate around the volume; Left}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{user's view.
    (b}\DIFaddendFL ) \DIFdelbeginFL \DIFdelFL{Venetain blind}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Details how these participants see through this lens}\DIFaddendFL , \DIFdelbeginFL \DIFdelFL{a non-flat plane, which allows }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{allowing for }\DIFaddendFL the \DIFdelbeginFL \DIFdelFL{cursor allows the user }\DIFdelendFL \DIFaddbeginFL \gls{hmd} \DIFaddendFL to \DIFdelbeginFL \DIFdelFL{navigate easier on all dimensions}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{function}\DIFaddendFL .
    \DIFdelbeginFL \DIFdelFL{Used }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{(c) The real-life implementation of the custom }\gls{hmd}\DIFaddFL{. This image has been modified and is used }\DIFaddendFL with permission from IEEE \textcopyright{} \DIFdelbeginFL \DIFdelFL{1993.}\DIFdelendFL \DIFaddbeginFL \DIFaddFL{2018.
    %DIF > \textbf{Note: May need permission for this, Technically it should be creative comments but it is not clear.}
    }\DIFaddendFL } 
    \DIFdelbeginFL %DIFDELCMD < \label{fig:Hui1993}
%DIFDELCMD < %%%
\DIFdelendFL \DIFaddbeginFL \label{fig:Singh2018}
\DIFaddendFL \end{figure}

\DIFdelbegin \DIFdel{One of the first instances of human interaction being a concept using }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{is the work by Hui }\DIFdelend \DIFaddbegin \DIFadd{The work that was started by Swan }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Hui1993} }\hskip0pt%DIFAUXCMD
on a cursor for these interactions.This cursor is displayed in }%DIFDELCMD < \autoref{fig:Hui1993} %%%
\DIFdel{and works similarly to how a mouse works on a 2D plane, but it had the ability to be rotated on another plane using another 1D input, like the scroll wheel on a mouse, to rotate the plane the mouse cursor was sitting on .
To inform the user of the depth of the plane was highlighted on the outside of the volume, and a variation blind effect would be used to prevent the cursor from moving too much while still being able to get everywhere~\mbox{%DIFAUXCMD
\cite{Hui1993}}\hskip0pt%DIFAUXCMD
. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Kersten et al.~\mbox{%DIFAUXCMD
\cite{Kersten2006} }\hskip0pt%DIFAUXCMD
performed one of the first studies ever to be done using }%DIFDELCMD < \gls{dvr}%%%
\DIFdel{.
This study was focused on how transparency affected depth perception when using }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{on a stereoscopic display.
The type of transparency used was commonly associated with direct volume rendering. 
The study design would slowly rotate a cylinder filled with Perlin noise~\mbox{%DIFAUXCMD
\cite{Perlin1989} }\hskip0pt%DIFAUXCMD
and then rotate it slowly in one direction.
To tell what direction a cylinder was rotating, users would have to understand the approximate position of elements in the noise.
Participants of this study had to do this when using a mono and stereoscopic display and between various amounts levels of opacity. 
Kersten }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Swan2007} }\hskip0pt%DIFAUXCMD
was later finalized by Singh et al.~\mbox{%DIFAUXCMD
\cite{Singh2018} }\hskip0pt%DIFAUXCMD
who developed the }\gls{ost} \gls{ar} \gls{hmd} \DIFadd{detailed in }\autoref{fig:Singh2018} \DIFadd{which was able to react to five different focal cues allowing for highly accurate visualizations. 
Due to its large weight, this headset was placed on a desk whose height could be adjusted to match the participants'. 
The near field depth perception of this headset was tested by a series of matching the graphical cue-like tasks similar to what was done prior in Swan }\DIFaddend et al.'s~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Kersten2006} }\hskip0pt%DIFAUXCMD
findings showed that }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{is much more effective on stereoscopic displays.
This shows that the real use case for these techniques may be within the use of stereoscopic devices. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Several years later, Kersten-Oertel }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
The first study tested the same conditions as Swan }\DIFaddend et al.\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{Kersten-Oertel2014} }\hskip0pt%DIFAUXCMD
looked at methods to tell depth within a sparse volume rather than an occluded one.
This was in the form of a set of Cerebral vascular volumes.
Five different depth cues and a baseline were utilized throughout this study : Edge Detection(Halos), Pseudo-Chromadepth, Fog, Kinetic depth, and Stereo Vision.
Kersten-Oertel et al.~\mbox{%DIFAUXCMD
\cite{Kersten-Oertel2014} }\hskip0pt%DIFAUXCMD
tested a combination of experts and novices separately on two different studies.
All of the studies utilized the same procedure, where two vessels were highlighted, and the participants guessed which one was closer to themselves.
One of these studies utilized each depth cue individually, while the other focused on their different combinations.
Individual Chromadepth, Fog, and Stereo were shown to be much more beneficial than the other cues when shown individually. 
While chroma depth and stereo seem to have showcased the most substantial values for the combination, Kersten-Oertel et al.~\mbox{%DIFAUXCMD
\cite{Kersten-Oertel2014} }\hskip0pt%DIFAUXCMD
study again shows why Stereo-vision of }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{is such a strong depth cue.
}\DIFdelend \DIFaddbegin \DIFadd{'s previous study~\mbox{%DIFAUXCMD
\cite{Swan2015}}\hskip0pt%DIFAUXCMD
.
This type of }\gls{ost} \gls{ar} \DIFadd{device was shown to be able to improve depth perception to about 1 cm accuracy in the near field, regardless of the user's age.
The second study evaluated how people of different ages performed the task, which showed that people's depth perception of }\gls{ar} \DIFadd{seemed to function fine regardless of age.
The final study changed the brightness of the graphical content, which showed no difference~\mbox{%DIFAUXCMD
\cite{Singh2018}}\hskip0pt%DIFAUXCMD
.
}\DIFaddend 

\DIFdelbegin \DIFdel{Another user study looking into the transparency created by using }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{was done by Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012}}\hskip0pt%DIFAUXCMD
. 
This system used two layers of volume rendering: an outer transparent layer and an inner occluded layer.
By having occluded surfaces, Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012} }\hskip0pt%DIFAUXCMD
could provide lighting to parts of the volume that were occluded by rendering the image throughout multiple passes.
%DIF < From this, they then ran a user study that investigated using the user's preferences to compare their shadowed-enabled volume rendering to a version that didn't have any shadows.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To evaluate the effectiveness of placing shadows in }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{Corcoran and }%DIFDELCMD < \\ %%%
\DIFdel{Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012} }\hskip0pt%DIFAUXCMD
ran a series of studies using a computer monitor running at approximately 20fps, each consisting of less than 20 participants.
The first one was designed to test the users' preferences}\DIFdelend \DIFaddbegin \DIFadd{Hua and Swan~\mbox{%DIFAUXCMD
\cite{Hua2014} }\hskip0pt%DIFAUXCMD
utilized the same apparatus and ran a study that asked users to tell the depth of an occluded object}\DIFaddend .
This was done by \DIFdelbegin \DIFdel{having the participants view the same object side by side. 
Participants were asked to say whether the shadowed-enabled or non-shadowed versions told them more about the volume. 
The next study looked at shape perception, which had participants arrange two images of similar body parts from two different datasets.
This shape perception study showed that shadows were not essential for shape perception, but instead, the }%DIFDELCMD < \gls{ui} %%%
\DIFdel{was.The next study looked at relative depth perception. One user chose a point on the screen that was closest to themselves, and they found that showing shadows significantly increased depth perception.
They note here that it was uncommon for participants to answer this incorrectly. 
The final experiment had users determine how far into a volume the point was by having them estimate the location of an artifact inside of the volume.
This task found that depth perception was not affected by the distance or the presence of shadows inside of the volume; rather, it may hinder the absolute depth perception. 
Overall, Corcoran and Dingliana~\mbox{%DIFAUXCMD
\cite{Corcoran2012} }\hskip0pt%DIFAUXCMD
show that shadows can help detail information in a Volume, but they don't necessarily improve perception.
}\DIFdelend \DIFaddbegin \DIFadd{allowing for the option of a temporary occlusion barrier. 
The authors found this headset was able to reduce the errors from a previous 4cm~\mbox{%DIFAUXCMD
\cite{Edwards2004} }\hskip0pt%DIFAUXCMD
on }\gls{vst} \gls{ar} \glspl{hmd}\DIFadd{.
}\DIFaddend 

\DIFdelbegin \DIFdel{Three studies to use }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{with }%DIFDELCMD < \gls{mr} %%%
\DIFdel{were done by Laha }\DIFdelend \DIFaddbegin \DIFadd{Whitlock et al.~\mbox{%DIFAUXCMD
\cite{Whitlock2018} }\hskip0pt%DIFAUXCMD
later looked at the difference between using controller gestures and voice commands to aid the placement of objects using a HoloLens.
Participants were asked to complete a series of precision tasks, including selecting, rotating, and translating virtual objects placed at different distances from each type of control interface.
Whitlock }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Laha2013, Laha2012, Laha2012a, Laha2014} }\hskip0pt%DIFAUXCMD
who looked at study was focused on determining the immersion of different }%DIFDELCMD < \gls{mr} %%%
\DIFdel{devices. 
To best determine the amount of immersion when using }%DIFDELCMD < \gls{mr} %%%
\DIFdel{devices, they enabled and disabled head tracking and limited the area that was displayed to the user to $360^{\circ}$, $270^{\circ}$, $180^{\circ}$, $90^{\circ}$. 
These conditions allowed them to determine the required or appropriate level of immersion that was preferable for each task.
all of their studies utilized a selection of open-source real-world data to do their studies and utilized a range of different tasks suited for each dataset on each one. The first study was focused on testing if there was any noticeable benefit to }%DIFDELCMD < \gls{mr} %%%
\DIFdel{when needing to utilize a }%DIFDELCMD < \gls{cave} %%%
\DIFdel{using these conditions~\mbox{%DIFAUXCMD
\cite{Laha2012}}\hskip0pt%DIFAUXCMD
.
This study caused them to notice that any combination of the two conditions was able to grant better results.
Laha }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Whitlock2018} }\hskip0pt%DIFAUXCMD
found that participants perceived embodied freehand gestures as the most efficient and usable interaction compared to device-mediated interactions. 
Voice interactions demonstrated robustness across distances, while embodied gestures and handheld remotes became slower and less accurate when the distance was increased. 
These findings emphasize the importance of selecting appropriate interaction modalities based on distance when designing the studies in this thesis.
}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/AlKalbani2019.png}
    \caption[Several images from Al-Kalbani et al.'s~\cite{Al-Kalbani2019} study set up. ]{
    \DIFaddFL{Several images from Al-Kalbani et al.'s~\mbox{%DIFAUXCMD
\cite{Al-Kalbani2019} }\hskip0pt%DIFAUXCMD
study set up. 
    (a) displays grasp measurements required for Grasp Aperture (GAp) and grasp middle point (gmp)
    (b - m) images of participants completing the task, (b - g) without shadows, (h-m) with shadows.
    (n) the interaction space in reference to the Kinect. Used with permission from IEEE \textcopyright{} 2019}}
    \label{fig:AlKalbani2019}
\end{figure}

\DIFadd{Many previous papers showcased how the depth perception of computer graphics can be realized when they are placed in the real world. However, very few of these focused on what happens if these graphical objects react to the real world, such as having them leave a shadow over the ground.
Using }\gls{ost} \gls{ar} \DIFadd{devices is difficult as they cannot display darker shades of colors.
Al-Kalbani }\DIFaddend et al.\DIFdelbegin \DIFdel{'s~\mbox{%DIFAUXCMD
\cite{Laha2013} }\hskip0pt%DIFAUXCMD
next study utilized the NVis SX111 a (}%DIFDELCMD < \gls{vst} \gls{ar} %%%
\DIFdel{display)under similar circumstances.
This study found that the performance of their participants was improved by providing the most immersion possible~\mbox{%DIFAUXCMD
\cite{Laha2013}}\hskip0pt%DIFAUXCMD
. 
The final study ran by Laha et al.~\mbox{%DIFAUXCMD
\cite{Laha2014} }\hskip0pt%DIFAUXCMD
looked at what happened when iso-surfaces were used as the data set, requiring them to change the tasks required and also had them reintroduce stereo vision back to the conditions.
This study observed that the combination of all of the results was the most suitable for the tasks required to interact with volumetric data visualizations. 
}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Al-Kalbani2019} }\hskip0pt%DIFAUXCMD
ran a study that looked at the accuracy of participants trying to grab hold of virtual 3D shapes using a HoloLens2 as shown in }\autoref{fig:AlKalbani2019} \DIFadd{(b - m).
To do this the placement of their hands was situated around the virtual cube as shown in }\autoref{fig:AlKalbani2019} \DIFadd{(a) using a remote sensor (a Kinect 2~}\footnote{\url{https://en.wikipedia.org/wiki/Kinect}}\DIFadd{) placed just over 2 meters away as shown in }\autoref{fig:AlKalbani2019} \DIFadd{(n). 
The results from this study showed that participants tended to overestimate the area they had to grasp.
Drop shadows were appreciated when they were visible to the participants and increased the amount of time required to grasp the object, but they did not improve the participant's accuracy.
Users also underestimated the depth required by 2cm. 
}\DIFaddend 

\DIFdelbegin \DIFdel{Afterward, Laha et al.~\mbox{%DIFAUXCMD
\cite{Laha2016} }\hskip0pt%DIFAUXCMD
created a taxonomy of the different types of tasks that are possible when using }%DIFDELCMD < \gls{mr} %%%
\DIFdel{to remove the domain dependency that exists with empirical studies.
This was done by consulting 167 people using a questionnaire regarding how this taxonomy should be shaped. 
This was done with the aim to allow basic types of interactions to be considered similar to other types of interactions for different fields. 
These different types were classified as:
}%DIFDELCMD < \begin{itemize}
\begin{itemize}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Searching:}} %DIFAUXCMD
\DIFdel{Searching for the presence or absence of an object, or counting the amount of a given object.
}%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Pattern Recognition:}} %DIFAUXCMD
\DIFdel{Looking for trends like what side of the data set are there more blood vessels or repetition and asking the participant how many times certain items appear. 
}%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Spatial Understanding:}} %DIFAUXCMD
\DIFdel{This section is for tasks that require the participant to understand the orientation or position of a feature in the dataset. %DIF < Spatial understanding can be split into three subcategories:
    %DIF <  \begin{itemize}
    %DIF <      \item \textit{Absolute} Participants Locate a feature in the dataset that is the highest, lowest, furthest, etc., from the participant viewpoint or in the dataset.
    %DIF <      \item \textit{Reative:} Participants Judge if one feature in the dataset is in front, behind, higher, or lower than another object.
    %DIF <      \item \textit{Intersection:} Participants are asked if two objects are intersecting with each other. 
    %DIF <  \end{itemize}
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Quantitative Estimation:}} %DIFAUXCMD
\DIFdel{These focus on tasks that have the users estimate the properties of a feature of the dataset. 
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\textbf{\DIFdel{Shape Description:}} %DIFAUXCMD
\DIFdel{Requires the user to describe a shape they are viewing. 
}
\end{itemize}%DIFAUXCMD
%DIFDELCMD < \end{itemize}
%DIFDELCMD < %%%
\DIFdel{They also note two possible viewing styles: Egocentric and exocentric. They also note how the different dimensionality of the data should be considered. 
Overall, this study paper has informed the design process of the later studies. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  sketch-based interactions
\DIFdel{Sketch-based interactions allow for an easy interaction with volumes.
Shen }\DIFdelend \DIFaddbegin \DIFadd{It is common in }\gls{ar} \DIFadd{experiments to display objects as if they are hovering in reality; however, since this itself is not a natural appearance of the virtual object, it may be misleading the end user. 
This is even more relevant when using }\gls{ost} \gls{ar} \glspl{hmd} \DIFadd{as they show virtual objects superimposed over the participants' vision. 
Rosales }\DIFaddend et al.\DIFdelbegin \DIFdel{'s~\mbox{%DIFAUXCMD
\cite{Shen2014} }\hskip0pt%DIFAUXCMD
split the various use cases this technique could be used into seven different categories: segmentation, matching coloring, augmentation, and illustration. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{An early example of research that tried to inspire the fundamental approaches to creating the  was inspired by research from
Feng et al.~\mbox{%DIFAUXCMD
\cite{Feng20152548} }\hskip0pt%DIFAUXCMD
created a system that projected a grid over the area of the volumetric area. 
This was chosen to help highlight objects and clearly present the depth to different areas when faced with users who do not understand the depth of objects within the volume.
This system worked by projecting a 2D grid on the first available }%DIFDELCMD < \gls{voxel_g} %%%
\DIFdel{in the ray marching algorithm when it was in the range of the grid texture. 
They note in their findings that chroma color could be utilized to highlight depth perception}\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Rosales2019} }\hskip0pt%DIFAUXCMD
set out to test this hypothesis by testing if a participant's depth perception was more accurate by having participants blindly walk to a position where they believed they saw a virtual cube that was either hovering or on the ground. 
Whether an object was on or off the ground, participants tended to underestimate the depth they needed to move towards, but targets off the ground they judged to be closer to them}\DIFaddend . 

\DIFdelbegin \DIFdel{Almost all interactions with }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{require good depth perception, even when they are using a 2D display. 
Grosset }\DIFdelend \DIFaddbegin \DIFadd{Adams }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
created a user study that aimed to test if the depth of field could be utilized with }%DIFDELCMD < \gls{dvr}%%%
\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Adams2022} }\hskip0pt%DIFAUXCMD
also ran a study looking at how depth perception could be influenced by having the graphics respond to the real world by investigating depth perception with virtual objects placed on and off the ground}\DIFaddend . 
This study \DIFdelbegin \DIFdel{looked at replicating a person's ability to focus on an element based on how deep it was in the volume that they were focused on.
This was tested using a Dynamic and a Static experiment. 
The Dynamic Experiment allowed the user to control the depth of field by tracking their eyes. 
The other experiment by Grosset }\DIFdelend \DIFaddbegin \DIFadd{design utilized a 3 x 2 x 2 x 2 design looking at three distances in the action space (3m, 4.5m, and 6m), the presence of shadows, causing the virtual object to hover off the ground, and if it was being viewed by a }\gls{vst} \DIFadd{or an }\gls{ost} \gls{ar} \DIFadd{display. 
The results from Adams }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
set them at the most optimal place.
While there was not much difference between the different conditions, both studies provided similar answers.
Whether this technique was static or dynamic didn't make much difference between various conditions. 
However, they did notice a difference between the different sets of data. 
This was likely because each dataset they utilized presented a vastly different type of visualization, which utilized different depth cues to various levels~\mbox{%DIFAUXCMD
\cite{Grosset2013}}\hskip0pt%DIFAUXCMD
.
}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Adams2022} }\hskip0pt%DIFAUXCMD
showed an underestimation of the values of over 17\%. 
There was a small increase in accuracy regarding the presence of shadows, showing a significant improvement of 2\%.
}\DIFaddend 

\DIFdelbegin \DIFdel{Roberts et al.~\mbox{%DIFAUXCMD
\cite{Roberts2016} }\hskip0pt%DIFAUXCMD
investigated the impact of different types of reconstruction filters for }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{graphics using a questionnaire.
%DIF < The different reconstruction filters they used as the conditions for their study were: b-spline, Trilinear, CatMull-Rom, Int B-spline, and Welch.
These reconstruction filters included B-spline, Trilinear, CatMull-Rom, Int B-spline, and Welch filters.
Participants were asked to view several open-source datasets and rate their depth quality, layout, sharpness, and jaggedness. 
This paper did not have many major findings. B-spline was preferable for the tasks they suggested but not to a significant amount.
What was interesting about this research was that the datasets seemed to have more of an impact than the conditions themselves did, much like Grosset }\DIFdelend %DIF > Similar to Swan et al.~\cite{Swan2007}, Medious et al.\cite{Medeiros2016} and Adams et al.~\cite{Adams2022}, Ping et al.~\cite{Ping2020} aimed to determine what the limits of depth perception were possible to observe between \gls{ar} and \gls{vr} and if do different depth cues have a different impact than others.
\DIFaddbegin \DIFadd{Similar to Swan }\DIFaddend et al.\DIFdelbegin \DIFdel{'s~\mbox{%DIFAUXCMD
\cite{Grosset2013} }\hskip0pt%DIFAUXCMD
study.
}%DIFDELCMD < 

%DIFDELCMD < \begin{figure}
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Englund2018.jpg}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[Conditions used in Englund et al.'s~\cite{Englund2016, Englund2018} studies.]{%
{%DIFAUXCMD
\DIFdelFL{Conditions used in Englund et al.'s~\mbox{%DIFAUXCMD
\cite{Englund2016, Englund2018} }\hskip0pt%DIFAUXCMD
studies.
    (a) Direct Volume Rendering (DVR), (b) Depth of Field, (c) Depth Darkening, (d) Volumetric Halos, (e) Volume Illustration, and (f) Volumetric Line Drawings. Used with permission from John Wiley and Sons \textcopyright{} 2016}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:Englund2018}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{To get around the issues that Roberts }\DIFdelend \DIFaddbegin \DIFadd{~\mbox{%DIFAUXCMD
\cite{Swan2007}}\hskip0pt%DIFAUXCMD
, Medeiros }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Roberts2016} }\hskip0pt%DIFAUXCMD
and Grosset }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Medeiros2016}}\hskip0pt%DIFAUXCMD
, and Adams }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Grosset2013}}\hskip0pt%DIFAUXCMD
, Englund }\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Adams2022}}\hskip0pt%DIFAUXCMD
, Ping }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Englund2016, Englund2018} }\hskip0pt%DIFAUXCMD
instead created 15 volume rendered images in which they took photos off each of their each of their sides.
Each of these volumes was in the shape of a cube, which allowed them to take 90 images of different volumes, which they then utilized in a questionnaire.
%DIF < A sample of these volumes can be seen in ~\autoref{fig:Englund2018}.
They then took these images with six different }%DIFDELCMD < \gls{dvr} %%%
\DIFdel{visualization techniques seen in }%DIFDELCMD < \autoref{fig:Englund2018}%%%
\DIFdel{: Depth of Field, Subtractive Color Blending, Additive Color Blending, White Outlines, Toon Shading, and Black Outlines. 
This questionnaire had participants perform }%DIFDELCMD < \gls{twofc} %%%
\DIFdel{questionnaire for multiple depth perception studies if participants could visually orient the images in the same positions and how attractive the different conditions they presented were}\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Ping2020} }\hskip0pt%DIFAUXCMD
aimed to determine what differences in depth perception can be observed between }\gls{ar} \DIFadd{and }\gls{vr}\DIFadd{, and whether different depth cues have varying impacts}\DIFaddend .
This study \DIFdelbegin \DIFdel{found that depth darkening and Halo's gave the best sense of depth perception out of all the conditions , and the occlusion cue-in seems to be the most effective way to determine depth perception based on it. Orienting the shapes was easiest to determine using the toon shading, as that method highlighted the shapes of the foreground objects the best. Most users preferred depth darkening. 
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The Englund et al.'s~\mbox{%DIFAUXCMD
\cite{Englund2018} }\hskip0pt%DIFAUXCMD
study design and conditions were later consulted on by three experts. 
This clearly explains both the Subtractive Color Blending and Black Outlines. 
One main issue an expert noted was that as these sources of data only represented a single image, they didn't accurately represent the interaction that people would have with volume data, as this was expected to be utilized to properly view the data, like being able to rotate the volume. 
The same expert noted that the Toon shading hid aspects of the volume due to its high opacity.
One expert noted that the outlines would be better suited if they could react to the curve's outline.
Some of the most important expert findings from Englund et al.'s research~\mbox{%DIFAUXCMD
\cite{Englund2018} }\hskip0pt%DIFAUXCMD
was that by highlighting the edges of the objects, depth perception was most improved by communicating to the user where the front and back of the objects were. 
These factors likely were likely why the black outlines performed as well as they did.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\subsection{\DIFdel{Volume Rendering In Mixed Reality}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdel{Graphical capabilities require more processes running using }%DIFDELCMD < \gls{mr} \gls{hmd}%%%
\DIFdel{~\mbox{%DIFAUXCMD
\cite{Geng2013}}\hskip0pt%DIFAUXCMD
.
This is made much harder as many modern }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{headsets utilize mobile hardware even to produce the most simple 3D visualization~\mbox{%DIFAUXCMD
\cite{Zari2023}}\hskip0pt%DIFAUXCMD
, resulting in only a few papers on this topic.
Authors have, however, found ways of getting this technique to function on }%DIFDELCMD < \gls{mr} \glspl{hmd} %%%
\DIFdel{by utilizing the hardware in unique ways.
This has made possible research that spans gamification, medical assistance, collaboration, and high-fidelity graphics interaction, resulting in some interesting features.
}%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  extend this part Ross note
%DIFDELCMD < 

%DIFDELCMD < \begin{figure}
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width = \columnwidth]{Chapter2/Images/ImagesFromOtherWorks/Cecottietal.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[The virtual scene created by Cecotti et al.]{%
{%DIFAUXCMD
\DIFdelFL{The virtual scene created by Cecotti et al.~\mbox{%DIFAUXCMD
\cite{Cecotti2021} }\hskip0pt%DIFAUXCMD
(a) the 3D data, (b) the sagittal (y-z), (c) coronal (x-y), (d): transaxial (x-z) planes. Used with permission from IEEE \textcopyright{} 2021.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:Cecottietal}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Gamification is often used to increase student retention in education. 
One system created by Cecotti et al.~\mbox{%DIFAUXCMD
\cite{Cecotti2021} }\hskip0pt%DIFAUXCMD
allowed students to learn how to read medical images using 3D spaces as well as 2D ones. 
The scene shown in }%DIFDELCMD < \autoref{fig:Cecottietal} %%%
\DIFdel{shows the information presented to the students and how it allows them to map individual planes to a 3D object}\DIFdelend \DIFaddbegin \DIFadd{used three different depth cues (points, lines, and boxes) as conditions and four different illumination ranges which to change the texture of the surface (Ambient, Half-Lamber, Blinn-Phong, Cook-Torrane). 
Participants were asked to use these depth cues to align differently sized objects while they were between 1.75m and 7.35 m away from the shapes they were being asked to align.
Participants would control one of the shapes using a keyboard input}\DIFaddend , allowing them to \DIFdelbegin \DIFdel{learn how various organs are seen using 2D planes.
This is currently integrated into their institution's anatomy curriculum.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Pelanis }\DIFdelend \DIFaddbegin \DIFadd{move the virtual objects forward or backward.
Ping }\DIFaddend et al.\DIFdelbegin \DIFdel{~\mbox{%DIFAUXCMD
\cite{Pelanis2020} }\hskip0pt%DIFAUXCMD
asked a series of surgeons to compare how using a 3D rendering of }%DIFDELCMD < \gls{ct} %%%
\DIFdel{data compared to a more traditional 2D representation of the same }%DIFDELCMD < \gls{ct} %%%
\DIFdel{data when using an }%DIFDELCMD < \gls{ost} \gls{ar} \gls{hmd}%%%
\DIFdel{.
The }%DIFDELCMD < \gls{ct} %%%
\DIFdel{data consisted of images of 150 different legions. 
The medical practitioners correctly accessed 89 of the 150 legions using both formats, and there was no real }\DIFdelend \DIFaddbegin \DIFadd{'s~\mbox{%DIFAUXCMD
\cite{Ping2020} }\hskip0pt%DIFAUXCMD
results showed that there was little }\DIFaddend difference in the \DIFdelbegin \DIFdel{accuracy of the visualizations.
There was a difference between the time required, where MRI scans required an average time of 23.5 sections, while the }\DIFdelend \DIFaddbegin \DIFadd{depth cues that improved depth perception between the }\DIFaddend \gls{ost} \DIFaddbegin \DIFadd{and }\gls{vst} \DIFaddend \gls{ar} \DIFdelbegin %DIFDELCMD < \gls{hmd} %%%
\DIFdel{only took practitioners an average of 6 sections to diagnose.
Pelanis et al.~\mbox{%DIFAUXCMD
\cite{Pelanis2020} }\hskip0pt%DIFAUXCMD
highlights that there is still an advantage to using 3D graphics even when a person is highly trained. 
}\DIFdelend \DIFaddbegin \glspl{hmd}\DIFadd{, but they did note that illumination of the objects made a larger effect on a participants depth perception.
This effect was likely caused due to participants perceiving the objects were the same shape regardless of their size, they would all look alike unless and having more details on the shape may have given them more attributes they could match.
}\DIFaddend 

\DIFdelbegin \DIFdel{Cinematic Rendering is generally a costly method of viewing Volumetric data. It cannot be run in real time and requires a very powerful computer. 
Stadlinger et al.~\mbox{%DIFAUXCMD
\cite{stadlinger2021cinematic, Stadlinger2019} }\hskip0pt%DIFAUXCMD
utilized a workaround for this method where they took many images of the volume and rendered them on 2D planes for the user to see.
This allowed this technique to be viewed using a lowered-powered machine.
This made the processing so efficient that Steffen et al.~\mbox{%DIFAUXCMD
\cite{Steffen2022} }\hskip0pt%DIFAUXCMD
could display cinematic rendering on the }%DIFDELCMD < \gls{vr} %%%
\DIFdel{headset.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Steffen et al.'s~\mbox{%DIFAUXCMD
\cite{Steffen2022} }\hskip0pt%DIFAUXCMD
research had ten trained medical investigators look at ten patient cases using both a desktop display and }%DIFDELCMD < \gls{vr} \gls{hmd}%%%
\DIFdel{. 
When these users were asked to assess the volume details, they were more accurate when using a desktop.
They also noted that desktop displays offered better depth perception when viewing cinematic rendering.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Heinrich }\DIFdelend \DIFaddbegin \DIFadd{This finding caused Ping }\DIFaddend et al.~\DIFdelbegin \DIFdel{\mbox{%DIFAUXCMD
\cite{Heinrich2021} }\hskip0pt%DIFAUXCMD
performed a series of tests looking at the difference between depth perceptionbetween }%DIFDELCMD < \gls{ar} %%%
\DIFdelend \DIFaddbegin \DIFadd{\mbox{%DIFAUXCMD
\cite{Ping2020a} }\hskip0pt%DIFAUXCMD
to investigate how different shaders could influence depth perception.
This study had a similar physical setup as their previous study but only used a HoloLens~\mbox{%DIFAUXCMD
\cite{Ping2020}}\hskip0pt%DIFAUXCMD
.
The conditions were split into a 3 x 3 study setup using three different shaders: Half-Lambert, Blinn-Phong, and Cook Torrance, each colored as blue, yellow, or green, and displayed at either 25cm, 30cm, 35cm, or 40cm.
This study showed that participants seemed to be more accurate with the brighter-colored virtual objects (yellow or green).
Participants struggled to place larger objects, }\DIFaddend and \DIFdelbegin %DIFDELCMD < \gls{vr} %%%
\DIFdel{when looking at volumetric vascular structures rendered as an iso-surface.
They then utilized three different depth cues: Phong Shading, pseudo-chroma depth shading, and surrounding the volume with a Fog.
Users would be presented with a set of spheres located at the ends of all of the blood vessels and asked if they could sort them from nearest to farthest.
Using Pseudo-Chromadepth shading or Fog techniques clearly improved desktops, as these showed the z-axis. 
However, VR users rarely make any errors. 
This was because they could freely use an array of other depth perception cues, such as Convergence, Binocular Disparities, Motion Perspective, and even, in the case of Fog and Aerial Perspective.
In contrast, the desktop was limited to Occlusion, Real Size, and Density, which any display could also use.
However, this does show that just using a }%DIFDELCMD < \gls{ar} %%%
\DIFdel{device will improve the depth perception of a task using volume data, but it also indicated that the need limits required for testing medical data needed to become much more precise than before to enable the type of accuracy required by the medical field}\DIFdelend \DIFaddbegin \DIFadd{both the shaders that had more pronounced specular highlights were found to aid depth perception the most}\DIFaddend . 


\section{Research Gap}
This thesis looks at methods that can be used to allow a user to view \gls{dvr} visualizations within the real-life objects they have been created from when using an \gls{ost} Device. 
This first required a better understanding of how \gls{X-ray Vision} worked on \gls{ost} \gls{ar} \glspl{hmd}.
Then, it required taking the learnings from that and placing and implementing them in a way that could be utilized with \gls{dvr}. 
We would aim to test our findings to determine if these \glspl{X-ray Visualization} impact the user's experience.

At the time of starting the research (2020) for this thesis, there was little research on how \gls{X-ray Vision} within the near field functions when using \gls{ost} \gls{ar} \glspl{hmd}.
Some work was done using early versions of the technology within the action field prior~\cite{Blum2012, Rompapas2014, Erat2018}, but no research had been done in the near field. 
Around the same time as our own research, several other papers came out trying to solve this issue, highlighting the need to fill this gap~\cite{Gruenefeld2020, Martin-Gomez2021}.
These papers looked at methods and presented scenarios using static graphics overlaid onto the real world while having users test out various methods while constraining the users' actions by having them stand in one place and controlling how they interacted with the task~\cite{Gruenefeld2020, Martin-Gomez2021}. 
The research in this dissertation differs from these because these visualizations were in an environment that allowed for a more ecologically relevant scenario with few constraints on the user's freedom, presenting less biased results in favor of more realistic conditions. 
The difference between Geometrical Saliency and Visual Saliency was also compared to determine what makes the most appropriate \gls{X-ray Visualization} for an \gls{ost} \gls{ar} \gls{hmd}.
This was done by creating an algorithm that allowed for the calibration of a camera image over the view of the use of an \gls{ost} \gls{ar} \gls{hmd}.

Then, taking the lessons learned from our research on \gls{X-ray Vision} of \gls{ost} \gls{ar} devices, a range of \glspl{X-ray Visualization} was created and analyzed utilizing illustrative effects.
These were then tested using two separate studies; one of these was aimed at testing the user's ability to analyze the data, and the other tested how accurate the user's sense of depth was when looking at these objects.
As previously stated by Grosset et al.~\cite{Grosset2013}, the large range of distinct differences in publicly available data sets can cause wildly different results across methods that utilize the same condition.
So, in a bid to allow our datasets to be utilized in HCI studies, this type of visualization allowed us to perform tasks that could be repeated \DIFaddbegin \DIFadd{in a numerous (}\DIFaddend over a septillion\DIFaddbegin \DIFadd{) }\DIFaddend different ways and modified for many different studies.  

\DIFaddbegin \DIFadd{To clarify the main research gap addressed by this thesis, the key points are summarized as follows:
}\begin{itemize}
    \item \DIFadd{Lack of research on near-field X-ray vision in OST AR HMDs: Prior work (e.g., Blum~\mbox{%DIFAUXCMD
\cite{Blum2012}}\hskip0pt%DIFAUXCMD
, Rompapas~\mbox{%DIFAUXCMD
\cite{Rompapas2014}}\hskip0pt%DIFAUXCMD
, Erat~\mbox{%DIFAUXCMD
\cite{Erat2018}}\hskip0pt%DIFAUXCMD
) focused on the action field or static overlays, but little was known about how X-ray vision functions in the }\emph{\DIFadd{near field}}\DIFadd{.
    }\item \DIFadd{Limitations of prior studies: Contemporary work (Gruenefeld~\mbox{%DIFAUXCMD
\cite{Gruenefeld2020}}\hskip0pt%DIFAUXCMD
, Martin-Gomez~\mbox{%DIFAUXCMD
\cite{Martin-Gomez2021}}\hskip0pt%DIFAUXCMD
) constrained user freedom (static standing positions, restricted interactions), limiting ecological validity.  
    }\item \DIFadd{Need for ecologically relevant scenarios: Few studies tested X-ray vision in realistic, less constrained conditions, which may produce more representative user experiences.  
    }\item \DIFadd{Unclear best method for X-ray visualization: No prior comparative analysis between geometrical saliency and visual saliency for OST AR HMDs.  
    }\item \DIFadd{Lack of integration with Direct Volume Rendering (DVR): Previous AR visualization studies used static or simplified graphics; the use of DVR-based X-ray visualization in OST AR remains largely unexplored.  
    }\item \DIFadd{Dataset challenge: Public datasets vary widely, making reproducibility difficult; this research proposes repeatable tasks using DVR-based X-ray visualizations adaptable across many conditions.  
}\end{itemize}


\DIFaddend The end product of this thesis is several new \glspl{X-ray Visualization}. 
That can be used to overlay over the original object, person, or animal that had been scanned by a \gls{ct} or \gls{mri} scanner to provide the illusion of looking into the object.



% \section{Volume Rendering}
% This section should cover mainly Volume rendering in Human-computer interactions with 2 or 3 paragraphs dedicated to explaining some of the topics within the field.


% % What is augmented Reality
% %\gls{ar} refers to the ability to bring virtual objects into the real world \cite{Milgram1994}.
% %This can manipulate how we perceive reality and create new effects explaining new information regarding the real world.
% %It is one of the lowest forms of virtualization on Milgram's Mixed Reality Taxonomy~\cite{Milgram1994}. 

% % Millgrims AR
% These immersive technologies are part of Milgram et al.'s~\cite{Milgram1994} Mixed Reality Continuum, which can be seen in \autoref{fig:MillgirmsMixedReality}.
% This continuum spans between the real world and \gls{vr} is the furthest distance 
% \gls{ar} refers to the ability to bring virtual objects into the real world by overlaying them on someone's vision.
% While \gls{mr} talks about the spectrum between the real and virtual worlds, not including them. 
% This is for many different types of displays, given the types of interactions ranging from \gls{hmd}s, \gls{cave}s, Handheld Devices, \gls{sar} displays, and even some large monitors can be used as mixed reality devices. 
% %This thesis is primarily focused on \gls{ar} using a \gls{hmd}.

% %You will typically see \gls{ar} on smartphones and tablets with applications like Pokémon go ~\cite{pokemongo}.
% %These applications use the devices' sensors and cameras to get information about the real world and then overlay virtual objects to the display.

% \section{\gls{X-ray Vision}}
% % What use cases have been explored in \gls{X-ray Vision}, and what use cases are there left to explore
% \gls{X-ray Vision} in augmented reality has been thoroughly researched and utilized in many domains: in medical applications for diagnosis and surgery\cite{Bajura1992, Bichlmeier2007, Kalia2019, Phillips2021},
% in firefighting to allow firemen to see through smoke~\cite{CThru}, 
% in construction to view and look through walls to better understand a building's layout or understand the alternative interior layouts~\cite{Phillips2021, Ventura2009, Feiner, Rompapas2014},
% and guidance activities~\cite{DePaolis2017, Zollmann2014}.
% The main goal of this thesis is to view these issues as they primarily present to the medical field. Still, our findings could also be useful for any of the prior mentioned fields as all of them can make use of trying to perform \gls{X-ray Vision} of geometrically ambiguous or volumetric data.

% \section{Review of \gls{X-ray Visualization}s in Augmented Reality}
% \subsection{Rationale}
% \gls{X-ray} has been one of the first goals in Augmented reality and over the past 28 years many solutions have been developed and worked on. However, since this is such a long lasting field of research within \gls{ar} it is rapidly becoming harder to keep track of and understand the current state of research in the field.

% \subsubsection{Objectives}
% \begin{itemize}
%     \item To determine the breadth of work done to date in \gls{X-ray Vision} and create an understanding of the work that has been done throughout the field and an understanding of the work left to do in the field.
%     \item The technologies that have been used to create the effect of \gls{X-ray Vision}. 
%     \item Trends the industry has been through. 
%     \item How many and what \gls{X-ray} visualizations have been created.
%     \item Where any of these types of visualizations are tested with a specific industry in mind.
%     \item What has been the most common method of verification of \gls{X-ray} visualizations that has been done to date.
%     \item We also want to focus on the various experiments that have been run of \gls{X-ray Vision} and some details in regards to them. 
%     \item We also can look at the habits of people citing each other in the field.
% \end{itemize}

% \subsection{Methods}
% The following protocols are based on the prisma SRC\cite{Page2021} checklist.
% \subsubsection{Protocol}
% Prisma SRC \cite{Page2021}

% \subsubsection{Eligibility Criteria}
% \begin{itemize}
%     \item \gls{X-ray Vision} needed to be included in the study as placing virtual objects behind real-world objects.
%     \item Augmented Reality needed to be included in the paper. However, the application of \gls{ar} was not essential to the papers included but they did have to test theories of AR.
%     \item Papers that focused on overlaying or superimposing data that would exist within a physical over real-world objects were not included.
%     \item Studies that were included in multiple papers were only recorded once all papers that highlighted the same research have been included in this review as the content between both articles provides different amounts of information.
%     \item If the focus on \gls{X-ray Vision} was light compared to other elements of the study and two or more reviewers agreed on it then the work was passed on in favor of more concise and understandable data.
% \end{itemize}

% \subsubsection{Information Sources}
% The following databases where searched to compile for this study

% \begin{itemize}
%     \item ACM Digital Library
%     \item IEEE Xplore
%     \item Pub Med
%     \item Web of Science
%     \item Scopus
% \end{itemize}

% \subsubsection{Gray Literature}
% Google Scholar - Cross Ref Search

% \subsection{Search Protocol}
% The Ideal search terms we would have liked to be able to use would have been \gls{X-ray Vision} and Augmented Reality. However, extending the search from this point seemed logical so we recursively gathered up common keywords from the various results from valid papers and used them to extend our search.

% Due to the large field of research composing X-Ray visualizations Just the term X-ray was not included on its own unless the entry had another keyword of visualization. This was done since a generic search on X-ray and Augmented Reality would return thousands of invalid papers related to viewing medical data. 

% The terminology of \gls{X-ray Vision} has been through some iterations over the years and has not always been thought of as \gls{X-ray Vision} so opening it up to papers that look into the perception of occlusion techniques is also important. Especially since there may also be some versions of \gls{X-ray Vision} originating there.

% Second part of the search function was written to cover all of the different combinations of Augmented Reality that would return to me any field of augmented reality that would have included some form of \gls{X-ray Vision}. So both SAR and Augmented reality options were chosen for this search. 

% This search was duplicated throughout all databases in section .There were no filters applied to IEEE’s search query. This found us 101 search results. ACM’s was restricted to terms that could be found Titles Abstracts and Keywords that returned us 40 unique results in total. 

% \subsubsection{Subsequent Searches}
% Two papers were also added due to being both referenced by many of our papers and being valid within our search crita. Which leads us to add these papers into our final collection of evidence. These papers could be found in the database we had chosen to use but however due to their age the terminology they use made them difficult to search for other than by title. 

% Papers that cited the valid papers were then found on google scholar and any paper that had received over 4 citations from our initial pool of search results was then investigated. If any of these papers were valid they would go through the same review process as the previous valid papers and if any of them were past another cross reference search was run to find more papers that were not previously found could be made valid until no new papers were found.

% \subsubsection{Selection of Sources of Evidence}
% The evidence was first evaluated by title and abstract. If a reviewer thought the paper may have been relevant to this research it was given a full text read where it was they would critique if it met our eligibility criteria. It would then be independently reviewed by one other reviewer on the paper and if they agreed it was added to the collection of evidence. 

% \subsection{Data Charting}
% This was done by me.

% \subsection{Data Items}
% From each paper I collected the following data items:
% \begin{itemize}
%     \item \textbf{BibTex:} Was used as a unique ID;
%     \item \textbf{Title:} for ease when navigating the data;
%     \item \textbf{Results Type:} What kind of research was done for this study (Case, Technical, User Evaluation or other);
%     \item \textbf{AR used:} Did the paper use OSTAR or/and VSTAR or/and SAR;
%     \item \textbf{Vision Classification:} stereo or mono;
%     \item \textbf{Device Information:} what kind of device was used;
%     \item \textbf{Technique(s):} What \gls{X-ray Vision} Visualizations where used in this paper;
%     \item \textbf{Was first seen:} Was the techniques seen in this paper first seen here;
%     \item \textbf{Ran Study:} Was a user or case study ran for this study;
%     \item \textbf{Results Proven:} A brief overview of what did this paper prove;
%     \item \textbf{Limitations:} What did this paper list as limitations;
%     \item \textbf{Real World Use Cases:} What industry was this research aimed at.
% \end{itemize}

% A list of all of the papers that have cited a paper within the original database was also collected to determine the fields where the techniques are most in demand. 

% \subsubsection{Data collected that was related to the user studies}
% All studies that conducted user studies had the following data extracted from them where possible:
% \begin{itemize}
%     \item \textbf{BibTex:} Was used as a unique ID;
%     \item \textbf{Amount Of Participants:} How many participants were recruited for this study;
%     \item \textbf{Group Size Of Participants:} Were these participants placed into groups and how big were they;
%     \item \textbf{Amount Of Independent Variables:} How many independent variables were in this study;
%     \item \textbf{Numerical Breakdown Of Variables as Categories:} A breakdown of the independent variables into their classifications.
%     \item \textbf{Amount Of Iterations Per Condition:} How many iterations of each set of independent variables was each user sent though.
%     \item \textbf{Conditions:} A written down set of the experiments in English;
%     \item \textbf{Analysis used:} What type of analysis did were used for the results;
%     \item \textbf{Keywords Related To Study:} Between or within participants;
%     \item \textbf{Goal Of Study:} What was the aim of the study in a word or two;
%     \item \textbf{Summary Of Study:} A short written summary of the experiment;
%     \tiem \textbf{The Distance Stimulus Was Placed:} Where where the virtual articfacts the users where expected to interact with placed;
% \end{itemize}

% \subsection{Results}
% \subsubsection{Selection of sources of evidence}
% The following search query was used throughout all databases it was chosen by looking at a series of key words from 20 select papers 

% \textbf{Search Query:} ("\gls{X-ray Vision}" OR "X-Ray Visualization" OR ("occlusion" AND "perception") OR (Visualization AND X-ray) OR "see-through vision") AND ("AR" OR "Augmented Reality" OR "Mixed Reality" OR “SAR” OR "spatial augmented reality”)

% \begin{figure}
%     \centering
%     \includegraphics{Chapter2/Images/DraftOfReivewFormatForXray vision.png}
%     \caption{Process for the selection of results}
%     \label{fig:TheSelectionOfResultsFromLitReview}
% \end{figure}

% \subsubsection{Characteristics of sources of evidence And Results of individual sources of evidence}
% Notes about how various citations in the field are conducted

% What type of \gls{ar} devices were used.

% Most research in \gls{X-ray Vision} is not tailored towards any field however there have been some works done in Construction, Maintenance, Roadwork, Medicine, driving, navigation and safety.

% Information regarding citations eg. how many times the field has been cited as a whole and within the field. What is the highest cited paper in the field.

% \subsection{Types of \gls{X-ray Vision} classifications}

% \subsection{A summary of User studies in \gls{X-ray Vision}}
% Go over the information gathered regarding \gls{X-ray Vision} studies.

% There tends to be three main areas of interest within \gls{X-ray Vision} research and they are Usability Depth perception and general perception of objects within the X-ray able object.

% Details about participant information. Amounts of participants and how they were grouped creating how many random variables. Were the studies done within or between subjects?

% Details about the amount of independent variables how they tended to break down and details about the amount of repetitions

% \subsection{Types of \gls{X-ray Vision} classifications}
% \textbf{Virtual hole}
% The first type of \gls{X-ray Vision} to be done. This was used in medical \gls{X-ray Vision}. 

% \textbf{Cutaways}
% Similar to virtual holes but not used to much in Augmented reality compared to Virtual reality or in general graphics design. This methodology uses abstract shaped holes within the original object creating more areas that can occlude the view from people. 

% \textbf{Block Effect}
% The uses of showing occlusion by using quads. 

% \textbf{Light Grid}
% Similar to the block effect but to increase the saturation for on areas on a object where they are hit. 

% \textbf{Alpha Blending/Levels of Transparency}
% It is possible to show some level of X-ray visiulistation just by adjusting the transparency of a physical object or the transparency of the occluded object. Sometimes this is dynamically adjusted to look more correct with the  

% \textbf{Wire-Frame}
% The Act of placing a wireframe over an area of transparency. Showing a virtual example of depth. 

% \textbf{Edge based}
% By highlighting the effect of edges of an object we are able to create the appearance that an object is behind the object. 

% \textbf{Tunneling}
% Similar to a virtual hole tunneling is an \gls{X-ray Vision} technique that is able to convey depth though a building or a wall to the user. 

% \textbf{Melting}
% Showing an effect similar to by showing parts of the occluded object and parts of the occluded object the object that needs to be viewed through. 

% \textbf{Saliency}
% Occludes objects based on the level of salincy detected over that area of the users view. 

% \textbf{Auxiliary Augmentation}
% This type of X-ray visualization shows how depth can be demonstrated through an object based on its relationship to other objects around it. 

% \textbf{Ghostings}
% Ghostings work similarly to sailency but are developed mainly for the purposes for images and monoisotopic displays

% \textbf{Adaptive Ghosting}
% Simply but this method combines Kalkofens Edge based techniques with Zolmans Ghosting techniques to apply a minimal cut out effect with a higher level of effect to the effect.

% \textbf{Random Dot}
% Highlighting various pixels on the surface of objects in order to provide a stereo stopic effect of occlusion for the user.

% \textbf{Vengeance Based}
% Vengeance based \gls{X-ray Vision} is designed to have people need to change and move their focus away from the foreground to focus on the background image. 

% \textbf{Silhouette}
% Ozgur et al. created an effect that only uses a light amount of a \gls{X-ray Vision} technique that highlights the area of effect around the object to show the most effective but minimal amount of occlusion.

% \textbf{Multi Masking}
% A proposed extension to edge based displays that also extra information relating to the faces of objects that may also be occluded. 

% \textbf{P-Q Space}
% This method can be used when applying different types of information to volumetric fields of data that may not share the same amount of realism. It finds regions where the depth of the image has changed dramatically on the surface body and highlights them to the end user. This type of \gls{X-ray Vision} is more designed for repairing the depth perception of two virtual objects that may not share the same type of rendering style. By having this space

% \subsubsection{A summary of User studies in \gls{X-ray Vision}}
% Go over the information gathered regarding \gls{X-ray Vision} studies.

% There tends to be three main areas of interest within \gls{X-ray Vision} research and they are Usability Depth perception and general perception of objects within the X-rayable object.

% Details about participant information. Amounts of participants and how they were grouped creating how many random variables. Were the studies done within or between subjects?

% Details about the number of independent variables how they tended to break down and details about the number of repetitions

% Performing Questionnaires and running Anovas and T tests were the most common form of data analysis in the field. 


% \subsubsection{Synthesis of results}
% Talk about the risk of bias in the current studies

% \subsection{Discussion}
% Summery of all the results before

% \section{conclusion}
 \newpage 
    %\include{Chapter2/BackgroundBrief}
    %\include{Chapter2/ReviewofX-rayVisualisationsinAugmentedReality}
    \glsresetall
    %\include{Chapter3/Chapter3Brief}
     \newpage \chapter{Spatial Estimation In Augmented Reality Aided X-Ray Vision} \label{Chap:X-ray Implemntion}

This chapter explores the parts of \gls{X-ray Vision} that are functional or essential to providing the illusion of looking through an object with the aim of understanding how the visualization can support improved spatial estimation accuracy.
\DIFdelbegin \DIFdel{This draws }\DIFdelend \DIFaddbegin \DIFadd{While limiting it's self of OSTAR devices, and utilizing techniques from prior research on other devices this research is able to draw }\DIFaddend knowledge from some of the most cited \glspl{X-ray Visualization} from \DIFdelbegin %DIFDELCMD < \autoref{chap:Background}%%%
\DIFdel{'s }\DIFdelend \autoref{sec:BackXRayVision} and looks at how users experience placing virtual objects behind or inside a physical world object.
\DIFdelbegin \DIFdel{Aiming }\DIFdelend \DIFaddbegin \DIFadd{The study presented in this chapter aims }\DIFaddend to understand the depth and spatial perception challenges and their spatial relationships \DIFaddbegin \DIFadd{when using }\gls{ost} \gls{ar} \gls{X-ray Vision} \DIFadd{by utilizing a method to migrate video footage over and recalibrate it to their real world vision}\DIFaddend .
Using the findings from this research informs the \glspl{X-ray Visualization} that should be made for \gls{dvr} using \gls{ost} \gls{ar} presented in \autoref{Chap:VolumetricX-rayVision}.

% These results will be evaluated by asking a user to recreate a physical scene within an occluded space using \glspl{X-ray Visualization}.
% This is different to previous works in the space because participants allow users to move around almost freely encouraging participants to use the visualizations in a natural manner.
% This allowed participants to utaize individual strategies and showcasing strengths and weaknesses of the \glspl{X-ray Visualization} regardless of how the user interacted with it. 

Overlaying three-dimensional virtual content on the physical world can be problematic in terms of aligning the perceived depth of the virtual and physical information in AR. 
This is due to AR devices' limitations to present data to people intuitively, and misalignment between the physical and virtual \DIFaddbegin \DIFadd{world }\DIFaddend will present this information as being further away~\cite{Fischer2020a}. 
This is made worse when virtual objects are placed beyond the physical object's bounds~\cite{Bajura1992} as the user's sense of depth is influenced by their ability to recognize visual relationships. When visual cues in AR are not carefully designed, the user may perceive virtual objects to be smaller instead of further away~\cite{Berning2014a}. 
X-ray vision is one approach explored to address some of the challenges of correctly perceiving where virtual objects are positioned in the physical world.

\glspl{X-ray Visualization} have utilized a form of visual saliency~\cite{Kalkofen2007, Sandor2010, Zollmann2010, Kalkofen2013} that was possible by using a \gls{vst} \gls{ar} display that would render the world again from the perspective of a camera.
\gls{ost} devices make this difficult since the user can dynamically see the world through the display. The system does not have a strong picture of the visual saliency the users are looking at. 
This is challenging as there are many different shapes of human heads with eyes placed at different positions; the position that visual objects appear in using an \gls{ost} \gls{ar} \gls{hmd} is it little better than a guess. 
Little focus is on research merging physical and virtual objects with \gls{ost} \gls{ar}, considering depth has been explored.

To make a system more accessible, a system was designed that took \gls{ost} \gls{ar} sensor calibration pipeline that can translate camera footage to a person's sight. 
\gls{ost} \gls{ar} \glspl{X-ray Visualization} benefit from better adapting to changes in the real world and adjusting the visualization to suit what the user is perceiving and is helpful outside of \gls{X-ray Vision} as it allows creation of visualizations or to make virtual objects react to actual world virtual stimuli that any computer vision technology can create.
\gls{ost} \gls{ar} is necessary for several tasks that require the user's vision to be unobstructed, like driving\cite{Gabbard2014}, Surgery\cite{Rolland2000}, and Security operations\cite{Phillips2020} where VST AR HMDs are not suited. \gls{vst} technologies suffer latency issues and are vulnerable to hardware failure, which could be catastrophic for mission-critical tasks like surgery.
\gls{ost} does not suffer from these limitations and may be better suited to the applied tasks above, motivating to investigate further the use of \gls{X-ray Vision} techniques on \gls{ost} technologies.
To adapt \gls{vst} \gls{ar} \glspl{X-ray Visualization}, this research had to overcome several challenges, such as representing the visualization within the correct depth of field and in the appropriate shape the user perceives it to be, which will be explained in depth in this Chapter.


\section{Updates Since Running This Study}

% Gruenefeld et al.~\cite{Gruenefeld2020} performed depth perception studies using an OST headset utilizing x-ray vision, including Grid and Cut-out effects. 
% The grid effect, placed on the ground and a perpendicular wall, could be used to approximate a relationship between the wall and the object. In contrast, the cut-out visualization provided a hole in the wall, which the user could look through to see where the object was on the other side.
% Against a baseline that pointed participants to the object with a red arrow and displayed a perpendicular line going from the far point of the arrow to the bottom of the visualization. 
% Gruenefeld et al.'s~\cite{Gruenefeld2020} found that the weakest of the depth cues were the wireframe and the cut-out. This was because neither of these cues conveyed clear depth indications to the user~\cite{Gruenefeld2020}.

% Another set of studies published by Martin-Gomez et al.~\cite{MartinGomez2021} studied the difference between several \glspl{X-ray Visualization} (None (Superimposition), virtual hole, ghosting, and Random Dot) on both VST AR and OST AR devices in the near field. 
% They found that users better-used \gls{X-ray Vision} on a VST AR headset than on an OST AR device.
% This prompted another study investigating different rendering techniques for \gls{X-ray Vision} (shading, hatching, ghosting) and brightness levels, finding that bright, clear objects work best in OST AR.

Since running this study, several similar studies have published their results as are detailed in \autoref{sec:ComparisionOfDisplays}. 
This research maintained its novelty by evaluating the effects of various VST AR effects, such as Saliency and Edge-Based techniques.
These techniques have not been compared against \textit{Random Dot} and wire-frame \glspl{X-ray Visualization}.
Moreover, previous studies commonly restricted the user's movement while investigating the different approaches to depth cues. 
Rather, This work focuses on a more natural interaction with these effects, looking at a less restrictive solution to test the uses of various \glspl{X-ray Visualization} to understand better users' natural processes for dealing with different types of occlusion.


\section{Adopting X-ray Visualizations for Augmented Reality} \label{sec:X-ray Vis}

% paragraph explaining 
% why,
% partial occlusion
% 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend This research chose to focus on auxiliary effects (using groups of objects) \DIFdelbegin \DIFdel{, which is an effect that most }\DIFdelend \DIFaddbegin \DIFadd{and }\glspl{X-ray Visualization}\DIFadd{. 
many }\DIFaddend medical applications will likely cause \DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{some auxiliary effects in the data, because most medical applications will require the user to look through a large amount of data to find a small part of interest. 
Auxiliary effects work by introducing additional reference objects or visual cues into the scene, helping users to better judge spatial relationships and depth.
In the context of volume rendering, auxiliary effects can include the placement of markers, or anatomical landmarks within or around the volume. 
These added elements provide extra context, making it easier for users to locate, interpret, and interact with regions of interest within complex volumetric data. 
By enriching the visualization with supplementary information, auxiliary effects help users navigate dense or ambiguous data, reduce cognitive load, and enhance overall spatial understanding.
}

\DIFadd{The }\glspl{X-ray Visualization}\DIFadd{s chosen for this study were:
}\DIFaddend \begin{enumerate}
    \item Edge and Saliency techniques occluded large amounts of the data behind the objects, while the other two drew lines over the object you were trying to look through. Creating a High Occlusion Model and a Low Occlusion Model for dynamic effects.
    \item The other consideration was using geometric saliency vs. visual saliency (or Real-world Overlays vs. Computer Vision Enabled \gls{X-ray Vision} techniques). Since there is no record of using Computer Vision Enabled \gls{X-ray Vision} techniques on stereoscopic \gls{ar} devices.
\end{enumerate}
Hole-in-the-world visualizations were ruled out as limiting the view of the visualization, requiring more volume data to be produced. Four visualizations were chosen due to the differences in their designs.
Instead, a back face to this was applied to the cube, which has been shown to improve depth perception~\cite{Lerotic2007}.

Four types of \gls{X-ray Visualization} techniques \DIFaddbegin \DIFadd{are }\DIFaddend utilized for use with \gls{ost} \gls{ar} displays: \textit{Random Dot}, \textit{Tessellation} (similar to wireframe), \textit{Edge-Based}, and \textit{Saliency}.
These visualizations were chosen because they were all only utilized up to this point for \gls{vst} \gls{ar} \gls{X-ray Vision} Papers.
\textit{Edge} and \textit{Saliency} used Computer Vision-Enabled Techniques, while the other two utilized a Real-world overlay (\textit{Random-Dot} and \textit{Wireframe}). This could then further be split as one from each of these groups blocked out a large portion of the user's vision (\textit{Random Dot} and \textit{Saliency}), while the other used thin lines to show contrast to the world via the display (\textit{Edge} and \textit{Wireframe}).

% Placed here to help with the flow of  content should logically be lower
\begin{figure}[tb]
    \centering
    \includegraphics[width = \textwidth]{Chapter3/Images/Visualizations/OtsukiRandomDot.png}
    \caption[a) Otuski et al.'s~\cite{Otsuki2015} version Random Dot b) A image of the \textit{Random Dot} visualization used in this chapter.]{a) Otuski et al.'s~\cite{Otsuki2015} version Random Dot b) A image of the \textit{Random Dot} visualization used in this chapter. Taken using a HoloLens2 under study conditions. a) Was produced by By Otuski et al.~\cite{Otsuki2015} and is licensed under a Creative Commons Attribution licence}
    \label{fig:RandomDotImage}
\end{figure}

Any virtual object that is partially occluded by another virtual object is in front of a given object~\cite{Bajura1992, Vishton1995}. 
However, to make this a seamless experience, it is required to experience this.
Research on all these techniques has shown the importance of partial occlusion, but no work has been done comparing their ego-centric constraints to this point~\cite{Sandor2010, Dey2012, Dey2014, Zollmann2014, Tsuda2005}.
While other studies have researched the impact of the amount of occlusion, these forms of \gls{X-ray Vision} can bring~\cite{Santos2016}, which have been used and considered when designing the parameters used when adapting these visualizations.
However, all these studies were much more controlled than ours and were run using a range of \gls{vst} devices. 

\DIFaddbegin \DIFadd{Regardless, some of the technical considerations for each of these visualizations have been adapted around the same environmental considerations to ensure the visualizations are as fair as possible.
Firstly every visualization utilized a back face to the cube, because it has been shown to improve depth perception~\mbox{%DIFAUXCMD
\cite{Lerotic2007}}\hskip0pt%DIFAUXCMD
.
The back face was a contrasting pink color that contrasted with the virtual objects and the large Voronoi cube (shown in }\autoref{fig:Basic image of setup}\DIFadd{).
Placing a plane on any face of the cube that is facing away from the user that is visible in AR. 
Once a virtual object goes past the back face, it will begin to clip behind this object until it is no longer visible to the user.
}

\DIFadd{The X-rayable area was the 0.6m x 0.6m x 0.6m area within the cube between the backface of the effect and all the }\glspl{X-ray Visualization} \DIFadd{except for the baseline }\DIFaddend No \gls{X-ray Visualization} (referred to as \textit{None})\DIFdelbegin \DIFdel{was used as the baseline for this study, }\DIFdelend \DIFaddbegin \DIFadd{.
}\textit{\DIFadd{None}}  \DIFaddend which provided no occlusion over any part of the virtual objects within the x-rayable space. 
Simply, it superimposes the visualization of the objects.
\DIFdelbegin \DIFdel{This had the effect of making it }\DIFdelend \DIFaddbegin \DIFadd{To the user }\textit{\DIFadd{None}} \DIFadd{would likely appear to the user to }\DIFaddend seem as if the back plate of the X-rayable area was in front of the x-rayable area \DIFaddbegin \DIFadd{even though it is technically in the same area for each condition}\DIFaddend .

\subsection{Random Dot X-ray Visualization}

\textit{Random Dot} appears as a grid of square dots that can be turned on or off. 
Generally, these are colored a slightly translucent shade of black \autoref{fig:RandomDotImage}. 
The instance of \textit{Random Dot} used in this study uses the translucent shade of white \autoref{fig:RandomDotImage} b, since \gls{ost} \gls{ar} headsets can not render black.
Random Dot was \DIFaddbegin \DIFadd{a }\textit{\DIFadd{real world overlay }\gls{X-ray Vision} \DIFadd{technique}} \DIFadd{was }\DIFaddend used because it relied on \gls{slam}~\cite{Karlsson2005, Klein2007} for positioning and obscured some of the user's vision into the X-rayable space and geometrical saliency, akin to \textit{Saliency}. 
The implementation of this visualization followed the published description of the visualization by Otsuki et al.~\cite{Otsuki2016, Otsuki2017}, with a scale for different-sized dots and density, 
a low-resolution texture (50 x 75 px), and randomly made half of the pixels either transparent white or clear.
%This resolution was chosen because it  best fits our real-world environment.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Chapter3/Images/Visualizations/livingstoneWireframe.png}
    \caption[a) Livingston et al.'s~\cite{Livingston2003} initial version of the wireframe \gls{X-ray Vision} effect b) A image of the \textit{Tessellation} visualization used in this chapter.]{a) Livingston et al.'s~\cite{Livingston2003} initial version of the wireframe \gls{X-ray Vision} effect b) A image of the \textit{Tessellation} visualization used in this chapter. Taken using a HoloLens2 under study conditions. a) Was used with permission from IEEE \textcopyright{} 2003}
    \label{fig:TesselationImage}
\end{figure}

\subsection{Tessellation X-ray Visualization}

Wireframes have long been used as \glspl{X-ray Visualization}, giving the user some identification of where the real world is compared to the virtual world and being able to provide some identification of minor occlusion~\cite{Tsuda2005, Webster1996}.
%DIF > The \textit{Tessellation} visualization shown in \autoref{fig:TesselationImage} shows a similar but different take on the same properties as a wireframe model was chosen over a wireframe visualization to allow for flexibility~\cite{Hettinga2018} by allowing geometrically salient patterns without needing to add any extra polygons.
The \textit{Tessellation} visualization shown in \autoref{fig:TesselationImage} shows a similar but different take on the same properties as a wireframe \DIFdelbegin \DIFdel{model was chosen over a wireframe visualization to allow }\DIFdelend \DIFaddbegin \DIFadd{visualization but is using a geometry shader and calculating a uniform dimension for each of the tiles to become between each edge of the shape. 
This allowed }\DIFaddend for flexibility~\cite{Hettinga2018} by allowing geometrically salient patterns without needing to add any extra polygons.

\textit{Tessellation} subdivides a wireframe into more triangles, covering more area. 
\DIFaddbegin \DIFadd{Generally, this is done to increase the quality of a virtual object by creating increasing the polygon count without adding extra geometry~\mbox{%DIFAUXCMD
\cite{Hettinga2018}}\hskip0pt%DIFAUXCMD
, but in this case, it was used to create a wireframe-like effect that could be adjusted to cover more or less of the cube.
}\DIFaddend This visualization subdivided an existing wireframe further and would normally be designed to allow for a dynamic range of quality to be applied to a virtual object~\cite{Hettinga2018}.
\DIFaddbegin 

\DIFaddend A uniform Tessellation algorithm was used to ensure the triangles were placed in an even and logical manner. 
\DIFaddbegin \DIFadd{The uniform triangular tessellation using fractional-odd partitioning on triangle patches where each original triangle gets subdivided into smaller triangles. This was set to produce five regular and evenly distributed across the entire surface~\mbox{%DIFAUXCMD
\cite{flick_2017}}\hskip0pt%DIFAUXCMD
.
}\DIFaddend Allowing complete flexibility to manually manipulate the size of the effect to choose the amount of the cube covered by the effect.
For this application, a uniform \textit{Tessellation} was utilized to keep the quality increased amount and aimed to create five new lines coming out of each edge~\cite{flick_2017}. %This is technically where I got the idea for this effect
This allowed a visualization seen in \autoref{fig:TesselationImage} like a Wireframe and retained geographical sense while maintaining realistic and continuous while also manipulating the total number of lines, allowing for more partial occlusion. 

\subsection{Edge-based}

\DIFaddbegin \DIFadd{The }\DIFaddend \textit{Edge-Based} \DIFaddbegin \DIFadd{visualization }\DIFaddend places a white line over all areas that show contrast between sets of \DIFdelbegin \DIFdel{neighboring }\DIFdelend \DIFaddbegin \DIFadd{neighbouring }\DIFaddend pixels. 
This visualization was selected\DIFaddbegin \DIFadd{, since }\DIFaddend as seen in \autoref{fig:edgeBasedImage}, the \textit{Edge-Based} visualization occludes very little of the user’s vision and uses salient points of interest in the user's own point of view to provide the visualization.
Designed initially by Kalkofen et al.~\cite{Kalkofen2007}, this visualization provides just enough occlusion to show virtual objects as opposed to the visually salient parts of real-world objects, effectively guiding users to discern the presence and location of virtual entities.
The Sobel algorithm was utilized for this implementation with a delta X and delta Y of 5 (performing the algorithm over a 5 by 5 grid).
The Sobel algorithm was chosen over other edge-based detection algorithms due to how well it performed in parallel, using only a single step, unlike other edge detection algorithms, such as the canny edge detection. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{Chapter3/Images/Visualizations/KalkofenSaliency.png}
    \caption[a) Kalkofen et al.'s~\cite{Kalkofen2007} version of \gls{X-ray Vision} b) A image of the \textit{Edge-Based} Visualization used in this study.]{a)Kalkofen et al.'s~\cite{Kalkofen2007} version of \gls{X-ray Vision} b)A image of the \textit{Edge-Based} Visualization used in this study. Taken using a HoloLens2 under study conditions. a) Was produced by Kalkofen et al.'s~\cite{Kalkofen2007} and was used with permission from IEEE \textcopyright{} 2007}
    \label{fig:edgeBasedImage}
\end{figure}

\subsection{Saliency}

Visual saliency describes how much a given object or region in a scene may stand out or attract attention to the viewer. 
As mentioned in previous chapters, Sandor et al.~\cite{Sandor2010} found that by using \DIFdelbegin \DIFdel{the objects in }\DIFdelend by showing attention-grabbing objects in the foreground, you could create an \gls{X-ray Vision} visualization that communicates to users with a good level of depth perception where an object exists. 
\textit{Saliency} was chosen for this study because it partially occludes some of the user's vision, like \textit{Random Dot}, and uses the visually salient regions in the user's own point of view, similar to the ~\textit{Edge-based} visualization.
Tian et al.'s~\cite{Tian2009} Saliency algorithm was adapted to run in parallel on a GPU.
This first required the HSL (Hue Saturation and Intensity) value~\cite{Valensi1945} to be extracted from the RGB color using a similar method to Saravanan et al.~\cite{Saravanan2016}.
The values of the individual Hue, Saturation, and Intensity Contrast were then calculated along with the Dominance of both the warm colors and the Saturation and Intensity.
These methods were then normalized and weighted by utilizing a static set of values that were calculated prior based on the lighting of the study environment. 
The study environment provided consistent lighting and colors to ensure these values remained consistent.

Unlike the other visualizations, the \textit{Saliency} algorithm would utilize salient features from the real world to occlude the virtual objects~\cite{Sandor2010}.
VST AR devices normally do this by \DIFdelbegin \DIFdel{reducing by }\DIFdelend rendering the real world partially on top of the virtual world. 
\DIFdelbegin \DIFdel{This won't work in using }%DIFDELCMD < \gls{ost} \gls{ar} %%%
\DIFdel{as the real world is not rendered using a stereoscopic video feed}\DIFdelend %DIF > This will not work in using \gls{ost} \gls{ar} as the real world is not rendered using a stereoscopic video feed, it is a person's actual vision. 
\DIFaddbegin \DIFadd{In VST AR, depth is reconstructed from a stereoscopic camera feed, but in OST AR the user sees the real world directly through transparent optics.
Making it difficult to know what the user is looking at and how to occlude the virtual objects over users vision of the real world}\DIFaddend . 
\DIFdelbegin \DIFdel{It is a person's actual vision . 
}\DIFdelend To get around this issue, the visualization used in \autoref{fig:saliencyImage} was used. 
Instead of producing saliency over the top of real-world constraints, this version of saliency would make more salient areas of real-world objects more opaque and others less opaque by rendering a black area over the x-rayable area. 
Rendering the salient areas as black enabled the system to interpolate between fully occluding the object and being completely transparent without the user being able to view these abilities. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\textwidth]{Chapter3/Images/Visualizations/SandorSaliency.png}
    \DIFdelbeginFL %DIFDELCMD < \caption[a) Sandor et al.'s~\cite{Sandor2010} version of Saliency. b) An image of the \textit{Saliency} Visualization used in this study.]{%
{%DIFAUXCMD
\DIFdelFL{a) Sandor et al.'s~\mbox{%DIFAUXCMD
\cite{Sandor2010} }\hskip0pt%DIFAUXCMD
version of Saliency. b) An image of the }\textit{\DIFdelFL{Saliency}} %DIFAUXCMD
\DIFdelFL{Visualization used in this study. Taken using a HoloLens2 under study conditions. a)Was produced by Sandor et al.'s~\mbox{%DIFAUXCMD
\cite{Sandor2010} }\hskip0pt%DIFAUXCMD
and was used with permission from IEEE \textcopyright{} 2010}}
    %DIFAUXCMD
\DIFdelendFL \DIFaddbeginFL \caption[Two images that use saliency detection. a) Sandor et al.'s~\cite{Sandor2010} version of Saliency. b) An image of the \textit{Saliency} Visualization used in this study]
    {
        \DIFaddFL{Two images that use image saliency detection to find area's of human interest within the images to occlude virtual objects.
        a) Sandor et al.'s~\mbox{%DIFAUXCMD
\cite{Sandor2010} }\hskip0pt%DIFAUXCMD
version of Saliency which changes the opacity of objects in the foreground to reveal another video image in the background. b) An image of the }\textit{\DIFaddFL{Saliency}} \DIFaddFL{Visualization used in this study which increases the occlusion of the real world objects by creating a transparent mask over the top of the objects from drawing back in the screen a color that the HoloLens is unable to display at varying angles. b) Was taken using a HoloLens2 under study conditions.
        In this setup, the large cube was decorated with a Voronoi pattern of brightly coloured tiles.
        These coloured tiles acted as salient features: the irregular edges, strong contrasts, and varied hues provided multiple points of visual interest that the saliency algorithm could reliably identify. This ensured consistent saliency detection across the cube’s surface, while also preventing participants from relying on simple geometric cues such as uniform grids.
        a) Was produced by Sandor et al.'s~\mbox{%DIFAUXCMD
\cite{Sandor2010} }\hskip0pt%DIFAUXCMD
and was used with permission from IEEE \textcopyright{} 2010
    }}
    \DIFaddendFL \label{fig:saliencyImage}
\end{figure}

%, so they would suit the study environment better. 
% The rectangular shape of the dots was chosen due to the user’s general orientation to the cube. 
%No immersive OST AR applications have ever overlaid a picture of the real world 
\textit{Edge-Based} and \textit{Saliency} \glspl{X-ray Visualization} required major additions to their calibration to make them work with OST AR devices are described in detail in the next section (\autoref{sec:X-ray VST Overlay}).

\section{Rendering Considerations} \label{sec:X-ray VST Overlay}
To take advantage of \glspl{X-ray Visualization} that were traditionally designed for VST AR systems, A system that could transmit camera data over to an OST AR display was required. 
%This would allow the system to highlight areas of high visual impact. 
The following system was designed to display the output of a video feed over a user's vision for x-ray vision, allowing the system to highlight areas of interest in the user's vision. However, its potential uses are not limited to this.

\subsection{Video Image Overlay Method} \label{sec:X-ray VST Overlay Method}
% What is the problem that this area is addressing
Calibrating video see-through techniques, such as \textit{Saliency} or \textit{Edge-Based} visualizations for see-through video headsets and mobile devices~\cite{Avery2009, Sandor2010}, presents the need for a change in calibration as a 2D image will need to be able to respond to the user's depth cues to present the correct affect~\cite{Li2012}. 
This system would need to accommodate the difference between the distortion of human sight~\cite{Peek2014}, reacting to their movements as fast as possible~\cite{Wang2023, Li2012} while understanding the depth of field.
To date, no algorithms for \textit{Saliency} or \textit{Edge-Based} visualizations were found that made use of bifocal vision\cite{Otsuki2016, Otsuki2017}, with each eye requiring its display that would not overlay onto a video feed but the participants’ view of the real world. 
The closest method previously used was Hamadouch's~\cite {Hamadouche2018} method, which displayed \textit{Edge-Based} \gls{X-ray Vision} using projectors in collaboration with the HoloLens2. 
Our technique extended this concept by applying \gls{X-ray Vision} visualization to a virtual projector, creating the illusion of depth within the augmented world and aligning the virtual objects with the view of the physical world through the headset. 

% Explaining at a high level what the distortion was doing
As illustrated by \autoref{fig:FrameByFrameGuide}, each time a new image from the video feed was received, the image was filtered to represent either a \textit{Saliency} or \textit{Edge-Based} map and then distorted to align with the users' vision. 
This image was then used as input into a virtual projector from the user's perspective but only interacting with the X-rayable object.
Outputting either a color-based saliency map for the \textit{Saliency} condition or a Sobel edge detection for the \textit{Edge-Based} visualization, 
This was then distorted for each of the user's eyes. 
For \textit{Saliency}, a shadow occludes the graphics rendered within the cube, while a white light was projected for an \textit{Edge-Based} visualization.

\begin{figure}
    \centering
    %\includegraphics[width=\columnwidth]{Chapter3/Images/ViewPerspectivesBetter.png}
    \includegraphics[width=\columnwidth]{Chapter3/Images/ViewPerspectivesBetterNoDepthPlane (1).png}
    \caption[This image illustrates how different fields of view (FOV) were managed to work with each other.]{This image illustrates how different fields of view (FOV) were managed to work with each other. 
    The yellow line indicates the user's entire bifocal field of view ($130^\circ$ vertically).
    The red lines show the Field of View (FOV) of the HoloLens2 from the user's point of view ($29^\circ$ vertically).
    The blue lines show the FOV of the ZED Mini from the position it was mounted ($54^\circ$ vertically).}
    \label{fig:ViewPerspectives}
\end{figure}

\DIFaddbegin \DIFadd{The HoloLens2 was used as the OST AR device, and a ZED Mini stereo camera~}\footnote{\url{https://www.stereolabs.com/en-au/store/products/zed-mini}} \DIFadd{was used to capture the video feed.
The ZED Mini was chosen due to its small size, stereoscopic video feed however this this use the inbuilt funciationlity of the was not required as the hovermap was able to provide this and the distortion algorithm is incorrect for this use case.
The ZED Mini was mounted on the front of the HoloLens2, as close to the user's eyes as possible, as shown in }\autoref{fig:ViewPerspectives} \DIFadd{using a custom 3D printed mount described in }\autoref{sec:X-rayAddionalSensors}\DIFadd{.
}


\DIFaddend % Explaining how the initial offset was fixed
On an OST AR display, it is impossible to mount a camera that will see the world from the same perspective as the user, so these cameras \DIFdelbegin \DIFdel{are placed }\DIFdelend need to be placed away from the user's view. 
First, the offset needs to be applied to the virtual world to overlay the image from the camera.
\DIFdelbegin \DIFdel{The camera output is then rendered based on the location where the image was taken, invisible to the user for the first frame until it is projected on the next frame in the same place the image was taken.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{The difference between the views of the HoloLens2's Field of View (FOV) and the camera's FOV can also impact the amount of the virtual vision that can be overlaid.  
}%DIFDELCMD < \autoref{fig:ViewPerspectives} %%%
\DIFdel{shows that given the camera has a wide enough field of view, these issues can only occur when objects are rendered close to the user, given there is a larger discrepancy will ever be seen if users move to close to the object.
}%DIFDELCMD < 

%DIFDELCMD < \begin{figure}
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\columnwidth]{Chapter3/Images/FrameByFrameGuide.png}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption[This diagram explains the frame-by-frame process that the system used to process images]{%
{%DIFAUXCMD
\DIFdelFL{This diagram explains the frame-by-frame process that the system used to process images; each frame away from receiving the first images is labeled up the top of the diagram. Each system task is listed in a square box. ZED Mini describes the process where the ZED Mini provides the system with the image. Filter explains whether the Sobel edge-based filters are applied. The render state is when the final scene is distorted and rendered to be overlaid onto the user's vision.}}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:FrameByFrameGuide}
%DIFDELCMD < \end{figure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend % Talking about the radial distortion
Radial distortion is required to match the virtual representation of the real world so that the virtual content the camera produced is accurately aligned with the user's vision~\cite{Brown1966, Zhang1998, DeVilliers2008}. 
By applying this to the visualization frame, the edges of the display were matched to the user’s vision of the real world. 
Since the eye movements of the user required some input from their bifocal cues, the distance-based distortion algorithm~\ref{alg:distortionAlgorithm}) was applied to the visualization from the headset. 
This required the distance from the user and the visualization to be tracked, as the difference in perspective between the camera and the \DIFdelbegin \DIFdel{user }\DIFdelend \DIFaddbegin \DIFadd{headset }\DIFaddend would need to be compensated for. 
Influencing the value along the input camera image's vertical axis (height position).
\DIFdelbegin \DIFdel{These values were based on a series of previously known correct values at certain distances that were found using preliminary human testing at different distances.
Explaining where users expect to where users should expect to see the visualization at different depths.
}\DIFdelend Linear interpolation was used to estimate any unrecorded values between the known ones.
This gave the user the impression that the projection was aligned appropriately on the box.

\DIFaddbegin \DIFadd{The used parameters used to correct the distortion and the perspective offset calculated from the participants subjective feedback during a short test after the pilot study.
Each of the participants used in the pilot study was asked to view the visualizations overlaid on the box from 3 different distances (0.5m, 1m, and 1.5m) at three different orientations on the box with the request provide feedback on the alignment of the visualization.
This feedback was used to adjust the distortion values for the given depth away from the camera was positioned repairing the offset between the camera and the HoloLens display as well as translating the image distortion to from the camera to match the user's vision.
}

\DIFaddend \begin{algorithm}
    \caption[Algorithm for the radial distortion used by the camera.]{Algorithm for the radial distortion used by the camera. \DIFaddbegin \DIFadd{This is a standard GPU variant of the Radial Distortion Algorithm~\mbox{%DIFAUXCMD
\cite{Kang2001, Liu2025, Brown1971}}\hskip0pt%DIFAUXCMD
. }\DIFaddend All variable values are Vector2s, except for D, which is the distortion value and indicates the level of distortion required at a given pixel. To correctly distort the image to resemble human sight, outlet areas of the viewing area, depending on the viewpoint of the user, will need to use barrel distortion, while other inner areas will use pincushion distortion.}
    \label{alg:distortionAlgorithm}
    \begin{algorithmic}[1]
        \State $uv \gets (uv - 0.5) \times (D_z + 0.5)$
        \State $ruv \gets {DScale_x}_y \times (uv - 0.5 - {DCenter_x}_y)$
        \State $ru \gets \text{length}(ruv)$

        \If{$Pincushion\_Distortion$} 
        \State $uv \gets uv + ruv \times ((\tan(ru \times D_x) \times (1.0 / (ru \times D_y))) - 1.0)$
        \Else 
        \State $uv \gets uv + ruv \times (((1.0 / ru) \times D_x \times \arctan(ru \times D_y))) - 1.0)$
        \EndIf
        \State $return \gets uv$
    \end{algorithmic}
\end{algorithm}

\DIFaddbegin \DIFadd{In order to make the visualization appear as if it were being projected from the user's perspective, the system needed to understand where the user was looking and how far away they were from the box however since the pipeline took approximately 50ms to reach the display it was important that the system could predict the closest point the user would have been looking at when the image was taken.
to accomplish this the camera output is then rendered based on the location where the image was taken, invisible to the user for the first frame until it is projected on the next frame in the same place the image was taken.
This allows the user to see the image from the camera as if it were being projected from the user's perspective.
}\autoref{fig:ViewPerspectives} \DIFadd{illustrates that the Camera had a wider field of view than the HoloLens2, so the user could move around within the FOV of the camera without losing the visualization.
One issue to this methodology is that the field of view of the }\gls{X-ray Visualization} \DIFadd{is limited to interactions that are approximately within 10 cm from the box past this point the visualization will be cut off by the edge of the camera's field of view.
}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter3/Images/FrameByFrameGuide.png}
    \caption[This diagram explains the frame-by-frame process that the system used to process images]{\DIFaddFL{This diagram explains the frame-by-frame process that the system used to process images; each frame away from receiving the first images is labeled up the top of the diagram. Each system task is listed in a square box. ZED Mini describes the process where the ZED Mini provides the system with the image. Filter explains whether the Sobel edge-based filters are applied. The render state is when the final scene is distorted and rendered to be overlaid onto the user's vision.}}
    \label{fig:FrameByFrameGuide}
\end{figure}

\DIFaddend % I'm thinking about drawing a diagram of this
The \gls{ost} \gls{ar} \gls{hmd} used for this experiment (a HoloLens2) required a tethered connection to receive a video feed from the camera as initial testing from the inbuilt camera showed several issues: The battery life of the devices decreased dramatically. The performance of the system dropped to a maximum frame rate of 30\gls{fps}; the image had a low quality, and it had been extensively processed. 
A tethered connection would inevitably cause a delay in when the system would need to be known.

The delay between the camera to the computational server and the display was tested using a system similar to Gruen et al.'s\cite{Gruen2020} system and found the camera (the ZED mini) produced a video lag of approximately 50ms from when the image was captured to being displayed by the OST AR Headset (HoloLens2). 
The lag is noticeable when viewing within an OST AR headset. 
To overcome this, a 5 Gbit/s USB connection between the OST AR Headset and the PC was used to render the visualization, enabling a rapid and stable visualization between the system and the OST AR Headset.
Most of the lag occurred between when the picture was taken and when the system could process it.

To compensate for this lag, the asynchronous re-projection algorithm was implemented, which recorded the position the user was looking at each frame using the manner seen in \autoref{fig:FrameByFrameGuide}, and used the position recorded three frames before the picture was acquired in the systems calculations~\cite{VanWaveren2016,  Landvoigt2017}. 
This approach enabled the system to place the image at the approximate position it was taken since the re-projection accounted for ~48ms (3 frames) of the delay.

% talking about how the noise reduction was managed
This system also introduced noise created from a) the difference between the estimated and actual positions of where the picture was taken and b) the user's involuntary head movements (e.g., \Gls{microsaccades}~\cite{Martinez-Conde2013}). 
To repair the damage done by the difference between the estimated and actual positions, the position and orientation where each image frame should have been were rendered, and the image was distorted to match that position. 
%calculate where each image frame would have been rendered. 
The system would then render the frame it received at the position where the user was looking four frames ago.
This would give the system a frame to apply the Sobel or saliency filter and apply the distortion mentioned earlier.

% talking about how the output was smoothed
To help with the \gls{microsaccades}, the position of the output image was first sent into a one euro filter~\cite{Casiez2012}. 
The filter provided the flexibility to allow large sweeping head movements to appear as if they had no latency but filtered values when the user stopped moving. 
This kept the image where the participant expected it without restricting their movement. 

% Explaining the preliminary study that was used to determine the exact parameters
A preliminary study of 5 participants was used to determine the correct parameters for the filter. 
Each participant used the system for 5 to 10 minutes and was able to set the parameters. 
This resulted in the following parameters being selected:
One euro filter’s frequency was set to 60, with a min cut-off set to 1, the beta was set to 200, and the D cut-off was set to 1.
These settings would turn off the filter when the user was making large motions like turning but would keep the image stable when they were standing relatively still. 

% The frequency with this system updated the image
The system was sent a new image every frame that corresponded to the position the user was in approximately 50ms prior. 
The \textit{Edge-Based} and \textit{Saliency} visualizations had an approximately 50ms slower start-up time than the wireframe and \textit{Random Dot} visualizations. 
The system ran on the HoloLens with a constant frame rate of 60 frames per second. 
Our system could achieve this consistently across all visualizations, which was consistent with the camera, allowing for a close match to the user's perceived real-world view and the input video feed.

\section{Pilot Study} \label{sec:X-ray Pilot}
%derived our hypotheses based on a preliminary study that was run 
A pilot study was conducted prior to the main study to test the viability of the \glspl{X-ray Visualization} running on an \gls{ost} \gls{ar} \gls{hmd} on a cohort of 5 participants to test depth perception. 
This was done by comparing the participants' performance of a \gls{vst} \gls{ar} \gls{hmd} against their performance using an \gls{ost} \gls{ar} \gls{hmd} to determine if there were any distinct differences between the implementations on a given headset.
Similar to the design of studies done by Otsuki et al.\cite{Otsuki2017} and Martin-Gomez et al.\cite{MartinGomez2021} using the large Voronoi cube (shown in \autoref{fig:Basic image of setup}) where the participants were asked to guess what geometric shapes where inside or outside of the box if they could determine it.

The participant sat in a chair 1.5 meters away from the cube and was required to say if any of the three provided objects (a sphere, a cube, and a star) were inside it.
No object would appear to be partially inside of the cube
The answers they could give were to say if they were certain it was inside the cube, they were certain it was outside the cube, or they were unsure if it was inside or outside the cube. 
These objects were randomly placed either inside or outside of the object.

Both the \gls{vst} and \gls{ost} systems used a ZED mini~\footnote{\url{https://store.stereolabs.com/en-au/products/zed-mini}} to provide the system with visual information.
The \gls{vst} \gls{ar} \gls{hmd} was built from a Vive pro~\footnote{\url{https://www.vive.com/au/}} using a ZED Mini to provide \gls{ar} support.
The \gls{ost} \gls{ar} \gls{hmd} was a HoloLens2~\footnote{\url{https://www.microsoft.com/en-au/HoloLens}} using a ZED Mini as the system's visual input. 

\DIFdelbegin \DIFdel{This pilot }\DIFdelend \DIFaddbegin \DIFadd{The pilot study was tested using a Friedman test with a post-hoc Wilcoxon signed-rank test with a Bonferroni correction to determine if there were any significant differences between the different visualizations and devices.
The results of the pilot study are to remain confidential due to the ethics agreement; however we did find that Saliency and Random Dot were the most effective visualizations for depth perception.
Regarding the differences between the displays this pilot }\DIFaddend had similar results to Martin-Gomez et al.'s\cite{MartinGomez2021} test regarding both devices, where both devices produced a significantly different effect in regard to depth-based accuracy, with VST AR being more accurate but having many more uncertain answers.
%No other metric showed a significant difference related to depth. 
%The visualization effect was insignificant; however, 
\textit{Saliency} proved the most effective \gls{X-ray Visualization} for depth perception, with \textit{None} being the worst. 
A System Usability Scale (SUS) showed that people preferred to use the \DIFdelbegin \DIFdel{camera-based effects}\DIFdelend \DIFaddbegin \DIFadd{Computer Vision-Enabled Techniques (}\autoref{sec:ComputerVisionEnabledTechnqiue}\DIFadd{)}\DIFaddend .
Possibly due to the small amount of participants these results showed very few significant differences(p-value $>$ 0.05).

\section{User Study} \label{sec:X-ray User Study}



\begin{figure}
    \centering
    \includegraphics{Chapter3/Images/Visualizations/None.png}
    \caption{A image of the Baseline (\textit{None}) condition}
    \label{fig:BaselineImage}
\end{figure}
\DIFaddbegin 

\DIFaddend A placement task assessed how well participants could place virtual objects within a large physical cube decorated with a colorful Voronoi pattern (shown in \autoref{fig:Basic image of setup} and \autoref{fig:BaselineImage}). Users could walk freely within a prescribed area in the study environment detailed in \autoref{fig:Study One Set up}.
\DIFaddbegin \DIFadd{This allowed for a more natural interaction with the virtual objects and the environment and provided a more realistic scenario for how these visualizations would be used in practice than testing depth based alignment tasks.
Allowing movement was essential because depth judgments in AR rely heavily on motion parallax and viewpoint changes, which cannot be experienced in a seated, fixed-position setup. Furthermore, in practical AR applications users naturally move around objects to inspect them from multiple perspectives, so incorporating movement provided both stronger perceptual cues and a more ecologically valid evaluation of the visualization techniques than a purely seated, static study.
In order to ensure that participant specific skills like hand-eye coordination, spatial reasoning, and motor skills did not affect the results of the study, a within-subjects design was used where each participant experienced all conditions of the study, along with a random effects style analysis to control for individual participant bias.
}

\DIFadd{The pilot study described in }\autoref{sec:X-ray Pilot} \DIFadd{which was used informed the design of this study and to provide a comparison between the two types of }\gls{ar} \DIFadd{devices.
Since for this Dissertation there was a requirement to use an }\gls{ost} \gls{ar} \DIFadd{device to enable these visualizations, it was decided that the main study would only use an }\gls{ost} \gls{ar} \DIFadd{device so the all that was required was to compare the different }\gls{X-ray Visualization} \DIFadd{techniques.
While alignment tasks or comparisons to }\gls{vst} \glspl{hmd} \DIFadd{could provide baseline performance measures, they would not isolate the contribution of visualization techniques.
The study therefore focused on comparing alternative visualizations within }\gls{ost} \gls{ar}\DIFadd{, as this directly addressed the research question of how X-ray visualizations perform in see-through displays.
}


\DIFaddend Participants were either assisted with one of the four \gls{X-ray Visualization} techniques \textit{Random Dot} (\autoref{fig:RandomDotImage}), \textit{Tessellation}, \textit{Edge-Based} (\autoref{fig:edgeBasedImage}), and \textit{Saliency} (\autoref{fig:saliencyImage}) as described in the \autoref{sec:X-ray Vis}, or they saw no augmented visualization(\autoref{fig:BaselineImage}).
Moreover, additional virtual reference objects were placed in the cube to assess their impact on placement accuracy. Additional measures included the participant's cognitive load, usability indicators, and rate of movement. 

%This user study was designed to evaluate the effects of \textit{Edge-Based}, \textit{Saliency}, \textit{Random Dot}, and \textit{Tessellation} \glspl{X-ray Visualization} on the participant's ability to place a virtual icosahedron object precisely within the larger cube. Moreover, additional virtual reference objects were placed in the cube to assess their impact on placement accuracy. Additional measures included the participant's cognitive load, usability indicators, and rate of movement. 

\subsection{Research Questions}
Our research questions were as follows:
\begin{enumerate}[label=RQ.\arabic*]
    \item Is there a difference in accuracy when placing a virtual object when different \gls{X-ray Visualization} effects are used?
        \begin{enumerate}[label=RQ.1.\arabic*]
            \item Do different \gls{X-ray Visualization}s affect the various axes (vertical, horizontal, and depth) differently?
        \end{enumerate}
    \item What effect does adding more reference objects within an object have on this task?
    \item Do participants move differently when they are presented with different \gls{X-ray Visualization}s?
    \item What are the participants' perceived differences of the \gls{X-ray Visualization}s on an OST headset?
\end{enumerate}

\subsection{Hypotheses} \label{sec:X-ray Hypotheses}
The hypotheses of the study were as follows:
% %--- Group these and relate them to the research questions in a logical order ---[noitemsep]
\begin{enumerate}[label=H.\arabic*]
    \item Participants will place the virtual icosahedron closer to the correct target position when they are using the \textit{Saliency} visualization (\textbf{R.1}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: Both Sandor et al.~\mbox{%DIFAUXCMD
\cite{Sandor2010} }\hskip0pt%DIFAUXCMD
and Dey et al.~\mbox{%DIFAUXCMD
\cite{Dey2014} }\hskip0pt%DIFAUXCMD
found that }\textit{\DIFadd{Saliency}} \DIFadd{was the most effective }\gls{X-ray Visualization} \DIFadd{for depth perception in their study, and it performed the best in our preliminary tests.
        }}
     \DIFaddend \item Participants' vertical and horizontal placement of the virtual icosahedron will be further from the correct target position when using the \textit{Saliency} visualization (\textbf{R.1.1}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: }\textit{\DIFadd{Saliency}} \DIFadd{has the most potential occlusion, and I hypothesize that users may struggle to place the object accurately on the X and Y axis.
        }}
     \DIFaddend \item Participants' depth axis of the virtual icosahedron closer to the correct target position when they are using the \textit{Saliency} visualization (\textbf{R.1.1}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: }\textit{\DIFadd{Saliency}} \DIFadd{provides a powerful depth cue that is able to assist with more accurate placement of objects~\mbox{%DIFAUXCMD
\cite{Sandor2010, Dey2014}}\hskip0pt%DIFAUXCMD
.
        }}
     \DIFaddend \item Participants will place the virtual icosahedron closer to the correct target when reference objects and any \gls{X-ray Visualization} are present (\textbf{R.2}).
%     \item The presence of the virtual reference objects will improve precision for all visualizations, except for the \textit{None} visualization.
     \item Participants will take less time when the reference objects are added to the virtual scene (\textbf{R.3}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: The introduction of the virtual objects should result in an overall improvement in efficiency from the users as they have a better set of references to view the position of the study environment~\mbox{%DIFAUXCMD
\cite{Vishton1995}}\hskip0pt%DIFAUXCMD
.
        }}
     \DIFaddend \item Participants will move less when the reference objects are present (\textbf{R.3}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: Embodied cognition suggests that participants should more less when they have more information to complete a task~\mbox{%DIFAUXCMD
\cite{Wilson2002}}\hskip0pt%DIFAUXCMD
.
        }}
     \DIFaddend \item Participants will stand farther back from the Voronoi cube when more reference objects are present (\textbf{R.3}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: Participants will likely want to concentrate on more than one detail at a time. They will need to view the box more as a whole.
        }}
     \DIFaddend \item Participants will find \textit{Saliency} subjectively difficult to use and require a higher cognitive load than other \gls{X-ray Vision} effects (\textbf{R.4}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: Although }\textit{\DIFadd{Saliency}} \DIFadd{may provide a powerful depth cue that is able to assist with more accurate placement of objects, I suspect it requires much more attention, which may result in greater cognitive effort~\mbox{%DIFAUXCMD
\cite{Sandor2010, Zollmann2014, Santos2015}}\hskip0pt%DIFAUXCMD
.
        }}
     \DIFaddend \item Participants will prefer \gls{X-ray Vision} effects which occlude less of their vision (\textbf{R.4}).
        \DIFaddbegin \textit{
            \DIFadd{Rationale: It is likely that since participants are required to be in close proximity to the large physical cube with a Voronoi pattern, they will prefer to either use }\textit{\DIFadd{Tessellation}} \DIFadd{or possibly }\textit{\DIFadd{Edge Based}} \DIFadd{conditions.
        }}
\DIFaddend \end{enumerate}

\subsubsection{Placement of the Object}
I hypothesize that users can place the object better using \textit{Saliency} similar to Sandor et al.'s~\cite{Sandor2010} results compared to the \textit{Edge-Based} visualization on \gls{vst} \gls{ar}, and because it performed the best in the preliminary tests that focused on depth perception. 
I expect to get similar results from the position of objects using \textit{Saliency} when placing objects (\textbf{H.1}). 
Due to \textit{Saliency} having the most potential occlusion, I hypothesize that users may struggle to place the object accurately on the X and Y axis (\textbf{H.2}) but will be able to place the object accurately along the depth (z) axis (\textbf{H.3}).
\textit{Saliency} and \textit{Random Dot} may yield similar results since they have very similar opacity levels. 
I expect an improvement in depth perception due to the impact the reference objects would have on relative size and density because of findings shown by Cutting et al.~\cite{Vishton1995} showing the benefits regarding relative size and density to depth perception and Kyto et al.~\cite{Kyto2014} finding showing that this was enough to create a \gls{X-ray Visualization} using only the depth cues of relative size and density (\textbf{H.4}).
The benefits of \gls{X-ray Vision} have been seen to be highly effective at any distance~\cite{Sandor2010, Dey2014, MartinGomez2021}. 
Overall, all of the  \gls{X-ray Vision} visualizations should perform better than the baseline visualization (\textit{None}).

\subsubsection{User Movement and Task Completion}
Introducing the virtual objects should result in an overall improvement in efficiency from the users as they have a better set of references to view the position of the study environment~\cite{Vishton1995}. 
Therefore, participants will likely have \DIFdelbegin \DIFdel{more }\DIFdelend \DIFaddbegin \DIFadd{improved depth perception }\DIFaddend when they have fewer objects to look at (\textbf{H.6}).
Moving less should enable them to complete tasks faster (\textbf{H.5}).
However, I hypothesize that this will also cause the users to want to stand further away from the object (\textbf{H.7}) as they try to concentrate on more than one detail at a time. They will need to view the box more as a whole. 

\subsubsection{Subjective Analysis}
%The perception of difficulty does not cause the difficulty of a task.
Although \textit{Saliency} may provide a powerful depth cue that is able to assist with more accurate placement of objects, I suspect it requires much more attention, which may result in greater cognitive effort (\textbf{H.8})~\cite{Sandor2010, Zollmann2014, Santos2015}. 
Results from other devices show \textit{Saliency} utilizes depth cues in a powerful manner that should lead to more accurate placements~\cite{Sandor2010, Zollmann2014, Santos2015, Kalkofen2013}. Still, the higher occlusion it produces will likely require in the near field will require much more attention to use as it relies on the user's understanding of \textit{Saliency} will need to match the output from a given algorithm~\cite{Bruce2009, VanDyck2021}.
It is likely that since participants are required to be in close proximity to the large physical cube with a Voronoi pattern, they will prefer to either use \textit{Tessellation} or possibly \textit{Edge Based} conditions (\textbf{H.9}).


\subsection{Participants} \label{sec:X-ray Participants}
The study recruited 22 participants between the ages of 22-44 ($mean = 29.35, \sigma = 6.43964$).
Two of the participants were female, and 20 were male. All participants were required to have a normal or correct vision regarding depth perception. 
This was determined by user self-reporting before the study.
This was verified by visually examining their final placements against all other participants to look for outliers where the task was not performed correctly.
Two male participants were removed from the study as their data showed they either did not understand the study procedure or their sight was impaired.

\subsection{Study Design}  \label{sec:X-ray Design And Implemention}
% tools the user was originally provided with
The Zed mini allowed the system to observe the local environment to produce the \glspl{X-ray Visualization} (\textit{Edge-Based} and \textit{Saliency}). At the same time, a Vive controller was used to give users better control over the placement of the virtual icosahedron inside the large colorful box.
All the additional sensors were attached using 3D-printed mounts \DIFaddbegin \DIFadd{(described in }\autoref{sec:X-rayAddionalSensors}\DIFadd{) }\DIFaddend to ensure repeatability and reliability\DIFaddbegin \DIFadd{.
}\DIFaddend %(more information can be found in \autoref{sec:X-rayAddionalSensors}).

% An explanation of why I used the HoloLens and an explanation of other findings I may have run into. 
Rather than creating a new AR device to work, this study aimed to use off-the-shelf components best. 
This meant that this study required the use of several consumer-level components that needed to work together.
The HoloLens2 was used as the AR device in this study because it allowed additional sensors positioned relative to the display (to allow for more precise controls and a faster image input), the refresh rate on the device when running \textit{Saliency} and \textit{Edge-Based} algorithms was adequate (approximately 30 \DIFdelbegin \DIFdel{fps}\DIFdelend \DIFaddbegin \gls{fps}\DIFaddend ), it was possible to offload GPU processing to a tethered machine (increasing the frame rate to 60 \DIFdelbegin \DIFdel{fps}\DIFdelend \DIFaddbegin \gls{fps}\DIFaddend ).
This slower performance is due to the processing the HoloLens requires for each frame before the image can be displayed on the HoloLens2. 
Instead, a Zed Mini was used, which could be processed concurrently on a remote device in less time.

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{Chapter3/Images/HeadsetWithSensors.png}
    \caption{A image of the hardware used for this study on a glass mannequin head from three different angles.}
    \label{fig:X-ray Headset}
\end{figure}

% why the HoloLens was chosen over the Magic Leap
The choice of off-the-shelf devices for this study meant that it was not possible to take full advantage of some possible depth cues, specifically accommodation and convergence, since very few of the available headsets provide adequate accommodation.
I did consider the Magic Leap~\footnote{\url{https://www.magicleap.com}} as an alternative because it utilizes two focal planes, which would allow for some use of the accommodation depth cue. 
However, the two focal planes would have greatly complicated the setup this system would have required.
While the two depth planes of the Magic Leap were potentially useful, this technology would have complicated the camera pass-through to the user's vision and reduced the available frame rate, potentially causing issues for the study~\cite{Wang2023}.

\subsubsection{Vive Controller Set Up}
% What is the issue with using gestures in this study? 
The HoloLens 2 has an intuitive interaction design but has limited accuracy, especially at a distance.
MRTK utilizes Interpolation to create a smoother sense of motion within AR, making objects feel like they are drifting through the air. 
This works well for placing items in an approximate location but does not allow the participant to make precise controls, which is challenging. 
Generally, when using HoloLens2 to do this, it is recommended that you use long-distance motions for precise controls, but you would rather grab objects and place them. 
This was not an option, as participants could not touch the physical objects. 
So, a new system was built to view the participants' interactions. 

Rather than use the MRTK controls for this study, a Vive controller was utilized that controlled a ray out of the end of the controller, allowing participants to interact with the inside of the cube.
Using controllers enabled a more precise and predictable interaction than the default controls from MRTK. 
% Explaining how the Vive controller works
A separate VR system would be used to track the position of the Vive puck and controller.
It would then correct this position and transform it to be relative to where the puck should be relative to the HoloLens2, and send it to the main system as the position and rotation of the main controller.
%This allowed participants to interact with the virtual objects via the Vive controller. 
The system would then portray a ray coming out of the end of this controller. 
To lower the sensitivity of the Vive sensors and the HoloLens2 connection, a one-euro filter~\cite{Casiez2012} was used to remove any noise caused by the Vive's hardware that may be transmitted to the HoloLens2. 

\subsubsection{3D printed mounts} \label{sec:X-rayAddionalSensors}
To attach the sensors to the HoloLens2, 3D-printed mounts were required to fit comfortably onto the HoloLens2 with minimal movement due to head motions. This allowed the system to know where the camera was for each frame and where the controllers were located. Compared to the headset. Allowing for the controllers to be tracked in relation to the \gls{hmd}.

\begin{figure}[bt]
    \centering
    \includegraphics[width=0.9\columnwidth]{Chapter3/Images/RapidPrototypeing.jpg}
    \caption{A image of the prototype models developed to prototype the final ZED MINI camera mount}
    \label{fig:RapidPrototypeingZedHeadMount}
\end{figure}

% Technical
The Zed mini was required to capture as much of the participants' view as possible to locate it as close as possible to the users' eyes.
This meant the zed camera needed to be placed on top of the front of the HoloLens2, where two slots allow for the small items to be mounted.
To make this work, a keyhole-fit mechanism was utilized to hold these items in the same place without any chance of movement.
This required precise knowledge of the physical parameters of the HoloLens2. 
To be able to detect both the holes of the headset to establish that rapid prototyping was enacted, the development of a series of 3D printed models is required to try to determine the parameters of the HoloLens2 headset, which can be seen in figure \autoref{fig:HoloLens2Dimentions}.
The final version of this mount can is available open source~\footnote{\url{https://www.thingiverse.com/thing:4561113}}\DIFaddbegin \DIFadd{\textsuperscript{,}}\DIFaddend \footnote{\url{https://github.com/tomishninja/HoloLens2-Sensor-Mount-Repository}}

\begin{figure}[tb]
    \centering
    \includegraphics[width=0.9\columnwidth]{Chapter3/Images/HoloLens2Dimentions.png}
    \caption{A diagram of the base dimensions for the HoloLens2 calculated utilizing 3D prototyping. The top of the HoloLens2 Mount can be seen shaded in yellow.}
    \label{fig:HoloLens2Dimentions}
\end{figure}

The 3D-printed back Vive puck mount sat on the back of the HoloLens2 in \autoref{fig:X-ray Headset} was used to keep the Vive Puck visible and in the same place relative to the user.
This model was made open source to allow others to utilize this system~\footnote{\url{https://www.thingiverse.com/thing:4657299}}.
This design was created using photogrammetry. This version did not require such a complex design, and it acquired as many points of reference as possible to ensure the design's accuracy. 
This design utilized a two-piece system that could pressure clamp the mount if necessary to allow for more flexibility.
The areas of the design this would work with where could be found with the screw that could connect to a Vive puck as well as the top and bottom.
%The final design was built to be extendable if required, with two hinges on either side to allow for support if needed, but was deemed unnecessary and was removed in favor of having less weight.

\subsubsection{Environment and Apparatus}  \label{sec:X-ray Implmention Task And Reference}

\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{Chapter3/Images/UserStudyRunning (1).png}
    \caption[The study environment used for spatial estimation experiment.]{The study environment used for the spatial estimation experiment. Left is the large physical cube with a Voronoi pattern (for tracking) where \gls{X-ray Vision} techniques are performed. Right is a reference scene with movable 3D-printed objects that need replicating during the task.}
    \label{fig:Basic image of setup}
\end{figure}

% explaining the general physical environment
The physical study environment consisted of two cubes placed in an area where the participant could freely walk. The area where the participant could walk was constrained so that they were at most 1m from either of the physical scenes. The study area arrangement is depicted in \autoref{fig:Study One Set up}.

% a Deeper look into the cubes
The large Voronoi cube and reference scene, shown in \autoref{fig:Basic image of setup}, were very distinct in appearance, measuring 60cm along each axis.
The reference scene on the right-hand side of \autoref{fig:Basic image of setup} was the reference scene with a felt base. 
Smaller objects held up by stilts were placed in the cube that the participant would use as a reference.
The other large Voronoi cube was a brightly colored cube decorated in a Voronoi pattern designed to support \textit{Edge-Based} and \textit{Saliency} effects. 
This cube held the virtual objects displayed by the HoloLens2 that the user could interact with.

% Explain what these cubes did
The large physical cube with a Voronoi pattern was designed to be a 1:1 physical representation of the virtual world displayed in the brightly colored Voronoi cube. 
These cubes were designed to hold three geometric objects: a cube, a sphere, and an icosahedron.
The 3D-printed objects within the reference scene were ivory-colored to ensure that the shadows on the objects would be similar to the virtual ones displayed by the HoloLens2 and presented in the same orientation as the virtual objects. 
During the preliminary tests, no participant noted that the bright colors on the Voronoi cube were distracting.
Each stilt featured a square base measuring 100cm$^2$, which ensured that the center of each object was positioned at least 5cm away from any other object.
This enforced a minimum distance of 4 cm from any other object (except cube boundaries), ensuring that some spatial estimation was required to place the object.

% dimensions of the apparatuses
The geometric objects were identical in the physical reference scene and the virtual space. 
Each object had similar dimensions. 
The sphere (113.3cm$^3$) and the icosahedron (109.9cm$^3$) were made to be the largest size possible that would fit into the dimensions of the cube-shaped object \DIFdelbegin \DIFdel{scene}\DIFdelend (216cm$^3$).
The placement of each reference object was decided randomly along all axes while accounting for collisions and ensuring that each object was wholly within the cube.

\subsection{Study Variables} \label{sec:X-ray Design}
This section details the variables considered in this study. The independent variables are used to create each condition that will be tested throughout this study. The dependent variables will state the metrics that were required to track to answer the prior research questions. 

\subsubsection{Independent Variables} \label{sec:X-ray Design IV}
%DIF > This study utilized a 2 by 5 study design with both baselines (with no reference objects and no \gls{X-ray Visualization}. 
\DIFaddbegin 

\DIFaddend This study utilized a 2 by 5 \DIFdelbegin \DIFdel{study designwith both baselineswith }\DIFdelend \DIFaddbegin \DIFadd{design, with two separate baselines. 
The first baseline related to the presence of reference objects: participants either had }\DIFaddend no reference objects \DIFdelbegin \DIFdel{and no }\DIFdelend \DIFaddbegin \DIFadd{or two reference objects (a cube and a sphere) available in the scene (see }\autoref{fig:Basic image of setup}\DIFadd{). 
The second baseline related to visualization: participants either viewed the scene with one of four }\DIFaddend \gls{X-ray Visualization} \DIFdelbegin \DIFdel{. }\DIFdelend \DIFaddbegin \DIFadd{techniques (}\textit{\DIFadd{Random Dot}}\DIFadd{, }\textit{\DIFadd{Tessellation}}\DIFadd{, }\textit{\DIFadd{Saliency}}\DIFadd{, }\textit{\DIFadd{Edge-Based}}\DIFadd{) or with no visualization at all (the baseline condition for this factor, }\textit{\DIFadd{None}}\DIFadd{).
}

\DIFadd{Thus, the design contained two orthogonal baselines:
}\begin{itemize}
    \item \DIFadd{Reference baseline: no cube or sphere present.
    }\item \DIFadd{Visualization baseline: no }\gls{X-ray Visualization} \DIFadd{applied.
}\end{itemize}

\DIFaddend The conditions tested in this design are as follows:
\begin{itemize}
    \item (2) The presence of the reference objects: two reference objects (a cube and a sphere) and no reference objects (Shown in \autoref{fig:Basic image of setup}). 
    \item (5) X-ray visualization: \textit{Random Dot}, \textit{Tessellation}, \textit{Saliency}, \textit{Edge-Based} and \textit{None}. These were all previously mentioned and described in \autoref{sec:X-ray Vis}
\end{itemize}
\DIFdelbegin \DIFdel{Hypotheses 1 to 7 }\DIFdelend \DIFaddbegin \textbf{\DIFadd{H.1}} \DIFadd{to }\textbf{\DIFadd{H.7}} \DIFaddend utilize both of these conditions, while \textbf{H.8} \DIFaddbegin \DIFadd{\& }\textbf{\DIFadd{H.9}} \DIFaddend only uses the \glspl{X-ray Visualization}.

\subsubsection{Dependent Variables}  \label{sec:X-ray Design DV}
The dependent variables state the different tracked measures utilized to confirm this research's hypotheses. 

\paragraph{Quantitative Variables}
\begin{itemize}
    \item \textbf{Accuracy}: Accuracy was measured as the distance between the center of the actual placement position and the center of the target position (\textbf{H.1}, \textbf{H.2}, \textbf{H.3} \& \textbf{H.4}).
    \item \textbf{Perspective Accuracy by axis}: Perspective accuracy was measured as the distance between the actual placement position (along the horizontal, vertical, and depth axes) and the target position from the participant's perspective at the time of placement (\textbf{H.2}, \textbf{H.3} \& \textbf{H.4}).
    \item \textbf{Time}: Overall time taken between the user starting the condition and the time the user chose to end the condition. The time the user spent holding the object for each condition \DIFdelbegin \DIFdel{to determine }\DIFdelend was recorded (\textbf{H.5}). 
    \item \textbf{Distance Moved}: Throughout each task, data was gathered from the headset regarding the distance the headset was moved every frame. The total sum of this was used to determine how much various users felt the need to move around to get an understanding of various scenes (\textbf{H.6} \& \textbf{H.7}). 
    \item \textbf{Distance Icosahedron Was Moved}: For each task, users would be given as many opportunities to move the icosahedron as they felt necessary. Whenever the user moves the icosahedron, the distance it moves will be tracked similarly to its movement. Allowing for an assessment of the movement required to place the shape in every instance.
\end{itemize}

\paragraph{Subjective Variables}
\begin{itemize}
    \item \textbf{PAAS Questionnaire}: To gain an understanding of the possible cognitive loads undertaken via the various \gls{X-ray Visualization}s, the PAAS mental effort scale\cite{Paas1992} was utilized (\textbf{H.8}).
    \item \textbf{\gls{sus}}: A \gls{sus} questionnaire was utilized to determine how easy the various \gls{X-ray Visualization}s where to use (\textbf{H.8}).
    \item \textbf{Favorite \gls{X-ray Visualization}}: At the end of each study, users were asked to state what \gls{X-ray Visualization} was their favorite (\textbf{H.9}).
    \item \textbf{Comments and Feedback}: At the end of each study, users were asked to give feedback in the form of written text after completing the study  (\textbf{H.9}). 
\end{itemize}

\subsection{Task} \label{sec:X-ray User Study Procdedure}

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{Chapter3/Images/SkectchOfStudyAreaForStudy1.png}
    \caption[A top down view of the study setup for the spatial estimation experiment color coded in to the different areas of note]{A top down view of the study setup for the spatial estimation experiment color coded in to the different areas of note: a) Colourful Voronoi Cube; b) Black Inverse Reference  Cube; c) Area the participant could traverse; d) Examiner area; e) Main Processing Server; f) Projector setup; g) Vive sensors}
    \label{fig:Study One Set up}
\end{figure}

% Explaining the Vive setup
Participants were asked to perform a demographics survey at the start of each session. Then, they were asked to \DIFdelbegin \DIFdel{don }\DIFdelend \DIFaddbegin \DIFadd{wear }\DIFaddend a HoloLens2 with an HTC Vive puck and a Zed mini mounted on it and given an HTC Vive controller for hand interactions with the study.
Allowing a pointer to come out of the controller to interact with the box's interior.
Participants were given a tutorial on how to use the controller for the task. 
This explanation included:
\begin{itemize}
    \itemsep-0.15em 
    \item When the pointer interacts with the icosahedron, it changes its color and surrounds the object with a transparent bubble.
    \item Using the touchpad on the controller, the ray cast could be grown (up to 1 meter in length) and shrunk as necessary. %, allowing the participants to interact within the box from any angle. 
    \item Participants could start and end each iteration by pressing the menu button on the Vive controller.
\end{itemize}

% Explaining the task the user had to do
Participants were instructed to place the virtual icosahedron in the same position in the Voronoi cube as the physical icosahedron was positioned in the physical reference scene. 
The other reference objects and their physical counterparts in the inverse cube were also pointed out to them. 
In each iteration of the study, one of four different \glspl{X-ray Visualization} or no visualization would be randomly chosen and shown on the outside of the cube. 
The initial position of the virtual icosahedron was at 5cm along all axes measured from the bottom front corner of the target Voronoi cube. 
If virtual reference objects were present for the iteration, they would be located at the correct relative position indicated by their physical counterparts.
All positions of the geometrical objects were generated before the study using a pseudo-random selection. 

A \DIFaddbegin \DIFadd{randomized order of conditions was used instead of a Latin balanced square. This choice was made to maintain flexibility in participant scheduling and accommodate uncertainty in recruitment numbers. Balanced Latin squares require fixed multiples of participants to achieve complete counterbalancing, which can become problematic when participant availability is unpredictable or when demographic representation is uneven. Randomization ensured that order effects were distributed across the sample while allowing the study to adapt to potential imbalances in participant demographics caused by pandemic-related constraints.
}

\DIFadd{A }\DIFaddend \gls{sar} calibration guidance tool was used to position the physical objects in 3D space by the examiner within the reference scene~\cite{Bimber2004}.
The desired height position of each object was indicated using a wireframe representation of these models to ensure that they were in the correct position and facing the correct way.

% what the users were expected to do each time
During each task, the participants could freely move around the area shown in \autoref{fig:Study One Set up} but were not allowed to step outside the area.
Participants could reposition the icosahedron as often and as far as they wanted \DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{and had no time limit to complete the task.
This was aimed to reduce any pressure on the participants to complete the task quickly and allow them to focus on accuracy while also allowing them to move around the cube as much as they felt necessary focusing on how well they could perform the task with no restrictions, creating a more realistic experience.
}\DIFaddend 

% participant rigor
Participants were given three practice iterations of each task before data collection began.
Participants were given instructions and guidance during these practice iterations. 
Following the practice iterations, participants were presented with 10 iterations for each \gls{X-ray Visualization} effect. 
This would then be split into two randomly interleaved groups: one with two reference objects and one without any reference objects.
%Five of these would include two extra reference objects, and five would only include the icosahedron but no reference objects presented. 
After completing the ten iterations, the participant answered a visualization technique questionnaire, including a System Usability Survey~\cite{Brooke1996SUSA}, the PAAS subjective rating scale~\cite{Paas1992}, and several other custom questions. 
This procedure was repeated randomly for each \gls{X-ray Visualization} and the baseline condition.
In total, each participant spent an average of approximately 96 minutes ($\pm 61.52$) doing in this study, with the fastest participant spending about 31 minutes in the study and the slowest taking almost 4 hours and 21 minutes to complete this task. 

\section{Results}  \label{sec:X-ray Results}

\begin{figure*}[bt]
    \centering
    %\includesvg[width=\textwidth]{Chapter3/Images/DistanceToGoalGraphEffectOnlyVersion.svg
    %\includegraphics[width=\textwidth]{Chapter3/Images/AccuracyPlotOverallX-rayVision.pdf}
    \includegraphics[width=\textwidth]{Chapter3/Images/Impact of X-ray Visualization on Placement Accuracy.pdf}
    \caption[A graph representing the accuracy of participants' placement based on the distance the object was placed from the goal position within the large physical cube with a Voronoi pattern using the \glspl{X-ray Visualization}.]{
    A graph representing the accuracy of participants' placement based on the distance the object was placed from the goal position within the large physical cube with a Voronoi pattern using the \glspl{X-ray Visualization}. 
    The error bars indicate each condition's confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the top of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:X-ray Distance To Goal X-ray vision effect plot}
\end{figure*}


The analyses followed a within-group design, with a single group of participants completing all conditions (presence of reference objects and \glspl{X-ray Visualization}) \DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{to answer hypotheses }\textbf{\DIFadd{H.1}} \DIFadd{to }\textbf{\DIFadd{H.7}}\DIFadd{. 
}\gls{lmm} \DIFadd{was used to analyze the data, with participants as a random effect and the presence of reference objects and }\glspl{X-ray Visualization} \DIFadd{as fixed effects while allowing for repeated measures.
This allowed for the differences between each participant to be accounted for while still being able to analyze the effects of the independent variables
}\DIFaddend 


All results with a p-value $< 0.1$ are reported in this section. 
Any p-value $< 0.05$ is considered significant.
All results from the post hoc analysis in this section can be found in the appendix of this thesis. 

\subsection{Quantitative Results} \label{sec:X-ray Quantitative Results}
\begin{table*}[!b]
  \centering
  \scriptsize
  \addtolength{\tabcolsep}{-2.75pt}
  \DIFdelbeginFL %DIFDELCMD < \caption[Table of \glspl{X-ray Visualization} used in this study. Showing the mean ($\mu$), standard deviation ($\sigma$), and the median(M) of the distance from the correct target position of all the \glspl{X-ray Visualization} from the user's sight.]{%%%
\DIFdelendFL \DIFaddbeginFL \caption[Table of \glspl{X-ray Visualization} used in this study. Showing the mean ($\mu$), standard deviation ($\sigma$), and the median (M) of the distance from the correct target position of all the \glspl{X-ray Visualization} from the user's sight.]{\DIFaddendFL 
  Table of \glspl{X-ray Visualization} used in this study. Showing the mean ($\mu$), standard deviation ($\sigma$), and the median (M) of the distance from the correct target position of all the \glspl{X-ray Visualization} from the user's sight (x, y, and z) and the distance to the goal (*). \DIFaddbeginFL \DIFaddFL{All measurements are in cm.
  }\DIFaddendFL %The top row of images shows the \glspl{X-ray Visualization} used in this study from the viewpoint of the HoloLens2's camera.
  The second column from the left indicates the presence of the reference objects (T indicates the presence of reference objects, while F indicates the absence of all reference objects in the scene).
  }


  \begin{tabular}{"c|c"c|c|c"c|c|c"c|c|c"c|c|c"c|c|c"}
    \cline{3-17}
    \multicolumn{2}{c"}{} & 
    \multicolumn{3}{c"}{Random Dot} & 
    \multicolumn{3}{c"}{Tessellation} &
    \multicolumn{3}{c"}{Edge-Based} & 
    \multicolumn{3}{c"}{Saliency} & 
    \multicolumn{3}{c"}{None} \\ 
    \cline{3-17}
    \multicolumn{2}{c"}{} & 
    $\mu$ & $\sigma$ & M & 
    $\mu$ & $\sigma$ & M & 
    $\mu$ & $\sigma$ & M & 
    $\mu$ & $\sigma$ & M & 
    $\mu$ & $\sigma$ & M \\ 
    \hline 
    \multirow{2}{*}{*)} & F 
    & 5.82 & 2.84 & 5.09
    & 5.47 & 2.38 & 5.25
    & 6.19 & 2.99 & 5.68 
    & 6.09 & 2.40 & 5.75 
    & 5.78 & 2.57 & 5.65  \\ 
    \cline{2-17} & T 
    & 4.24 & 1.97 & 4.09 
    & 4.58 & 2.18 & 4.30
    & 4.46 & 2.15 & 4.17 
    & 4.86 & 2.76 & 4.41 
    & 4.27 & 2.02 & 3.90  \\ 
    \hline 
    \multirow{2}{*}{x)} & F 
    & -0.69 & 3.19 & -0.88 
    & -0.08 & 3.34 & -0.24
    & -0.04 & 3.64 & 0.02 
    & -0.21 & 3.49 & -0.68 
    & -0.26 & 3.40 & -0.37  \\ 
    \cline{2-17} & T 
    & -0.19 & 2.86 & -0.16 
    & 0.07 & 3.35 & -0.31
    & 0.00 & 2.85 & -0.15 
    & 0.05 & 3.22 & -0.04 
    & 0.39 & 2.54 & 0.36  \\ 
    \hline 
    \multirow{2}{*}{y)} & F 
    & -0.98 & 2.90 & -0.98 
    & 0.03 & 3.05 & 0.09
    & -0.93 & 2.66 & -1.13 
    & -0.89 & 3.01 & -0.83 
    & -0.34 & 2.56 & -0.55  \\ 
    \cline{2-17} & T 
    & -0.46 & 2.22 & -0.21
    & -0.49 & 2.25 & -0.14
    & -0.26 & 2.36 & -0.03 
    & -0.79 & 2.36 & -0.56 
    & -0.07 & 2.56 & -0.12  \\ 
    \hline 
    \multirow{2}{*}{z)} & F 
    & 0.15 & 2.55 & 0.25 
    & -0.33 & 2.84 & -0.65
    & -0.76 & 3.21 & -0.84 
    & 0.00 & 3.18 & 0.24 
    & -0.48 & 2.84 & -0.89  \\ 
    \cline{2-17}  & T 
    & 0.02 & 2.32 & 0.27 
    & 0.17 & 2.13 & 0.15 
    & -0.59 & 2.35 & -1.15 
    & -0.08 & 2.96 & -0.61 
    & -0.00 & 2.37 & -0.04  \\ 
    \hline 
  \end{tabular}
  \label{tab:MeanAndMedianAccuracyValues}
\end{table*}

% \begin{figure}[tb]
%     \centering
%     %\includesvg[width=\textwidth]{Chapter3/Images/DistanceToGoalBasedOnUsersPerspectiveAndAxis.svg}
%     %\includegraphics[width=\textwidth]{Chapter3/Images/AccuracyPlotXAxisRefrenceObjects.pdf}
%     \includegraphics[width=\textwidth]{Chapter3/Images/Impact of Reference Objects on Placement Accuracy On X-axis.pdf}
%     \caption{
%         A graph representing the accuracy of participants' placement based on the distance the object was placed from the goal position on the X-axis within the large physical cube with a Voronoi pattern using the \glspl{X-ray Visualization}.
%         Positions were obtained from the final placement of the object.
%         The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%     }
%     \label{fig:X-ray AccuracyPlotXAxis plot}
% \end{figure}

\begin{figure}[bt]
    \centering
    \includegraphics[width=\textwidth]{Chapter3/Images/Impact of X-ray Visualization on Placement Accuracy on the X axis.pdf}
    \caption[A Graphs representing the accuracy of participants' placement based on the distance they are from the goal position on the X-axis within the large, physical cube with a Voronoi pattern]{
    A Graphs representing the accuracy of participants' placement based on the distance they are from the goal position on the X-axis within the large, physical cube with a Voronoi pattern using the presence of the reference objects.
    Positions were obtained from the final placement of the object.
    The error bars indicate each condition's confidence levels (CL = 95\%).
    }
    \label{fig:X-ray AccuracyPlotXAxis plot}
\end{figure}

A \gls{lmm} was used to examine the differences across all variables measured within this section. 
\DIFdelbegin \DIFdel{This approach }\DIFdelend \DIFaddbegin \glspl{lmm} \DIFadd{are now standard for analyzing repeated‐measures data in behavioral and cognitive experiments, because they allow for both fixed effects of experimental manipulations and random effects (e.g. subject‐level variability). 
This approach has several advantages over traditional methods such as ANOVA, including the ability to handle unbalanced data, account for individual differences, and model complex hierarchical structures~\mbox{%DIFAUXCMD
\cite{Meteyard2020, Bell2019, Singmann2019}}\hskip0pt%DIFAUXCMD
.
Unlike traditional repeated-measures ANOVA, }\glspl{lmm} \DIFadd{are flexible to unbalanced data and missing observations, which was advantageous in this study context~\mbox{%DIFAUXCMD
\cite{Li2012a}}\hskip0pt%DIFAUXCMD
.
}

\glspl{lmm} \DIFaddend uses a similar method to linear regression ($y = X\beta + \varepsilon$) that is given fixed and random effects. Fixed Effects may not vary per condition; and random effects are estimated using \DIFdelbegin \DIFdel{$\beta \sim \mathcal{N}(\mu, \sigma^2)$}\DIFdelend \DIFaddbegin \DIFadd{$\beta \sim \mathcal{N} (\mu, \sigma^2)$}\DIFaddend , allowing the system to account for different variances between results of the random effect (mixed effects)~\cite{Models2006, Singmann2019}.
Overall, the model predicts the outcome variable (y) using a matrix of predictor variables (X) against a single column of fixed effects regression coefficients ($\beta$); The effect from the random effects (Z) is then multiplied to the random effect of a given fixed effect ($\gamma$), this value is then added against a vector column of the residuals values ($\varepsilon$) for each fixed effect. 
\[y = X\beta + Z\gamma + \varepsilon\]
This approach accounts for individual differences between participants in repeated measures on both normally distributed and non-normally distributed data and enables the examination of the effect of the number of objects\DIFdelbegin \DIFdel{.}\DIFdelend ~\cite{Kaptein2016, Adams2022, Cameron2005}.
The model was specified with the factors of the presence of virtual reference objects and \gls{X-ray Visualization} effect techniques, the dependent variable of the accuracy, viewpoint accuracy, distance moved, distance away, or time. 
The model was specified with fixed effects of x-ray visualization, the presence of reference objects, and an interaction effect between them, with a random effect of participants on the intercept. 
Significance values were extracted using Type II Wald chi-square tests using the following algorithm: 
\[ W = g(\hat{\theta})^{ \intercal } \left[ G V(\hat{\theta}) G^{\intercal} \right]^{-1} g(\hat{\theta})
\]
\DIFdelbegin \DIFdel{Where G is }\DIFdelend %DIF > Where G is the Jacobian matrix of $g(\theta)$ with respect to $\theta$, it's a matrix of partial derivatives of $g(\theta)$ the elements of each parameter.
\DIFaddbegin \DIFadd{Here, \( G \) denotes }\DIFaddend the Jacobian matrix of \DIFdelbegin \DIFdel{$g(\theta)$ }\DIFdelend \DIFaddbegin \DIFadd{\( g(\theta) \) }\DIFaddend with respect to \DIFdelbegin \DIFdel{$\theta$, it's a matrix of partial derivatives of $g(\theta)$ the elements of each parameter .
}\DIFdelend \DIFaddbegin \DIFadd{\( \theta \).
Each element of \( G \) represents the partial derivative of one component of \( g(\theta) \)
with respect to a corresponding parameter in \( \theta \), that is,
}\[
\DIFadd{G_{ij} = \frac{\partial g_i(\theta)}{\partial \theta_j}.
}\]

\DIFaddend pairwise post-hoc comparisons were conducted using Tukey’s \gls{hsd} for multiple comparisons to further validate p-values which is shown below:
\[ q = \frac{\bar{X}_i - \bar{X}_j}{\sqrt{\frac{MS_{\text{e}}}{n_{\text{t}}}}} \]
$q$ is the test statistic,
$\bar{X}_i - \bar{X}_j$ are the two groups being compared,
$MS_{\text{e}}$ is the mean square error from the groups calculated from the \gls{lmm},
$n_{\text{t}}$ is the total number of observations.

% \begin{figure}[tb]
%     \centering
%     %\includesvg[width=\textwidth]{Chapter3/Images/DistanceToGoalBasedOnUsersPerspectiveAndAxis.svg}
%     %\includegraphics[width=\textwidth]{Chapter3/Images/AccuracyPlotYAxisXRayVisionEffects.pdf}
%     \includegraphics[width=\textwidth]{Chapter3/Images/Impact of \gls{X-ray Visualization} on Placement Accuracy On Y-axis.pdf}
%     \caption{
%         A graph representing the accuracy of participants' placement based on the distance the object was placed from the goal position on the X-axis within the large cube with a Voronoi pattern using the \gls{X-ray Vision} visualizations.
%         Positions were obtained from the final placement of the object.
%         The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%         %Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:X-ray AccuracyPlotYAxis plot}
% \end{figure}

\begin{figure}[bt]
    \centering
    \includegraphics[width=\textwidth]{Chapter3/Images/Impact of X-ray Visualization on Placement Accuracy on the Y axis.pdf}
    \caption[Graphs representing the accuracy of participants' placement based on the distance they are from the goal position on the Y-axis within the large physical cube with a Voronoi pattern.]{
        Graphs representing the accuracy of participants' placement based on the distance they are from the goal position on the Y-axis within the large physical cube with a Voronoi pattern using either different (Right) \glspl{X-ray Visualization} or (Left) the presence of the reference objects.
        Positions were obtained from the final placement of the object.
        The error bars indicate each condition's confidence levels (CL = 95\%).}
    \label{fig:X-ray Reference Objects AccuracyPlotYAxis plot}
\end{figure}

\begin{figure*}[tb]
    \centering
    %\includesvg[width=\textwidth]{Chapter3/Images/DistanceToGoalBasedOnUsersPerspectiveAndAxis.svg}
    %\includegraphics[width=\textwidth]{Chapter3/Images/AccuracyPlotZAxisXRayVisionEffects.pdf}
    \includegraphics[width=\textwidth]{Chapter3/Images/Impact of X-ray Visualization on Placement Accuracy on the Z axis.pdf}
    \DIFdelbeginFL %DIFDELCMD < \caption[Graphs representing the accuracy of participants' placement based on the distance the object was placed from the goal position on the X-axis within the large physical cube with a Voronoi pattern]{%%%
\DIFdelendFL \DIFaddbeginFL \caption[Graphs representing the accuracy of participants' placement based on the distance the object was placed from the goal position on the Z-axis within the large physical cube with a Voronoi pattern]{\DIFaddendFL 
        Graphs representing the accuracy of participants' placement based on the distance the object was placed from the goal position on the \DIFdelbeginFL \DIFdelFL{X-axis }\DIFdelendFL \DIFaddbeginFL \DIFaddFL{Z-axis }\DIFaddendFL within the large physical cube with a Voronoi pattern using either different (Right) \glspl{X-ray Visualization} or (Left) the presence of the reference objects.
        The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
        Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:X-ray AccuracyPlotZAxis plot}
\end{figure*}

% \begin{figure*}[bt]
%     \centering
%     %\includesvg[width=\textwidth]{Chapter3/Images/DistanceToGoalBasedOnUsersPerspectiveAndAxis.svg}
%     %\includegraphics[width=\textwidth]{Chapter3/Images/AccuracyPlotZAxisXRayVisionEffects.pdf}
%     \includegraphics[width=\textwidth]{Chapter3/Images/Impact of Reference Objects on Placement Accuracy On Z-axis.pdf}
%     \caption{
%         A Graph representing the accuracy of participants' placement based on the distance they are from the goal position on the Z-axis within the large physical cube with a Voronoi pattern using the presence of the reference objects.
%         The error bars indicate each condition's confidence levels (CL = 95\%).
%         Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:X-ray AccuracyPlotZAxis plot}
% \end{figure*}

\subsubsection{Accuracy of Placement (H.1)} \label{sec:X-ray Quantitative Results Placement Accuracy}
%\paragraph{Distance To the Goal: }

Analysis of the accuracy of the user's placement of the virtual icosahedron object as compared to the target shape's position
% The model for the generic distance between the user's point of placement and the correct target position 
showed a significant fixed effect of the \gls{X-ray Visualization} effects ($\chi$2(4, N= 20) = 13.897, p  = 0.007) and the presence of the additional reference objects ($\chi$2(1, N= 20) = 77.641, p $<$ 0.0001), with no significant interaction effect. 

Post-hoc pairwise comparisons showed significantly improved accuracy when additional reference objects were present (p $<$ 0.0001, df = 989, t = 8.771), which can be seen on the right side of \autoref{fig:X-ray Distance To Goal X-ray vision effect plot}.
\textit{Saliency} showed significantly lower accuracy than \textit{None} (p = 0.0358, df = 989, t = -2.853), and \textit{Saliency} was also significantly less accurate than \textit{Tessellation} (p = 0.0414, df = 989, t = 2.8). 
The differences \DIFdelbegin \DIFdel{between all of the accuracy of placement of }\DIFdelend \DIFaddbegin \DIFadd{in placement accuracy using of }\DIFaddend different \gls{X-ray Vision} effects can be viewed in the left side of \autoref{fig:X-ray Distance To Goal X-ray vision effect plot}

\paragraph{Placement Accuracy from User Viewpoint by Axis (H.2 \& H.3):}
The placement accuracy of the virtual icosahedron object was also analyzed in relation to the participant's viewpoint at the time of placement.
The initial placement provides insight into how accurately an object can be positioned within a given space, allowing for measuring distance along each axis. This enables the determination of how precisely actions were perceived at the final moments of placement and helps isolate the areas most affected by different visualizations.

Positions were transformed into the relative space of the headset using its position and orientation matrix just after the user placed the object to compute the actual and correct final placement. The distance was then calculated along the three axes of the user's headset: the x-axis (horizontal) and y-axis (vertical) are the horizontal and vertical axis of the participant's view, respectively, and the z-axis was the direction the participant was looking.

The viewpoint accuracy on the x-axis, the \gls{lmm} showed users were significantly more accurate when the reference objects were present ($\chi$2(1, N= 20) = 5.6295, p  = 0.0184), while the \gls{X-ray Vision} effect ($\chi$2(4, N= 20) = 5.8363, p  = 0.2117) and the interaction effect ($\chi$2(4, N= 20) = 0.4646, p  = 0.9768) was not found to be significant.
The post-hoc comparison for the presence of reference objects showed a significant improvement in accuracy with reference objects present (p $=$ 0.0184, df = 989, t = -2.362). All these results can be seen in \autoref{fig:X-ray AccuracyPlotXAxis plot}. 

Regarding the viewpoint accuracy on the y-axis, the \gls{lmm} showed a significant difference between different \gls{X-ray Vision} effects ($\chi$2(4, N= 20) = 13.8972, p  = 0.0076) and no significance for the presence of reference objects($\chi$2(1, N= 20) = 77.6411, p  < 0.0001) and the interaction effect ($\chi$2(1, N= 20) = 5.1383, p  = 0.27340). 
The post-hoc comparisons between all of the \gls{X-ray Vision} effects showed no significant values.
There was some variability between \textit{None} and \textit{Saliency} (p = 0.0551, df = 989, t = 2.697) and \textit{Saliency} and \textit{Tessellation} (p = 0.0985, df = 989, t = -2.469). 
The left-hand side of \autoref{fig:X-ray Reference Objects AccuracyPlotYAxis plot} shows that where Saliency was slightly less accurate, but it was not significantly different.
Whereas, the right side of \autoref{fig:X-ray Reference Objects AccuracyPlotYAxis plot} does present some benefits to having reference objects in a scene. 

The viewpoint accuracy on the z-axis model showed a significant fixed effect of the \gls{X-ray Visualization} effects ($\chi$2(4, N= 20) = 11.9119, p $=$ 0.0066). The presence of the reference objects showed no significant effect ($\chi$2(1, N= 20) = 0.0002, p = 0.9878).
The interaction effect ($\chi$2(4, N= 20) = 2.5402, p = 0.6374) showed not significantly different.
The post-hoc effect showed that the \textit{Edge-Based} visualization was significantly less accurate at presenting depth than \textit{None} (p = 0.0499, df = 989, t = -2.734) and \textit{Saliency}(p = 0.0048, df = 989, t = -3.476).
Some variation could also be found when comparing \textit{Edge-Based} to \textit{Random Dot} (p = 0.0661, df = 989, t = -2.628) and \textit{Tessellation} (p = 0.0782, df = 989, t = -2.563). All these effects can be seen in \autoref{fig:X-ray AccuracyPlotZAxis plot}.

\begin{figure}[bt]
    \centering
    %\includesvg[width=\columnwidth]{Chapter3/Images/TimeRequiredToCompleteGraphChangedColor.svg}
    %\includegraphics[width=\columnwidth]{Chapter3/Images/TimeRequiredToCompleteReferenceObjects.pdf}
    %Impact of X-ray Visualization on the Time Required to Complete the Task.pdf
    \includegraphics[width=\columnwidth]{Chapter3/Images/Effect of the Time Required for Participants to Move the Icosahedron.pdf}
    \caption[Two box plots analyzing the time it took to complete the task compared between different (Right) \glspl{X-ray Visualization} and (Left) with and without the reference objects.]{
    Two box plots analyzing the time it took to complete the task compared between different (Right) \glspl{X-ray Visualization} and (Left) with and without the reference objects.
    The error bars indicate each condition's confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:TimeRequiredToCompleteReferenceObjects}
\end{figure}

\subsection {Time Required (H.5)}
This section focuses on answering \textbf{H.5} in detail by looking at how long this task took between the different conditions and if \DIFdelbegin \DIFdel{the amount }\DIFdelend this changed the quantity of time they spent moving the icosahedron around. 

\paragraph{Time Required for task completion (H.5): } \label{sec:X-ray Quantitative Results Placement Time}

% \begin{figure*}[tbp]
%     \centering
%     %\includesvg[width=\columnwidth]{Chapter3/Images/TimeRequiredToCompleteGraphChangedColor.svg}
%     %\includegraphics[width=\columnwidth]{Chapter3/Images/TimeRequiredToCompleteGraphXRayEffects.pdf}
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of \glspl{X-ray Visualization} and the Time Required to Complete the Task.pdf}
%     \caption{
%     A box plot analyzing the time it took to complete the task compared between different \glspl{X-ray Visualization}.
%     The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%     Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:TimeRequiredToCompleteGraphXRayEffects}
% \end{figure*}

% The amount of time each iteration took
The completion time for each iteration was measured from the time the participant pressed the start button until the time they pressed the end button. The model of completion time showed significant fixed effects of both the \gls{X-ray Visualization} effects ($\chi$2(4, N= 20) = 24.441, p $<$ 0.0001) and the presence of the reference objects ($\chi$2(1, N= 20) = 14.2427, p = 0.0002).
No significant interaction effect between \gls{X-ray Visualization} and the presence of the reference objects was found ($\chi$2(4, N= 20) = 2.4244, p = 0.6582).

Post-hoc pairwise comparisons of the \glspl{X-ray Visualization} showed \textit{None} was significantly faster than Edge (p $<$ 0.0001, df = 983, t = -4.693), \textit{Saliency} (p = 0.0039 df = 983, t = -3.536) and \textit{Tessellation} (p = 0.0396 df = 983, t = -2.817). Comparisons of the presence of reference objects showed participants were significantly slower with reference objects present (p = 0.0002, df = 989, t = -3.757).
These results and \autoref{fig:TimeRequiredToCompleteReferenceObjects} indicate that \DIFdelbegin \DIFdel{there may }\DIFdelend these \glspl{X-ray Visualization} may slow down the user slightly to allow them to achieve a similar level of accuracy. 

\paragraph{Time Participants Spent Moving the Object (H.5)} 
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Chapter3/Images/Effect of Reference Objects onthe Participant's Movements.pdf}
    \caption[Graphs representing the percentage of time participants spent moving the icosahedron for each task for each \gls{X-ray Vision} effect.]{Graphs representing the percentage of time participants spent moving the icosahedron for each task for each \gls{X-ray Vision} effect and (Right) the percentage of time required to move the icosahedron that participants moved the icosahedron using either different (Right) \glspl{X-ray Visualization} or (Left) the presence of the reference objects.
    The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).}
    \label{fig:SpeedUserMovedObject}
\end{figure}

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on the Percentage of Time Participants Moved the Icosahedron.pdf}
%     \caption{
%     A box plot showing the number of percentage of time participants spent moving the icosahedron for each task for each X-ray vision effect
%     The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%     Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).}
%     \label{fig:SpeedUserMovedObjectXray}
% \end{figure}

All of the times when a user would press the button to pick up an object and put the same object down were recorded and summarised across each task to record the time spent on each task.
This was calculated as a percentage of the time the participants moved the icosahedron each time.
This enabled the identification of which aspects of the task posed greater challenges for participants: placing the icosahedron or viewing and confirming its correctness.
The \gls{lmm} showed a significant fixed effect on the \gls{X-ray Visualization} condition ($\chi$2(4, N= 20) = 36.4188, p $<$ 0.0001) and the presence of the reference objects ($\chi$2(1, N= 20) = 7.7862, p = 0.005). No significant interaction effect was found. ($\chi$2(4, N= 20) = 4.9161, p = 0.296).

% \begin{figure}[tp]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on the Percentage of Time Participants Moved the Icosahedron.pdf}
%     \caption{
%     A box plot of percentage of time required to move the icosahedron that participants moved the icosahedron based on the presence of reference objects
%     The error bars indicate each condition's confidence levels (CL = 95\%).
%     Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:SpeedUserMovedObjectReferenceObjects}
% \end{figure}

Post-hoc pairwise comparison of the \glspl{X-ray Visualization} showed participants held the object for significantly less time for \textit{Random Dot} than \textit{Edge} (p $=$ 0.0130, df = 989, t = -3.183), \textit{Tessellation} (p = 0.0364, df = 989, t = 2.847), and \textit{Saliency} (p $< 0.0001$, df = 989, t = 5.475), \textit{Edge} than for \textit{Saliency} (p = 0.0012, df = 983, t = 3.847), and \textit{None} than for \textit{Saliency} (p = 0.0012, df = 983, t = 3.847).
\autoref{fig:SpeedUserMovedObject} shows that the more occlusion a 
visualization had caused\DIFdelbegin \DIFdel{the participants to hold the icosahedronlonger. }\DIFdelend \DIFaddbegin \DIFadd{, the longer the participants were holding the icosahedron. %DIF > the more occlusion a visualization had caused the participants to hold the icosahedron longer.
}\DIFaddend Post-hoc comparisons of the presence of reference objects showed participants were significantly slower without reference objects present (p = 0.0056, df = 989, t = 2.778).

% \paragraph{Time Participants Spent Moving the Object} \label{sec:TimeSpentMovingTheObject}

% All of the times between when a user would press the button to pick up an object and put the same object down were recorded and summarised across each task to record how much time in each task.
% the \gls{lmm} showed a significant fixed effect the \gls{X-ray Visualization} condition ($\chi$2(4, N= 20) = 33.0345, p $<$ 0.0001).
% No significant effect regarding the presence of the reference objects ($\chi$2(1, N= 20) = 1.8470, p = 0.1741), and no significant interaction effect was found. ($\chi$2(4, N= 20) = 3.6116, p = 0.4611).

% \begin{figure}[tb]
%     \centering
%     %\includesvg[width=\columnwidth]{Chapter3/Images/PlotofSpeedUsersMovedTheObjectShowingX-rayvisionEffect.svg}
%     %\includegraphics[width=\columnwidth]{Chapter3/Images/TimeRequiredToPlaceThe IcosahedronX-rayVisionEffects.pdf}
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of \glspl{X-ray Visualization} on the Time Spent Moving Icosahedron.pdf}
%     \caption{
%     A box plot showing the number of seconds participants spent moving the icosahedron for each task for each X-ray vision effect
%     The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%     Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).}
%     \label{fig:SpeedUserMovedObjectXray}
% \end{figure}

% Post-hoc pairwise comparison of the \glspl{X-ray Visualization} showed participants held the object for significantly less time for \textit{None} than \textit{Edge-Based} (p $=$ 0.0130, df = 989, t = -3.183), \textit{Random Dot} (p = 0.0364, df = 989, t = 2.847), and \textit{Saliency} (p $< 0.0001$, df = 989, t = 5.475). Users also moved significantly faster using \textit{Tessellation} than for \textit{Saliency} (p = 0.0012, df = 983, t = 3.847).
% Some variation was also found between Random Dot and Saliency (p = 0.0661, df = 989, t = -2.628), but it was insignificant.
% These differences can be seen in \autoref{fig:SpeedUserMovedObjectXray}.
% Post-hoc comparisons of the presence of reference objects showed participants were significantly slower without reference objects present (p = 0.0118, df = 989, t = 2.524), which can be seen in \autoref{fig:SpeedUserMovedObjectReferenceObjects}.

% \begin{figure}[tp]
%     \centering
%     %\includesvg[width=\columnwidth]{Chapter3/Images/PlotofSpeedUsersMovedTheObjectShowingReferenceObject.svg}
%     %\includegraphics[width=\columnwidth]{Chapter3/Images/TimeRequiredToCompleteGraphReferenceObject.pdf}
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on the Time Spent Moving Icosahedron.pdf}
%     \caption{
%     A box plot of Time required to move the icosahedron that participants moved the icosahedron based on the presence of reference objects
%     The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%     Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:SpeedUserMovedObjectReferenceObjects}
% \end{figure}

%
% % Removing this and instead looking at the speed the user could interact with this object.
% % but the comparison between the results is interesting
%
% \paragraph{Amount Icosahedron was Moved} \label{sec:X-ray Quantitative Results Time The Object was hold}

% \begin{figure}[tb]
%     \centering
%     %\includesvg[width=\columnwidth]{Chapter3/Images/TimeRequiredToPlaceIcosahedron.svg}
%     %\includegraphics[width=\columnwidth]{Chapter3/Images/AmountOfMovmentToPlaceIcosahedron.pdf}
%      \includegraphics[width=\columnwidth]{Chapter3/Images/Impacted}
%     \caption{The amount participants moved the Icsoahedron during each iteration}
%     \label{fig:SpeedUserMovedObjectXray}
% \end{figure}
% % The time users moved the held item
% In addition to completion time, the duration the user spent holding the object was also analyzed. The \gls{lmm} showed a significant fixed effect between the x-ray vision effects ($\chi$2(1, N= 20) = 25.3929, p $<$ 0.0001). No significant effect was shown regarding the reference objects or ($\chi$2(4, N= 20) = 0.0014, p = 0.9702),
% and no significant interaction effect was found. ($\chi$2(4, N= 20) = 4.7149, p = 0.3178). 

% Post-hoc pairwise comparison of the \glspl{X-ray Visualization} showed participants moved the icosahedron for significantly less time between \textit{Saliency}, and \textit{None} (p $<$ 0.0001, df = 989, t = 4.903) and \textit{Random Dot} (p = 0.0364, df = 989, t = 3.352), and \textit{Saliency} (p = 0.0074, df = 989, t = -5.475). Some variation was also noticed between for \textit{Tessellation} than for \textit{Saliency} (p = 0.0012, df = 989, t = -3.847), and \textit{Saliency} and \textit{Edge-Based} (p = 0.0721, df = 989, t = -2.595). These comparisons can be viewed in \autoref{fig:SpeedUserMovedObjectXray}.



% \begin{figure}[!bthp]
%     \centering
%     %\includesvg[width=\columnwidth]{Chapter3/Images/s.svg}
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on the Participant's Movements.pdf}
%     \caption{
%         The distance participants walked for each iteration was based on the differences between each \gls{X-ray Visualization} they were experiancing.
%         The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%         Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:DistanceMoved}
% \end{figure}

\begin{figure}[!bt]
    \centering
    \DIFdelbeginFL %DIFDELCMD < \includegraphics[width=\columnwidth]{Chapter3/Images/Effect of Reference Objects onthe Participant's Movements.pdf}
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \includegraphics[width=\columnwidth]{Chapter3/Images/Effect of Reference Objects onthe Participant's Movements V1.pdf}
    \DIFaddendFL \caption[The distance participants walked for each when (Left) different \glspl{X-ray Visualization} were displayed to them or (Right) the differences when reference objects were or were not available.]{
        The distance participants walked for each when (Left) different \glspl{X-ray Visualization} were displayed to them or (Right) the differences when reference objects were or were not available.
        The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
        Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:DistanceMoved}
\end{figure}

\subsubsection{User Behaviour Results (H.6 \& H.7)} \label{sec:UserBehaviourResults}
This section is focused on how the different conditions affected the participant's ability to move around the environment and where they felt comfortable standing. 


\paragraph{Distance Moved (H.6):} \label{sec:X-ray Quantitative Results DistanceMoved}

The \gls{lmm} for the distance moved throughout each iteration by the participants showed a significant fixed effect between the \gls{X-ray Visualization} effects ($\chi$2(4, N= 20) = 52.8212, p $= 0.0043$) and a significant difference between the presence of the virtual reference objects ($\chi$2(1, N= 20) = 7.8110, p = 0.0052). Still, no significant interaction effect between the \gls{X-ray Visualization} effect and the presence of the reference objects was found $\chi$2(4, N= 20) = 0.4301, p = 0.9799.
Post-hoc pairwise comparisons found that users moved significantly more when the reference objects were present (p = 0.0055, df = 989, t = -2.782).
Comparisons of the combined \gls{X-ray Visualization} effects and the presence of the reference objects showed significance between the \textit{Edge-Based} and \textit{Saliency} Visualizations (p $=$ 0.003, df = 989, t $=$ 3.610) as well as some variance between \textit{Edge-Based} and \textit{Saliency} (p $<$ 0.0806, df = 989, t $=$ 2.550), and \textit{Random Dot} and \textit{Saliency} (p $<$ 0.0957, df = 989, t $<$ 2.481) but no significance.
Plots illustrating these effects can be seen in \autoref{fig:DistanceMoved}.

% \begin{figure}[!btp]
%     \centering
%     %\includesvg[width=\columnwidth]{Chapter3/Images/DistanceTraveledViolinPlot.svg}
%     %\includegraphics[width=\columnwidth]{Chapter3/Images/DistanceUsersMovedDuringTheTaskReference Objects.pdf}
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on the Participant's Movements.pdf}
%     \caption{
%         The distance participants walked for each iteration was based on the differences when reference objects were or were not available.
%         The error bars indicate each condition's confidence levels (CL = 95\%).
%         Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:DistanceUsersMovedDuringTheTaskReference}
% \end{figure}

\paragraph{Distance from Box (H.7)} \label{sec:X-ray Quantitative Results Distance Stood Away From the Box}
\begin{figure}[!btp]
    \centering
    %\includesvg[width=\columnwidth]{Chapter3/Images/DistanceTraveledViolinPlot.svg}
    %\includegraphics[width=\columnwidth]{Chapter3/Images/DistanceUsersMovedDuringTheTaskReference Objects.pdf}
    %\includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on Participant Distance from Colorful Voronoi Box.pdf}
    \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on Participant Distance from Colorful Voronoi Box.pdf}
    \caption[Box plots represent the distance participants walked for each iteration.]{
        The distance participants walked for each iteration was based on the differences between (Right) each \gls{X-ray Visualization} they expressed or (Left) when reference objects were unavailable.
        The error bars indicate each condition's confidence levels (CL = 95\%).
        Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:DistanceAwayFromBox}
\end{figure}

The \gls{lmm} of the distance the participants stood away from the large physical cube throughout the task on average showed a significant fixed effect of the \gls{X-ray Visualization} effects ($\chi$2(1, N= 20) = 14.1305, p $=$ 0.0067). Both the presence of reference objects ($\chi$2(4, N= 20) = 0.5301, p $=$ 0.4666) and the interaction effects these ($\chi$2(4, N= 20) = 0.5013, p $=$ 0.9733) showed no significance.
Post-hoc pairwise comparisons of the \gls{X-ray Visualization} effects showed a significant result between \textit{Random Dot} and \textit{saliency} (p $=$ 0.0217, df = 989, t $=$ 3.021). 
Some variation was found between \textit{Edge-Based} and \textit{Random Dot} (p $=$ 0.0875, df = 989, t $=$ -2.716) and \textit{None} and \textit{Saliency}  (p $=$ 0.0652, df = 989, t $=$ 2.633) but no significance. Comparisons between \gls{X-ray Vision} effects can be seen on the right side of \autoref{fig:DistanceAwayFromBox}.

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/X-ray Visualization on Participant Distance from Colorful Voronoi Box.pdf}
%     \caption{
%         The distance average distance participants stood away from the large Voronoi box for each iteration was based on the differences between each \gls{X-ray Visualization} they were experiencing.
%         The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%         Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:DistanceAwayFromBox}
% \end{figure}

\paragraph{Speed Object Was Moved (H.6)}\label{Sec: speed user move the object}

% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of \glspl{X-ray Visualization} on the Velocity Participants Moved the Icosahedron.pdf}
%     \caption{
%         A box plot representing the speed/velocity in which the object was moved based on the icosahedron between each \gls{X-ray Visualization} they were experiencing.
%         The error bars indicate each X-ray visualization's confidence levels (CL = 95\%).
%         Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
%     }
%     \label{fig:}
% \end{figure}

The speed at which the users moved the item was also measured by the sum of the times the user moved the object divided by the amount of distance the icosahedron would have been moved.
Understanding this provides insights into when participants felt confident in selecting a location for the icosahedron or whether the visualization enhanced their spatial awareness.
The \gls{lmm} showed a significant fixed effect between the \gls{X-ray Vision} effects ($\chi$2(1, N= 20) = 18.2724, p $=$ 0.0015) and a significant fixed effect was shown regarding the reference objects \DIFdelbegin \DIFdel{or }\DIFdelend ($\chi$2(4, N= 20) = 8.9925, p = 0.0097). No significant interaction effect was found. ($\chi$2(4, N= 20) = 1.8465, p = 0.764). 

Post-hoc pairwise comparisons found users \DIFdelbegin \DIFdel{moved significantly }\DIFdelend \DIFaddbegin \DIFadd{movement was both significantly different and }\DIFaddend faster when reference objects were present (p = 0.0102, df = 989, t = 2.574), which can be seen in \autoref{fig:WithReferenceObjects}.
The post-hoc comparisons for the \gls{X-ray Vision} effects \DIFdelbegin \DIFdel{showed }\DIFdelend show that \textit{Random Dot} was significantly faster both than \textit{Saliency} (p = 0.0017, df = 989, t = -3.754) and \textit{Tessellation} (p = 0.0088, df = 989, t = -3.303), which can be seen in \autoref{fig:WithReferenceObjects}.

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter3/Images/Impact of Reference Objects on the Velocity Participants Moved the Icosahedron.pdf}
    \caption[Box plots representing the speed/velocity in which the object was moved based on the icosahedron.]{
        Box plots representing the speed/velocity in which the object was moved based on the icosahedron: (Left) between each \gls{X-ray Visualization} they were experiencing or (Right) whether reference objects were present.
        The error bars indicate each condition's confidence levels (CL = 95\%).
        Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:WithReferenceObjects}
\end{figure}

\subsection{Subjective  Results (H.8 \& H.9)} \label{sec:X-ray Subjective Results}
% \begin{figure}[htp]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/XrayPaasResults.pdf}
%     \caption{A bar plot showing PAAS cognitive load results acquired after each participant finished using an \gls{X-ray Visualization} in the study. Higher scores indicate a higher cognitive load required. Error bars indicate $\pm$ standard deviation.}
%     \label{fig:Study 1 PASS Reslts Plot}
% \end{figure}

\DIFaddbegin \DIFadd{This section focuses on answering }\textbf{\DIFadd{H.8}} \DIFadd{and }\textbf{\DIFadd{H.9}} \DIFadd{in detail by looking at the subjective data collected throughout the study.
All subjective data was found to be non-normally distributed using a Shapiro-Wilk test (p $<$ 0.05). 
Therefore, a non-parametric Friedman test was used to determine if there were any significant differences between the various conditions.
Mixed Effect Models were not used since the data was represents the participant's subjective opinion of each condition rather than a measurable quantity as such treatment of the participant as a random effect was not necessary.
}

\DIFaddend The PAAS results (shown in the upper left plot of \autoref{fig:SubjectivePlots}) showed a significant difference between the \glspl{X-ray Visualization} using a Friedman rank sum test
($\chi$2(4, N= 20) = 41.185, p $<$ 0.0001).
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied. 
Comparisons showed significantly increased cognitive load between 
\textit{Saliency} and \textit{Edge-Based} (p = 0.0208),
\textit{Saliency} and \textit{None} (p $<$ 0.0001),
\textit{Saliency} and \textit{Random Dot} (p = 0.0003),
\textit{Saliency} and \textit{Tessellation} (p $<$ 0.0001).

\begin{figure}[htp]
    \centering
    \includegraphics[width=\linewidth]{Chapter3/Images/SubjectiveGraphs.pdf}
    \caption[Three plots displaying the subjective results collected throughout the course of this study.]{Three plots displaying the subjective results collected throughout the course of this study. 
    (Upper Left) PAAS cognitive load results were acquired after each participant finished using an \gls{X-ray Visualization} in the study. Higher scores indicate a higher cognitive load required.
    (Lower Left) The System Usability Scale (SUS) results were acquired after each participant finished using an \gls{X-ray Visualization} in the study. Higher scores indicate better usability.
    (Right) A tally of participants' favorite visualizations was totaled by having them choose their favorite ones. Error bars show standard error values.
    Error bars indicate $\pm$ standard deviation. Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:SubjectivePlots}
\end{figure}

% A chi-square showed highly there was a significant relationship between different visualizations and the users confidence  $\chi$2(4, N= 20) = 61.495, p $<$ 0.00001,
% \textit{Saliency} was found to be much lower than the other other visualizations. 

%
%Kendalls's rank correlation was conducted to determine if the final Sus Scores.
%The SUS scores between \gls{X-ray Vision} effects and the final SUS scores showed no correlation.
The System Usability Scale results (shown in the lower left of \autoref{fig:SubjectivePlots}) showed a significant difference between the \glspl{X-ray Visualization} using a Friedman rank sum test
$\chi$2(4, N= 20) = 45.234, p $<$ 0.0001.
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied, resulting in a significantly lower usability score for \textit{Saliency} than \textit{Edge-Based} ($p = 0.0012$), \textit{None} ($p = 0.0002$), \textit{Random Dot} ($p = 0.0002$), and \textit{Tessellation} ($p = 0.0002$).

% \begin{figure}[btp]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/XraySusResults.pdf}
%     \caption{A box plot showing the System Usability Scale (SUS) results acquired after each participant finished using an \gls{X-ray Visualization} in the study. higher scores indicate better usability. Error bars indicate $\pm$ standard deviation.}
%     \label{fig:SUS Reslts Plot}
% \end{figure}

At the end of each study, users were asked if they could state their favorite visualization and explain why. The results are tallied and displayed the results in the right-hand side plot in \autoref{fig:SubjectivePlots}. 
No significant difference was found between the \gls{X-ray Vision} effects when using a Chi-Square Test $\chi$2(4, N= 20) = 2.5, p $=$ 0.6446 \DIFdelbegin \DIFdel{. When }\DIFdelend \DIFaddbegin \DIFadd{when participants where }\DIFaddend asked why this visualization seems to have been their favorite. The results can be seen in \Cref{app:Chapter3Comments}. 

Participants would seem to have picked \textit{Tessellation} and \textit{None} more often as they were less occlusive and did not interfere with the task. \textit{Tessleation} seems to have provided users with enough visual cues to be able to navigate with it easily, making it the second most preferred method of visualizing the internal structures of an element. Users also reported liking \textit{Random Dot} since its design matched the shape of the box the most. User comments found in \Cref{app:Chapter3Comments} \DIFaddbegin \DIFadd{show that }\DIFaddend \textit{Saliency} \DIFdelbegin \DIFdel{were }\DIFdelend \DIFaddbegin \DIFadd{was }\DIFaddend criticized for blocking out too much of the object underneath. At the same time, several participants liked how unobtrusive the edge-based visualization was, but some participants noticed a slight delay while using both it and \textit{Saliency}.


% \begin{figure}[btp]
%     \centering
%     \includegraphics[width=\columnwidth]{Chapter3/Images/FavoriteXrayVis.pdf}
%     \caption{
%         This bar chart shows a tally of participants' favorite visualizations, which was totaled by having them choose their favorite ones. Error bars show standard error values
%     }
%     \label{fig:plotsOfFavoritevisualizations}
% \end{figure}

\section {Discussion}  \label{sec:X-ray Discussion}
The findings from \autoref{sec:X-ray Results} present a broad spectrum of findings, which this section discusses in detail.
Notably, it was found that while \textit{Saliency} was beneficial for depth perception, it hindered the general placement of objects and was generally found to be the hardest to use when evaluating its usability and participants' cognitive loads.

%H.1 Participants will place the virtual icosahedron closer to the correct target position when they are using the Saliency visualization.
%H.2 Participants’ vertical and horizontal placement of the virtual icosahedron will be further from the correct target position when using the Saliency visualization.
%H.3 Participants’’ depth-based placement of the virtual icosahedron closer to the correct target position when they are using the Saliency visualization.
\subsection{Accuracy of Placement (H.1)}
The results shown in \autoref{sec:X-ray Quantitative Results Placement Accuracy} from this study did not support hypothesis one but partially supported hypothesis three. The right-hand side of \autoref{fig:X-ray AccuracyPlotZAxis plot} shows \textit{Saliency} positively affected depth perception compared to the edge-based visualization.
\autoref{tab:MeanAndMedianAccuracyValues} shows that while there seems to be a lot of difference between \textit{Saliency's} median and mean values being over 3 cm different, whereas other conditions throughout this study were relatively consistent.
This shows that users were very inconsistent with saliency, with their results sometimes being very accurate and sometimes being very inaccurate. 
This is likely because the occlusion created by the saliency covered larger areas of the visualization, showing that more occlusion can lead to better depth perception.

\subsection{Placement Accuracy from User Viewpoint by Axis (H.2 \& H.3)}
The right-hand side of \autoref{fig:X-ray AccuracyPlotXAxis plot} shows that \textit{Saliency} was the least accurate visualization on the horizontal axis. 
This can be explained better in-depth when looking at insignificant trends on the vertical axis (shown in \autoref{fig:X-ray Reference Objects AccuracyPlotYAxis plot}) and the values for x and y in \autoref{tab:MeanAndMedianAccuracyValues} where both random dot and saliency show a worse mean and standard deviation values.
This is likely due to the high occlusion in this X-ray visualization.
The effects that occluded the least gave the user a better understanding of space, where the shapes were inside the large physical cube with a Voronoi pattern. 
Still, the results from \autoref{fig:X-ray AccuracyPlotXAxis plot} and \autoref{fig:X-ray Reference Objects AccuracyPlotYAxis plot} lacked the significance to support hypothesis two. 

%DIF > The results in \autoref{sec:X-ray Quantitative Results Placement Accuracy} and \autoref{tab:MeanAndMedianAccuracyValues} seem to indicate that when using an OST AR device, the edge-based visualization provides the worst depth perception objects when it is not being used with the reference objects there are no reference objects to view. 
The results in \autoref{sec:X-ray Quantitative Results Placement Accuracy} and \autoref{tab:MeanAndMedianAccuracyValues} \DIFdelbegin \DIFdel{seem to indicate thatwhen using an OST AR }\DIFdelend \DIFaddbegin \DIFadd{indicate that, on an }\gls{ost} \gls{ar} \DIFaddend device, the edge-based visualization \DIFdelbegin \DIFdel{provides the worst depth perceptionobjects when it is not being used with the reference objects there are }\DIFdelend \DIFaddbegin \DIFadd{produced the poorest depth perception, particularly when }\DIFaddend no reference objects \DIFdelbegin \DIFdel{to view}\DIFdelend \DIFaddbegin \DIFadd{were present}\DIFaddend .
This could be due to two reasons: as mentioned previously, prior work has shown that more occlusion allows for better depth perception~\cite{Otsuki2017}, provided the visualization does not occlude too much~\cite{Santos2016}; the other reason may be due to the edge-based visualization in this instance doesn’t represent the shape of the cube well \DIFdelbegin \DIFdel{do }\DIFdelend \DIFaddbegin \DIFadd{due }\DIFaddend to the mosaic nature of the pattern~\cite{Avery2009}. 
For this research, a Voronoi effect was chosen to be used on the large physical cube, as I hypothesized to aid both the \textit{Saliency} and \textit{Edge-Based} \gls{X-ray Vision} effects by creating more areas for them to highlight; however, due to the non-uniform pattern on the box, the fact participants were looking down at it and within the box and rather all may show a weakness in this format when compared to a brick wall~\cite{Avery2009, Sandor2010}. 
Previous studies indicate \textit{Edge-Based} visualization can tell a user if an object is in front of or behind another object quite well~\cite{Kalkofen2007, Avery2009, Zollmann2014}. Still, the results from this study indicate the impact on depth perception can be slightly misleading. 
These findings may also be true for the \textit{Saliency} visualization; however, the improvement which can be seen in \autoref{fig:X-ray AccuracyPlotZAxis plot} and lowest mean accuracy showed in \autoref{tab:MeanAndMedianAccuracyValues} illustrates that the depth perception \textit{Saliency} presents may make it more viable for certain situations.

%H.4 Participants will place the virtual icosahedron closer to the correct target when reference objects are present.
\autoref{sec:X-ray Quantitative Results Placement Accuracy} shows that users did perform significantly better with reference objects, supporting the fourth hypothesis. 
Overall, it seems that introducing the reference objects may have allowed participants to place the icosahedron about 1 cm closer to the target position. 
Interestingly, though, it seems to have much more to do with the placement of all three axes, especially the user's horizontal axis, which can be seen in \autoref{fig:X-ray Reference Objects AccuracyPlotYAxis plot} where a significant difference is less accurate than \textit{None} and \textit{Tessellation}. 
%Since no interaction effects were noticed, I cannot conclude that these reference objects have any combined effect with the \gls{X-ray Vision} effects. 
Still, it does seem clear that users can more accurately place objects when they have more than one reference in the static state. 

\subsection{Time Required (H.5)}
%H.5 Participants will take less time when the reference objects are added to the virtual scene.
\autoref{sec:UserBehaviourResults} showed that all the fifth, sixth, and seventh hypotheses were not supported as \autoref{fig:TimeRequiredToCompleteReferenceObjects} \DIFaddbegin \DIFadd{shows that users }\DIFaddend seemed to take \DIFdelbegin \DIFdel{users }\DIFdelend slightly longer (about 3 seconds) to complete each task when reference objects were present. They seemed to have moved somewhat less and were closer to the large physical cube with a Voronoi pattern. 
Due to the relatively similar results, they seem to indicate that participants were not utilizing the depth cues of relative size and density because they were either ineffective or not being used. 
Participants \DIFdelbegin \DIFdel{seemed to use the objects instead }\DIFdelend \DIFaddbegin \DIFadd{primarily used the visualizations }\DIFaddend to improve their spatial awareness of the object. \DIFdelbegin \DIFdel{Surprisingly, this led to them not using the objects as a depth cue but as a measuring tool. }\DIFdelend \DIFaddbegin \DIFadd{In some cases, this seemed to involve treating the visualizations as implicit measuring aids rather than relying on them directly as depth cues. This observation, while not systematically verified, suggests that users may appropriate visualization features in ways that were not originally intended.
}\DIFaddend 


\subsection{User Behaviour Results (H.6 \& H.7)}
%H.6 Participants will move less when the reference objects are present. 
%H.7 Participants will stand farther back from the Voronoi cube when more reference objects are present.
There was no significant effect on the differences that participants moved between when using different \gls{X-ray Vision} effects. 
Still, no effect was found, which indicated that when a more occlusive \gls{X-ray Vision} effect was used (\textit{Saliency} and \textit{Random Dot}), participants did get slightly closer to the large physical cube with a Voronoi pattern. 
This was likely because it allowed them to look into the box easier, not because it encouraged them to stand closer to the box. 

%H.9 users will move the reference object the slowest when the transparency \gls{X-ray Vision} effect is being used. 
Other behavioral findings which were observed \DIFaddbegin \DIFadd{in }\DIFaddend \autoref{sec:UserBehaviourResults} were that users were significantly faster when using no \gls{X-ray Vision} effect (shown in \autoref{fig:TimeRequiredToCompleteReferenceObjects}). 
It seems that viewing the \gls{X-ray Vision} effect slightly delays users' movement as they comprehend the layout of the various visualizations, making more straightforward visualizations like that have a predictable pattern like \textit{Random Dot} and \textit{Tessellation} easier to use. 
This could also be seen with the speed they moved the icosahedron faster while using \textit{Random Dot} (shown in \autoref{fig:WithReferenceObjects}). 
Since \autoref{fig:SpeedUserMovedObject} showed participants held the \DIFdelbegin \DIFdel{less }\DIFdelend \DIFaddbegin \DIFadd{virtual objects for the least amount of }\DIFaddend time with no \gls{X-ray Visualization} effect and \textit{Tessellation}, it seems that \textit{Random Dot} effect may have been as a guideline to determine where to place the object much like the reference objects.

\subsection{Subjective Results (H.8 \& H.9)}
% Preferences
%H.8 Participants will find Saliency subjectively difficult to use and require a higher cognitive load than other \gls{X-ray Vision} effects
%I hypothesized that participants would find \textit{Saliency} hard to use (\textbf{H.8}), and 
The results from the qualitative data (reported in \autoref{sec:UserBehaviourResults}) seem to show that participants would find \textit{Saliency} hard to use (\textbf{H.8}). 
The upper left plot of \autoref{fig:SubjectivePlots} illustrated how the PAAS scores varied between all conditions
which was also reflected in the SUS score shown in the lower left plot in \autoref{fig:SubjectivePlots}. 

All conditions showed a significant difference between them and \textit{Saliency}, indicating they tended to dislike the \textit{Saliency} visualization on the \gls{ost} \gls{ar} device when they were close to the visualization tracking smaller objects. 
Feedback from this visualization noted that the occlusive nature of this visualization led them to lose track of various virtual objects during the study and possibly caused some frustration (\textbf{H.9}). 
It should be noted that this observation was not universal, as one participant stated, "It was easier to position the objects accurately as opposed to the others" about \textit{Saliency}. 
Our participants reported that \textit{Random Dot} and \textit{Saliency} blocked their vision too much, which may have been the reason for its lack of popularity. 
\textit{Random Dot} may be improved if the number of dots was increased and their size decreased depending on the participant's distance from the object. 

The questionnaires and user comments showed \textit{the None} or baseline \DIFdelbegin \DIFdel{condiation }\DIFdelend \DIFaddbegin \DIFadd{condition }\DIFaddend was viewed as the most preferred visualization next to \textit{Tessellation}. This further indicates that \glspl{X-ray Visualization} that \DIFdelbegin \DIFdel{occludes }\DIFdelend \DIFaddbegin \DIFadd{occlude }\DIFaddend less would be preferred. 
However, the visualizations occluded the most improved depth perception the most. 
It would seem that moving forward to research these visualizations that it would make sense not just to consider depth perception but also other tasks that may be incorporated into these studies. 

\subsection{Summary of Discussion}
% More occlusion can help with depth perception but hinders placement.
% Users don't want their vision to be occluded in an obtrusive way. 
% I cannot accurately translate VSTAR technologies to OSTAR using off-the-shelf hardware. 
% The Presence of occlusion is important but not needed.
% Participants don't focus on the physical world when using OSTAR devices. This means that they may mean that they use the depth plane for perception a lot.
% Movement is an extremely powerful depth cue; this may extend to interaction is a powerful depth cue. 
% Allowing people to choose their own path to follow is probably not a great idea. as it allows for problem-solving.

What was unexpected but interesting about this study is that having no \gls{X-ray Visualization} did not do poorly at this task. 
Unexpectedly, participants with no \gls{X-ray Visualization} seemed to have performed fine throughout the study.
No participants commented that the objects felt like they were in the wrong place, which did not prevent anyone from performing the task. While it did not aid people in the task they were given, it was never a detriment.
It seems like the stereoscopic effects from the display may have fixed a major issue \DIFaddbegin \DIFadd{with }\glspl{X-ray Visualization} \DIFadd{by providing more depth perception cues than other devices showing that indeed that just enough reality may be enough in some instances.
Showing that depth cues other than occlusion may be just as important as occlusion for }\gls{X-ray Vision}\DIFaddend .
\DIFdelbegin \DIFdel{To this end, the occlusion only needs }\DIFdelend \DIFaddbegin 

\DIFadd{A plausible explanation for the }\textit{\DIFadd{None}} \DIFadd{condition’s success lies in how participants processed spatial information.
Those exposed to the }\glspl{X-ray Visualization} \DIFadd{may have experienced divided attention between the virtual and physical environments, while those without overlays could focus on the more consistent depth cues provided by the stereoscopic display.
For instance, motion parallax and binocular disparity are potent depth cues that can be effectively utilized in the absence of occlusive overlays.
Display brightness may also have played a role, as the reduced contrast between virtual and real content could have created a natural “transparency effect,” partially replicating the purpose of an X-ray view.
This might explain why some users still valued the X-ray representations even though they were not strictly necessary for task completion.
}

%DIF >  Upon reflection, the \textit{None} condition may have performed so well in comparision to the other \glspl{X-ray Visualization} because the participants may not have been able to focus on either the physical or virtual world when viewing the other \glspl{X-ray Visualization}.
%DIF >  This may have led them to rely on the depth plane of the display to help them understand where the object was in space. 
%DIF >  One more consideration is that the brightness of the display may not have been high enough to requrire X-ray vision to be required as the virtual objects would have been less visbile than the real world objects creating the same effect used by Transparancy Based X-ray Visulizations.
%DIF >  This didn't completely fix the requirement as some users still preferred the \glspl{X-ray Visualization} effects, but it may have reduced the need for them.

\DIFadd{The proposal here is that the AR effect for }\glspl{X-ray Visualization} \DIFadd{effects should be designed to be as unobtrusive as possible.
Rather than trying to create a perfect }\gls{X-ray Visualization} \DIFadd{effect, the goal should be to create an effect that is just good enough to provide the necessary information to the user without overwhelming them or blocking their view of the real world.
Using occlusion only }\DIFaddend to illustrate where an item is compared to the physical object. It does not need to observe it, although that would provide better depth perception.

\section{Future Considerations}

% Overall issues with the design
This study has several limitations that may have impacted this work. 
The gender gap among participants may have influenced some of the findings \DIFaddbegin \DIFadd{however there is no evidence of this}\DIFaddend .
The large physical cube with a Voronoi pattern and the \glspl{X-ray Visualization} were designed with heavy consideration from Otsuki et al.\cite{Otsuki2017} and Santos et al.\cite{Santos2016}.
Experiments with different implementations and settings for these conditions could result in different results, and it would be interesting to see if research like the work from Santos et al.\cite{Santos2015} on illumination and opacity has the same results on other devices.

% Reducing movement data
In future studies, I would consider restricting movement around the larger cube. 
For this study, it was valuable to see how well a participant could place a virtual object in an augmented world using \gls{X-ray Vision} while being able to move in the physical space, which mimics real-world settings.
Although this allowed the participants to employ their own problem-solving techniques,
this made analysis difficult as each participant undertook their own strategies, resulting in a wide range of results. 
This was a benefit to this research as it showcases data that could easily be translated to a real-world setting. Still, it would have been possible to obtain more consistent and statistically stronger results by restricting the users' movement to a smaller space.

% issues regarding camera latency
\DIFdelbegin \DIFdel{These are great steps to correct }\DIFdelend %DIF > These are great steps to correct latency for this study, but it did not eliminate it completely. 
\DIFaddbegin \DIFadd{Great steps were taken to reduce }\DIFaddend latency for this study \DIFaddbegin \DIFadd{by developing a system that presented a video feed where it was captured with a assumption this will not be at real time (as presented in in }\autoref{sec:X-rayAddionalSensors}\DIFadd{)}\DIFaddend , but it did not eliminate it completely.
Mounting a camera to the head of the participants was a useful attempt to transfer VST AR effects to an \gls{ost} \gls{ar} headset, but it was not without difficulties as several participants commented that they noticed \DIFdelbegin \DIFdel{this}\DIFdelend \DIFaddbegin \DIFadd{some latency between their vision and the camera}\DIFaddend .
Camera technology is not yet on par with the precision you can gain from using an \gls{imu} like the one found on the HoloLens (approx. 1000 fps)~\cite{Guo2022, Matyash2021}, and relying on cameras creates a noticeable lag between the augmented world and the physical objects as participants move about. 
To some extent, a lag of less than 16ms was purposeful since the experiment tested these visualizations as they would need to run in a real-world environment.
I could have used a pre-generated texture that applied the effect to the outside of the box.
This would allow for these methods to be tested but would not be applicable to any real world scenarios in the near future~\cite{Geng2013}. 

% How could the camera have tracked the system better
A system that allowed direct access to the \gls{imu} would have been ideal.
If it was possible to know the exact millisecond or better when a photo was taken, the system could have placed the image in the exact right spot rather than being several milliseconds late. 
Further corrections could also be made if more information was utilized between each frame by taking into account velocity or advanced machine learning techniques for predicting user movements like the work from Gamage et al.\cite{Gamage2021} and Lee et al.\cite{Lee2021}.
This allows the system to place the overlay texture in almost the exact place for the appropriate millisecond. 
Another option around this issue would have been not placing the camera sensor in a stationary position, similar to \cite{Hamadouche2018}.
This would have removed any issues regarding movement but would have made the system much less flexible. 

\DIFaddbegin \DIFadd{Overall the lack of a time limit lead to a wide variety of strategies being used by participants causing a lot of variance in the time taken to complete the task.
This was purposeful to allow participants to take their time and ensure they were accurate and demonstrate utilize their own sense of depth perception.
This lead which may have been a confounding factor in the results, as some participants took their time to ensure they were accurate, while others rushed to complete the task quickly.
}

%DIF >  talking about the unknown effects that could have impacted this study if it was researching a different effect.
\DIFadd{This study allowed for a wide range of interactions with the experimental setup.
While this does provide a more realistic scenario, it also indicates that we may not have been researching spatial awareness as much as we were problem-solving or another cognitive task.
This could have been mitigated by restricting the movement of the participants or by having a more structured task.
Moving forward it would make sense to run a more restricted version of this study.
}

\DIFaddend \section{Conclusion}
This research has shown that the \gls{X-ray Visualization} effects that work on VST AR devices can be transferred to OST AR devices, but they have limited benefits over \DIFdelbegin \DIFdel{SLAM-based methods}\DIFdelend \DIFaddbegin \DIFadd{real world overlay x-ray visualizations}\DIFaddend .
This study has shown that there is a large difference between different methods of \gls{X-ray Vision} effects and how they are displayed. This study found that occluding larger parts of the internal object will improve depth perception but at the cost of vertical and horizontal accuracy. 
When using stereoscopic Ocular See-Through (OST) Augmented Reality (AR) displays, users seem to value seeing the object's geometric structure, which can be seen through participants' answers to the subjective questionnaires and their performance with the edge-based visualization. 

The main contributions of this chapter are:
\begin{itemize}
    \item More occlusion can help with depth perception but hinders placement.
    \item Users don't want their vision to be occluded in an obtrusive way, meaning it needs to be genuinely salient \DIFaddbegin \DIFadd{(aligned with the user’s attention and task relevance, not just visual distinctiveness) }\DIFaddend to the user for saliency to work correctly. 
    \item The presence of occlusion for \gls{X-ray Vision} is essential but unnecessary to cover the entire field.
    \item A system design that allows computer vision effects to be overlaid on a user's vision.
%     \item Participants don't focus on the physical world when using OSTAR devices. This means that they may mean that they use the depth plane for perception a lot.
%     \item Allowing people to choose their own path to follow is probably not a great idea. as it allows for problem-solving.
\end{itemize}

This chapter has provided insights into which \DIFdelbegin \DIFdel{elements of an }%DIFDELCMD < \gls{X-ray Visualization} %%%
\DIFdel{are acceptable and which are not in }\DIFdelend \DIFaddbegin \DIFadd{rendering techniques are effective as }\glspl{X-ray Visualization} \DIFadd{(such as }\textit{\DIFadd{Saliency}}\DIFadd{, }\textit{\DIFadd{Edge-Based}}\DIFadd{, }\textit{\DIFadd{Tessellation}}\DIFadd{, and }\textit{\DIFadd{Random Dot}}\DIFadd{). 
It has also clarified which perceptual outcomes are important for achieving }\DIFaddend \gls{X-ray Vision}\DIFdelbegin \DIFdel{.
}\DIFdelend \DIFaddbegin \DIFadd{, namely accurate depth perception, alignment with the real world, and unobtrusive integration into the user’s view.
%DIF > This chapter has provided insights into which elements of an \gls{X-ray Visualization} are acceptable and which are not in \gls{X-ray Vision}.
}\DIFaddend This study has clarified that while occlusion can aid depth perception, it hinders general spatial awareness of a given space.
Making it less than an idea to find a usable solution for X-ray vision.
This study also showed that Computer Vision enabled \glspl{X-ray Visualization} may not have a place on \gls{ost} \gls{ar} \glspl{hmd} and should likely not be considered until the video camera hardware has progressed further so it can better keep up with human sight. 

% Moving forward, will look at how \gls{dvr} can be utilized with X-ray vision. This is a very different form of rendering that does not utilize polygonal shapes. 
% The designs for these artifacts will need to be crafted by utilizing basic geometry instead. 
% These methods will also need to be efficient to allow work alongside real-time \gls{dvr} to help promote movement to better allow for a clear understanding of the data within the object. 

% \begin{itemize}
%     \item More occlusion can help with depth perception but hinders placement.
%     \item Users don't want their vision to be occluded in an obtrusive way. 
%     \item The Presence of occlusion is important but not needed.
%     \item Participants don't focus on the physical world when using OSTAR devices. This means that they may mean that they use the depth plane for perception a lot.
%     \item Movement is an extremely powerful depth cue; this may extend to interaction is a powerful depth cue. 
%     \item Allowing people to choose their own path to follow is probably not a great idea. as it allows for problem-solving.
% \end{itemize} \newpage 
    \glsresetall
    %\include{Chapter4/Chapter4Brief}
     \newpage \chapter{Designing X-ray Visualizations with Volume Rendering} \label{Chap:VolumetricX-rayVision}
% Introduction to this section: why is this chapter required
This chapter presents an investigation into the use of volume rendering to support \glspl{X-ray Visualization} by providing an explanation of the technical aspects of volume rendering and motivating the opportunity of volume rendering. 
Then, it presents a summary of volume rendering techniques and algorithms designed and demonstrated for use on stereoscopic displays, which form the basis of the research in this dissertation.
Finally, drawing from the knowledge from the literature review (\autoref{chap:Background}) and the previous study (\autoref{Chap:X-ray Implemntion}) to create a new form of X-ray Visualizations the \glspl{virt}.


% Firstly, we will provide a grounding for the unlying technology that the X-ray visualizations being tested in this study will use.
The \glspl{virt} are designed for \gls{mri} and \gls{ct} data when displayed using \gls{dvr} and how to view the inside of a solid object that they are captured from.
The image data from \gls{mri} and \gls{ct} scanners has been chosen as the main focus as these devices are the most commonly used forms of volume rendering that are used to look inside of objects rather than trying to visualize or make other data readable.

%These techniques can likely be extended to other uses for \gls{dvr} as they all utilize a similar geometrical space.
%two methods into something were brought up. One of them would be to place cameras behind the real world and create a scene, which does not work well when you can not access the inside of the object.
% Causes this chapter to focus on creating X-ray visualizations that you can see into.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Chapter4/Images/CTScannersCropped.jpg}
    \caption[A patient receiving a scan in the \gls{ct} scanner]{A patient receiving a scan in the \gls{ct} scanner. Left) shows the environment within the Scanning Room. Right) is one example of an operation area for a given \gls{ct} scanner. Provided by NIH Clinical Center}
    \label{fig:CTScanner}
\end{figure}


To understand what is inside solid objects, a 3D scanning technique will need to be utilized, allowing people to observe an area using a method that human sight can use.
Some options exist for the creation of this volume rendering data. 
Firstly, they can be created manually or from the result of a simulation~\cite{Avila1994} or by using other solutions like tracking the speed of signals between different areas~\cite{Adib2013}.
Electronic microscopes can visualize the data they see as a volume~\cite{Goodsell1989, Khlebnikov2013}.
\gls{gpr} can be used to understand what is below the ground, from pipes and artifacts to different layers of ground sediments~\cite{Maloca2018, VanSon2018, Baker2007}.
\gls{mri} and \gls{ct} scanners are more commonly used to see the inside of people~\cite{Alhazmi2018, Zhang2011} and most other types of materials~\cite{Okuyan2014, Groger2022, Vicente17}.

% DataFromScans.png
\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{Chapter4/Images/DataFromScans.png}
    \caption[Left) \gls{mri} of a skull; Center) Cerebral angiography, arteria vertebralis sinister injection; Right) CT scan of human lungs.]{
    Several different types of 2D data: 
    Left) \gls{mri} of a skull~\footnotemark[1];
    Center) Cerebral angiography, arteria vertebralis sinister injection~\footnotemark[2];
    Right) CT scan of human lungs~\footnotemark[3].
    All images are licensed under Creative Commons Attribution licences}
    \label{fig:CTScans}
\end{figure}

\footnotetext[1]{\url{https://www.flickr.com/photos/reighleblanc/3854685038}}
\footnotetext[2]{\url{https://en.m.wikipedia.org/wiki/File:Cerebral\textunderscore angiography,\textunderscore arteria\textunderscore vertebralis\textunderscore sinister\textunderscore injection.JPG}}
\footnotetext[3]{\url{https://commons.wikimedia.org/wiki/File:CT\textunderscore scan\textunderscore Iterative\textunderscore reconstruction\textunderscore \%28left\%29\textunderscore versus\textunderscore filtered\textunderscore backprojection\textunderscore \%28right\%29.jpg}}
% provides a high-level description of what the term volume rendering encapsulates (2 - 3 paragraphs)
Medical practitioners' current practice is to view medical data on a 2D display near the patient. 
This is shown in \autoref{fig:CTScanner} where the practitioner is in a separate room with a window between them and the patient looking at a series of slices from the scan.
Their concentration will be on this display, which will display images shown in \autoref{fig:CTScans}, where they can view the data from a series of 2D planes of view (axial, coronal, and sagittal). 
This type of exploration can be challenging to learn and interpret methods of data interaction~\cite{Brath2015, Vernon2002, Tang2019}.
The experience on 3D displays can be better suited for viewing 3D data~\cite{McIntire2012, Merino2018, Thomas2015}, and studies have shown that displaying this data over real objects can further aid this understanding~\cite{Martin-Gomez2021, Pratt2018, Fischer2020}. 
This makes volume rendering more suitable for \gls{mr} \glspl{hmd} as they utilize 3D displays. 
%This better experience still needs to be understood to place the internal data in the place it was pictured~\cite{Martin-Gomez2021, Pratt2018, Fischer2020}.

% To talk about issues in this field regarding testing
%This thesis focuses on looking into objects you usually could not and how this is best handled.
%We typically utilize \gls{mri} and \gls{ct} scanners to investigate areas inside of objects that we could not usually; however, this experience is not simple to deal with. 
\gls{mri} and \gls{ct} scanners are utilized to investigate areas inside objects that could not usually be seen; however, this experience is not simple.
%\gls{dvr} visualizations are suitable for hole-like X-ray visualizations since they naturally utilize the same geographic methods required.
Hole-like visualizations work by obscuring most of the data collected, which is required to create the \gls{X-ray Vision} effect. 
This is problematic as \gls{mri} and \gls{ct} scans generally only observe the immediate area.
Since they only show three axes, the only logical forms of entry are along the three planes (axial, coronal, and sagittal)~\cite{Joshi2013}.
3D objects also allow for navigating challenging anatomy that can take on multiple different shapes, like the liver~\cite{DePaolis2018}.

The radiation caused by \gls{ct} scanners can be harmful to the health of patients. Both \gls{mri} and \gls{ct} scans are time-consuming and expensive, limiting the amount of data that can be realistically collected when medical practitioners produce the \gls{mri} or \gls{ct} scans designed to diagnose and guide appropriate treatment.
This will generally result in a trade-off between the size and accuracy of the volumes these machines create. 
Since this supply is already limited, using a method of \gls{X-ray Vision} that requires excess information to be provided to adequately a hole-like \gls{X-ray Vision} technique in practice would require a change in best practice to collect more data, which would just be used to aid the illusion of depth perception. 
%The x-ray visualizations (the \glspl{virt}) avoid these issues and can be used without major changes to the current practices.

% % Methods that work for using this type of visualization to create \gls{X-ray Vision} effects
% Medical practitioners produce and create the \gls{mri} or \gls{ct} scans designed to diagnose and guide appropriate treatment.
% This is generally because there will need to be a trade-off between the size and accuracy of the volumes these machines create. 
% After all, the time required for these machines is valuable. Even though \gls{ct} scanners are faster, they also emit radiation, potentially harming the patient if the exposure lasts too long. 
% What is needed is an \gls{X-ray Vision} method that can be utilized for \gls{dvr}; it is important to consider the environment that can be made and used without major changes to the current workload.

This chapter aims to enable methods of \gls{X-ray Vision} that can be used with \gls{dvr} to create a method to view information created from \gls{mri} or \gls{ct} scanners using an \gls{ost} \gls{ar} \gls{hmd}.
This will utilize artistic effects (\glspl{virt}) because they do not obscure the focus of the user while still presenting the illusion of depth within the real world. 
However, before that goal is reached, there is a need to create and establish a method of real-time \gls{dvr} for stereoscopic displays. 
This chapter presents a novel apparatus to employ DVR on MR, allowing \glspl{virt} to be presented to users.

% Taling about the types of visualizations explored in this part of the paper.
% This Chapter discusses several \gls{X-ray Vision} types in \autoref{chap:Background}. 
% There are generally two methods of seeing things from other fields. 
% One would be placing cameras behind the real world, allowing for \gls{X-ray Vision}, or creating a scene on the other end by reconstructing the images or medical imaging devices. 
% This thesis will focus on the latter and look at how data from MRI or CT scanners can deliver \gls{X-ray Vision} for OST AR devices, which do not obscure any information but allow for a similar effect of \gls{X-ray Vision}. 

\section{Fundamentals of Volume Rendering}
%The volume dataset has some expectations. 
Volume datasets represent information relating to a given physical space~\cite{Kaufman2005}. 
They can either be viewed as a stack of 2D images, a singular 3D image, or a vector or scalar field. 
Volume datasets are known for representing \gls{ct} and \gls{mri}, but they are also used for displaying geometric data that consists of information that can be described in a voluminous way, such as meteorological data~\cite{Kaufman2005, Joshi2009}. 

\subsection{Technical Description of Volume Rendering}
Volumes are organized into a grid of \glspl{voxel_g}, each with its own position and value. 
These values can range in their purpose from Houndsvile units found in \gls{ct} scans~\cite{DenOtter2024}, relaxation times for \gls{mri} scans~\cite{Rinck2024}, and they can either contain normals or velocity when looking at such as meteorological data~\cite{Joshi2009}. 
Generally, a default value is chosen when no value exists in a particular area.
This makes volume data much less flexible than point cloud data but allows for more flexibility with the rendering process. 

Volumes can also be extended into having a fourth dimension, which will normally represent time, allowing them to change the visualization based on the amount of time that has passed. 
This can be used in medicine to give the surgeon a clear idea about how much organs in a patient may move normally~\cite{Langner2008, Gill2015} or to allow meteorologists to view the impact of phenomena like wind in real-time~\cite{Wang2018, HibbardL.1986}. 

\subsection{Visualizing Volume Data}
% Explaining quickly how rendering is usually done
To visualize a volume in 3D, it is preprocessed into an iso-surface or rendered directly. 
Creating an iso-surface makes it possible to use more traditional graphical rendering methods to visualize the volume, allowing these visualizations to be viewed using less computationally expensive at run time~\cite{Baoquan2016, Lorensen}.
Lowering the resulting polygon count of these volumes makes it possible to allow them to work on even less powered devices~\cite{Newman2006}.
However, preprocessing a volume into an iso-surface requires running time-consuming processes, which decreases and warps the amount of information seen. 
Typically, this type of visualization will be used to see only a few different surfaces of the volume~\cite{Newman2006}.
Showing multiple surfaces transparently also causes similar issues to traditional rendering, where an object is not entirely in front of or behind, and another transparent object is not.
When accurate results are required or when dealing with complex structures, \gls{dvr} is a more practical choice because it provides additional information that can be filtered in real time~\cite{Dai2021}.

\subsection{Direct Volume Rendering (DVR)}
% Start Talking about DVR
\gls{dvr} directly renders the data from the volume and displays it as a 3D image.
This allows for realistic images like those shown in \autoref{fig:CinimaticRendering}, which are accurate to the source with very low preprocessing costs, which consist of reading in the files to the \gls{gpu}.
\gls{dvr} has the drawback that it is slow to render with the images in  \autoref{fig:CinimaticRendering}, taking over 3 minutes to render~\cite{Eid2017}. 

An advantage to \gls{dvr} is that it provides a very flexible framework for rendering. 
It is capable of allowing versions of \gls{dvr} to focus on making the best image possible and others to focus on providing smooth interactions on lower-powered devices~\cite{Li2003, Morrical2019, Hadwiger2018, Deakin2019}. 
\gls{dvr} has many methods to make it work efficiently on mobile devices, with several recent studies getting \gls{dvr} to work on the HoloLens 1 and 2~\footnote{\url{https://www.microsoft.com/en-au/hololens}}~\cite{Jung2022, Cetinsaya2020}.

\subsection{Light Simulation Using Direct Volume Rendering}
Generally, \gls{dvr} simulates rays of light that travel through the volume.
Mathematically, moving along a given ray is the same even when working with non-light-based visualizations, such as determining the direction of wind from meteorological data.

These photons will traverse times across different distances in each render. 
When the photon collides with a value, several outcomes can occur. 
These can be categorized as Absorption, Out-Scattering, Emission, and In-Scattering.
Absorption characterizes the radiance of a given field, while emission is a separate radiance field that calculates the radiance added to the field. 

The collision between the ray or photons triggers light propagation and determines how much radiance can be seen throughout a volume~\cite{Chandrasekhar1950}. 
Any form of volume rendering can provide some form of absorption and emission of light. 
Absorption and emission methods are regularly used in real-time volume rendering as they are computationally inexpensive~\cite{Fong2017}. 
Scattering describes the phenomenon where light is deflected in different directions as it interacts with particles within the medium.
Scattering is computationally expensive, requires more computing power, and can be provided in real time. 

Even only utilizing one of these techniques can allow for some form of display using volume rendering, and since they can take drastically different amounts of time. 
This results in two different mechanisms for using DVRs. One of them is real-time, where a user interacts with and manipulates the display in real-time and moves around it. 
This version of \gls{dvr} can be utilized using \gls{mr} devices~\cite{Jung2022, TUKORA2020, Cetinsaya2020}.
However, real-time \gls{dvr} can tend to lack some realistic qualities. If creating realistic images is the goal, it is possible to make them, but it may require more processing time than would be accepted for a real-time application~\cite{Eid2017}. 

\subsection{Cinematic  Rendering}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Chapter4/Images/3D_Cinematic_Rendering_reconstructions_of_the_depressed_frontal_fracture.png}
    \caption[An example of cinematic rendering of a \gls{ct} scan of a patient with a sinus frontalis frontal bone fracture from different angles.]{An example of cinematic rendering of a \gls{ct} scan of a patient with a sinus frontalis frontal bone fracture from different angles.
    The following image by Eid et al.~\cite{Eid2017} is licensed under a creative-commons licence\footnotemark}
    \label{fig:CinimaticRendering}
\end{figure}

\gls{dvr}, which looks realistic, is also known as Cinematic Rendering and is used to create realistic images of a volume.
To achieve an image close to what is presented in \autoref{fig:CinimaticRendering}, some scattering processes must be considered. 
The scattering process will allow other elements to gain the light properties of the nearby objects it has collided with and those that collide with themselves~\cite{Fong2017}. 
This should change the angle at which the ray moves and consider what rays are being redirected. 
However, this will decrease the performance of the algorithm significantly, making it not sufficient for real-time \gls{dvr} and not possible for immersive \gls{mr} \glspl{hmd} yet~\cite{Ropinski2010, Corcoran2012}. Still, it may be possible in the near future to use more efficient algorithms~\cite{VanDamme2021}. 

% \gls{dvr} is another option for rendering a volume.
% Rather than using more traditional approaches, direct volume rendering aims to provide an approximation of your view of direct volume rendering.
% This is very computationally expensive but can produce some very high-quality images which are capable of resulting in images shown in \autoref{fig:CinimaticRendering} if given enough time. % find images to use for this 
% \autoref{fig:CinimaticRendering} is also able to show how direct volume rendering is able to highlight injuries faster and even ones that would either go unnoticed via looking at 2D slices through the visualization~\cite{Asel2018, Elshafei2019}.
% The biggest issue with images like this is that they can take longer to process for such high-quality images, making them not suitable for or \gls{mr} \gls{hmd}~\cite{Eid2017}. 


\subsection{Real Time Direct Volume Rendering}
\footnotetext{\url{https://commons.wikimedia.org/wiki/File:3D\textunderscore Cinematic\textunderscore Rendering\textunderscore reconstructions \newline \textunderscore of\textunderscore the\textunderscore depressed\textunderscore frontal\textunderscore fracture.png}}

Real-time DVR focuses on maximizing efficiency within tight time constraints, requiring iterative simulation of light for every pixel in each frame. 
This approach enables users to interact with data seamlessly, fostering a responsive user experience and facilitating effective communication.
Rendering a single light source using \gls{dvr} can be very computationally expensive~\cite{Ropinski2010}.
This makes images like \autoref{fig:CinimaticRendering}, which utilizes exterior lighting from multiple different sources, difficult to render in real time. 
It would require recalling the rays' trajectory from the light sources to the camera(s) (viewport). However, it does allow for images like \autoref{fig:RealTimeDVR} to be viewed and interacted with in real-time~\cite{Li2010c}.
The volumes seen in \autoref{fig:RealTimeDVR} are possible by skipping processes like scattering and removing exterior lighting from the algorithm~\cite{Li2010c}. 

Denser regions scatter light more because they contain more particles or materials to interact with the light.
For example, This causes areas of a \gls{ct} scans shown in \autoref{fig:CinimaticRendering} to render the skull much more clearly than the rest of the head.
It also prevents the skull from becoming more occlusive as it interacts with the light. 
Skipping the scattering process can be helpful when trying to create a transparent volume as scattering will tend to obscure structures that lay under more solid materials~\cite{Kniss2003, Li2010d}. 
This can be seen in \autoref{fig:CinimaticRendering} where the dense bone structure of the skull is entirely obscuring elements of the image like the brain.

\subsubsection{Optimization Methods}
Some approaches to \gls{dvr} cache the final product to memory. This allows for the fastest possible rendering time. If nothing has changed within a portion of an image that is being rendered, the ray should not need to be cast again, and that pixel can stay the same~\cite{Jabonski2016}.
If the user's viewpoint is relatively stable, rendering the object at a lower resolution and gradually raising it piece by piece can also work. 
However, these techniques struggle to function with immersive \gls{dvr}. 
Research has been done to get around this by pre-computing many of the angles that a user may look at the object and caching and then estimating the difference between the viewing angle and where the user is looking at~\cite{Tomandl2001}.
% I have seen versions of this that also use a low result texture to help mediate the process if it takes too long, but it likely was not published utilizing this feature.

Early ray termination stops the ray before it can no longer change its color~\cite{Matsui2004}. 
While \glspl{gpu} will keep using resources until each parallel thread has stopped if all the threads stop early, allowing for the overall processing time to be reduced. 
The ray can either be terminated once it has passed the volume boundary or if it is inside the volume. Optionally, the ray may also terminate when the pixel becomes wholly opaque and is not able to change its color anymore~\cite {Matsui2004}.
% Check this: Generally since GPUs run a series of pixels near each other, they will likely end at around the same time. 

%
%Precompuation:
%
%Spatial Data Structures
Other options for improving the speed of \gls{dvr} will require some level of preprocessing. 
This will consume resources when the data is loaded but will make interacting with the visualization far more responsive. 
An intuitive method for this is to use spatial data structures like Oct trees or KD trees to accelerate \gls{voxel_g} look-up times~\cite{GPU_Gems2}.
These can allow for processes that can skip sections of the volumes and allow for a higher resolution at the cost of using more of the \gls{gpu}'s memory~\cite{GPU_Gems2}.
% Building these data stores on GPUs is likely to be less efficient

%Adaptive Sampling
By utilizing spatial data structures, it is possible to skip space within the areas of the volume that are not empty but contain a known value to the system. 
This is called Adaptive Sampling~\cite{Heide2013}. 
This value can be found by segmenting the volume into parts that you wish to visualize together.
Which can be processed in real time~\cite{Schenke2005}.
Adaptive sampling can be utilized to provide a similar visualization to regular volume rendering, but it is not capable of producing the same output because there is no way to be able to linearly predict what the outcome would what the color should be for each \gls{voxel_g}~\cite{Morrical2019}.
Recent research by Kraft et al. has shown the effectiveness of calculating light effects like scattering with this effect~\cite{Kraft2020}.

%empty space skipping. 
A common way of utilizing adaptive sampling is to perform it in areas that are not being rendered. 
This is called "empty space skipping"~\cite{Li2003, Morrical2019, Zellmann2019}.
Empty space skipping tells the GPU that it has some amount of unrendered or empty space left before it needs to start rendering~\cite{Li2003, Hadwiger2018, Zellmann2019}.
Empty space within a volume will usually bring no change to the presentation of the given pixel as most \gls{dvr} algorithms usually consider all empty space to have the value of zero~\cite{Morrical2019, Deakin2019}.


\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/SphereMarchingVsRayMarching.pdf}
    \caption[Two different methods of moving along a ray.]{Two different methods of moving along a ray. Right) shows a rational volume rendering approach where the ray moves at a constant rate. Left) illustrates how empty space skipping can be utilized by measuring the distance to the closest object to determine the safe minimum amount it can move without hitting an edge.}
    \label{fig:SphereMarchingVsRayMarching}
\end{figure}

% Expand on the type of empty space skipping that is being used in this thesis
Some volumetric objects can utilize empty space skipping without requiring data structures~\cite{GPU_Gems2}. 
\glspl{sdf} are equations that represent the distance away from a shape from a point in space. 
Empty space skipping using \glspl{sdf} utilizes a technique called Sphere Marching, which looks at all of the \glspl{sdf} in the volume and determines what surface is the closest to it and moves that far forward as illustrated in the left side of \autoref{fig:SphereMarchingVsRayMarching}.
It typically terminates when it approaches the SDF at a certain distance to ensure the ray meets its target. 
This allows for opaque surfaces to be rendered directly. 

All of these methods allow for direct volume rendering, either by sacrificing image quality and accuracy or requiring more memory and preprocessing time. 
For real-time \gls{dvr} to work in this dissertation, it must be tailored to run at a reasonable frame rate without losing any visual quality. 

\section{Display Hardware for Stereoscopic Direct Volume Rendering}
Stereoscopic displays bring new challenges to volume rendering, requiring different methods to create them.
This section details three systems that utilize \gls{dvr} on novel displays that stereoscopic displays bring \gls{ct} and \gls{mri} data instead, with the goal of better understanding how to best visualize \gls{dvr} on these displays. 

%Three experimental displays have been created as part of this research to better understand how to best visualize \gls{dvr} on these displays.

\subsection{Designing Direct Volume Rendering for Volumetric Displays} \label{sec:VolumetricDisplays}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/stl_bones0.png}
    \caption[An image of a set of bones from a CT scan displayed as an iso-surface on a volumetric display (the Voxon).]{An image of a set of bones from a CT scan displayed as an iso-surface on a volumetric display (the Voxon). This image was provided by Voxon Ltd.}
    \label{fig:VoxonImageOfMedicalData}
\end{figure}

Volumetric displays such as the Voxon~\footnote{\url{https://voxon.co/}} generate a true 3D image by moving a screen rapidly up and down.
This creates a display that appears very similar to real-world 3D objects that can be viewed from almost any angle. 
A short project was undertaken to determine if it was possible to display \gls{ct} or \gls{mri} data using \gls{dvr} on a volumetric display. 
This included trying to visualize small \gls{dicom} files on the device. 
This project quickly failed due to limitations caused by this type of display and the required power. 

The lack of occlusion is a challenge with bringing \gls{dvr} to this technology. This is difficult due to the lack of occlusion possible on these displays, causing issues in determining the depth of certain objects~\cite{Geng2013}.
This is compounded by the fact that these \gls{dvr} objects require a dense grid of \glspl{voxel_g}, requiring a good sense of depth to distinguish the difference between different elements.
The relatively low refresh rate caused by all of these components makes this difficult, and this can be more challenging when projecting more than one color. 
Volumetric displays tend to require high-speed projectors, and many of these only produce a small amount of colors and shades due to their low bit rate~\cite{Nakagawa2023}. 
If it were possible to transfer a form of direct volume rendering to this technology, all this would need to be considered.

This type of display, however, could better be utilized by showing an adaptive version of an iso-surface-like matching cubes as can be seen in \autoref{fig:VoxonImageOfMedicalData}.
The design allowed for an interactive display that could show the most possible detail to the end users.
While \gls{dvr} is not well suited to volumetric devices, iso-surfaces are a viable option. 

% The Looking Glass Demo
\subsection{Autostereoscopic Displays} \label{sec:AutoStereoscopicDipslays}
% Talk stereoscopic displays in general
Auto-stereoscopic displays aim to let users see a 3D image without needing any special eyewear.
This makes auto-stereoscopic displays well-suited to shared experiences. 
This method could allow patients and a doctor to talk and view a \gls{dvr} visualization together, allowing for clearer communication between medical practitioners and patients~\cite{Ni2011, Abildgaard2010}. 

The display used for this project was a Looking Glass~\footnote{\url{https://lookingglassfactory.com/}}, which used a Lenticular-Based Display.
Lenticular Autostereoscopic displays present a series of images or views that are sliced and interlaced. 
As the viewer changes their viewing angle, different images are visible to each eye, creating a 3D effect.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/AutoStereoscopicDVRForMedince.png}
    \caption[A series of images showing the looking glass prototype utalizing \gls{dvr}.]{A series of images showing the looking glass prototype utalizing \gls{dvr} (a and b) show pictures of the display looking the same volume from two different angles (a) is from the left and b is from the right (c) shows the prototype's interface (d) shows the same image as seen in sections a and b. (e, f, and g) presents a collection of users using the system simultaneously.}
    \label{fig:AutoStereoscopicDVRForMedince}
\end{figure}

This project found that autostereoscopic displays are a good opportunity. They are similar to traditional displays but take advantage of many different views that humans require. 
Autostereoscopic technologies have a limited depth plane. Anything in the foreground or background of these technologies would be blurred out due to the binocular disparities caused by the mismatch between the two displays. 
This allows for \gls{dvr} to benefit from a more natural feeling of depth perception with very little work, as even if all the user can see is the first pane, the difference between the displays each user can see will still create this stereoscopic effect.

One challenge to note that \gls{dvr} has when being used on an Autostereoscopic display is rendering time.
Displays like the Looking Glass may have up to 100 different viewpoints, each of which needs to be rendered in each frame. 
This can make interacting with the volume directly a challenge.
Early testing of the demonstration and user testing of these systems show that frame rates over 20 \gls{fps} are tolerable or unnoticeable, and frame rates above 5 \gls{fps} can be tolerated. % I do not think anyone has done this research

Since this system functioned similarly to a traditional desktop display, caching was utilized to accelerate the frame rate when the \gls{dvr} visualization was static.
This caching allowed the \gls{dvr} to step through 256 times, even at a much higher-than-normal resolution. 
Early ray termination was utilized once the color could no longer be changed or when the ray left the \gls{aabb}. 
However, the step size was decreased to allow for more steps to allow a higher resolution from the device.

The prototype seen in \autoref{fig:AutoStereoscopicDVRForMedince} was designed as a second step to help us understand how Autostereoscopic displays should be built for surgeons and medical practitioners based on a prior investigation with two surgeons who wanted to learn if a 3D visualization could change the method they prepared for dangerous surgeries. 
The original prototype these surgeons interacted with utilized 3D visualization consisting of a stack of 2D planes whose orientation could be viewed from the Axial, Coronal, or Sagittal planes depending on the angle from which the volume was being viewed.
Most of this system's design functionalities have been informed by surgeons themselves.
Including the need for \gls{dvr} to be utilized along with traditional 2D \gls{ct} and \gls{mri} scans. 
They also wanted to restrict the volume they were looking into and control the system either from the 3D perspective for the patient's well-being or from the 2D perspective, which they had more experience with and gave them more precise control. 

As well as utilizing real-time \gls{dvr} for this project, this system required several more attributes. 
This system was required to be able to read any MRI or CT data and render it as a volume while using the same dataset to power other options.
This prototype could load many different volumes at once and enabled quick switching between them.
It utilizes real-time clipping planes and lets users change some of the properties of the transfer process (Opacity, Color, Gradient). 

In this state, it was demonstrated at Aus Medtech 2024, where it was shared with the Australian medical community.
From here, marketing and patient communication cases were suggested for the following product. 
It was also considered a mechanism for remote communication between medical practitioners and patients to discuss their medical scans.
This would help enable communication between hospitals to collaborate on a given patient with little concern. 
More research is required to determine the steps required for its use in diagnosing patients and surgical assistance.

% You could note that interactions with these technologies could be interesting

\subsection{Stereoscopic Head Mounted Displays} \label{sec:StereoScopicHMDsAndDVR}
% Talk about OST AR displays and the limitations. 
%Stereoscopic \glspl{hmd} tend to have a different set of limitations to the previous set of displays mentioned in \autoref{sec:AutoStereoscopicDipslays} and \autoref{sec:VolumetricDisplays}.
Stereoscopic \glspl{hmd} encompass any \gls{ar} or \gls{vr} \gls{hmd}, which places a different screen in each eye and tracks the user's movement.
Allowing the users to be fully immersed in the virtual environment. 
Similar to the autostereoscopic displays in \autoref{sec:AutoStereoscopicDipslays}, the volumes must be rendered at different levels, but there are due to the screen's position, some other tactics are required due to the different methods of interactions possible.

One difference between producing volume rendering on a \gls{hmd} rather than a screen-based device is that you can walk around the volume. 
They allow for better depth perception since they allow for binocular depth cues and motion parallax-based depth cues, which can be triggered even by utilizing the most minor movements. 
Rendering all these changes to the volume caused by these little movements makes using a \gls{dvr} difficult to visualize, causing most people to contain the research into this space into a cube~\cite{Fischer2020}.
Volume rendering is also usually visualized using a plane, but it requires a 3D object when using a Stereoscopic \glspl{hmd}. 

\autoref{fig: Examples of how volume rendering polygons} Illustrates the importance of utilizing the outer polygon shape to match as closely as possible to the outer shape of the polygon case to aid the system's own calculated binocular distortions~\cite{Heinrich2022, Kersten2006, Kersten-Oertel2014}.
Most volumes use a rectangular prism as their base shape due to its grid-like structure and the computational advantages it can assume.
Cubes also utilize the fewest polygons possible, which slightly improves rendering performance. Vertex operations only need to be called 24 for any view, so any eye sees the box.
This effect may not be evident when viewing the volume from a distance, but when close to the visualization, the volume may appear slightly displaced~\cite{Vishton1995}. 
Using a cube to display a volume generally provides acceptable depth on a stereo headset. Some users might experience distortion due to the absence of depth cues that the surrounding environment typically provides. 
Additionally, placing the volume within a more closely fitting mesh to its base shape can enhance near-field interactions, as the amount of white space constrains interactions with the volume or close examination on a conventional mesh. 
The systems in this thesis employ closer-fitting meshes for Volume Rendering.

\begin{figure}[!tbp]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/AlternativeDVRPerspectives.png}
    \caption[A third person view of Direct Volume Rendering using three different polygonal meshes.]{A third person view of Direct Volume Rendering using three different polygonal meshes. In the green box the outputs from the cameras by looking at each of the different meshes. The straight lines coming from the camera represent the camera frustum.}
    \label{fig: Examples of how volume rendering polygons}
\end{figure}

% % The pros and cons of using cubes for volume rendering
% This effect may not be evident when viewing the volume from a distance, but when close to the visualization, the volume may appear slightly displaced~\cite{Vishton1995}. 
% Conforming the outer shell of the volume has been proposed as a mitigation strategy to address this issue. 
% While using a cube to display a volume generally provides acceptable depth on a stereo headset, some users might experience distortion due to the absence of depth cues that the surrounding environment typically provides. 
% Additionally, placing the volume within a more closely fitting mesh to its base shape can enhance near-field interactions, as the amount of white space constrains interactions with the volume or close examination on a conventional mesh. 
% The systems in this thesis employ closer-fitting meshes for Volume Rendering.

\subsubsection{Challenges Regarding Frame Rate}
Wang et al.~\cite{Wang2023} state that \gls{vr} \glspl{hmd} require a frame rate of 120 fps to avoid simulator sickness and increase the speed of the user interaction.
This is much higher than a traditional display would be expected to run at, making it difficult for \gls{dvr} applications to run on stereoscopic \glspl{hmd}.
%Frame rate is the other issue that needs to be noted.
Having two separate displays that are being shown to users makes the frame rate twice as difficult to manage as it would generally be. The slower frame rate will cause the user to perceive a blurring effect~\cite{Zhao2022}.
Users move quite regularly and require the visualization to be refreshed as much as possible~\cite{Wang2023}.
This is a challenging task on \gls{ost} \gls{ar} \glspl{hmd} as many of the devices are designed to be mobile.
Calculating the position of real-world objects is also a challenge for \gls{ost} \gls{ar} \glspl{hmd}, which requires sensor information to locate real world elements rather than just being able to rely on RGB camera imagery~\cite{Zari2023}.

\subsubsection{Challenges related to Occular See-Through Head Mounted Devices}
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Chapter4/Images/ColorsOnDifferentDevices.png}
    \caption[This figure shows images from a computer monitor, an AR overlaid image, and the HoloLens2's display directly, featuring a box with a wireframe version of \gls{X-ray Vision} that can cast a shadow over the unseen area.]{This figure shows images from a computer monitor, an AR overlaid image, and the HoloLens2's display directly, featuring a box with a wireframe version of \gls{X-ray Vision} that can cast a shadow over the unseen area. A set of red, blue, and green lights is directed at this object, showing almost the full-color gambit.}
    \label{fig:ColorsOnDifferentDevices}
\end{figure}


%\gls{OST} displays have the same limitations as the \glspl{hmd} noted in \autoref{sec:StereoScopicHMDsAndDVR}. 
%These tend to come from their use of color. 
Realistic color representations are challenging on different displays, and \gls{ost} \gls{ar} devices further complicate this issue. 
Unfortunately, each device brings its problems, such as what color gambit they can display, the contrast they bring to the real world, how much of the users' real vision is occluded, and the specific configurations of how the virtual and the real worlds are viewed~\cite{Erickson2020, Lee2020, Ashtiani2023}. 
\autoref{fig:ColorsOnDifferentDevices} shows the gradual color change between a computer display and an OST AR display. The OST AR display amplifies the brightness of the colors than intended, whereas the VST AR overlay struggles to choose between the foreground and the background colors. 
The observed changes come from several hardware limitations and user preferences on the mentioned devices.

\autoref{fig:ColorsOnDifferentDevices} highlights a range of colors that \gls{ost} displays ca not properly display.
These relate to colors that are considered dull (colors that utilize a lot of brown or black) as projecting these colors vibrantly is difficult~\cite{Lee2020, Ashtiani2023}.
This makes these colors appear both dull and translucent~\cite{Erickson2020}.
This means that bright and clean colors are really the only viable solutions for these \gls{ost} devices.



\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/RealTimeVolumeRenderingExamples.png}
    \caption[This image showcases some examples of what is possible for real-time volume-rendered graphics working on an immersive \gls{mr} \gls{hmd} running at 60 fps on a GeForce RTX 2700 \gls{gpu}.]{
    This image showcases some examples of what is possible for real-time volume-rendered graphics working on an immersive \gls{mr} \gls{hmd} running at 60 fps on a GeForce RTX 2700 \gls{gpu}. a) heptane Gas simulation, b) CT scan of a monkey, c) Visible Human Male, d) CT scan of an orange, e) Upwind information from Hurricane Isabel,  f) Visible Human Female.}
    \label{fig:RealTimeDVR}
\end{figure}

\subsection{Implementation Details}

Applications of \gls{dvr} that were utilized in this dissertation utilized this version of \gls{dvr}, which can be viewed in \autoref{fig:RealTimeDVR}.
This version of \gls{dvr} was designed to be very efficient but also allowed for multiple colors to be represented using different ranges of \gls{hu} and \gls{tesla} values.
These colors utilized a look-up table, which held information relevant to the two different color blending techniques.

Before running this system, each \gls{voxel_g} was split into components either outside of the main body (outside), inside of the main body (inside), or inside the main body but touching the outside (surface) by running a volumetric recursive 12-directional \gls{bfs}. 
The distance between all outside and surface boundaries was then calculated and stored within the volume.
Then, the \gls{hu} or the \gls{tesla} values were pre-allocated with a known min and max attributed to the \gls{dicom} value before the system applied these values.
All values in the volume were then normalized and sent to the shader via a 3D texture.
The color values were used to store the \gls{voxel_g} data, where 1 unit stored the \gls{hu} or the \gls{tesla} values, and another unit stored the relative distance to the nearest surface \gls{voxel_g}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/TransperentSDFSphereAndRayMarching.png}
    \caption[A diagram of how ray marching works regarding this implementation of SDFs.]{A diagram of how ray marching works regarding this implementation of SDFs. 
    The dotted blue line circles are the ray logic, moving forward until the nearest part of the ray enters the volume. The green dots (Step Positions) represent each step the volume reads to produce the value for a single pixel. While outside of the SDF, sphere marching was utilized whenever outside of the object to reduce the number of steps required. If the ray is close to the object or inside of it, the algorithm becomes ray marching, where we move forward by a specified amount for each interaction.}
    \label{fig:SDFRayTracingDiagram}
\end{figure}

The relative distance to the nearest surface was calculated by normalizing the distance between two of the furthest corners on the volume boundaries and then normalizing all other distances to the same coordinates.
The shader also needed to perform the same calculation to accurately correct this distortion, allowing it to perform the space-skipping procedure shown in \autoref{fig:VoxelBasedSphereMarching}. 
% Talking about how this was implemented in this thesis
By having the relative distance from any space on the volume, a form of empty space skipping that resembled sphere marching (shown in \autoref{fig:SDFRayTracingDiagram}) was also utilized by calculating the distance from the center of a \gls{voxel_g}(\textbf{c}) to the center of a surface \gls{voxel_g}(\textbf{s}) allowing for large gaps to be quickly traversed~\cite{GPU_Gems2}. 
\(distance(\mathbf{c}, \mathbf{s}) = \sqrt{\sum_{i=1}^{n} (c_i - s_i)^2}\)
This requires preprocessing as all of the distances between the \glspl{voxel_g} that are on the surface (\textbf{s}) of the volume will need to be rendered, resulting in the \glspl{voxel_g} acting as an estimate to the next object, as shown in \autoref{fig:VoxelBasedSphereMarching}. 


The same implementation used for the empty space skipping could have also been utilized when traversing through the volume by grouping areas of similar \glspl{voxel_g} to allow for adaptive sampling.
This could have been done by setting a tolerance value and grouping all values that were of a similar \gls{hu} or \gls{tesla} value range, which could have been skipped, and the color value would have been calculated to be identical as if the system did the process the same.  
This was not utilized as it created visual artifacts that made it distinct from regular \gls{dvr}, potentially causing it to hide information~\cite{Morrical2019, GPU_Gems2}.
Lowering the algorithm's tolerance to react to smaller changes could have increased its accuracy, but due to the high variability of the data, it would have slowed down more than it was initially.

Two options for visualizing the colors: either it applied a flat range to a range of \glspl{hu} to isolate different parts of the volume, or it performed Interpolation between the different ranges to better highlight the different parts of the volume. 
The range base would color space within a range of two \glspl{hu}. This allowed the range-based method to ignore certain values within the volume and ignore areas like the bones.
The other option utilized linear interpolation between points. Each point would correlate to its own color, and \glspl{voxel_g} with that color would be the most colored in their given value. Each other color would change gradually and appear closer to its harboring color as the values grew closer to it. 
The linear interpolation volume transformation sets two capstone values less than the minimum, which are black but fully transparent, and any volume above the maximum, which is transparent and clear. 

Getting \gls{dvr} to run on stereoscopic \gls{ost} \gls{ar} devices requires displaying the visualization on two displays and updating them at least 30 times a second. 
Most \gls{ost} \glspl{hmd} are not able to run even the most efficient forms of \gls{dvr} in real time on these displays, as the ones used for this thesis utilized lower-powered mobile hardware~\cite{Jung2022}. 
It is for this reason that this version of \gls{dvr} is designed to run on desktop hardware weathered to an \gls{ost} \gls{ar} \gls{hmd}.

To lower the processing time, the version of \gls{dvr} scattering and other light effects were removed from the algorithm. 
Early ray termination was activated when the color had reached a point where it was not able to change from more steps, the ray leaves the \gls{aabb}, or if the ray required over 126 steps to complete. 
There was no system to cache the rendering, as even slight movements from the headsets would result in a different view of the volume.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/VoxelBasedSphereMarching.pdf}
    \caption[A 2D example of empty space skipping using a grid-based approach.]{A 2D example of empty space skipping using a grid-based approach. The distance that each moved forward would be equal to the distance to the nearest object. It then moves that much further each time until the distance to the next \gls{voxel_g} is beneath the given threshold, where the ray will then move at the given distance of that threshold.}
    \label{fig:VoxelBasedSphereMarching}
\end{figure}

% Conclude this section
%To get \gls{dvr} working on low-powered \gls{mr} \glspl{hmd}, it is possible its utility would be diminished~\cite{Jung2022}.
The version of \gls{dvr} created for this system is designed to take advantage of techniques that do not decrease the visualization quality in an immersive environment, so elements like space skipping and early ray termination were kept. 
This selection of parameters that controlled the quality of the visualization was tuned to produce an accurate image that can be visualized and modified in real-time on a range of different devices while still providing the best system results. 


\section{X-ray Vision Techniques for Direct Volume Rendering on Ocular See Through Augmented Reality Devices}
% Introduce this section and put forward the concepts that will be carried through
\gls{ost} \gls{ar} allows users to see the real world directly with virtual content overlaid onto it. 
However, in an \gls{ost}, these virtual images can be washed out by light from the real world, so \gls{X-ray Vision} cues are not as strong in an \gls{ost} \gls{ar} device compared to \gls{vst} \gls{ar}~\cite{Martin-Gomez2021}. 
\autoref{Chap:X-ray Implemntion} has shown that four issues need to be considered when creating \gls{X-ray Vision} visualizations for \gls{ost} \gls{ar} \glspl{hmd}:

% Mention again the lessons learned from OST AR Devices
\begin{itemize}
    \item The \gls{X-ray Vision} effect will not be affected by the user's view of the real world but rather by static visualizations. \textit{In \autoref{Chap:X-ray Implemntion} participants found both the camera-based \gls{X-ray Vision} (edge and saliency) effects to be more difficult to use, and they performed worse when using them.}
    \item The \gls{X-ray Vision} effect still needs to occlude the interior objects partially, but not the users' view inside of the object since this allows for additional depth cues like relative size and distance. 
    \textit{The reference objects in \autoref{Chap:X-ray Implemntion} proved to be beneficial to \gls{X-ray Vision} as they help users locate objects nearby in relation to the interior of each object. This is likely to be very important in volumes as surfaces can be difficult to discern. This effect is made more difficult when you consider all the different surfaces that can exist in volume.}
    \item The \gls{X-ray Vision} effect should also not use large objects as it is designed to be used up close to allow near-field interactions. 
    \textit{Participants mentioned in \autoref{app:Chapter3Comments} both Saliency and Random Dot occluded too much of the interior, causing participants to have issues interacting with the visualization when they were up close.}
    \item The effects would need to be reactive to the user and efficient enough to not impair the visualization's quality. 
    \textit{The None condition in \autoref{Chap:X-ray Implemntion} performed well in the subjective results. Participants noted this was due to it not occluding their vision in \autoref{app:Chapter3Comments}. To compensate for this, the \glspl{X-ray Visualization} in this chapter will only be observable within the peripheral vision of the participant.}
\end{itemize}
Considering these, it is also important to focus on what could and could not be done using volumetric rendering.

% Geometrical concerns
%On top of what we already understand about
The volumes know about their internal geometry, but they cannot adjust to the real-world external shell.
If the quality of the visualization is poor, the rays can also step over the object's surface, depending on the angle being viewed.
This will result in the visualizations not utilizing the device's built-in binocular distortion and the rest.

% issues with  transparency 
It is also important to consider transparency's impact on the interplay between \gls{dvr} \glspl{X-ray Visualization}. 
Studies have shown that transparency can be detrimental to the user's depth perception when done poorly, but it can be an effective depth cue when done well~\cite{Ping2020, Ping2020a, Pisanpeeti2017}. 
%To illustrate the effectiveness of the transparent \gls{dvr}, we show how any information configuration can be easily understood using this cue~\cite{Zhan2020}.
Transperent \gls{dvr} allows focus on important data while ensuring the overall information configuration remains easy to understand~\cite{Zhan2020}.
The \glspl{X-ray Visualization} presented in this section all ensure an interplay between transparent and opaque visualizations by making the illustrative effects opaque against a transparent volume.
Using occlusion to represent a clear foreground.

% How noticeable latency is 
Due to the noticeable latency, performing a single rendering pass with no processing stage was only possible~\cite{Guo2022, Matyash2021}. 
This means that a depth buffer could not be calculated in time for these effects, which restricts the visualizations to only understanding what information is related to the current \gls{voxel_g}'s details. 

% GPU concerns
\gls{dvr} is not as well designed for running on a \gls{gpu} as traditional polygonal assets are. 
%We have tried focusing on computational methods.
To enable this, methods that can be calculated in advance and utilize information from the texture memory have been prioritized. The memory is designed to use the RGB values of the texture as normal values when they correspond to edges, while the \gls{hu} or magnetic radiance value is stored as part of the texture information. Distance values are calculated only for the exterior surface, allowing them to be pre-calculated with minimal computational overhead.

% How noticeable
Given all these considerations, illustrative effects would make the most sense as a type of volumetric method because it allows for the appearance of transparency while using occlusive objects~\cite{Lawonn2018}, and it can be applied to volumetric objects~\cite{Pelt2008}.
Medical journals commonly use hatching~\cite{gray1877anatomy} and have commonly been used to translate volumetric data to a 2D interface~\cite{Gerl2006}.
There are also methods of producing these in monoscopic screens. However, if done in real-time, these should be able to provide a similar effect to a \gls{X-ray Visualization}.


% \section{Demo}
% This demonstration introduces a novel platform for Augmented Reality (AR) enabled \gls{X-ray Vision}. AR \gls{X-ray Vision} needs to show the correct depth via visualizing both the surface and the distance inside of the object~\cite{Avery2009}. Firefighters, medical practitioners, and security personnel, among others, have found benefits in using AR-enabled \gls{X-ray Vision} ~\cite{Bajura1992, Phillips2020, Held2019}. However, it is an open research question in the best way to implement AR \gls{X-ray Vision}.

% %This is different from other works, which superimpose content over the physical object creating, making the virtual content appear closer than it would normally~\cite{Blum2012, Avery2009, Bajura1992}. 
% This demo shows how an \gls{X-ray Vision} effect can be created with Direct Volume Rendering (DVR) present. Previous research has used \gls{X-ray Vision} with Video See Through (VST) displays~\cite{Rompapas2014} or used polygonal shells~\cite{Avery2009}, or a Tunneling method~\cite{Avery2009} to illustrate the depth and the physical layout of the virtual internal structures to the user. 
% Our work is novel since this is one of the first demonstrations to use DVR on an OST AR device in the last decade, and it is the only form of \gls{X-ray Vision} that utilizes volumetric data rather than preprocessed polygonal shells.

% Optical See-Through(OST) AR allows users to see the real world directly with virtual content overlaid onto it. However, in an OST, these virtual images can be washed out by light from the real world, so \gls{X-ray Vision} cues are not as strong in an OST AR device compared to VST AR ~\cite{Martin-Gomez2021}. So, our first aim is to show that \gls{X-ray Vision} and illustrative effects can be effective in an OST display. The second aim of this research is to illustrate the role that transparency can play in X-ray visualization. Studies have shown that if transparency is done poorly, it can be detrimental to the user's depth perception, but when done well, it can be an effective depth cue. To illustrate how effective the transparent DVR is, we show how any information configuration can be easily understood using this cue. We present a simple-to-understand volume structure that can be randomly generated to fit within a set of parameters for proof of concept or controlled user studies.

\begin{figure}
    \centering
    \includegraphics[width = \columnwidth]{Chapter4/Images/AmeliorateV2.png}
    \caption[Artistic images of anatomical images displayed as an \gls{X-ray Visualization} by Dr Joshua Luke Ameliorate.]{Artistic images of anatomical images displayed as an \gls{X-ray Visualization} by Dr Joshua Luke Ameliorate. These images are titled: left) Hypnotic; right) Infinite. They can be found at \footnote{\url{https://www.artstation.com/jameliorate}}. Used with permission from Dr. Ameliorate}
    \label{fig:Ameliorate}
\end{figure}

\section{VIRT: Volumetric Illustrative Rendering Techniques} \label{sec:X-ray vision}
% 
Throughout history, art has given depth to 2D surfaces; these works have simulated transparency and manipulated physics.
Inspiring this research to explore visualizations that covered all prior concerns was to utilize illustrative effects.
Throughout art, education, and pop culture, hatching and stippling have been used to discern surface curvature and surface texture while still allowing the viewer to look inside. 
\autoref{fig:Ameliorate} illustrates the added benefit that these techniques allow you to see through the surface of a girl and directly into her anatomy. 

% Talking about the theory of these systems, to begin with
The illustrations seen in \autoref{fig:Ameliorate} present the effect of looking through a solid matter using occlusive and semi-occlusive illustrative techniques by using partial occlusion, just like the visualizations seen in \autoref{Chap:X-ray Implemntion}. 
This shows that illustrative effects utilize the same effects to present the effect of looking through objects as the \glspl{X-ray Visualization} seen in \autoref{Chap:X-ray Implemntion}. 
%Since \glspl{virt} can illustrate the surfaces of objects without utilizing an occlusive barrier similar to the design of the X-ray visualizations seen in \autoref{Chap:X-ray Implemntion}. 
Similar to tessellation and random dots, they remain static in the real world. They can also be designed to allow for a gradient of transparency, occluding less of the area where the user is looking and more of the area where they are not. 
%The more artistic approach to the field also helps to convey more natural shapes rather than flat surfaces. 

Utilizing effects that are traditionally designed to be seen in 2D should also be more suitable for a \gls{dvr} solution like direct volume rendering. 
Given the 3D nature of many \gls{mr} displays, this is not guaranteed, and the experiments in the following chapters will investigate users' preferences for these effects when used as a visualization.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/HandDrawnVirts.png}
    \caption{Examples of hand-drawn illustrations using the illustrative techniques that inspired the various \glspl{virt} as they would be depicted within art.}
    \label{fig:HandDrawnVirts}
\end{figure}


%Explaining this system.
% Throughout the rest of this thesis, three different visualizations to illustrate \gls{X-ray Vision}: (1) a \textit{stippling} visualization that takes inspiration from Ghasemi et al.'s~\cite{Otsuki2017},
% (2) a \textit{cross-hatching} based which took inspiration from the wireframe effect shown in  (3), \textit{halo}. 
% Both of these effects provide a form of partial occlusion and use the geometric properties of the volume, like the surface curvature, the shape's size, and the user's perspective. 
% This gives the end user the perception that the visualization has been connected to the real world.

% These methods would give it 

\subsection{X-ray Vision of Empirical Volumes} \label{sec:XrayVisionCTAndMRI}
We will be showcasing these techniques using visualizations of \gls{mri} and \gls{ct} data. The data is preprocessed to calculate the areas defined using a tetrahedra-defining volume where surface normals would be expected and then calculate for all exterior \glspl{voxel_g} the distance away from the volume they were to allow for sphere marching. 
%A similar implementation to this was previously done by Rocha et al.~\cite{Rocha2011}.
By preprocessing the volume's surface and all of the \glspl{voxel_g} outside of this space, representing the skin, it is possible to determine the ray's relation to the surface. This shows an effect slightly over the top of the parts of the surface that are visible by the final volume. 


% % Talking about cross-hatching work
% The \textit{cross-hatching} visualization uses a grid with infinite depth facing the user. Lines on this grid were drawn along the interests of the grid, with thickness varying based on the curvature of the surface of the volume. The lines become invisible when facing the user so they can easily see into the volume. 

% % Talking about Cross hatching
% The \textit{stippling} visualization also uses a grid; however, this includes a sphere at a random place within its bounds, leading to variation in the size of the depth and a less ordered visualization. These dots transition from opaque to clear in the user's line of sight and the object's curvature to allow the user to view the inside of the object.


% 
% The internal contents of our visualizations are created from a set of randomly generated noisy hierarchical spheres. These are generated in real-time and must ensure that no object may touch another unless it is inside of it completely. These volumes will then be constructed as a small file and sent over a TCP connection to the system that renders the volume within the X-rayable fields.
% This allows users to view many different volumes appearing inside the various physical objects. 

\subsection{Halo} \label{sec: SDf Implmention Halo}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/HaloVis.png}
    \caption[The Halo \gls{virt} applied to the Visible Female data set overlaid over the 3D printed dataset to provide an \gls{X-ray Vision} effect.]{The Halo \gls{virt} applied to the Visible Female data set overlaid over the 3D printed dataset to provide an \gls{X-ray Vision} effect. This image was taken using the HoloLens2.}
    \label{fig:HaloX-rayVision}
\end{figure}

%Note other papers that have used this effect and areas where this implementation deviates from theirs. 
Halos's outlines and feature lines have been used to improve depth perception ~\cite{Bruckner2007, Zheng2013} and highlight areas of interest within volumes~\cite{Diaz2008, Piringer2004, Marriott}. 
On its own, a halo does not provide much depth perception, but when it is paired with a colored display, then it can distinguish what is in front and behind clearer than transparent objects can~\cite{Piringer2004}.
\textit{Halos} have also been shown to illustrate where the surface of an object is by highlighting the areas of high curvature~\cite{Bruckner2007}.
\textit{Halos} are also useful to give the viewer a clear indication of borders, allowing them to see what is in front or behind a given object.
Allowing \textit{Halos} to highlight the objects in the scene~\cite{Bruckner2007, Marriott, Baumeister2015, Joshi2008}.
\textit{Halo} can also use these clear indications of borders to provide a clearer indication of relative size and density, which was found to be a very powerful depth cue in \autoref{Chap:X-ray Implemntion}.

\subsubsection{Implementation}
%Implementation changes for this effect
Traditionally, the \textit{halo} effect has been utilized using two graphics passes~\cite{Bruckner2007}. 
The first is to build a volumetric depth map, and the second is to render the volume where needed. 
On an OST AR display, this is problematic as minor delays in latency are noticeable, so rather than rendering two passes of the volume, this dissertation presents a system that could draw the \textit{halos} without a depth map.

To help prevent this latency, an alternate shader was utilized that recorded two colors for the ray, one that would have the previous \textit{halo} the ray intersected with and one that did not.
This allowed the system to only show the final halo that was passed through. 
A halo would only be drawn when these rays got close to the end and hit the surface at almost a 90\textdegree angle.
This would be saved to the final output color, but if it were to enter the same object again or become a halo for another object, it would revert to the non-halo version of the output color.
For the purpose of visibility, all the \textit{halo's} 
However, it does have one caveat: it is possible for it to show outlines on the outer areas of the foreground with a very high level of curvature, presenting more information about the shape of the object but occluding the volume very slightly. 


%To help prevent this latency, a shader that recorded a \textit{halo} and one that did not was used.
If the ray were to enter the same object it was nearby, the \textit{halo} generated would be discarded; if the ray entered another shape or came into the range of another \textit{halo}, it would be saved. 
\textit{Halos} would be visualized when the dot product of the normal and the ray direction was between -0.1 and 0.1, about 90 degrees away from the user's line of sight. 
This did result in a bug/feature that can be seen in \autoref{fig:HaloX-rayVision}'s \textit{Halo} image where the curvature of the outer surfaces may show outlines in the foreground around areas where there was a high curvature on the surface causing the \textit{Halo's} color to be displayed on sharper contours of the volume facing the user.

%Parameters used specifically for this effect.
A \textit{Halo} would be produced when the dot product of the normal and the ray direction was between -0.1 and 0.1, about 90 degrees away from the user's line of sight. 
A white \textit{Halo} would be drawn if the direct distance to the SDF was less than the given \textit{Halo} tolerance.
All these variables were chosen to limit the number of floating point artifacts that could appear while keeping the size of the lines as uniform with the rest of the visualizations as possible. 

\subsection{Stippling} \label{sec: SDf Implmention Stippling}
%Note other papers that have used this effect and areas where this implementation deviates from theirs.
\textit{Stippling} can be utilized to show the features within an image~\cite{Kim2008} and depth by using several small dots and is capable of giving the artist a higher level of control over showing the surface range than depth perception.
\autoref{fig:HandDrawnVirts} shows how \textit{Stippling} can show curvature, surface depth, and different textures by changing the size, frequency, and pattern of the stipples~\cite{Preim2005}.
Using this effect to aid in the perception of 3D data is not common~\cite{Lawonn2018}, but it was chosen due to its similar depth perception properties as Otoski et al.'s~\cite{Otsuki2017} Random Dot \gls{X-ray Vision} effect~\cite{Lum2002, Salah2006}.

Stippling has not been as extensively tested for depth perception as any of the other \glspl{virt}~\cite{Lawonn2018}. It is more often used to demonstrate rougher surfaces to detail intricate details and decrease distortions when viewing iso-surfaces~\cite{Lawonn2018, Maciejewski2008}.
\textit{Stippling} allows the user to be able to see a series of textures that are laid over each other due to the high range of control it gives artists but also gives them the ability to show the roughness of the surfaces it is displayed upon~\cite{Lawonn2018, Maciejewski2008, Lum2002}.

% \textit{Stippling} allows for a transparent effect on the 3D object, creating a clear and simple effect with transparent objects~\cite{Yuan2005}. 
% \textit{Stippling} can also utilize partial occlusion and movement to indicate basic depth cues, allowing viewers to easily determine where objects are in relation to each other~\cite{Kim2008}.

\textit{Stippling} is used in graphics to showcase transparency~\cite{Pastor2004}, it is more frequently used to show details in textures much more precise than can be seen by shading alone~\cite{Martin2017}. 
In these cases, it would normally be referred to as dithering as it is used in a much less random manner~\cite{Ulichney1988}.
The particle occlusion formed by \textit{stippling} then clearly indicates what object is in front of another surface, avoiding screen door effects, unlike what the effect of rendering multiple transparent polygons may have on each other. 
This makes stippling a logical choice for an \gls{X-ray Visualization}.
%However, when given the wrong set of patterns or applications, stippling may be misleading as a depth cue, leading participants to mistake the location of a given object.


\subsubsection{Implementation}
This design of this effect took inspiration from two works by Lu et al.~\cite{Lu20, Lu2002} focused on a \gls{gpu} version of stippling and Ma et al.~\cite{Ma2018}, who developed a pre-calculated version of stippling that could be used for dynamic environments.
We also took inspiration from the work by Kim et al.~\cite{Kim2008}, who utilized different-sized dots to portray curvature and importance within a 2D image.
%Taking some light cues from works that focused on 2D implementations of this work to help provide some guidance for appropriate methods to adjust this method to work appropriately in a real-time immersive environment. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/StipplingOnCTData.png}
    \caption[The \textit{Stippling} \gls{virt} applied to the Visible Female data set overlaid over the 3D printed dataset to provide an \gls{X-ray Vision} effect.]{The \textit{Stippling} \gls{virt} applied to the Visible Female data set overlaid over the 3D printed dataset to provide an \gls{X-ray Vision} effect. This image was taken using the HoloLens2.}
    \label{fig:StipplingX-ray}
\end{figure}

%Implementation changes for this effect.
%Parameters used specifically for this effect.
The stipple effect was applied over a grid using a converged distribution (or blue noise)~\cite{Lloyd1982} to give the dots an evenly placed tone that was not too regular~\cite{Martin2017}. 
This was chosen over a feature-guided method as it would have required multiple rendering passes or generating a 3D texture over the volume\cite{Kim2008}. 
The blue noise algorithm based on Heitz's and Neyret's\cite{Heitz2018} was used to determine where a dot would be rendered over the shape's surface because it was efficient and well suited for \gls{gpu} architecture and was more visually uniform than traditional white noise~\cite{Ulichney1988}.
The final \textit{stippling} design can be seen in \autoref{fig:StipplingX-ray}.

Two major factors impacted the design: floating point errors and data legibility. Made it impossible to render as many dots as those created by Lu et al.~\cite{Lu20, Lu2002} and Ma et al.~\cite{Ma2018} due to a large number of floating point errors that seemed to be caused by rapid movements and stereopsis. 
To overcome this drawback, a graphics card with single or double-precision floating-point calculations would be required rather than the half-precision that could be utilized. 
As a result, the approach of "less is more" was adopted when rendering the dots while still keeping to guidelines proposed by Lu et al.~\cite{Lu20, Lu2002}, like having more dots when on parts of the volume's surface with high curvature. 
%Based on feedback from the initial pilot studies of the later studies in this thesis, the size of each dot was based on the radius and type of the object it was associated with. 
%Smaller objects had fewer larger dots, which were more densely clustered together, while the other outer regions utilized a sparse clustering of smaller of dots.
All the dots were housed in a grid whose sizes range between $96^3$ and $48^3$ units~\footnote{Real world sizes could vary depending on the volumes but were approximately $30^3$cm in this diseration}, ensuring that the outside of the grid was clear and easy to view while the inside was opaque with little to no floating point errors. 

%Using larger dots would have prevented users from distinguishing different objects within the volume. We found that various objects would seem to merge into one when we used larger dots, leading to worse-than-expected results from our pilot participants.

% The alternate version of this effect created for immersive headsets required us to place all the dots into a series of 3D grids that would warp their sizes depending on the curvature of the surface they were placed on and the size of the noisy sphere. 
% These sizes range between $96^3$ and $48^3$, ensuring that the outside of the grid was clear and easy to view while the inside was opaque.
% We would then place the dots in a random location within the grid close enough to the center to not hit the sides of the cell.


%We increased the size of our dots by 50\% for each layer, creating bigger dots that take up more space in a larger grid. We started with the smallest, densest grid possible that we could make, which was $96^3$. This allowed users to look through and have a clear indication regarding the outside volume.

% To differentiate the different layers of each type of object, we displayed the container regions showing the dots within a $64^3$ grid, and the regions that needed to be counted were displayed in a $48^3$ grid ~\cite{Lu20}. This ensured that the outside of the grid was clear and easy to view while the inside was opaque.

All of the dots were placed randomly within the grid.
to ensure that the dots did not overlap with other dots
To create the dots, first, a center point was picked where the outer edges of the dot would not hit the edge of the cell.
The dots were scaled based on their opacity, as the ones closest to the user would be made transparent, while the ones that were rendered further away or not facing the user were displayed as more open. 
%The further away the dots were from the user of the ray would determine how opaque they would appear to the user, but would not impact their size as this would cause the dots to appear and disappear as the user moved. 
This technique was also utilized in the next section (\autoref{sec: SDf Implmention Hatching}).% for the \textit{hatching} implementation as it better illustrated the shape of the noisy sphere objects. 
When the surface curvature increases, the amount of stipples that appear will decrease, similar to works by Lu et al.~\cite{Lu20, Lu2002}. 
This allows the dots to better represent shade and intensity over the volume\cite{Martin2017}. 

\subsection{Hatching} \label{sec: SDf Implmention Hatching}
%Note other papers that have used this effect and areas where this implementation deviates from theirs
Computer-generated hatching was originally formed by trying to make volumes or images look like they have an artistically drawn look to them\cite{Praun2001, Salvetti2020}.
In an artistic context, \textit{hatching} is considered a good method of showcasing transparency~\cite{Yuan2005}.
Different objects will utilize different patterns and densities of the hatching~\cite{Pelt2008}.
This allows for a large amount of flexibility and allows for more complex surfaces to be hatched.

\autoref{fig:HandDrawnVirts} shows how \textit{Hatching} is similar in utility to stippling; it can show differences between curvature position and lighting~\cite{Preim2005, Philbrick2019}.
It can be used to great effect to showcase curvature within using 2D mediums like books~\cite{gray1877anatomy} and desktop applications~\cite{Preim2005, Philbrick2019, Praun2001}, but there are very few examples of hatching being utilized in \gls{ar}. 
Current examples only show it being used as a medium to help create a painterly atmosphere within mobile \gls{ar}~\cite{Chen2011, Chen2012}.

In an artistic context, \textit{hatching} is considered a suitable method of showcasing transparency, which makes it well suited as an \gls{X-ray Vision} effect~\cite{Yuan2005}.
This is achieved by having the hatching of different objects in the hierarchy display a different width, line angle, and density of the hatching~\cite{Pelt2008, Frech1960}.
%This allows for a large amount of flexibility and allows for more complex surfaces to be hatched.

\subsubsection{Implementation}
Rather than try to make an artistically pleasing hatching algorithm, the implementation of \textit{hatching} in this dissertation aims to create a version of \textit{hatching} that is both simple to implement and understand and functional within MR environments for volumetric datasets.
%Implementation changes for this effect.
This method took advantage of \gls{dvr} and rendered a similar implementation of \textit{hatching} to work by Interrante et al.\cite{Interrante1997}. 
To do this, a grid of rectangular prims as SDF and rendered the area that would not be where the visualization could be presented, each with a deformed cube in the center to apply the hatching effect, creating the line effect seen in \autoref{fig:HatchingOnCTData}.
%to give the cubes a pointed edge; they were warped to grow larger when they collided with a surface with higher curvature. 

The \textit{hatching} effect was generated by creating a grid where every cell was 1.2\% of the size of the volume and had a box at the center of that which was 2.4\% of the volume. 
%These values were then times by the $1 + (\mbox{radius of the nearest object } \times 2)$ leading the larger object to have a cell size closer to 2.4\%.
%This resulted in values
This visualization will be shaded lighter when facing the user's direction. 
This is done by altering the transparency by the angle between the surface visualization and the user.
This was done by tracking the dot product, which was more transparent when the dot product grew lower.
This caused the \textit{hatching} effect to disappear when it was not at the point it was closest to the user.
Both of these parameters decrease when representing smaller objects than the object it was representing, causing the hatching effect to appear as a denser grid. 
After the dot product was greater than $0.25$ of the dot product, the hatching effect would incur a tapering effect.
%If the above was triggered when the ray entered a given tolerance where the dot product of the surface was normal and the participant's viewpoint was less than zero.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/HatchingOnCTData.png}
    \caption[The \textit{Hatching} \gls{virt} applied to the Visible Female data set overlaid over the 3D printed dataset to provide an \gls{X-ray Vision} effect.]{The \textit{Hatching} \gls{virt} applied to the Visible Female data set overlaid over the 3D printed dataset to provide an \gls{X-ray Vision} effect. This image was taken using the HoloLens2.}
    \label{fig:HatchingOnCTData}
\end{figure}

How to orientate the hatching quickly became an issue. 
Generally, hatching is designed to create a static image, so the lighting and shading would have been seen in a static place, with the light source pre-calculated before each image. 
%This would have made the volumes have a gradient of the effect, making parts of them almost impossible to see through and other areas of the hatching invisible. 
Instead, this version of \textit{hatching} treated the user's viewpoint as if it were a light source and made the effect from their point of view.
This made the visualization react to the users' behaviours and allowed them to have their view partially obscured while allowing them to look inside the volume.

%Parameters used specifically for this effect
Several aspects were considered to maintain the aesthetic elements required for \textit{hatching} when transitioning from a 2D to a 3D volume.
The \textit{hatching} effect utilized an SDF of a deformable square grid with infinite depth on the z-axis.
This grid was deformed based on the relationship between volume and the viewer and to have a larger, more opaque line, the further away from the user the normal was facing from the viewer. 
%Smaller objects in the volume would have a denser hash than the larger ones from the larger ones.
%Making it possible to perform the same algorithm to perform the previously mentioned prior in \autoref{sec:X-ray vision}.
%This is treated in the same way as the other volumes, which use a similar technique to showcase the X-ray vision effect. % Fix this Tom

\section{Conclusion}
To create \glspl{X-ray Visualization} for \gls{ost} \gls{ar}, an efficient method for rendering volumes directly to the screen using \gls{dvr} was developed. 
This method was successfully demonstrated on autostereoscopic displays and provided additional rendering time to generate three sketch-based \glspl{X-ray Vision} effects, collectively referred to as \gls{virt}. 
By leveraging sketch-based visualizations, observations were made that allow for the configuration of these conditions for \gls{ost} \gls{ar} devices.
Moving forward, this dissertation will further explore the utility of these \glspl{X-ray Visualization}.

The contributions of this chapter were:
\begin{itemize}
    \item An algorithm displaying how to run \gls{dvr} on a stereoscopic display with minimal distortions and low graphics requirements.
    \item A system designed using a stereoscopic display was to render \glspl{dicom} using \gls{dvr} while providing a reactive user interface, which allowed users to modify the volumes as they required.   
    \item Three new \glspl{X-ray Visualization}, the \glspl{virt}, have been created to allow for \gls{X-ray Vision} on volumes rendered in \gls{ost} \gls{ar} devices which adapt to user behaviours rather than using solely occlusion.
\end{itemize}
%They have designed them with medical needs in mind that can be applied to an extensive range of volumetric and spatial data.



%
%
%
%
%
\chapter{Random Volume Generator: Generating Irregular Hierarchical Objects for Controlled User Studies} \label{sec:VolumetricDataGeneration}

Finding volumetric data that can be utilized reliably for quantitative experiments is challenging.
Methods to obtain volume data are limited. Two methods of creating volume data can occur by using natural means either:
Patient data or artificial data. 
Patient data is generally collected from a spread of patients with similar symptoms\cite{Cocosco1997BrainWebOI} or collected data from relatively healthy patients~\cite{Ackerman1998}. 
The other method of producing volumetric data is to collect artificial data is collected by utilizing geometrical data to represent various natural phenomenon~\cite{neghip,dns,jicf_q}.
These tend to take the form of calculating known an empirically assessed theoretical physical concept, which in turn can be computationally expensive to produce and to manipulate. 


Controlled studies are challenging to perform with data that is limited in these settings because they rely on having hundreds to thousands sets of similar data based on a given condition.
The data needed to be plentiful enough that conditions could be replicated with different data sets over various conditions and conform to the properties of \gls{mri} and \gls{ct} scans while remaining intuitive enough for untrained users to be able to interoperate it.
This section details the methods focused on creating extensible and flexible volumetric data that participants can understand intuitively, regardless of their prior training. 

% Talk about the different types of data that can be read into the systems
Volumetric data is commonly used to interpret the structure of an object, like the human body, underground structures, or weather data.
Typically, the information utilized when requiring direct volume rendering is sourced from the real world using either \gls{ct}, \gls{mri}, or some form of radar technology.
These methods of obtaining volumetric data can be limited and challenging to produce in large quantities. Access to the machines and equipment capable of producing this data is limited, and the skills required make it difficult to automate. 
Most medical data needs to be anonymized, and the patient authorizes the use of it so it can be used for research.
Furthermore, very little volumetric data is explicitly designed for more controlled or quantized studies. 

The Open Scientific Visualization Database~\footnote{\url{https://klacansky.com/open-scivis-datasets}} has a wide selection of datasets available for science, but they are very different and can result in unexpected differences between the different visualizations when evaluated using a controlled study~\cite{Grosset2013}. 
Current solutions for generating artificial data require precise inputs, are challenging to manage, and are computationally expensive~\cite{Bossek2018, Haghighi2017, Meyer_and_Nagler_and_Hogan_2021, Patki2016}.
This section details a method of making simple-to-understand simulated volumetric data, a method to validate that the data is comprehendible, and a modular system allowing for flexible options to evaluate volumetric structures. 

Real-world volumetric data is challenging to share amongst other researchers due to its large file size and privacy concerns~\cite{Fabian2015}.
Medical data privacy is paramount, and there may be restrictions on what datasets can be distributed through research~\cite{Gillmann2021, Nature2023}.  
The size of the compressed data can easily be larger than one gigabyte for a given volume. 
Making sharing datasets difficult~\cite{Fabian2015}. 
%On top of this, high-resolution volumetric datasets file sizes can be greater than one GB in size for each image. 
This size becomes an issue for data sharing in the long term since storing large data sets will likely cost either the publisher or the researchers~\cite{Nature2023}, leading to many problems with long-term data availability. 

Current alternatives for creating controlled studies using volumetric rendering include adding extra information to the limited volumes via adding artificial data. 
These settings do well in recreating scenarios relating to the introduction of medical instruments to a volume and determining the reliability of these visualizations, but the interactions between other objects become more complex.
Another option can be seen by viewing research that utilized synthetically created data~\cite{Englund2016, Englund2018, Kersten2006}.

%that generates a series of random planes of data to view via a 2D display.
%Bifocal displays utilizing volumes within a cube can present noticeable optical artifacts, causing some users to have imprecise depth perception when using immersive displays. 

Synthetically created data has a couple of advantages. First, it allows you to modify and control the data you are using in a given study, allowing for more quantitative data to answer more specific questions. 
Allowing for more replicable and focused studies to be possible. 
Englund et al.~\cite{Englund2016, Englund2018} experiments generated data designed especially for the purposes of testing their research and rendered as static images of volumes for a range of two alternative forced-choice tasks. 
Kersten et al.~\cite{Kersten2006} utilized a fog-like volume made from Perlin noise within a cylindrical object and rotated it to see if users could tell the direction in which it was rotating.  

Creating synthetic data is a common tactic in many other fields as data of any type are hard to come by, and systems need to be tested using flexible mechanics and a wide range of parameters~\cite{Meyer_and_Nagler_and_Hogan_2021}.
These can range from systems designed to create structured databases~\cite{Meyer_and_Nagler_and_Hogan_2021}, networking data~\cite{Bossek2018, Haghighi2017}, or even modifying the original data for testing or publishing purposes~\cite{Patki2016}.

%\textit{find papers that have tried to solve areas in this area\\}
%Deep learning has also been heavily utilized to create various artificial volumes
There have been several attempts at using the tools \glspl{gan}~\cite{Goodfellow2020} and stable diffusion~\cite{Dhariwal2021} to create real examples of medical data~\cite{Pinaya2022, Ren2021, Togo2019, Nguyen2023}. 
These datasets provide use cases for training staff without the need to provide scans of real people, which can benefit tasks such as publishing and sharing medical data with a broader audience~\cite{Togo2019}, training radiologists~\cite{Nguyen2023}, and research~\cite{Nguyen2023, Ren2021}. 
These models utilize a large collection of existing data and aim to create a new data set, which tends to be indistinguishable from the real data~\cite{Togo2019, Ren2021}.
However, controlling the specific qualities in a manner that would be required for a study is not practical yet, and adjustments to this data for quantitative evaluations are not possible yet.

To solve these issues, the Random Volume Generator was created.
Taking into consideration the above parameters, I built a system to generate volumetric data that can be generated for studies in a unique but controllable mannor. 
This software is designed to act as a tool that can be used to generate a unique volume for each interaction of your study that can be customised to fit any type of research that you need to run. 
By utilizing \glspl{sdf} set in a hierarchy of customizable options.
Keeping the system modular enabled it to swap parts in and out, making it work for any study looking to evaluate volumetric graphics.

\section{Noisy Hierarchical Spheres} \label{sec:SystemOverview}
% \begin{figure}
%     \centering
%     \includegraphics{Chapter4/Images/ImageForResearch.png}
%     \caption{A large amount of the random objects that can be produced from this image are placed in shown all rendering together.}
%     \label{fig:enter-label}
% \end{figure}

% Explaining how the Perlin noise could work
To make these volumes as much like MRI and CT scans as possible, Perlin noise~\cite {Perlin1989} was used to deform a sphere object created via a spherical \gls{sdf} (A noisy sphere) set up in a hierarchical fashion.
Perlin noise~\cite{Perlin1989} is a close match for the noise found in medical data, and it has been utilized to simulate and create artifacts in read medical data for research purposes~\cite{Tomic2023, Lawson2024}.
This results in volumes that appear like the ones in \autoref{fig:noisySpheres}. 
Since they are \gls{sdf}, it is possible to accurately determine the angle of normals in the volume, allowing visualization-based experiments like \gls{X-ray Vision} studies (as seen in \autoref{fig:3DPrintedVersionOfTheSystem}).


We also considered the previous studies that have generated artificial volumes.
Englund et al.'s~\cite{Englund2016, Englund2018} 2D static images would not be translated well to a stereoscopic display as they can not respond to motion parallax.
Kersten et al.'s~\cite{Kersten2006} Perlin noise fog showed promise when using stereoscopic display. 
Perlin noise was adopted to generate synthetic volumes, mimicking the noisy characteristics of volume-rendered objects in applications such as MRI and CT scans.
This approach is advantageous since it can be rendered effectively on a variety of surfaces in real time.
%to some degree was instead chosen as it can be rendered in real-time, and they show that it is not limited to being rendered on planes or cubes.
Volume data tends to be inherently noisy and messy, making it more challenging to interpret, so Perlin noise-based objects make logical sense~\cite{Gillmann2021, Zehner2021}.
The ability to create a solid surface that could represent different materials or densities would be beneficial when trying to create iso-surfaces, and the ability to have identifiable pieces components is common in MRI and CT scans.

%And share qualities that require complex data analytical skills to interpret and tend to require researchers with a multidiscipline skill set. 
\section{Random Volume Generator's System Design}
%NEED INTRO HERE
The Random Volume Generator took on a complex but modular design system in \autoref{app:RandomGerneratingVolumesClassDiagram}. 
Which is designed to produce an array of volumes that can conform to a set of conditions.
This results in the output, which can be seen in \autoref{fig:noisySpheres}, whose shape is a "noisy sphere."~\cite{Perlin1989}.
% Things that happen before creating each volume

Noisy spheres, which are based on a sphere with random distortions, allow for an almost infinite variety of shapes. 
As such, each shape is irregular in nature, and a pseudo-random generation process determines the specific shape and appearance.
All while forcing the dataset to conform to a preconfigured set of requirements. 


% What does this system bring that the other systems didn't
%When designing these volumes, we needed to consider methods like Englund et al.'s~\cite{Englund2016, Englund2018} static images that do not work well on an immersive stereoscopic device. 
The Random Volume Generator is primarily aimed at researchers conducting human-computer interaction research investigating the traits and impacts of different displays and rendering styles~\cite{Rheingans2001, Svakhine2009}.
The volumes generated by the Random Volume Generator need to have generic qualities that other forms of volumes would typically have. 
This might include rounded edges, lumps and bumps around the edges, and a hierarchical set of objects.
We chose to base these volumes on a similar design to MRI and CT scans, but they also share qualities with electron microscopy visualizations~\cite{Nguyen2022}
and with minor modifications, they can be conformed to molecular systems~\cite{Goodsell1989}, meteorological data~\cite{HibbardL.1986, Lin2021}, as well as ground penetrating radar (GPR) data~\cite{Zehner2021}.
All these imaging techniques communicate information via geometrical methods, require a high degree of precision to generate, and are subjective to noise or inaccurate data.

% Make a new Image for this
\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/RandomlyGeneratedVolumes.png}
    \caption[The types of volumes the Random Volume Generation System system can produce.]{The types of volumes the Random Volume Generation System system can produce. There are noisy spheres on the bottom and right-hand sides using various illustrative effects (Outlines, Stippling, and Hatching).}
    \label{fig:noisySpheres}
\end{figure}


% the basic algorithm of the system
In each iteration of the generation process (shown in \autoref{fig:ActivityDiagramForGeneratingRandomSDF}), a top-level volume is created and added to the scene. Children within the volume are created recursively. The size and appearance of each volume are determined based on the probability distributions given as input to the generation process, whereby the size of the child volumes is constrained by their parent volume (if any). The number of children in the volume is drawn from a probability distribution for each volume.
In addition, global constraints govern the overall number of objects of each type at each level to ensure that the synthesized scene satisfies the desired properties.

The dimensions and placement of objects are determined at random such that each volume is wholly contained in its enclosing parent volume, and no objects are allowed to touch each other. These placement constraints are verified by a \gls{voxel_g}-based algorithm that examines possible intersections between volumes. The system provides a naive algorithm and a more efficient octree-based implementation. In case an object is found to violate the constraints, the object is moved to repair the situation.

\subsection{System Design}
%This system was designed to be modular.
The Random Volume Generator was designed to be modular by utilizing many system design patterns, interfaces to allow for interchangeable classes, and the use of functors to allow for customizable behaviours.
The system can be split into different parts, enabling adaptation to different requirements.
The different parts of the Random Volume Generator are classified as the Generator builder (used to construct the final structure of the Generation system), the Random Volume Builder and Validation (used to create and validate the success of the valid creation of a valid volume), and the system Outputs (used to tailor the outputs to the various studies that these volumes could be used for).
%All of these systems could then be further modified and changed to allow for more minor changes. 
Class diagrams showcasing the Random Volume Generator's structure can be found in the \Cref{app:RandomGerneratingVolumesClassDiagram}.

Four different types of noisy spheres can be made for the base version: outer, composite, leaf and some multipurpose spheres can also be added. 
%This system uses a series of visualizations that utilize a set of spheres that are deformed by Perlin noise~\cite{Perlin1989} and displays different hierarchies as different objects using different colors. 
To improve optical focus, using the method described in \autoref{Chap:VolumetricX-rayVision} where volumes were housed in meshes that represented the exterior of their shape to some degree, these objects are housed within a spherical mesh.
%\autoref{fig:DifferentVolumeRenderingViews} shows this allows for better bifocal viewing by allowing the mesh to align with the volume. 
Improving the distortion from the bi-optical, allowing for better depth perception~\cite{Kersten2006}.

%
%It needs to perform, and 2 to provide the correct verification for the stage of the volume.
The Unity game engine~\footnote{\url{https://unity.com/releases/2022-lts}} limits the amount of threads available to the Random Volume Generator is required to work within a single thread framework and utilizes active interfaces in tandem with a stateful logic system. 
Systems that handle the system's logic are functors, allowing a system to progress statefully.
Their behaviour is again modular, with the system's logic separated from their unique tasks. 
Allowing for a system capable of doing much more than its base functionality. 
Two simple examples of the methods are recursively placing an SDF within its parents so it takes as much space as possible.
The other can produce several meshes for 3D printing(seen in \autoref{fig:MeshVersionOfTheSystems}) while also providing their corresponding noise key, allowing for comparisons with real-world objects.

\subsection{Voxel-Based Verification of Volume Requirements}

The Random Volume Generator utilized a modular verification system to ensure the volume elements remained distinct to prevent any ambiguity between the objects.
% A brief overview of the volume rendering system
By looking at each \gls{voxel_g} and determining if they are sitting wholly inside each other and not slipping outside their parent object or touching any other object that is not a parent of theirs. 
%These rules can be easily exchanged for any class that inherits the Irules interface. 
The rules are checked via a C\# based system that emulates the same properties of Unity's high-level shader language (HLSL). 
It is designed to be extensible to allow for any SDF and potentially a different type of shader altogether. 

% Explaining how the root objects work
The verification methods for the root (outer) objects check that the volume fits within the mesh within which it is rendering while remaining as large as possible.
This mesh can either be a cube mesh or a spherical mesh.
If any \gls{sdf} is outside of the mesh, the object is discarded, and a completely new object is generated. 
The sphere mesh utilized a Fibonacci sphere algorithm to determine if the large outer sphere was within bounds~\cite{Gonzalez2010} whereas others, like the cube meshes, tested if the mesh was inside of \gls{aabb}.
In both conditions, if any point from any of these checks were found to have any SDFs within a range of them, it would then shrink the volume and perform the check again. 

% The different types of verification
The verification used for the leaf and composite objects can use two different verification methods. 
Either a brute force or progressive oct-tree searching method.
This can either happen linearly or in parallel for each \gls{voxel_g}.
The other method utilizes a partial linear oct-tree starting from one predefined depth in the oct-tree from one depth to another deeper one.
Studies in this thesis utilized an oct tree starting with one layer beneath its root $1/8^3$ to a leaf node that was of a \gls{voxel_g} with the comparable size of $1/512^3$.
This will perform a trimmed depth-first search. 
This search will determine if a group of \glspl{voxel_g} conform to SDF coordinates to ensure that all child nodes are completely within the parent's bounds. 
These brute force and progressive oct-tree can be combined for both high perception and speed.

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/UML Activity Diagrams (Community).pdf}
    \caption{An activity diagram showing the transition between the various states of the Random Volume Generation System}
    \label{fig:ActivityDiagramForGeneratingRandomSDF}
\end{figure}

\subsubsection{Random Volume Generation System Outputs}
\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/DiagramOfTheMeshes.png}
    \caption[A breakdown of the structures of the 3D meshes generated by the Random Volume Generator.]{A breakdown of the structures of the 3D meshes generated by the Random Volume Generator. a) presents the volume using transparent colors to show the different levels that meshes can be segmented, b) is the same volume but covered in a wireframe, c) is a close-up shot of the center b.}
    \label{fig:MeshVersionOfTheSystems}
\end{figure}

% 
The output from these files can provide mesh file outputs (.obj and .stl) or show DVR content in a JSON format.
The meshes are created via marching, running the iso-surface algorithm marching cubes~\cite{Lorensen} over the volume and can be created at any resolution required. 
The volume can be saved as a single mesh or a collection of smaller meshes (as shown in \autoref{fig:MeshVersionOfTheSystems}) spaced appropriately apart
allowing game engines and most displays to render them natively.
It can also create a 3D printable model (as seen in \autoref{fig:3DPrintedVersionOfTheSystem}).

The JSON file is designed to be read as input for a user study. 
It contains instructions on what condition the visualization was built for, the noise key, and any answers required for each volume, like volumetric information and the number of volumes contained under a certain circumstance. 
%This modular system can be redesigned for many DVR studies using Augmented Reality (AR) or Virtual Reality (VR) devices. 

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/3DPrintedObjectWithX-rayVision_ThesisVersion.png}
    \caption{(a) A 3D printed version of the model made from the Random Volume Generator's mesh output. (b and c) shows this model from the view of a Microsoft Hololens2 to create an \gls{X-ray Vision} effect using illustrative rendering.}
    \label{fig:3DPrintedVersionOfTheSystem}
\end{figure}

\subsection{Immersive User Interface Design} \label{sec:ImmersiveDesgin}
The Random Volume Generator's immersive design component is where the volumes are randomly generated while the user can change a set of parameters with the goal of creating an input file for the final system. 
By immersing researchers in the same environment as their users, they can experience the data set the same way their participants would while choosing parameters for various instances of the project.

The \gls{ui} elements have been generated from the Mixed Reality Tool Kit (MRTK)~\footnote{\url{https://github.com/microsoft/MixedRealityToolkit-Unity}} and have been laid out to keep the users focused on the volume. 
These elements include changing two list menus between a predefined set of conditions (this dissertation utilized \glspl{virt}) and updating input parameters (shown up the top and to the right of \autoref{fig:HoloLensInterface}).
All areas that can be modified via the selected parameters will be highlighted unless the "Custom Visualizations" option is enabled, showing the set of pre-computed visualizations that have been developed for the system.
Most of the input parameters users can be changed via to sliders (seen on the right side of \autoref{fig:HoloLensInterface}) that will allow them to choose between various sizes and amounts of objects that they want to exist at a time.

Since this interface exists in 3D, it is important to create a color interface that the interface suits this purpose. 
Unlike many color pickers used in \gls{vr}, which focus on a 2D input~\cite{Alex2020}, it was believed that this interface would benefit from giving the participants the ability to adjust the color with depth as of field as well. 
Requiring a volumetric color picker (shown down the bottom of \autoref{fig:HoloLensInterface}).
The colors for the elements of the volume can be chosen from a 3D color picker in a similar style to Kim et al.'s~\cite{Kim2022} color pickers designed for color blending.
% where direct volume rendering is used to identify an object's color.


The Random Volume Generator is available on GitHub.
The Random Volume Generation system~\footnote{\url{https://github.com/tomishninja/RandomlyGeneratingVolumes}} is built as a modular system built in the Unity game engine~\footnote{\url{https://unity.com/releases/editor/archive}}.

\section{Conclusion}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter4/Images/HoloLensTeaser.png}
    \caption{The HoloLens \gls{ui} of the random volume generation system. }
    \label{fig:HoloLensInterface}
\end{figure}

The Random Volume Generation System provides a method for generating volumes designed for different types of user studies that look into volumetric research in the 3D space. 
It provides a solution to the limited number of volumetric datasets that are available publicly and allows for more accurate results from controlled studies. 
This now lays the groundwork for future research to determine what \glspl{virt} the effects as \glspl{X-ray Visualization} have when used in tandem with \gls{dvr} utilizing a set of controls.
Moving forward, the goal is to test the perception and depth perception qualities of these visualizations on the general population by using these volumes with a series of tests designed to test the functionality of the \glspl{virt}. 

The Random Volume Generator provides researchers with:
\begin{itemize}
    \item A method for providing to recreate reproducible and a near-infinite amount of distinct volumes that can be easily transferred and stored for controlled studies. 
    \item An \gls{ar}/\gls{vr} interface to aid with the planning of these volumes.
    \item Open source access to the system and a guide to the modular components, which can be tailored for use with other studies.
\end{itemize} \newpage 
    \glsresetall
    %\include{Chapter5/Chapter5Brief}
     \newpage \chapter{Perception of Volumetric Illustrative Effects Visualizations within OST AR} \label{Chap:PerceptionStudy}

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/PerceptionStudyPhotos.png}
    \caption[This image shows the study environment in which the study was found in and all four of the conditions listed in \autoref{Chap:VolumetricX-rayVision}.]{
    These images show the study environment in which the study was found in and all four of the conditions listed in \autoref{Chap:VolumetricX-rayVision}.
    All these images were taken using an external HoloLens2's camera while observing a user interacting with the study.
    (a) Displays \textit{no \gls{virt}} and shows the participant performing the task while being observed by the researcher.
    (b) The \textit{Hatching} \gls{virt} illustrates a task perspective similar to the participants with all major interfaces visible in the photo.
    (c) An image of the participant counting the small green objects with their fingers while the visualization utilizes the \textit{stippling} \gls{virt}.
    }
    \label{fig:PerceptionStudyPhotos}
\end{figure}

% Explain this field
% Very few studies have researched how easily a person can look through \glspl{X-ray Visualization}.
% Direct Volume Rendering has several issues, one being that it is possible for viewers to miss key details within a volume. 
% This may likely be problematic on top issues like having an impaired sense of depth when previously tested in Augmented Reality~\cite{Sielhorst2006} that have been caused by \gls{dvr} objects on an \gls{ost} \gls{ar} \gls{hmd}s.

This chapter evaluates the \glspl{virt} presented in \autoref{Chap:VolumetricX-rayVision} to see if a measurable improvement can be observed when identifying individual graphical objects in a mixed reality environment.
This is critical for identifying foreign masses within the human body, allowing more rapid and accurate diagnosis and determining how compelling these visualizations would be for a given surgery. 
% Explain this study
% This chapter explores how well participants can determine what data exactly can be found in a given volume and identify individual objects when it is presented to them as a 3D volume rendering and
% \glspl{virt} are used to determine if they helped or hindered a person's ability to analyze a volume and how accurate that perception was visual.
By evaluating how participants comprehend the space and how they could categorize foreign masses, this research aims to determine if \glspl{virt} supports people's ability to understand data parameters.
%We then analyze this further by asking if users can accurately estimate the objects grouped into a smaller space.
% This provided us with some knowledge that depth perception is working better than 2cm accurately and also let us begin to understand the issues with visualizations like this.
%We were unsure if a person could clearly determine if an object was inside of another object or not. 

Illustrative rendering techniques enhance viewers' understanding of precise volume boundaries, facilitating the identification of relationships between objects. 
Practical applications of these techniques are evident in scientific illustrations, particularly in fields such as entomology and medicine~\cite{Philbrick2019, Maciejewski2008}. Artistic renderings of human anatomy have a long history, with "Gray's Anatomy"\cite{Coskun2022, Preim2005}.

% Why is this study needed
Understanding the content within a volume through an \gls{X-ray Vision} effect is important, as being able to see an object clearly is required for any visualization.
The more an effect distracts from the information it is trying to present, the less useful it becomes.
The following study aims to evaluate the effect these \gls{X-ray Visualization}s have on data, and if \glspl{virt} there are any issues that will need to be overcome to utilize any given \gls{virt} in a real-world scenario.

%
% Maybe move this stuff lower.
%

% Explaining how this fits into the study
% Findings from \autoref{Chap:X-ray Implemntion} indicated that the user's ability to look through does not seem dependant on the ability to look through the physical object, but Rather the visualization itself.
% The participants from this study seemed to struggle to focus on both the visualization and the real world simultaneously.
% This allowed us to counterbalance our study better and greatly expand the amount of data it could visualise. 

% Explain how This chapter is relevant to the rest of the thesis
%This chapter presents a study that takes the previously proposed VIRTs shown in "\autoref{Chap:X-ray Implemntion} \glspl{X-ray Visualization}" and asks participants to account for the amount of a type of artifact that appears.
%Since we didn't have to have a shape that represents the real-world body for this study, every iteration in the experiment used a unique set of shapes.

% The Road Map For the chapter.
%This chapter will elaborate on theoretical values and issues that \glspl{virt} have seen in other mediums(Mobile~\cite{Chen2012}, Desktop~\cite{Chen2012, Lawonn2013, Bruckner2007} and art~\cite{Rheingans2001, Gerl2006}) and what using these as an X-ray visualisation may cause.
%Then, 
%Details of the given user study's methodology, including the study environment, participant details, stimuli, and procedure, will be presented. 
%This will be followed by an analysis of this study's results, a detailed discussion, and lessons learned.

% Reintroducing the VIRTs in the sense of how easy they are to perceive
\section{Volumetric Illustrative Rendering Techniques}
\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingVolumesReducedImage.png}
    \caption{The conditions that were used in this study are displayed as the volumes they are represented as.}
    \label{fig:CountingConditions}
\end{figure}

% Talk about the difficulties this chapter is aiming to fix
Artistic effects are generally chosen over utilizing transparent objects because transparent objects can be difficult to place within a virtual space.
This is because it is difficult to even in the real world to determine if a transparent object is inside or behind another object~\cite{Pisanpeeti2017}.
While realistically rendering a scene in \gls{mr} will provide you with the best possible results regarding accuracy, research has determined that better spatial understanding can be found by using a sketch or cartoon-based rendering~\cite{Wijayanto2023}.
Neither stippling nor hatching has been used much using any \gls{mr} immersive devices, but it has seen a lot of use with 3D displays~\cite{Bui2015}.

Each different \gls{virt} uses different techniques to indicate the shape and surface of the object it is trying to represent. 
\textit{Halos} work well to highlight objects and have been shown to improve depth perception~\cite{Lawonn2018, Svakhine2009, Joshi2008, Shen2014}.
Both \textit{Stippling} and \textit{Hatching} are able to illustrate the location of major surfaces in relation to each other, which should make it simpler to determine which objects are which.
Stippling accomplishes this by applying stippling to each different type of object, allowing a better sense of space~\cite{Busking2007}, while Hatching accomplishes this by changing the angles of the lines~\cite{Frech1960}.

This Chapter focuses on the \glspl{virt} revealed in \autoref{Chap:VolumetricX-rayVision}: \textit{Halo}, \textit{Hatching} and \textit{Stippling}.%\autoref{sec:X-ray vision}
The design of these \glspl{virt} is shown in \autoref{fig:CountingConditions}, utilizing the volumes that could be created by the Random Volume Generation system shown in \autoref{sec:VolumetricDataGeneration}.


\section{User Study}  \label {sec: User Studies Study 1}


%Mention similar research (1 - 2 sentences)
%Explain what the goals of this study are
% What this study aimed at achieving 
%In our first study, we want to understand whether or not a participant can understand a complete volume and identify objects within that volume. 
To access the impact that \glspl{virt} ability to identify spatial relationships between the information within the volume two tasks were designed: one of them to determine how well a user could identify objects within a volume using \gls{ost} \gls{ar}; while the other one aimed at determining how well they could identify an a grouped hierarchy.
Volumetric rendering can make spatial perception difficult due to clustering, inaccurate depth perception, and orientation~\cite{Zhou2022, Zheng2013, Bruckner2006}. 
\gls{mr} \DIFdelbegin %DIFDELCMD < \gls{hmd}%%%
\DIFdel{s }\DIFdelend \DIFaddbegin \glspl{hmd} \DIFaddend greatly benefit these issues by enabling additional depth cues, including some stereotypical cues and motion parallax~\cite {Cutting1997, Vishton1995}. 
Still, \gls{ost} \gls{ar} \DIFdelbegin %DIFDELCMD < \gls{hmd}%%%
\DIFdel{s }\DIFdelend \DIFaddbegin \glspl{hmd} \DIFaddend tend to have a small color or gamut and less bright and opaque colors than traditional video screens/devices~\cite{Zhan2020, Xiong2021}.


\subsection{Research Questions}
This study was designed to better understand how partial occlusion in the form of \glspl{virt} affected a user's ability to understand the data within a volume. 
Most works on the usability and perception of these effects in \gls{X-ray Vision} have either looked into where and when \gls{X-ray Vision} is needed~\cite{Wang2023, Guo2023}, what are the tolerable parameters~\cite{Santos2016}, or how to interact in a space that you cannot physically access~\cite{Blum2012, Blum2012a}.
However, there has been limited work in general on the impact of the use of \gls{mr} \glspl{hmd} when using \gls{dvr} techniques~\cite{JunYoungChoi2018, Cecotti2021, Fischer2020} with very few papers focused on \gls{ost} \gls{ar}~\cite{Sielhorst2006}. 
This has led to a great deal of potential unknowns for how \glspl{virt} being ended \gls{ost} \gls{ar} may impact how users and the limited information which existed on the impact that \glspl{virt} could have on the perception of volumes answers, the following research questions were pursued by this study where: 

\begin{enumerate}[label=RQ.\arabic*]
    \item Can \glspl{virt} aid a person’s comprehension of a volume when determining individual objects using direct volume rendering?  
    \item What is the impact of different \glspl{virt} on participants' self-reported cognitive load and usability?
    \begin{enumerate}[label=RQ.2.\arabic*]
       \item Is this effect noticeable via the differences between participant behaviour (hand, head, and eye movements) between \glspl{virt}? 
    \end{enumerate}
\end{enumerate}

\subsection{Tasks}
\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/TaskDescription.png}
    \caption[A visual description of how the task users in this study were asked to conduct using a volume with 22 green artifacts.]{A visual description of how the task users in this study were asked to conduct using a volume with 22 green artifacts. On the left-hand side, users were asked to count all of the green artifacts, while on the left-hand side, they were asked to count all of the objects nested inside of the blue artifacts. All of the green cells that participants were asked to count for each task are marked with a white outline and counted with a number to the top right of each green artifact.}
    \label{fig:TaskDescription}
\end{figure}

% What inspired this study design and where can similar ones be found
The study design was based on work done in the field of visual analytics and was influenced by studies within the field of volume rendering and visual analytics~\cite{Laha2012, Laha2014, Laha2013}. 
To understand what the participants could comprehend about the volumes with a range of given \glspl{virt}. 
Investigating how well participants could record information within the volume using \glspl{virt}.
This was compiled by generating a set of 7680 volumes that all utilized the same range of data, allowing a unique volume that conformed to a set of predefined rules and had users report on the frequency of artifacts in the object.

\subsubsection{Count All}
In the first task, participants were asked to summarize a visual data set (shown in \autoref{fig:DifferentAmounts}) by asking them to "Count all of the small green objects throughout the entire volume".
This is called the "\textit{Count All}" task for simplicity.
The desired behaviour participant is illustrated on the left-hand side of \autoref{fig:TaskDescription}. 
This task was proposed in relation to \gls{dvr} usability by Laha et al.~\cite{Laha2016}, and a variation of it was used by Munzener et al.~\cite{Munzner2014}.
The aim of the question was to determine how easily people can identify individual objects within the volume and the effect of \glspl{virt} on their accuracy.

\subsubsection{Count Nested}
In the second task, participants were asked to count the "small green objects located within the larger blue objects." 
This was called the "\textit{Count Nested}" task. 
The desired behaviour participant is illustrated on the right-hand side of \autoref{fig:TaskDescription}. 
This task was intended to determine how accurately people understand the spatial relationships within a volume and the effect of \glspl{virt} on their accuracy. Specifically, the goal of this research was to understand if \glspl{virt} assists people in identifying if an object is in front of, inside, or behind another object. 


\subsection{Hypotheses} \label {sec: User Studies Hypothesis}
\begin{enumerate}[label=H.\arabic*, series=Hypotheses] 

    \item Participants will identify all of the objects more accurately using the \textit{halo} \gls{virt} in the \textit{Count All} task (R1);
    \textit{Piringer et al.~\cite{Piringer2004} have shown that the halo effect can allow for people to rapidly find objects within large 3D datasets and DVR visualizations}

    \item Participants will identify all of the objects faster using the \textit{halo} \gls{virt} in the \textit{Count All} task (R1).
    \textit{The same studies as above have shown that \textit{halo} can improve identification time ~\cite{Piringer2004}};

    \item Participants will identify all of the objects more accurately using the \textit{hatching} \gls{virt} in the \textit{Count Nested} task (R.1).
    \textit{These visualizations have been designed to enable better depth perception for users and should be able to aid participants when counting large amounts of objects};

    \item Participants will identify all of the objects fastest when either using the \textit{hatching} or \textit{stippling} \glspl{virt} in the \textit{Count Nested} task (R.1).
    \textit{As in the previous study in this dissertation and in other similar studies, the visualizations should enable a more accurate view of the dataset. This effect can be seen without volume rendering in works that don't use volume rendering, and I believe that this will carry over into this experiment~\cite{Martin-Gomez2021}};

    \item Participants will move their hands, eyes, and heads at a lower rate when they are using the \textit{halo} \gls{virt} (R.2.1).
    \textit{The embodied cognition field looks at instances when the body plays a significant causal role in a person's cognitive processing~\cite{Raab2019, Wilson2013}. I expect that people would move their heads and eyes more rapidly when trying to solve a difficult problem~\cite{Wilson2013}. This is likely to take the place of hand motions to help them count, like using their fingers to count and pointing at the objects~\cite{Wilson2013}, but could also be seen in with rapid eye movements};

    \item The \textit{halo} \gls{virt} will be the most preferable and least cognitively demanding for identification tasks (R2);
    \textit{When other studies have utilized similar methods, they have found that a silhouette or outline has been more appealing to participants~\cite{Martin-Gomez2019, Fischer2020a}};

\end{enumerate}

\subsection{Participants} \label {sec: User Study Perception Participants}
24 participants were recruited for this study from a pool of students, faculty and staff from the University of South Australia aged between 21 and 37 years old ($mean = 26.95, \sigma = 4.47$), 6 female, 18 male, with little(9) to no (15) experience with medical data.
Their experience using MR systems varied, with 8 people using them daily, 5 using them weekly, 6 using them monthly, 2 using them rarely, and 3 using them for the first time.
All participants were asked to declare if they had any major vision impairments that could not be corrected during the recruitment process. If this was the case, they would have been asked not to participate in the study. 

\subsection{Procedure}  \label {sec: User Studies Study 1 Design}
 %PreAt the start of starting the study, participants were required to read and fill out an information sheet, content form, and demographic survey. 
Participants were asked to sit down and complete eight training exercises to familiarise them with the task of each condition containing either 13 or 14 objects to count in total.
No data was recorded during this phase, and the participants were encouraged to talk to the examiner. 
After this, participants were allowed to take a break before starting the study.

\subsubsection{Task} \label{sec:PerceptionTaskDesgin}

\begin{figure}[bt]
    \centering
    \includegraphics[width = \columnwidth]{Chapter5/Images/DifferentAmountsOfVIRTs.png}
    \caption{5 volumes each using \textit{no \gls{virt}} each with a different number of green objects to count.}
    \label{fig:DifferentAmounts}
\end{figure}

% the task users were asked to perform
Each participant was asked to \textit{count all} of the small green objects throughout the scene (\textit{Count All}) or only the green objects located within one of the blue regions (\textit{Count Nested}).
%To not confuse the participants, we would have them answer each question in a group of 15 and display the question on a monitor for them to remind them each time.
To ensure clarity, participants would be tasked with answering the same question for each \gls{virt} at the same time before moving forward to the other question for that same \gls{virt}. 
This group of 15 would consist of 3 repetitions of 5 conditions shown in \autoref{fig:DifferentAmounts}, each containing 14, 16, 18, 20, and 22 objects to count.
The order in which the \glspl{virt} was presented was completely counterbalanced.

This was done to ensure the participants were aware of which task they were being asked to perform.
Every time the question changed, the examiner would verbally inform the participant. 
The order in which these questions were asked throughout the study would be shuffled to remove possible learning effects. 
The question would also be displayed on the nearby computer screen if the participant was confused, and this would be changed for a prompt telling the participant the question would change each time.
This allowed us to test how well participants could identify objects within the scene and the spatial location and hierarchy of the objects within the scene.

% Data collection
Participants were given a large red button (called the task button) that allowed them to start each iteration presenting the visualization.
Participants were required to press the task button before giving their answer.
Participants would then be prompted by a computer monitor where they would input their answer via a keyboard.
During the times between the task buttons being pressed, the time is taken to start, and it would finish when they pressed it again to signal the end of the condition. 
When Participants are prompted to provide their answer via the nearby keyboard and monitor. 
When Participants pressed the task button again, the next iteration of the study would begin. 
their hand, head, and gaze movements were tracked. 
%Their voice was also recorded - Extend on this tom, and the examiner would take notes regarding any unusual behaviour they presented, such as counting with their hands or voice.

%Questionnaires and Post-Study
Between each condition, participants were asked to take a questionnaire that consisted of a PAAS~\cite{Paas2003}, a SUS~\cite{Lewis2018}, and a question promoting them to quantify between 0(worst) and 5(best) how well they could observe all the objects in the volume as well as giving them the option to tell us what they liked and disliked about the \gls{virt} which is reported in \Cref{app:Chapter5Comments}.
The post-study questionnaire asked the participants to rank how they believed they performed with all of the \glspl{virt}, and how easy they were to use, and they were given the opportunity to explain their answers (reported in \Cref{app:Chapter5Comments}). These answers were then used to confirm a correlation between how the participants answered each condition.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/SketchOfStudy2TableLayoutPerception.png}
    \caption[The layout of the study area (the participants desk space) the participants had around them for the perception experiment.]{The layout of the study area (the participants desk space) the participants had around them for the perception experiment. Distances are in center meters}
    \label{fig:PerceptionStudySpace}
\end{figure}

\subsubsection{Pilot Study}
%Study Settings and Parameters
The amount of objects in this study was decided by comparing this study research to similar studies in graph visualizations that would use up to 20 nodes in their visualizations~\cite{Yoghourdjian2018}. 
A separate pilot study was conducted using 5 participants (1 female 4 male) who were aged between 21 - 57 years old. 
This study used a similar procedure to the one listed in \autoref{sec:PerceptionTaskDesgin}, which used a wider range of values from 9 green objects to 27.
Only the \textit{no \gls{virt}} condition was used in the pilot study.

The results from this study showed that they struggled to count past 18 when presented with the baseline condition.
All visualizations were randomly generated for each study iteration, following a predefined set of rules laid out in the previous chapter (\autoref{Chap:VolumetricX-rayVision}).
This caused the decision to have numbers ranging around 18 objects the participants needed to count. 
%Allowing for some conditions that we were confident participants could perform well and others they would have likely struggled with when using the \textit{no \gls{virt}} condition. 

\subsection{Study Environment}

The study took place in a dimly lit room with multiple light sources (windows and stage lighting) and utilized the space shown at the centre of the room shown and detailed in \autoref{fig:PerceptionStudySpace}.
The visualization would appear 15cm above the visualization marker.
Participants could move the computer screen, keyboard, and button to a preferred location, but each user studied started in the same position.
Participants were seated at a desk in the center of the room and observed throughout the study by a researcher as shown in \autoref{fig:PerceptionStudyPhotos} a). The study was facilitated and built upon using the Unity Engine\footnote{\url{https://unity.com/releases/editor/whats-new/2019.4.3}}, with the Microsoft Hololens 2\footnote{\url{https://www.microsoft.com/en-us/hololens/}}  (\autoref{fig:PerceptionStudyPhotos}(b)) as the display modality via a wired connection utilizing the Holographic Remoting Player\footnote{\url{https://learn.microsoft.com/en-us/windows/mixed-reality/develop/native/holographic-remoting-player}} to a desktop PC. The desktop PC featured an Intel i5 with an Nvidia Geforce GTX 2070 GPU, running at between 60-90 FPS during the study. 
Participants would either be given a task button on either their left or right side for this study.

\section{Results} \label{sec:sdfResultsPerception}
This section contains the analysis of the data collected in both studies.
All values with a p-value $<$ 0.1 have been reported, while only p-values $<$ 0.05 are considered significant.


\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingEverythingMainResults.pdf}
    \caption[The accuracy, or difference between the participant's and real answers, when counting all green objects within the volume by the \glspl{virt} and the number of objects they were asked to count.]{The accuracy, or difference between the participant's and real answers, when counting all green objects within the volume by the \glspl{virt} and the number of objects they were asked to count.
    Confidence intervals (CL = 95\%) calculated from the post-hoc emmeans between all conditions are shown as boxes on each error bar.
    Significance bars have been omitted as all conditions are significant}
    \label{fig:CountingEveryThingMainResultsAllShownLineGraph}
\end{figure}


%We were most interested in for the object identification study.
In this study, while the participant was observing the volume, the system tracked: 
\begin{itemize}
    \item The participants' accuracy. The absolute difference between the amount of green objects the participants counted($p$) and the actual amount of objects there was($a$);
    \item The time required for a participant to count the number of green objects.
    \item and user behaviour details were recorded this included information required to track:
        \begin{itemize}
            \item Eye Movements
            \item Head Movements
            \item Hand Movements
        \end{itemize}
\end{itemize} 
For the analysis of errors and time, linear mixed models were conducted. This approach accounts for individual differences between participants in repeated measures and allows us to also examine the effect of the number of objects.~\cite{Kaptein2016, Adams2022}.
% A mixed linear model examined the differences across all variables measured for \autoref{sec:sdfResultsPerceptionAccuracy} and \autoref{sec:sdfResultsPerceptionTimeRequired}. 
% This model accounted for the individual differences between the participants and allowed us to permit model specification-based interaction effects if possible, allowing us to look into the effect the number of objects may have on the VIRTs.
%Allowing us to look into the number of objects' effect on the VIRTs.
Both models were specified with the fixed factors of the \glspl{virt} designed in \autoref{Chap:VolumetricX-rayVision}, the number of objects the user was required to count, and the interaction effect between them plus a random effect of the participant on the intercept. Significance values were extracted using Type II Wald chi-square tests, and where appropriate, pairwise post hoc comparisons were conducted using Tukey’s \gls{hsd} for multiple comparisons.
%Results for the two questions asked
%"How many smaller green blobs are there?"

\subsection{Accuracy (H.1 \& H.3)} \label{sec:sdfResultsPerceptionAccuracy}

% (Moved this from above as it only applies to this subsubsection
The accuracy of the participant responses was measured as the difference between the correct answer and the answer given by the participant. The model for accuracy in the \textit{Count All} task showed a significant fixed effect between the \glspl{virt} ($\chi$2(3, N = 24) = 777.083, p $<$ 0.0001), and the number of objects also showed a significant fixed effect ($\chi$2(4, N = 24) = 146.190, p $<$ 0.0001), with a significant interaction effect (($\chi$2(3, N = 24) = 34.605, p $<$ 0.0001)). 
These results are illustrated in \autoref{fig:CountingEveryThingMainResultsAllShownLineGraph}.

The post hoc pairwise comparisons between the \glspl{virt} showed significant differences for all combinations (p $<$ 0.0001). 
\textit{Halo} had the highest accuracy, followed by \textit{no \gls{virt}}, \textit{stippling}, and \textit{hatching} in descending order.
Comparison between the different numbers of objects showed significant differences between 14 and 22, 16 and 24, 16 and 22, and 18 and 24 objects (p $<$ 0.0001), 14 and 28 (P = 0.0005), 16 and 18 objects (p = 0.0247), and 23 and 25 objects (p = 0.0033). The accuracy was lower when the participant had more objects to count in all of these pairs.
% These results showed that the more objects that existed, the less accurate the users tended to be.

The Post hoc Pairwise comparison for the interaction between the conditions had 138 significant effects with a p-value $<$ 0.05. 
To summarize these effects, most visualizations showed a significant difference in themselves when the difference between the number of objects the participant had to count was greater than 4, regardless of \glspl{virt}.
The one exception to this rule was the \textit{halo} visualization, which showed no significant differences against itself when the participant was required to count any number of objects.
A full breakdown of the results can be found in \Cref{apendix:ErrorWhenCountingEverything}.


%For clarity noticable trends noticed in \Cref{apendix:ErrorWhenCountingEverything}
For conditions with fewer objects to count, \textit{hatching} was unlikely to be significant when compared to conditions using \textit{stippling} with more objects to count. 
There was a high level of significance for the inverse, though (when there were fewer objects to count using \textit{stippling} than \textit{hatching}).
Generally, \textit{hatching} and \textit{stippling} showed a very significant difference when compared to \textit{no VIRT} and \textit{halo}. 
All other interaction pairs not previously mentioned showed a significant difference from one another. A full table of these effects can be found in the supplemental materials.

For the \textit{Count Nested} task, the model showed a significant fixed effect between the \glspl{virt} ($\chi$2(3, N= 24) = 347.053, p $<$ 0.0001). No significant effect of the number of objects or significant interaction effect was shown. 
The post hoc pairwise comparison of \glspl{virt} showed significant results between \textit{halo} and \textit{hatching}, \textit{halo} and \textit{stippling}, \textit{hatching} and \textit{no \gls{virt}}, \textit{hatching} and \textit{stippling}, \textit{no \gls{virt}} and \textit{stippling} (p $<$ 0.0001), and \textit{halo} and \textit{no \gls{virt}} (p = 0.0001). 
%The results were similar to when all green objects were counted.
\textit{Halo} had the highest accuracy, followed by \textit{no \gls{virt}}, \textit{stippling}, and \textit{hatching} in descending order. 
%This likely indicates that participants had no real issue determining what was inside the blue objects. Instead, the \glspl{virt} themselves appear to have a bigger impact on the ability to count the objects accurately.
%However in this case, as can be seen in \autoref{fig:CountingWithinTheBlueMainResults}, it was uncommon for participants to get any errors when using the \textit{halo} \gls{virt}. 


\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingNestedMainResults.pdf}
    \caption{The accuracy of participant responses when completing the \textit{Count Nested} task.
    The error bars indicate each \gls{X-ray Visualization}'s confidence levels (CL = 95\%).
    Significance bars have been omitted as all conditions are significant}
    \label{fig:CountingWithinTheBlueMainResults}
\end{figure}

\subsection{Time Required (H.2 \& H.4)} \label{sec:sdfResultsPerceptionTimeRequired}
% The time it took to answer these questions.
The model for the time spent on the \textit{Count All} task showed a significant effect between the number of objects viewed ($\chi$2(4, N= 24) = 85.9544, p $<$ 0.0001). There was no significant fixed effect between the four conditions and no significant interaction effect. 
% To further evaluate these findings, post hoc pairwise comparisons using Tukey’s HSD for multiple comparisons. 
The post hoc pairwise comparison of the different numbers of objects showed significant differences between 14 and 22, 16 and 22 (p $<$ 0.0001), 14 and 20 (p = 0.0008), and 18 and 22 (p = 0.0055). 
%These results show that the more objects

\begin{figure}[bt]
    \centering
    %\includegraphics[width=\columnwidth]{Chapter5/Images/CountingOnlyWithinTheBlueTimeTaken.pdf}
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingTimeRequired.pdf}
    \caption[The time required to complete each interaction of a task for each \gls{virt} for both the \textit{Counting Everything} and \textit{Counting Nested} tasks.]{The time required to complete each interaction of a task for each \gls{virt} for both the \textit{Counting Everything} and \textit{Counting Nested} tasks.
    The error bars indicate each \gls{X-ray Visualization}'s confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:CountingOnlyWithinTheBlueTimeTaken}
\end{figure} the participants had to count, the longer it took them to complete the task, particularly when the count differed by four objects.

The model for the time spent on the \textit{Count Nested} task showed a significant fixed effect between the four \glspl{virt} ($\chi$2(3, N= 24) = 85.9544, p $<$ 0.0001). In contrast to counting all objects, the fixed effect number of objects shown only showed some variability ($\chi$2(4, N= 24) = 8.5963, p $=$ 0.07202), and no significant interaction effects were found. 
The post hoc comparison between the \glspl{virt} showed significant differences between \textit{halo} and \textit{hatching}, \textit{halo} and \textit{stippling}, \textit{hatching} and \textit{no \gls{virt}} (p $<$ 0.0001), \textit{hatching} and \textit{stippling} (p = 0.0010), and \textit{no \gls{virt}} and \textit{stippling} (p = 0.0143). 
%\autoref{fig:CountingOnlyWithinTheBlueTimeTaken} shows \textit{halo} was the fastest condition for participants, closely followed by \textit{no \gls{virt}}, with \textit{stippling} and \textit{hatching} being significantly slower than both. 

A Kendall correlation test was used to determine if the time spent by each participant correlated to their accuracy. 
When only counting the green objects contained within the larger blue objects, a significant relationship between the time required and range of error was found with a low to moderate amount of correlation $\tau = -0.18$ (z = -9.3074, p $<$ 0.0001), showing that participants tended to do better when they took more time some of the time. 
When counting all of the objects, almost no relationship was found between the time required and accuracy $\tau = -0.03831766$ (z = -1.8507, p = 0.06422), suggesting that taking more time when performing this task did not help improve accuracy. 

%(Optional) present analysis for voice and gesturing analysis. 
\subsection{User Behavioural Analysis (H.5)} \label{sec:PerceptionBehavioralResults}
This section focuses on how the participants' behaviour reacted to the four conditions. 
Over the course of the overall distance, participants would have moved their heads, gaze, and hands where tracked. 
This is then divided by the time each condition took to allow us to view the average speed at which participants moved.
The headset's and hand's distance was tracked by calculating the amount its position had moved relative to the visualisation between every 16ms and summing these results.
When the hands were not in front of the user, their distance was not tracked.
Eye Gaze's distance was measured by tracking the distance a user's eye gaze would move within a 2m radius from the participant's current position in each frame minus the same distance that the head gaze would have provided. 

\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingHeadVel.pdf}
    \caption[Box plots showing how much each participant moved their head to view the volume between the different \gls{virt} conditions for both the \textit{Counting Everything} and \textit{Counting Nested} tasks.]{Box plots showing how much each participant moved their head to view the volume between the different \gls{virt} conditions for both the \textit{Counting Everything} and \textit{Counting Nested} tasks.
    The error bars indicate each \gls{X-ray Visualization}'s confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:HeadsetMovementPerception}
\end{figure}

% The model for the users head motion when counting nested
The model for the participants' head motion velocity for the \textit{Counting Nested} task showed a significant fixed effect between the various \glspl{virt} ($\chi$2(3, N= 24) = 54.9095, p $<$ 0.0001). 
The fixed effect number of objects and interaction effects both found no significant effects. 
The post hoc comparison between the \glspl{virt} showed significant differences between \textit{hatching} and \textit{no \gls{virt}} (p $<$ 0.0001), \textit{no \gls{virt}} and \textit{Stippling} (p = 0.0001), \textit{Halo} and \textit{no \gls{virt}} (p = 0.0047), \textit{Halo} and \textit{hatching} (p = 0.0005), \textit{Halo} and \textit{no \gls{virt}} (p = 0.0049), \textit{Hatching} and \textit{Stippling} (p = 0.0205).
\autoref{fig:HeadsetMovementPerception}'s Counting Nested objects graph shows that having no \gls{virt} leads to an unpredictable reaction from the user behaviour while adding a \gls{virt} tends to restrict this.

% Results for distance head moved for motition velocity
The model for the participants' head motion velocity for the \textit{Counting Everything} task showed a significant fixed effect between the various volumetric Illustrative Renderings ($\chi$2(3, N= 24) = 32.2482, p $<$ 0.0001). In contrast to counting all objects, the fixed effect number of objects and interaction effects were found to have no significant effects. 
The post hoc comparison between the \glspl{virt} showed significant differences between \textit{halo} and \textit{stippling}, and \textit{hatching} and \textit{stippling} (p $<$ 0.0001), \textit{no \gls{virt}} and \textit{Hatching} (p = 0.0042), \textit{Halo} and \textit{no \gls{virt}} (p = 0.0047).
\autoref{fig:HeadsetMovementPerception}'s Counting Everything graph shows that the \textit{Hatching} and \textit{Stippling} \glspl{virt} required participants to move more than other \gls{virt} such as \textit{halo} or \textit{no \gls{virt}}.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingHandVel.pdf}
    \caption[A graph showing the speed at which participants moved their hands in this experiment when the different \glspl{virt} were being utilized for both the \textit{Counting Everything} and \textit{Counting Nested} tasks.]{A graph showing the speed at which participants moved their hands in this experiment when the different \glspl{virt} were being utilized for both the \textit{Counting Everything} and \textit{Counting Nested} tasks.
    The error bars indicate each \gls{X-ray Visualization}'s confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:PerceptionSpeedHandsMoved}
\end{figure}

% Results for Hand Motion
The model for the participants' head motion velocity for the \textit{Counting Everything} task showed no significant results. 
However, The model for the \textit{Counting Nested} task showed a significant fixed effect between the \glspl{virt} ($\chi$2(3, N= 24) = 11.7432, p $=$ 0.0083), but both the fixed effect for the different amounts of countable objects and the interaction effect showed no significant results.
The post hoc comparison between the various \glspl{virt} showed that when participants were using the \textit{Halo} \gls{virt}. 
\autoref{fig:PerceptionSpeedHandsMoved}, they would move their hands significantly faster than they would during the \textit{Stippling} task (p = 0.0057).

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingEyeVel.pdf}
    \caption[Box plots relating to the speed at which participants moved over different eye gazes.]{Box plots relating to the speed at which participants moved over different eye gazes.
    The error bars indicate each \gls{X-ray Visualization}'s confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:PerceptionEyeGaze}
\end{figure}

% Results for Eye gaze for counting everything
The model for the velocity of the participants' eye gaze for \textit{Counting Everything} task showed a significant fixed effect between the different \glspl{virt} ($\chi$2(3, N= 24) = 19.347, p $=$ 0.0002) and the number of objects in total ($\chi$2(4, N= 24) = 30.759, p $<$ 0.0001). No significant interaction effect was found.
The post-hoc comparison between the \glspl{virt} showed that participants significantly moved their eyes more when using stippling compared to \textit{Halo} (p = 0.0007) and \textit{Hatching} (p = 0.0010). Some variation was also found between \textit{No \gls{virt}} and \textit{Stippling} (p = 0.0918). 
Whereas the posthoc comparison for the number of objects \textit{14} countable objects showed participants moved their eyes significantly less compared to \textit{18} (p = 0.0013), \textit{20} (p = 0.0153) and \textit{22} (p = 0.0013). The same phenomenon was found with \textit{16} countable objects as participants moved their eyes significantly fewer results against \textit{20} (p = 0.0177) and \textit{22} (p = 0.0024).

% Results for eye gaze when counting nested objects
The model for the velocity of the participants' eye gaze for \textit{Counting Nested} task showed a significant fixed effect between the different \glspl{virt} ($\chi$2(3, N= 24) = 28.1722, p $<$ 0.0001) and no significant fixed effect was found with the number of objects to count and the interaction effect.
The post-hoc comparison between the \glspl{virt} (shown in right hand side of \autoref{fig:PerceptionEyeGazeAmountOfObjectsToCount}) showed that \textit{Stippling} was significantly faster compared to \textit{Halo} (p = 0.0004) and \textit{no \gls{virt}} (p $<$ 0.0001). Participants moved significantly slower when using \textit{Hatching} when compared to \textit{no \gls{virt}} (p = 0.0108). Some variation could also be seen between \textit{Hatching} and \textit{Halo} (p = 0.0602). 

\begin{figure}[tb]
    \centering
    %\includegraphics[width=\columnwidth]{Chapter5/Images/PerceptionEyeGaze.pdf}
    \includegraphics[width=\columnwidth]{Chapter5/Images/EyeGazeAmountOfObjectsToCount.pdf}
    \caption[A plot showing the difference in velocity between the user's eye movements when different amounts of objects existed for them to see when counting everything.]{A plot showing the difference in velocity between the user's eye movements when different amounts of objects existed for them to see when counting everything.
    The error bars indicate each \gls{X-ray Visualization}'s confidence levels (CL = 95\%).
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:PerceptionEyeGazeAmountOfObjectsToCount}
\end{figure}

\subsubsection{Subjective Results (H.6)} \label{sec:PercptionSubjectiveResults}
% What is the content in this section
This section focuses on how the participants themselves perceive the various \glspl{virt}.
By asking participants to complete the PAAS Cognitive load scale and a System Usability Scale (SUS) questionnaire. 
Paired with the asking participants "\textit{How easy was it to look at objects inside of other objects using this visualization?}".
It was possible to gain further insights into the participants' actions and behaviours. 


The results from the PAAS questionnaire for the counting only within the blue showed a significant difference between different illustrative visualizations using a Friedman rank sum test ($\chi2(3, N= 24)= 46.493, p < 0.0001$).
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied, comparisons showed significantly increased cognitive load between \textit{hatching} and \textit{halo} (p $<$ 0.0001), \textit{no \gls{virt}}, and \textit{halo} (p = 0.0008), \textit{stippling} and \textit{hatching} (p = 0.0003), \textit{no \gls{virt}} and \textit{hatching} (p = 0.0031). 
The PAAS questionnaire for the counting everything question showed a significant difference between different illustrative visualizations using a Friedman rank sum test ($\chi$2(3, N= 24)= 43.674, p $<$ 0.0001).
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied, comparisons showed significantly increased cognitive load between \textit{Hatching} and \textit{Halo} (p $<$ 0.0001), \textit{no \gls{virt}} and \textit{Halo} (p = 0.0008), \textit{Stippling} and \textit{Hatching} (p = 0.0041), \textit{no \gls{virt}} and \textit{Hatching} (p = 0.03618), and \textit{Hatching} and \textit{no \gls{virt}} (p = 0.0064).
All of the PAAS results found, shown in \autoref{fig:CountingPAASResults}, indicate that participants feel that the \textit{halo} \gls{virt} required the least cognitive load, while the \textit{hatching} and \textit{no \gls{virt}} \glspl{virt} were felt to require a much greater cognitive load.

All the PASS results for both were highly correlated $\tau = 0.77$ (z = 9.9632, p $<$ 0.001) to each other. showing that regardless of the task the participants were presented with, they felt these visualizations took a similar cognitive load.
% PASS - Separated between conditions
The correlation between the PAAS Questionnaire Results and how well participants believed they did on each condition. A significant negative relationship between them $\tau = -0.49$ (z = -6.4262, p $<$ 0.001). Showing participants' feelings at the end of the study and showcasing an accurate representation of how they viewed their performance.

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingPAAS.pdf}
    \caption[Outcomes from the PAAS questionnaire in the counting study. Lower results indicate lower cognitive load and higher results indicate higher cognitive load.]{Outcomes from the PAAS questionnaire in the counting study. Lower results indicate lower cognitive load and higher results indicate higher cognitive load.
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:CountingPAASResults}
\end{figure}

% SUS - Separated between conditions
The results from the SUS questionnaire showed a significant difference between different \gls{virt} using a Friedman rank sum test ($\chi2(3, N= 24)= 54.728, p < 0.0001$)
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied, comparisons showed significantly increased cognitive load between \textit{hatching} and \textit{halo} (p $<$ 0.0001), \textit{no \gls{virt}} and \textit{halo} (p $<$ 0.0089), \textit{stippling} and \textit{halo} (p = 0.015) \textit{stippling} and \textit{hatching} (p = 0.0019), and \textit{no \gls{virt}} and \textit{hatching} (p = 0.0022) (see \autoref{fig:CountingSUSResults}).

% Correlation between PAAS and SUS to the final questionnaire
The correlation between the SUS Questionnaire Results and how easily participants found the visualizations to use was tested where there was a significant relationship between them $\tau = 0.48$ (z = 6.2974, p $<$ 0.0001). Showing a mildly strong positive correlation between participants' feelings about their ability to use the \glspl{virt} for this task.

\begin{figure}[!b]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingSUS.pdf}
    \caption[Results of the SUS questionnaire for the counting study.]{Results of the SUS questionnaire for the counting study.
    Significance differences are displayed as the lines on the right side of the graph stars indicate significance (* = p $<$ 0.05; ** = p $<$ 0.01; *** = p $<$ 0.001).
    }
    \label{fig:CountingSUSResults}
\end{figure}

Responses for the question "\textit{How easy was it to look at objects inside of other objects using this visualization?}" were analyzed using a Friedman rank sum test and showed a significant difference between the various \gls{virt} ($\chi$2(3, N= 24)= 49.392, p $<$ 0.0001). 
Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied. Comparisons showed participants found it significantly more challenging to see through the \textit{hatching} and \textit{halo}, \textit{no \gls{virt}} and \textit{halo} (p $<$ 0.0001), \textit{stippling} and \textit{halo} (p = 0.0012), \textit{stippling} and \textit{hatching} (p = 0.0005), and \textit{no \gls{virt}} and \textit{hatching} (p = 0.0030) \glspl{virt} (see \autoref{fig:CountingSeeThroughResults}).

%(Optional) Correlation between the voices and gestures and PAAS questionnaire

\section{Discussion} \label{sec:Disscussion}

\begin{figure}[!tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter5/Images/CountingEase.pdf}
    \caption{Results for the question \textit{"How easy was it to look at objects inside of other objects using this visualization?"}}
    \label{fig:CountingSeeThroughResults}
\end{figure}

The main findings from the \autoref{sec:sdfResultsPerception} show that the \textit{Halo} \gls{virt} was the most efficient and preferred of all of the visualizations when using \gls{ost} \gls{ar} devices.
Both the \textit{Hatching} and \textit{Stippling} seemed detrimental to this task. 
The following sections will detail how these conclusions were reached based on the five hypotheses.

\subsection{Accuracy (H.1 \& H.3)}
% talking about the results from the perception study as a whole
The results in \autoref{sec:sdfResultsPerceptionAccuracy} support \textbf{H.1} with the \textit{halo} effect improving the counting task performance.
In the \textit{Count Nested} task, when counting only the objects within the larger blue regions, \textbf{H.3} was unsupported as the \textit{halo} did not significantly affect performance.
Results in both tasks seem to indicate similar results between the \gls{virt} conditions.
This indicates that participants did not find the extra information too distracting when using different \glspl{virt}, but also shows that the \textit{halo} \gls{virt} is constantly easy to use. 

% isolating these down to just the insights we gained about the Halo visualization
\textit{Halos} gave participants a better understanding of the volume. 
Especially when identifying the approximate location of the green objects. 
In the \textit{Count Nested} task, participants could not count the wrong amount of objects. 
This effect can be seen in the lack of significant interaction effects between the different amounts of objects shown in \autoref{fig:CountingEveryThingMainResultsAllShownLineGraph}. 
Compared to using \textit{no \gls{virt}} at all, the \textit{halo} technique performs significantly better with high amounts of data. 
This indicates that the \textit{halo} visualization supports understanding a much larger amount of objects than any of the other \glspl{virt}.
Participants mentioned that \textit{halos} gave them a clear and easy-to-understand way to conceive the volume and had almost no critiques.

% talking about the findings regarding hatching
\textit{Hatching} is not intuitive when used in MR, as participants seem to have found it misleading or confusing.
\autoref{fig:CountingEveryThingMainResultsAllShownLineGraph} indicates \textit{Hatching} effect adds to the complexity just as much as adding four more objects to count. 
It leads to much poorer results than \textit{stippling}, a visualization that was hypothesized to be similar. 
As it covers the object, participants seem to find it hard to track what participants are looking at while they move their heads around.

\subsection{Time Required (H.2 \& H.4)}
The results found in \autoref{sec:sdfResultsPerceptionTimeRequired} do not support \textbf{H.2} as no significant differences were found regarding the time results for the \textit{Count All} task. 
However, in the \textit{Count Nested} task, \autoref{fig:CountingOnlyWithinTheBlueTimeTaken} showed participants were fastest when using the \textit{halo} visualization and slowest using the \textit{hatching} visualization, which also did not support \textbf{H.4}. 
It showed us that the volume rendering effect was quite strong as participants were more accurate when using no Volumetric Illustrative Rendering Techniques (\gls{virt}) than \textit{hatching} and \textit{stippling}, but they were also significantly faster when using this \glspl{virt}.
This may mean that these visualizations gave them some false information regarding where the objects were.


\subsection{Participant Behaviour (H.5)}
% headset motion behaviours
Overall, \autoref{sec:PerceptionBehavioralResults} showed that embodied cognition was not a major factor for this study. 
Participants tended to move faster with the \textit{Counting Nested} task than they did for the \textit{Counting Everything} across all of the behaviours that were measured.
Participants did not seem to point at any objects to count them or to use their hands in any context. 
Participants tended to move their heads less when there were more objects to view.
Overall, \textbf(H.5) was shown to be false.
The exact types of tasks that are required to trigger Embodied Cognition when looking at visualizations are still unknown~\cite{Jang2017}.
It is likely that the task chosen was either not cognitively difficult enough for the participants or the metaphors and clustering chosen to utilize did not trigger a correct metaphor~\cite{Wilson2013}. 
Although \autoref{fig:HeadsetMovementPerception} head motion seems to have been utilized. 
lowering the amount of movement required for participants to be able to see the volume of all the objects in the volume, but it seems to have required them to move more to confirm their location in the \textit{Counting Nested} task.

% Talking about the motion behaviour results in more detail
All the graphs in \autoref{fig:HeadsetMovementPerception}, \autoref{fig:PerceptionSpeedHandsMoved}, and \autoref{fig:PerceptionEyeGaze} show that \textit{Counting Nested} and \textit{Counting Everything} caused participants' 
tended to some difference between both tasks these results can be seen in \autoref{fig:CountingOnlyWithinTheBlueTimeTaken}, \textit{Halo} where it can be seen that \textit{Halo} caused the participants to move around the slowest/least.
\autoref{fig:HeadsetMovementPerception} illustrates for the \textit{no \gls{virt}} condition, participants expressed a large range of hand and head speeds when they felt that they needed to move around more with the \textit{Hatching} and \textit{Stippling} \gls{virt}.
This change in behaviour is likely because depth perception was not a huge concern when counting all of the items, so they just needed to focus on identifying various objects.
It does identify that participant's movements are less erratic when \glspl{virt} are utilized.

% Eyegaze discussion
The Velocity of participants' Eye Gaze seems to have functioned differently on both their hand and head velocities.
\autoref{fig:PerceptionEyeGazeAmountOfObjectsToCount} indicates the participants' eye movements are not reliant on the number of objects in the scene as their eye movement velocity seems to stabilize when they have more objects to count. 
This can also be noted as \autoref{fig:PerceptionEyeGaze} showed participants moved their eyes less on the counting everything task, but \textit{Stippling} \gls{virt} seemed to require participants to move their eyes more. 
This may have been to get a better idea of the structure the participants were looking at or to look through various stipples due to their larger sizes, or it could have been due to the stippling task having more, requiring more movement to see around.

% Eyegaze in relation to the amount of object
The increased movement in the eye gaze, which is seen in \autoref{fig:PerceptionEyeGazeAmountOfObjectsToCount}, may indicate a sign of embodied cognition where the users seem to be struggling to count the largest numbers for the counting everything task~\cite{sep-embodied-cognition, Wilson2013, Raab2019}.
However, these results are not almost inverted for the \textit{Counting Nested} task, where stippling causes the least amount of eye movement.
This may be caused by having the occlusion showing what objects are nested more clearly than they would have been shown with the \textit{Halo} or \textit{no \gls{virt}} conditions. 
%This can also be seen to a lesser degree in \autoref{fig:CountingEveryThingMainResultsAllShownLineGraph} where the more objects increased the difficulty as expected.
%This seems to hit a point at eighteen to twenty objects, making this task much more difficult, like the pilot and other studies that have run similar studies~\cite{Yoghourdjian2018}.

\autoref{fig:PerceptionSpeedHandsMoved} There seems to be little difference between the \glspl{virt} when participants move their hands. 
When performing the \textit{Counting Nested} task, it seems the participants moved their hands much more when stippling \gls{virt}.
This may have been to compensate for the lack of eye movement, indicating when counting fewer objects users prefer to track objects by using their hands rather than moving their eyes when they are using the \textit{stippling} \gls{virt}.

\subsection{Subjective Results (H.6)}
The results in \autoref{sec:PercptionSubjectiveResults} showed that both \textit{Hatching} and \textit{Stippling} were perceived to be much harder to complete the tasks with.
This was a reasonable response since users seemed to struggle most with these tasks.
The correlation between the PAAS and SUS questionnaires with the post-study questionnaire seems to have also indicated that participants were sure about this decision in the moment regardless of the order they experienced the \glspl{virt}.

% Talking about some qualitative findings
In \Cref{app:Chapter5Comments}, participants were asked to explain their responses. Participants who disliked the \textit{hatching} visualization mentioned that it was distracting, some of them had to move their heads in strange ways to be able to tell what objects were what, and overall the visualizations were criticized for being too opaque, participants were pleased however that is was easy to tell the boundary of the shapes, the depth within the objects and Participants made it clear what objects where distinct.
Interestingly, the \textit{halo} and \textit{hatching} \glspl{virt} moved dynamically, but only the \textit{hatching} \gls{virt} was criticized for it.
This is likely due to the lack of occlusion that the \textit{halo} has, while the \textit{hatching} \gls{virt} covers a larger part of the face of the surface.
When Fisher et al.~\cite{Fischer2020a} performed their user calibration study, they also found that participants preferred to see an outline over all other conditions. 
If an athletically pleasing way of statically using the \textit{hatching} \gls{virt} is found, it would likely show different results. 

% talking about the insights gathered about the stippling and hatching visualization
\textit{Stippling} was still significantly less accurate and slower than using none of the \glspl{virt}. Performing better than \textit{hatching}, this seems to be because it was static, and the dots would not move or rearrange. 
In all cases, \textit{stippling} was worse than having \textit{no \gls{virt}} or using the \textit{halo} \gls{virt} when counting objects.
A large issue mentioned by the participants about this visualization was that it made it difficult to tell where the edges of the objects were. 
%One addressable issue to this algorithm is that it should probably get much dense when objects much denser when they are reaching the outset of the objects
Several participants mentioned that the dots allowed them to easily identify the exact location of the objects within the volume. 
This was because this visualization was relatively stable. While various dots would appear and disappear, they would not move like \textit{hatching}, allowing the participants to identify patterns of the objects. 

% % Needs to have a conclusive paragraph
% \section{Limitations and Future Work}

% This paper discusses the limitations of viewing volumetric data in augmented reality without utilizing real-world counterparts to determine the accuracy possible when viewing volumetric data on an OSTAR device. 
% The next step that will be needed to be taken for this research would be to start working with studies like blind reaching and perceptual matching tasks to further determine what accuracy is possible when interacting with these volume renderings in the real world~\cite{Jamiy2019}.

% \subsection{Summary of Discussion}
% The \gls{Halo} \gls{virt} is the most effective technique for object perception in \gls{ost} gls{ar} environments, offering high accuracy and ease of use. In contrast, Hatching and Stippling were found to be less effective, increasing task difficulty.
% While the \textit{Hatching} \gls{virt} was criticized for being overly opaque and distracting, \textit{Stippling} provided a stable visualization but still reduced accuracy. 
% These results are likely applicable using either \gls{vr} or \gls{vst} \gls{ar} devices, but further research should be done to clarify these results. 
% Overall, these findings suggest that Halo-based visualizations are the most promising for enhancing spatial awareness in AR applications, while alternative techniques require further refinement to improve usability and effectiveness.


\section{Conclusion}
The \textit{Halo} \gls{virt} is very well suited to allowing people to determine \gls{ost} \gls{ar} while other \glspl{virt} seem to make it. 
This chapter showed the \textit{Halo} \gls{virt} improved the participants' performance when interoperating the volume.
Using either hashing or stippling with these effects made it considerably harder, especially when they were asked to find more objects.
This shows the \textit{Halo} \gls{virt} is the most useful \gls{X-ray Visualization} to use if you, a user, are required to inspect a volume.
Allowing for better communication of diagnosis for medical professionals~\cite{Mandalika2018} and clearer provide more detailed information for fields like education~\cite{Jang2017, Cheung2021}, material science~\cite{Vicente17, Groger2022, Okuyan2014} and geology~\cite{Zehner2021, Mathiesen2012}. 
Moving forward, this disertaion will look at the impact of \glspl{virt} on depth perception and test their viability with being used for a more interactive purpose.

This research showed:
\begin{itemize}
    \item Participants can better observe objects within a volume when using the \textit{Halo} \gls{virt}, and this effect seems to become stronger when more items are being visualized. 
    \item The \textit{Stippling} and \textit{Hatching} \glspl{virt} impaired the user's ability to access what objects were inside of the object based on what was in the volume. 
    \item Participants found the \textit{Halo} \gls{virt} to be preferable compared to the other \glspl{virt}, but the \textit{Hatching} \gls{virt} seemed to cause some discomfort with participants while performing this task.
\end{itemize} \newpage 
    \glsresetall
    %\include{Chapter6/Chapter6Brief}
     \newpage \chapter{The Limits of Depth Perception when Using Volumetric Illustrative Rendering Techniques} \label{chap:DepthPerception}
% What is this chapter looking into?
It has been shown that X-ray visualizations have improved the perceived depth mismatch caused by a visual misalignment when a virtual object is rendered behind a real object~\cite{Bajura1992, Avery2008, Sandor2010, Kalkofen2009}. 
\autoref{Chap:X-ray Implemntion} shows that \glspl{X-ray Visualization} techniques can impact depth perception, which is important to understand before they are proposed for activities that require precise hand-eye coordination tasks, especially activities like surgery. 
Occlusion is a powerful depth cue that uses virtual graphics to block physical world objects to influence the perceived depth. 
However, it can also obscure key information in some scenarios.
%Occlusion is a powerful depth cue but can obscure key details. 
The study presented in this chapter advances the current research knowledge by exploring the impact of \glspl{virt} on a user's depth perception when paired with \gls{dvr}.
%This study examines the impact of \glspl{virt} on a user's depth perception when paired with \gls{dvr}.

\begin{figure}[tb]
    \centering
    \includegraphics[width = \columnwidth]{Chapter6/Images/DepthPerceptionStudyPhotos.png}
    \caption[This figure shows the environment in which this study took place and presents each of the \gls{virt} conditions utalized in this study.]{This figure shows the environment in which this study took place and presents each of the \gls{virt} conditions utalized in this study. 
    All these images were taken using a HoloLens2 camera, viewing a user engaging with the study. 
    (a) Illustrates the \textit{Halo} \gls{virt} and shows a participant pressing one of the task buttons present in the study. 
    (b) Shows the \textit{Stippling} \gls{virt} being observed by the participant. 
    (c) Displays the \textit{No \gls{virt}} condition from the participant's perspective. 
    (d) Is an image of the \textit{Hatching} \gls{virt}.}
    \label{fig:DepthPerceptionPhotos}
\end{figure}

\gls{dvr} on desktop displays provides a good sense of depth when looking through an object. Our understanding of how it is affected when using a \gls{ar} \gls{hmd} is relatively unknown past the understanding that the previous devices struggled to run the visualizations efficiently enough~\cite{Sielhorst2006}.
Prior literature that focused on how humans understood how \glspl{virt} affects depth perception tends to include the following:
\begin{itemize}
    \item Spatial awareness is improved in VR when removing distracting details from the physical environment~\cite{Wijayanto2023};
    \item Illustrative geometric effects like those found in \gls{X-ray Vision} can better join the virtual world and the virtual world together by providing a clear reference where both of the relationship between them~\cite{Martin-Gomez2019};
    \item Illustrative effects are utilized in medical \gls{ar} as they are more tested at dealing with the depth mismatch when displaying structures that are located within each other since they can illustrate depth perception in ways that are not possible with transparency alone~\cite{Hansen2010, Lawonn2013};
    \item \autoref{fig:DepthPerceptionPhotos} Shows illustrative effects are also pleasing to see rendered inside of other objects as they are better designed to showcase ~\cite{Lawonn2017};
    \item Illustrative effects can convey curvature clearer than shaded meshes~\cite{Lowonn2013};
\end{itemize}
However, the literature has not discussed the impact of illustrative effects in tandem with \gls{dvr} regarding depth perception.
If \glspl{virt} are to be found useful for high-precision tasks, their accuracy with depth perception should not decrease depth perception and ideally improve or remove any mismatch.
\autoref{Chap:X-ray Implemntion}, demonstrated that having any \gls{X-ray Vision} effect impaired depth perception.
\autoref{Chap:VolumetricX-rayVision}, it was shown that both \textit{Stippling} and \textit{Hatching} may be detrimental to understanding the layout of a given volume.

This chapter's investigation required suitable methodologies to measure the impact of VIRTs, with the expectation that they would be used in a precise and stressful situation.
Similar to the study design used by Nagata~\cite{Nagata1983} and Chen et al.~\cite{Chen2018}, a \gls{twofc} psycho-physical experiment was conducted.
Nagata~\cite{Nagata1983} showed the participants an image or video displayed, either on an image, television, or stereoscopic display between two separate experiments to determine the impact of motion and stereopsis on depth perception.
The results from Nagata~\cite{Nagata1983} formed the foundations upon which a lot of depth perception research is based today, detailing the maximum thresholds of motion, binocular parallax, accommodation, and convergence while also stating some of the conditions that affect these factors.
Chen et al.~\cite{Chen2018} conducted a depth perception experiment examining the impact of binocular distortion using motion and focusing on creating depth cues that conflicted with each other by presenting slightly different phenomena in each eye showing that a more occlusive element would normally appear closer to the viewer if only shown in one eye.

% based on psycho-physical experimenting principles conducted, searching for the \gls{jnd} and \gls{pse} of the default volume rendering and all the \glspl{virt}.
% Due to the lack of similar literature in this area, it is first important for us to determine the thresholds where \gls{dvr} is able to provide.


% This chapter aims to determine the impact of using \glspl{virt} as an \gls{X-ray Visualization} to depth perception when compared to just using plan \gls{dvr} within a near field.
% The participant comments from \autoref{Chap:X-ray Implemntion} explained that participants struggled to notice if the \gls{X-ray Vision} effect was working or not due to their limited focus on the \gls{ost} \gls{ar} \gls{hmd}'s screen.
% So, this visualization was tested without \gls{X-ray Vision} as it would only distract the user from the algorithm they were performing. 

% %This chapter aims to Investigate the accuracy of depth perception that \glspl{virt} can achieve.
% %To ensure this is handled in the best possible way, we have designed this study to purely analyze the \gls{dvr} algorithm to accommodate a method of 
% By utilizing a study has been designed to evaluate how well \gls{dvr} can convey an accurate depth perception on \gls{ost} \gls{ar} devices. 
% It also explains at what thresholds \gls{ost} \gls{ar} displays can convey an accurate portal of depth in any situation.
% % Need to combine these
% The first \gls{X-ray Vision}~\autoref{Chap:X-ray Implemntion} user study observed vision techniques influenced the depth perception to a level users could place a visualization to tell if an object is inside of another object.
% This chapter aims to determine the impact of using \glspl{virt} in an \gls{X-ray Visualization} when compared to using plan \gls{dvr} within a near field.

\section{Volumetric Illustrative Rendering Techniques Impact on Depth Perception on Ocluar See Though Devices}

There are several challenges using current \gls{ost} displays that make it difficult to present virtual information with intuitive depth cues.
Current \gls{ost} \gls{ar} displays display blacks and darker colors as transparent, making shading realistically difficult.
Since \glspl{virt} can utilize bright occlusive colors, they can act as an alternative to shadows when used for \gls{ost} \gls{ar} displays. 
\glspl{virt} can mitigate many of the issues regarding transparency with \gls{ost} \gls{ar} displays.
This allows \glspl{virt} to convey a sense of depth that would not otherwise be possible when only using \gls{dvr} graphics.

Previous literature shows that \glspl{virt} are able to provide a better opportunity to convey depth perception within 2D mediums.
\textit{Stippling} has been shown to allow the user to better understand an object's surface and how it is shaped and enables a user to see layers between objects~\cite{Lawonn2018, Maciejewski2008}. 
\textit{Hatching} has been used to show depth in several textbooks~\cite{gray1877anatomy, Vinson1967, Philbrick2022} and communicates depth by overlapping lines and using different angles, leading to darker objects that are deeper into the object and hidden from the light~\cite{Ritter2006}.
\textit{Halo's} and feature lines have been shown not just to be an effective \gls{X-ray Vision} technique to comprehend and is also a method for more clearly indicating parts of the various parts of the systems~\cite{Bruckner2006} that can also be used to determine the approximate depth of objects~\cite{Salah2006}.
% Kalia2019 was removed from the ciation on the line above

% All of the above \glspl{virt} have shown that they are capable of showing distinctive artifacts in the previous chapter \autoref{Chap:PerceptionStudy}.
% Only hatching and Stippling have had their effect on depth research.

%When working with direct volume rendered images, Halo's and feature lines have been shown not just to be an effective \gls{X-ray Vision} technique but also as a method for more clearly indicating parts of the various parts of the systems. The effects highlighting the outside of an object have been previously seen by \cite{Bruckner2006}.


\section{User Study}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthBasedVolumesReducedImage.png}
    \caption{The \glspl{virt} used for this study}
    \label{fig:DepthBasedVolumesReducedImage}
\end{figure}

% To determine the performance of each \gls{virt}, this study performs a two-alternative forced-choice (2AFC) psycho-physical study detailing the limits of the \gls{dvr} on an ocular see-through device. The participants will be asked to judge what is closer to them in a virtual scene with only a single noisy sphere.
% Each of these noisy spheres where using each of the 3 different \glspl{virt} and a baseline (no \gls{virt}).


% explaining the limitations taken into account
%This is one of the first studies using modern headsets focused on viewing volume rendering using a \gls{ar} device.

\subsection{Research Questions}
This study investigates the accuracy that is achievable when using \glspl{virt} in \gls{dvr} to answer the following research questions:
%Research Questions
\begin{enumerate}[label=RQ.\arabic*, resume=ResearchQuestions]
    \item What is the minimum difference in depth that participants can reliably distinguish between volumetric objects, independent of any given \gls{virt}? 
    \item What impact do the \glspl{virt} have on the participant's ability to distinguish the difference in depth of volumetric objects reliably?
    \item What is the impact of different \glspl{virt} on the participants' self-reported cognitive load and usability when determining the depth of a volumetric object?
    \begin{enumerate}[label=RQ.3.\arabic*]
        \item Is there a noticeable effect between participant's behavior (hand, head, and eye movements) between different \glspl{virt}?
    \end{enumerate}
\end{enumerate}

To determine the performance of each \gls{virt}, this study conducts a \gls{twofc} psycho-physical study detailing the limits of depth perception when utilizing both \gls{dvr} and \glspl{virt} on an ocular see-through device. 
The participants are asked to judge what is closer to them in a virtual scene with only a single noisy sphere. Each of these noisy spheres is presented using one of the three different \glspl{virt} and a baseline (no \gls{virt}).

% talk about people who have done similar studies

A psycho-physical \gls{twofc} questionnaire study presents participants with pairs of options and requires them to select the one that best corresponds to their judgment. 
This study design utilized a \gls{twofc} questionnaire to determine at what depth a user could no longer judge the difference in depth between two objects.
This choice was influenced by research done in \gls{X-ray Vision} investigating similar research investigating the effect visual cues can have on depth perception in \gls{ar}~\cite{Otsuki2017, Krajancich2020} and research looking at methods capable of influencing depth perception in general~\cite{Chen2018, Adams2021}.
This type of study enables us to determine what level of depth perception is achievable when using an OST AR headset with volume rendering. 

% high-level overview of the study design used for this study
The threshold at which a user can no longer reliably judge the difference between two conditions in a \gls{twofc} experiment is referred to as the Just Noticeable Difference (JND). The point where the two objects look identical to the Point of Subjective Equality (PSE).
Hence, these studies are aimed at finding the limits of human perception and have been performed in many depth perception studies to determine the thresholds of human perception of various objects~\cite{Nagata1983, Chen2018}. 


\subsection{Hypothesis} \label{sdf: User Studies Study 2 Hypothesis}
\begin{enumerate}[label=H.\arabic*]
    \item participants will be unable to distinguish differences in depth less than 1.25cm with any VIRT (R1).
    \textit{Studies observed depth perception using AR within the near field can achieve accuracy of just over 2cm~\cite{Al-Kalbani2019} to just under 3mm~\cite{Swan2015}. Our study aimed to improve depth perception through volume rendering alone. There is a chance that the transparency of the volume will hinder the precise nature of nature possible};

    \item Either the \textit{hatching} or \textit{Stippling} VIRTs will have the lowest Just Noticeable Difference (JND).
    \textit{The use of hatching-like visualizations to show depth perception is a helpful depth perception cue when trying to detail the interior of an object~\cite{Martin-Gomez2021}};

    \item Either the \textit{hatching} or \textit{Stippling} VIRTs will have the closest Point of Subjective Equality (PSE) to zero (R2).
    \textit{The PSE and the JND are likely going to be correlated in this experiment}

    \item Participants will be able to determine depth faster using the \textit{Hatching} VIRT (R2). 
    \textit{Previous work by Martin-Gomez et al.~\cite{Martin-Gomez2021} has shown that Hatching can aid depth perception in AR \gls{X-ray Vision} when used to demonstrate depth through/within an object, and results form \autoref{Chap:X-ray Implemntion} showed that a similar technique was the most accurate among the investigated techniques};

    \item Participants will move their heads, hands, and eyes faster when using the \textit{Halo}. \gls{virt} (R.3.1). 
    \textit{Embodied cognition did seem to place a role in \autoref{Chap:PerceptionStudy} where we found high rates of movement on more challenging conditions. It is likely that a difficult problem~\cite{Wilson2013}. Embodied condition in this study would likely indicate that the participant was struggling and trying to align the virtual world with something real~\cite{Wilson2013}, but could also be seen in rapid eye movements};

    \item The \textit{halo} VIRT will be the most preferable and the least cognitively demanding for the depth perception task (R3).
    \textit{When other studies have utilized similar methods, they have found that a silhouette or outline has been more appealing to participants~\cite{Martin-Gomez2019, Fischer2020a}.};

\end{enumerate}

\subsection{Participants} \label {sec: User Study Depth Perception Participants}
24 participants were recruited for this study from a pool of students, faculty, and staff from the University of South Australia, each aged between 19 and 37 years old ($mean = 26.79, \sigma = 4.96$), 7 female, 17 male, with little (8) to no (16) experience with medical data.
Their experience using \gls{mr} systems varied, with 8 who used \gls{mr} daily, 5 who used \gls{mr} weekly, 7 who used \gls{mr} monthly, 1 who used \gls{mr} rarely, and three participants whose it was their first time using MR.
All participants had to be asked to declare if they had any major vision impairments that could not be corrected during the recruitment process. If this were the case, they would have been asked not to attend the study. 


\subsection{Study Design} \label{sdf: User Studies Study 2 Study Design}
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthPerceptionNoEffectConditions.png}
    \caption[An example of what the volume looked like from the outside for each depth]{An example of what the volume looked like from the outside for each depth (displayed in the columns shown in cm) from three sides (a front-on view (Front), a birds-eye view(Top), and a side-on view(Side)). All of these images use the same noise calculation. The reference object is the blue object on the right-hand side of each volume.
    The left side of this image shows the blue variable object when it is closest to the participant, while the right side shows when it is the furthest away.}
    \label{fig:AllDepthsImage}
\end{figure}

This study utilizes the noisy sphere detailed in \autoref{sec:VolumetricDataGeneration} and asks participants to assess what blue object in the volume was closest to them.
This not only allows for validation of the structure of the volumes but also controls the noise and shape of the various cells within the volume. It also allows the creation of volumes with elements of similar but slightly differing artifacts, creating a linear range of volumes shown in \autoref{fig:AllDepthsImage}.
Every distinct iteration of this study utilized a unique volume that could be adjusted to suit any condition. 
The row labelled "Front" in \autoref{fig:AllDepthsImage} shows how difficult this choice made it to tell various depths when they are displayed in 2D. 
However, MR allows for a much greater sense of the depth of these volumes than this image can convey. 
This study aims to learn how good this sense of depth is~\cite{Heinrich2021}. 

\subsubsection{Task}
% What this study entailed in detail
%This assessment was done using the noisy spheres developed in \autoref{sec:VolumetricDataGeneration} and was utilized again for this study. % with some slight modifications.
Two task buttons were made available to participants to press with either of their hands. Participants would press the task button on the same side as the blue object they found closer to them to answer this.
The 3D object was rendered in one utilizing one of the depths shown in \autoref{fig:AllDepthsImage} that participants would be asked to judge would sit at various points along a depth axis and move between 2.75cm and -2.75 cm, with 0 being used as a baseline for each comparison. In comparison, the other side was variable and could be placed within 2.75cm on either side of the reference object.

\autoref{fig:AllDepthsImage} shows all the different conditions from three different angles. 
The reference object would randomly be swapped half the time to avoid bias from handedness.
The system would save and convert the participant's answer to whether or not the participant thought the variable side was in front of or behind the reference object (Shown in \autoref{fig:DepthPerceptionFrontAndBackComparision}), as the participant was not informed what side the reference object was on. 
%The noise used to create these volumes would be drawn from two sets of random values generated from two unique hashes, One for the outside and one for the internal objects.
They used the technique detailed in \autoref{sdf: User Studies Study 2 Depth Implementation Alterations}, and both objects in each condition appeared identical. %as each position used the same noise for both conditions.

\begin{figure}[tb]
    \centering
    \includegraphics[width = \columnwidth]{Chapter6/Images/DepthPerceptionFrontAndBackComparision.png}
    \caption[The furthest extremes of the visualization shown in enlarged using the \textit{No \gls{virt}} condition.]{The furthest extremes of the visualization shown in enlarged using the \textit{No \gls{virt}} condition. (Columns) represent the depth of the objects; and (Rows) represent different viewpoints of the volume. (Top) the viewpoint the participants could see; and the (bottom) a bird's eye view of the volume.}
    \label{fig:DepthPerceptionFrontAndBackComparision}
\end{figure}

\subsubsection{Procedure}
% what the participants had to do
Participants repeated this process for 80 iterations (ten different depths, two sides, repeated four times) for each condition, after which they were asked to answer a questionnaire. 
%They would then need to follow the main process for each condition by doing a questionnaire after completing 80 iterations (ten different depths, two sides, repeated four times) for each condition.
At the end of each condition, they would perform a SUS and PAAS questionnaire. 
At the end of the study, they would be asked to complete a post-study questionnaire to gain their final understanding of the entire study.

% Pre Study
Before the study, participants were required to read and fill out an information sheet detailing the study, sign a consent form, and complete the demographic questionnaire. 
The structure of the study was explained to the participants. 
For the training task, they were then asked to sit down and sent through 16 different training exercises that covered conditions that were 5cm away from the reference object (both behind and in front of the reference object).
No data was recorded during this phase, and the participants were encouraged to ask the examiner questions. 
Subsequently, participants were allowed to take a break before starting the actual study.

\subsubsection{Determining Maximum Thresholds}
% Talk briefly about the pilot studies and how I chose the final parameters
Since there was little prior work on depth perception using quantitative methods utilizing \gls{dvr} on similar devices, a pilot study was run to inform the selection of parameters for the main study.
While the pilot study was conducted under similar conditions to the main study, its primary purpose was to refine the experimental parameters by identifying the depth thresholds at which participants began to struggle with determining whether an object was in front or behind the baseline object.
This study consisted of four participants, each presented with the same three \glspl{virt} (Stippling, Hatching, and Halo) and the baseline condition.
The most extreme depths were over 5cm away from the reference object (in front and behind). 
This study ran for a total of 36 iterations comprising nine different evenly spaced depth levels, presented from two sides and repeated twice. 

The data from the pilot study showed participants tended to struggle to accurately determine depths within 2.5cm of the reference object. 
This informed our choice to place both endpoints 2.75 cm away from the reference object. 
We also chose to increase the number of depths the participant would view to 10 because participants showed frustration when the objects seemed to be at an identical depth, and the data gathered from this was not as useful as participants could not determine in this case which volume was the baseline as they where both identical and the baseline was on a random side. 
The longest it took to determine any depth was just under 20 seconds, and iterations for the study were based on the findings from this pilot.

\subsection{Study Environment}

\begin{figure}
    \centering
    \includegraphics[width = \columnwidth]{Chapter6/Images/SketchOfStudy2TableLayout.PNG}
    \caption{Top-down view of the layout of the participants' study area (their desk).}
    \label{fig:depthPerceptionStudySpace}
\end{figure}

This study occurred in the same lit study room as the previous one in \autoref{Chap:PerceptionStudy}. It contained multiple light sources (windows and stage lighting) and utilized the space shown at the center of the room shown in \autoref{fig:depthPerceptionStudySpace}.
Participants were seated at a desk in the center of the room and observed throughout the study by a researcher. 
The visualization would appear 15cm above the visualization marker on the desk.
Participants could move the buttons to any area on the desk they chose, but they always started in the position identified in \autoref{fig:depthPerceptionStudySpace}.
This study was facilitated and built upon using the Unity Engine\footnote{\url{https://unity.com/releases/editor/whats-new/2019.4.3}}, with the Microsoft Hololens 2\footnote{\url{https://www.microsoft.com/en-us/hololens/}}  (\autoref{fig:PerceptionStudyPhotos}(b)) as the display modality via a wired connection utilizing the Holographic Remoting Player\footnote{\url{https://learn.microsoft.com/en-us/windows/mixed-reality/develop/native/holographic-remoting-player}} to a desktop PC. The desktop PC featured an Intel i5 with an Nvidia Geforce GTX 2070 GPU, running at 60-90 FPS during the study. 
The computer allowed participants to answer questionnaires without leaving their seats, but they were free to do so.  

\subsection{Generation and Placement of Volumes} \label{sdf: User Studies Study 2 Depth Implementation Alterations}
\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/Non Euclidean Direct Volume Rendering.png}
    \caption[An Example of Euclidean \gls{dvr} and non-Euclidean \gls{dvr} of the same volume.]{
    An Example of Euclidean \gls{dvr} and non-Euclidean \gls{dvr} of the same volume. The left side shows the direction of the Euclidean method of the transparent sphere casting method, whereas the right side shows the non-euclidean version of ray tracing that was used. Blue circles show the sphere marching into the \gls{sdf}, green dots show the ray marching steps, and the black line shows the ray these elements are following.
    }
    \label{fig:NonEuclideanDVR}
\end{figure}

% Explain the purpose of the section
This study aims to allow us to understand the limits of depth perception using \gls{dvr} on an \gls{ost} \gls{ar} \gls{hmd}.
This required a visualization that could be modified in a controlled way but would still appear to look individual each time it was created. 
%As this is an in-depth perception study; there was a much more predictable method of assigning values to allow better us to understand what depths could or could not be understood purely by looking at them. 
The volumes utilized in this study used a set of controllable parameters, enabling us to conduct a highly controlled depth perception study. 
This section explains the methods that were used to create and manage the volumes required for such a study. 

%Explain what was altered from the previous study to depth perception one
Applying Perlin noise throughout the volume led to differently shaped objects appearing within the volume at different areas.
This would have made testing depth perception to an accurate level a difficult feat since differently shaped objects would have presented a different front, back, and center of a shape.
%To overcome this, both objects seen on either side were placed in the same area in the same place as if they were in different locations.
To ensure a consistent comparison between both objects, noise was applied to them as if they were in the center, while they appeared in different positions visually.

% Explain the non-ego-centric effect (Split into two paragraphs for readability)
%Several changes were made to the behavior of the two inner regions. 
This required several changes to how the volume was rendered compared to how it was detailed in \autoref{sec:VolumetricDataGeneration}.
Adjusting the offset of the ray's position left and right by approximately a quarter of the diameter of the volume allowed us to render the same shape in two separate positions using the same noise.
This triggered each of these sides to render at the same time when the ray would display the volume when the ray when the offset was acceptable. 
This created two identical sides, a variable and a reference object side.

Both the variable object and the reference object were placed in a position where they would not extend beyond the outer red volume (Seen in \autoref{fig:NonEuclideanDVR}) and could not intersect with each other.
Each side of this volume consisted of an outer green object identical to the other side and a smaller blue object that could be moved closer or further away from the participant. 
If the small blue object and other objects had the same noise and were in the same location, they would look identical. However, since they are always positioned differently, they appear as objects with distinct shapes. 
Regardless of their shape, these objects were consistently placed, causing them to appear either in front of or behind the reference object.

% Explain why I reversed the hierarchy
The overall implementation is unchanged regarding the red outer volume from what was mentioned in \autoref{sec:VolumetricDataGeneration}.
In the previous chapter, the color green was chosen because the contrast between red and blue colors was not as great as that between green and both the red and blue colors. 
This same logic was applied to this study, but since there were fewer objects for the participants to consider and they all used the same template, contrasting the green against the blue and red colors ensured the volume was easy to view.
For this study, placing the blue object inside the green one further aided the contrast between the colors, making the blue object stand out more.

%\label{sec:sdfResultsDepthPerception}
%Show results for the depth perception study\\

\section{Results} \label{sdf: Results}
This section presents the values collected throughout this user study.
All values within this with a p-value $<$ 0.1 have been noted. All p-values $<$ 0.05 are considered significant. 


\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{Chapter6/Images/DepthResultsPerCondition.pdf}
    \caption[Four graphs which show impact to the depth perception caused by each condition in this study's response data, and sigmoid functions for each VIRT.]{Four graphs which show impact to the depth perception caused by each condition in this study's response data, and sigmoid functions for each VIRT. The top and bottom grey lines indicate the area where the participants could not accurately determine the difference in depth. The cross in the center of the image shows the exact point of zero, where participants should be unable to determine a difference. The confidence interval (CL 95\%) is shaded for each condition, and each point on the plot represents a participant's mean answer for this condition.}
    \label{fig:DepthResultsPerCondition}
\end{figure*}

\subsection{Psychometric Analysis of Depth Perception} 
A psychometric analysis of the participant response data was conducted to investigate the \gls{twofc} results. The amount of time that was required to determine the difference between the depths of the objects and the baseline used a linear mixed model and their details surrounding how much the participants' heads, hands, and eyes moved during this study. This approach accounts for individual differences between participants in repeated measures and allows us to examine the effect of the differences between the variable object and the baseline one~\cite{Kaptein2016, Adams2022}. 



\begin{figure}[!b]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthPerceptionResultsOneGraphWithMagnifier.pdf}
    \caption{Magnified view of the sigmoid functions showing the offset from zero and is the largest divergence for the JND at the 75\% line. }
    \label{fig:DepthPerceptionResultsOneGraphWithMagnifier}
\end{figure}


\subsubsection{Psychometric Analysis(H.1 \& H.2)} \label{sec: Psychometric Analysis}

\begin{table}[t]
    \centering
    \caption[The results from the point of subjective equality (PSE) in cm across all of the VIRTs.]{The results from the point of subjective equality (PSE) in cm across all of the VIRTs. The standard error of the mean (SEM) and the inferior and superior confidence interval (CI) are calculated.}
    \begin{tabular}{r|rrrr}
        Conditions & No VIRT  &  Halo/Outline & Hatching & Stippling \\
        \hline
        \hline
        PSE(cm) & -0.0855 & -0.0127 & -0.0849 & -0.1353 \\
        SEM (cm) & 0.0468 & 0.0446 & 0.0447 & 0.0447 \\
        \hline
        CI Superior & 0.0025 & 0.0800 & 0.05240 & -0.0476 \\
        CI Inferior & -0.172 & -0.1055 & -0.1724 & -0.2231 \\
    \end{tabular}
    \label{tab:PSEResults}
\end{table}

A sigmoid function was fitted to the participant responses as shown in \autoref{fig:DepthResultsPerCondition}. This allows us to determine at which point they could not tell the difference in depth between two virtually created volumetric objects.
% depths apart. 
%, allowing us to know at what point users cannot determine the different depths of an object within a volumetric structure. 
This method was chosen because it provides a good approximation of human behaviour~\cite{Harvey1986}.

In \autoref{fig:DepthResultsPerCondition}, the distance in front of or behind the ground truth is plotted on the horizontal axis. This value increases as the test object moves further away from the participant than the reference object at position zero. Likewise, the value decreases into the negative range as the object gets closer to the participant.
In theory, the further the absolute distance between the reference object and the test object, the more easily the participants can tell if it is in front or behind. 

We computed the point of subjective equality (PSE) of all the conditions. This is where the sigmoid function intersects at 50\% between in front or behind, indicating a position in participants where participants viewed the objects as equal.
This indicates if the object was viewed to be in front or behind where it was expected to be.
The results collected are displayed in \autoref{tab:PSEResults}.
%The PSE participants saw a disparity of -0.855 (SEM = 0.468, 95\%[0.0025, -0.172]) when no illustrative effect was used. 
%When Halo's PSE disparity was = -0.0127 (SEM = 0.0446, 95\% [0.0800, -0.1055]), The PSE disparity for Hatching was -0.0849 (SEM = 0.0447, 95\% [0.05240, -0.1724]), and The PSE disparity for Stippling was -0.1353 (SEM = 0.0447, 95\% [-0.0476, -0.2231]).
Using a pairwise T-test, which showed a significant difference between \textit{Halo} and \textit{stippling} (p = 0.0303) and \textit{hatching} and \textit{stippling} (p = 0.0490). 

We additionally computed the perceptual limits of the JND of all the VIRTs.
This limit was set for both 75\% of the time they guessed that the object was behind the reference object and 25\% % for when they thought the object was in front of the reference object. 
%The JND disparity for each condition was -0.7691 (SEM = 0.046, 95\%[-0.67, -0.86]) when no illustrative effect was used. 
%When Halo's JND disparity was = -0.8506 (SEM = 0.0473, 95\% [0.0800, -0.1055]), The JND disparity for Hatching was -0.7349 (SEM = 0.04326, 95\% [-0.6501, -0.8197]), and The JND disparity for Stippling was -0.7613 (SEM = 0.0459, 95\% [-0.6712, -0.8513]).
The results collected are displayed in \autoref{tab:JNDResults}.
Using a pairwise comparison, I found significance between \textit{halo} and \textit{hatching} p = (0.0498)

\subsubsection{Time Required (H.3)}

\begin{table}[!b]
    \centering
    \caption[Results for the just noticeable difference (JND) in cm across all of the \glspl{virt}.]{Results for the just noticeable difference (JND) in cm across all of the \glspl{virt}. The standard error of the mean (SEM) and the inferior and superior confidence interval (CI) are calculated.}
    \begin{tabular}{r|rrrr}
        Conditions & No VIRT  &  Halo/Outline & Hatching & Stippling \\
        \hline
        \hline
        JND(cm) & -0.7691 & -0.8506 & -0.7349 & -0.7613 \\
        SEM (SEM) & 0.0460 & 0.0473 & 0.04326 & 0.0459 \\
        \hline
        CI Superior & -0.6700 & 0.0800 & -0.6501 & -0.6712 \\
        CI Inferior & -0.8600 & -0.1055 & -0.8197 & -0.8513 \\
    \end{tabular}
    \label{tab:JNDResults}
\end{table}

%note times that it took for these to complete.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthTimeTaken.pdf}
    \caption{The mean time required for each VIRT in the depth perception study.}
    \label{fig:DepthTimeTaken}
\end{figure}
A \gls{lmm} was used to evaluate the different times required for each interaction between the different \glspl{virt} and depths experienced by the participants.
Using each participant as their random variable will reduce any differences in behaviours between participants unique to the participants~\cite{Kaptein2016}. 
The time required to determine what object was in front for each task showed a significant fixed effect between the VIRTs ($\chi$2(3, N= 24) = 32.2482, p $<$ 0.0001). The different depths showed no significant fixed effect, and no significant interaction effects were found. 
Post hoc pairwise comparisons were done using Tukey's HSD for multiple comparisons to further evaluate these findings. 
The post hoc pairwise comparison between VIRTs showed significant differences between \textit{Halo} and \textit{no VIRT} (p = 0.0047), \textit{halo} and \textit{stippling} (p $<$ 0.0001), \textit{hatching} and \textit{no VIRT} (p = 0.0042), \textit{hatching} and \textit{stippling} (p $<$ 0.0001).

\subsubsection{User Behavioral Analysis (H.4)} \label{sec:PerceptionbehaviouralResults}

\begin{figure}[!b]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthHeadSetVelocity.pdf}
    \caption{Box plots showing the average head movement velocity for each condition}
    \label{fig:HeadsetMovementDepth}
\end{figure}

This section focuses on how participants moved when the various \glspl{virt} were being used, with a focus on known behaviours that exhibit embodied cognition~\cite{Raab2019, Wilson2013}. 
This was done by measuring the overall distance participants moved their heads, gazes, and hands each iteration. 
This is then divided by the time each condition took, allowing us to view the average speed at which participants moved.
Tracking the average velocity is important for this particular study as there was no real task to be completed other than looking at two objects and pressing a button. A higher average velocity would have indicated a struggle with using the visualization~\cite{Raab2019, Wilson2013}. 
The headset and hand distances were tracked by calculating the amount of their position that had moved relative to the visualization between every 16ms and summing these results.
Their distance was not tracked when the hands were not in front of the participant.
Eye Gaze's distance was measured by tracking the distance a participant's eye gaze would move within a 2m radius from the participant's current position in each frame minus the same distance the head gaze would have provided. 

Each set of analyses for this section utilizes a \gls{lmm} to evaluate the different Speeds of hand motion, eye gaze, and head gaze between the different \glspl{virt} and depths experienced by the participants.
Using each participant as their random variable will reduce any differences in behaviours between participants unique to the participants~\cite{Kaptein2016}. 
Post hoc pairwise comparisons are run using Tukey's HSD for multiple comparisons on all significant findings found using the \gls{lmm}. 

% The model for the user's head motion when counting nested
The \gls{lmm} for the participants' head motion velocity while performing the task across all of the conditions showed a significant fixed effect between the various \glspl{virt} ($\chi$2(3, N= 24) = 123.8529, p $<$ 0.0001). 
The different depths showed no fixed effect, and no interaction effect was found.
The post hoc comparison between the \glspl{virt} showed significant differences between \textit{Halo} and \textit{no \gls{virt}} (p = 0.0001), \textit{Halo} and \textit{Hatching} (p $<$ 0.0001), \textit{Halo} and \textit{Stippling} (p = 0.0001), \textit{Hatching} and \textit{Stippling} (p = 0.0205), \textit{No \gls{virt}} and \textit{Stippling} (p = 0.0492).
These differences can be seen in \autoref{fig:HeadsetMovementDepth} as they moved considerably more when they were using the Halo \gls{virt} than in other conditions.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthHandVelocityThesis.pdf}
    \caption{Box plots relating to the speed at which participants tended to move their hands.}
    \label{fig:DepthPerceptionHandMovement}
\end{figure}

% Results for Hand Motion
The \gls{lmm} for the participants' hand motion velocity across the conditions showed a significant fixed effect between the \glspl{virt} ($\chi$2(3, N= 24) = 23.4039, p $<$ 0.0001), no significant fixed effect for the different depths was found. 
The interaction effect showed no significant results.
With the post hoc comparison between all of the \glspl{virt} showing \textit{No \gls{virt}} and \textit{Hatching} (p $<$ 0.0001), \textit{Stippling} and \textit{Hatching} (p $=$ 0.0062), and \textit{Halo} and \textit{Hatching} (p $=$ 0.0185).
These results can be seen in \autoref{fig:DepthPerceptionHandMovement} where it can be seen that participants moved their hands slower when using \textit{Hatching}.

% Results for Eye gaze for counting everything
The \gls{lmm} for the velocity of the participants' eye gaze velocity across all conditions showed a significant fixed effect between the different \glspl{virt} ($\chi$2(3, N= 24) = 60.5829, p $=$ 0.0001) but no significant effects were found when analyzing the different depths and the interaction effect.
\autoref{fig:DepthPerceptionEyeGaze} post-hoc comparison between the \glspl{virt} showed that participants moved their eyes significantly more when using. \textit{Halo} compared to all other \glspl{virt}, revealing: \textit{Halo} and \textit{No \gls{virt}} (p $<$ 0.0001), \textit{Halo} and \textit{Hatching} (p = 0.0001), and \textit{Halo} and \textit{Stippling} (p $<$ 0.0001).


\begin{figure}[tb]
    \centering
    %\includegraphics[width=\columnwidth]{Chapter5/Images/PerceptionEyeGaze.pdf}
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthEyeMotionVelocityThesis.pdf}
    \caption{The difference in velocity between the participant's eye movements.}
    \label{fig:DepthPerceptionEyeGaze}
\end{figure}

% Subjective Data for depth perception
\subsubsection{Subjective Results (H.5)} \label{sec:DepthSubjectiveResults}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthPaasScoreThesis.pdf}
    \caption{Results from the PAAS questionnaire for the depth perception study.}
    \label{fig:DepthPerceptionPAASGraph}
\end{figure}

The analysis of responses to the PAAS questionnaire showed a significant difference between different illustrative visualizations using a Friedman rank sum test ($\chi2(3, N= 24)= 22.109, p = 0.0001$).
Post-hoc analysis with pairwise Wilcoxon signed-rank tests were conducted with a Bonferroni correction applied, and comparisons showed significantly increased cognitive load for \textit{Stippling} when compared to \textit{no VIRT} (p $<$ 0.0073) (see \autoref{fig:DepthPerceptionPAASGraph}).

We tested the correlation between the PAAS Questionnaire Results to identify if cognitive load correlated with how well participants believed they did on each condition. I found a significantly negative correlation between both of these conditions $\tau = -0.32$ (z = -4.2966, p $<$ 0.0001), showing that participants tended to think they did better when they felt the task required less cognitive load regardless of their actual performance.

\begin{figure}[!b]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthSusScoreThesis.pdf}
    \caption{Results from the SUS questionnaire for depth perception study.}
    \label{fig:DepthPerceptionSUSGraph}
\end{figure}

The results from the SUS questionnaire showed no significant difference between different illustrative visualizations using a Friedman rank sum test ($\chi2(3, N= 24)= 5.8969, p = 0.0957$) (see \autoref{fig:DepthPerceptionSUSGraph}).

We tested the correlation between the SUS Questionnaire Results and how well participants believed they did on each condition and found a significant relationship between them $\tau = -0.22$ (z = -6.0749, p $<$ 0.0017). 
This showed a mismatch between the system usability scores for each condition and how the participants felt about the usability when looking back on all the conditions. This lower correlation may indicate that the participants were unsure about their preferences for this task. 

\begin{figure}[bt]
    \centering
    \includegraphics[width=\columnwidth]{Chapter6/Images/DepthEaseOfVewingThesis.pdf}
    \caption{Results for the question "\textit{How easy was it to look at objects inside of other objects using this visualization?}"}
    \label{fig:How Easy was this visualization to see through depth perception}
\end{figure}

Responses for the question "\textit{How easy was it to look at objects inside of other objects using this visualization?}" were analyzed using a Friedman rank sum test and showed a significant difference between the various VIRTs ($\chi$2(3, N= 24)= 21.44, p $<$ 0.0001) (see \autoref{fig:How Easy was this visualization to see through depth perception}).

Post-hoc analysis with pairwise Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied. Which found a significant difference when trying to perform this task using \textit{Hatching} compared to \textit{no VIRT} (p = 0.0061) and \textit{Stippling} when compared to \textit{no VIRT} (p = 0.0005).
\autoref{fig:How Easy was this visualization to see through depth perception} showed that for this task, participants found it much more difficult to comprehend the internal structure of the volume when using \textit{no VIRTs}.

\section{Discussion}
The following sections present the findings of this study based on a comprehensive analysis of the participant's performance when using different \glspl{virt} across accuracy, time requirements, behavioral patterns, and subjective feedback. 
Overall, we found that all conditions performed similarly in accuracy. However, participants noted that \textit{Halo} was more difficult to utilize inside of this particular study.

\subsection{Psychometric Analysis (H.1 \& H.2)}
Participants could reliably tell if a volumetric object was in front or behind an object when it was less than 1 cm away from the reference object, which is relatively consistent with all \glspl{virt}, which seem to all follow a similar curve. 
This exceeds the estimates shown in \textbf{H.1}. 
\autoref{fig:DepthResultsPerCondition} and \autoref{tab:JNDResults} shows that utilizing \textit{no VIRT} performed very similarly to both the \textit{Stippling} and \textit{hatching} VIRT, meaning that the depth cue from the coming from the volume rendering was much stronger than the effect of the partial occlusion they provided. 
Another possibility is that the depth cues \textit{Hatching} and \textit{Stippling} provided were not good enough to overcome the drawback of occluding a small subsection volume, resulting in minimal improvement.
In my opinion, while difficult to test, I assume it is for both of these reasons. Hatching and Stippling provide a better depth cue, but the partial occlusion is not working as well as the \gls{dvr} itself. 
Leading to the depth cue of aerial perspective, which may provide a more precise form of depth perception than partial occlusion.

% Talk about the JND results
While the results for \textbf{H.2} showed a slight improvement to the JND when \textit{hatching} or \textit{Stippling} was used, I could not support either \textbf{H.2}. 
Instead, both \textit{Hatching} and  \textit{Stippling} conditions showed similar comparable patterns in \autoref{fig:DepthResultsPerCondition} except for a significant difference between \textit{Stippling} and \textit{halo} 's \gls{jnd}. 
\autoref{fig:DepthPerceptionResultsOneGraphWithMagnifier} shows us that participants struggled to tell the difference between the two objects more when the variable object was behind the reference object by over 1 mm. 
Participants also took significantly longer to determine the depth when answering these questions, leading us to find that the \textit{Halo} visualizations may harm depth perception.

% Talk about the improvement of the PSE
Interestingly, the \textit{Halo} VIRT also had the \gls{pse} closest to zero (0.0127), indicating that participants had equally no perception of where the object was on either side of zero.
The other visualizations ($>0.8mm$) and especially \textit{stippling}($>1.3mm$) were considered to be in front of where the variable objects were.
A virtual object appearing slightly closer than its actual position is a common effect in AR~\cite{Jamiy2019, Jamiy2019b}.
The \textit{halo} effect made it almost equally ambiguous where the center was, as it was the least accurate most consistently.
In theory, this could be because the participant was still underestimating; it is possible that they are using the \textit{Halo} as a depth cue and placing, which would move the center of the object about 0.5mm forward (a similar effect should happen to the other VIRTs, though). 
The significant difference between the PSE between the \textit{Halo} and \textit{hatching} and \textit{Stippling} shows that \textit{Stippling} seems to cause participants to systematically underestimate depth relative to the \textit{Hatching} and \textit{Halo} VIRTs.
This highlights an improvement that \textit{Halo} and \textit{Hatching} both seem to improve depth perception when the objects are near the user, but they tend to perform worse when it is far away.
In contrast, \textit{Stippling} performed worse when it was closer to the participant. 

\subsection{Time Required (H.3)}
\autoref{fig:DepthTimeTaken} shows participants took longer viewing the both \textit{halo} and \textit{hatching} \glspl{virt}.
The hatching \gls{virt} was likely due to the time how distracting participants found it, which is further described in \autoref{sec:PerceptionDiscussionSubjectiveResults} and \Cref{app:Chapter6Comments} whereas the Halo seems to be due to the challenges experienced in \autoref{sec: Psychometric Analysis}.
Highlighting the issues participants had distinguishing the depth, which is reflected in \autoref{fig:DepthPerceptionResultsOneGraphWithMagnifier}.
In summary, the findings relating to the time taken for each section indicate the nuanced relation between the \glspl{virt} and how they noticeably impact depth perception and usability. 

\subsection{User Behavioral Analysis (H.4)}
% Talk about the users' behavior throughout the study
Much like the previous chapter (\autoref{Chap:PerceptionStudy}), participants demonstrated a clear increase in activity when they used the \textit{Halo} \gls{virt}, where they felt like they had less control. 
This supports \textbf{H.5} by showing the impact of embodied cognition in regards to depth perception as participants commented that it was challenging to perform the task in \autoref{app:Chapter6Comments}.
Generally, the impact of hand-based movement was much lower in this study, as hand motions provided little aid for this type of task.
However, the impact of the accelerated gaze and head motion was considerably accelerated when they were trying to better understand the depth of various objects in the scene, and the accelerated movement can be seen as a side effect of that notion~\cite{Vishton1995}. 

\subsection{Subjective Results (H.5)} \label{sec:PerceptionDiscussionSubjectiveResults}
% Talk about the subjective findings
Participants indicated in \autoref{app:Chapter6Comments} that the \textit{stippling} visualization was the most straightforward visualization to use for this task.
This goes against our original hypothesis and shows that while participants understand the difference between the different \glspl{virt}, participants performed much better when using the \textit{Halo} visualization. 
However, for both time and accuracy (\gls{jnd}), \textit{hatching} outperformed the \textit{halo} VIRT. 
This does not support \textbf{H.5} as we found that participants did not prefer the \textit{Halo} \gls{virt} when trying to perform depth perception. 

It seems that the \textit{Hatching} \gls{virt} in \autoref{sec:DepthSubjectiveResults} was disliked because it followed the participant around similar comments relating to slow rendering and instability were noted inside of this study similar to \autoref{Chap:PerceptionStudy}.
A similar phenomenon has been found in previous studies on other devices, which have noted that Hatching looked like viewing a screen in front of the image~\cite{Lawonn2018}.
While this implementation seemed to remove the screen door effect, having the \textit{Hatching} follow the user was still not ideal, according to the participant feedback in \autoref{app:Chapter6Comments}.
It is quite possible that the results for \textit{Hatching} could be improved if a different design for real-time 3D Hatching was used. 
This would likely require a move away from the 2D artistic approach rather than using volumetric geometry relying on a surface based geometric approach like  Gerl et al.~\cite{Gerl2012}'s or the purely geometrical one similar to the tessellation \gls{X-ray Vision} effect used in \autoref{Chap:X-ray Implemntion}.
By providing a version of the \textit{Hatching} \gls{virt} that could remain static, finding a method to improve these results may be possible.

\section{Conclusion}
% Talking wrapping up the discussion
The \textit{Stippling} \gls{virt} was seen as the best \gls{virt} to use for depth perception, especially when considering participant feedback. 
Technically, the \textit{Hatching} \gls{virt} may have slightly outperformed \textit{Stippling}, but participants spent more time working with \textit{Hatching} and found it more challenging to judge depth to use than \textit{Stippling}.
Overall, it seems that the advantage that depth perception grants when using \gls{dvr} is quite high, and this is likely too high to see much more improvement with other depth perception effects.
This is positive regarding \gls{X-ray Vision} effects as this shows that \textit{stippling} and \textit{Hatching} can both function as \gls{X-ray Vision} techniques for \gls{dvr} without hindering depth perception below what the devices are capable of delivering.

Overall, this research showed that:
\begin{itemize}
    \item This study has showed that the Halo \gls{virt} may be slightly detrimental to precision tasks;
    \item While hatching and stippling perform similarly but are better suited to precise tasks;
    \item I observed that utilizing the transparent nature of depth perception may make for a more precise depth cue than any of the others. 
\end{itemize}


 \newpage 
    %\include{Chapter7/Chapter7Brief}
    \glsresetall
    %\include{ChapterFinal/ConclusionBrief}
     \newpage \chapter{Conclusion}
% Talk about the thesis overall
This thesis adapted three new X-ray visualization techniques (Halo, Hatching, Stippling) for use in \gls{ar}, testing their strengths and weaknesses and showcasing the environments in which they can be used.
The research in this dissertation shows that \glspl{virt} provides \glspl{X-ray Visualization}, which can both aid or hinder a user's comprehension of a volume. 
% Road map.
This chapter summarizes this thesis's novel contributions and is followed by potential future research and future research it has enabled.

\section{X-ray Vision Evaluation}

% Intro
The field of \gls{ar} enabled \gls{X-ray Vision} is one of the oldest research areas in \gls{ar}. 
This thesis has provided literature information on collating research done in prior fields to influence our choices regarding the research on \glspl{X-ray Visualization}. 
Which informed the initial study presented in \autoref{Chap:X-ray Implemntion}
Plenty of studies have researched depth perception. The initial goal of this research was to determine how \glspl{X-ray Visualization} affects a user in a more ecologically relevant scenario, leading us to answer the following question: 
"What is the impact on spatial estimation when using \gls{X-ray Vision} effects who use different design methodologies?".

This dissertation presents a study in \autoref{Chap:X-ray Implemntion} aimed at judging depth perception and how X-ray visualization affects a user's ability to place an object accurately. 
This is one of the first to allow users to view computer vision-enabled (Saliency or Edge detection) \glspl{X-ray Visualization} on \gls{ost} \gls{ar} devices. 
This experiment showed that \glspl{X-ray Visualization} hampered spatial estimation and that this effect grows larger as they occlude more.

% Why Were the X-ray Visulizaitons Chosen
\autoref{chap:Background} stated many different X-ray effects. 
This dissertation focused on allowing users to experience as much visualization as possible without obscuring the data. 
This is essential because Medical Data can be dangerous and expensive. Current procedures only allow data to work with what is currently possible. 
To that end, this dissertation focused on visualizations that allowed the user to view most of the visualizations.
A study focused on auxiliary effects was conducted, motivated by situations found in medical environments.  High and low occlusion models and different saliency techniques were compared. Four diverse visualizations were chosen, and a back face was applied to improve depth perception.

% Talk about the system utilized to make this possible.
Being able to adapt \gls{vst} \gls{ar} techniques to \gls{ost} \gls{ar} alone may allow for many extensions of many systems in the future.
This system did allow us to utilize Computer Vision Enabled \gls{X-ray Vision} techniques.
While improvements could be made to this system, it does show that it is possible to visualize Computer Vision Enabled \gls{X-ray Vision} techniques on \gls{ost} \gls{ar} displays. 

% Evaluation of these methods
To ensure this study's ecological relevance, a placement study was performed, which had participants accurately replicate a physical scene inside a large box. 
% What was learned to answer the above question
%To answer research question one: "What is the impact on spatial estimation when using \gls{X-ray Vision} effects who use different design methodologies?"
This study showed that Auxiliary Augmentation effectively improved spatial estimation and provided an \gls{X-ray Vision} cue. 
These results led us to learn that while visualizations that occluded more provided a better sense of depth perception, they did not, and the spatial awareness they provided was reduced. 
It also became clear that the 90fps of the Microsoft HoloLens was not fast enough to provide a good user experience for the Computer Vision-Enabled \gls{X-ray Vision} Technologies.
This told us that Real-world overlays were still required and could be tailored to provide more accuracy, but the participants' feedback also informed us that they did not require the whole visualization to be covered. 

\section{X-ray Visualizations for Direct Volume Rendering}
%R.2 How can Volumetric Illustrative Effects be adopted to become \gls{ost} \gls{ar} \gls{X-ray Vision} effects?
The lessons learned from \autoref{Chap:X-ray Implemntion} and the lessons found in the prior literature in \autoref{chap:Background} several requirements were found that enforced the design of the \glspl{X-ray Visualization}. 
\gls{dvr} also required visualizations that could take the effect of an object that utilized a ray-cast geometry rather than a polygonal one. 
These constraints indicated the utility of specific \glspl{virt} that took inspiration from illustration techniques that could depict see-through or glass-like objects.

This required modifying several illustrative techniques to work with \gls{dvr} and adjusting the parameters of \gls{X-ray Vision}.
Resulting in the question of
"How can Volumetric Illustrative Rendering Techniques be adopted to become \gls{ost} \gls{ost} \gls{X-ray Vision} effects for \gls{dvr} visualizations?"
To answer this, the techniques of Saliency, Hatching, and Halo were adapted to work with \gls{ost} \gls{ar} \glspl{hmd}.
Implementations of all these effects previously existed, but they needed to be extensively adapted to be displayed using \gls{dvr} and ready to use for \glspl{X-ray Visualization}.

\section{Evaluation of Volumetric Illustrative Rendering Techniques}
%How can Volumetric Illustrative Effects be adopted to become OST AR X-ray Vision effects?

Volumetric data for quantitative evaluation makes it difficult to acquire the required data since finding unique but repeatable data is not realistic. 
Furthermore, designing a set of initial products to scan and use is a time-consuming and expensive task that comes at the time of these in-demand machines.  
To remedy this, this thesis contributes the design of the Random Volume Generation System, which was designed to produce pseudo-random generated volumes for use for \gls{hci} studies.
These volumes were in the form of a hierarchical placement of random noisy spheres but could also be replaced with any required shape because this system was designed to be modular.
This consideration was made to allow for easy modification in many different experiments.

By using the Random Volume Generation System, it made it possible for two studies to be run using an \gls{ost} \gls{ar} \gls{hmd}. 
One of these was focused on determining how well a person could recall information through these effects. 
In contrast, the other determined the minimum threshold depth participants could reliably tell between two volumetric objects while using am \gls{ost} \gls{ar} \gls{hmd}.

\subsection{Perception}
%Can an X-ray Vision effect facilitate a user’s understanding of spatial relationships?
Comments from the \gls{X-ray Vision} user study indicated that all of the \gls{X-ray Vision} effects impair the user's ability to view a system.
This issue is one of the primary reasons for visualizing data using \gls{dvr}.
This led us to ask the question, "Can an \gls{X-ray Vision} effect facilitate a user’s understanding of spatial relationships when using Direct Volume Rendering?"
To determine what \glspl{virt} were better able to convey the information within the volume.

Testing how clearly users could see into the volume when a \gls{virt} was applied to it, I conducted a test that could view a volume.
The participants' perceptions were then evaluated by having them perform a counting task under two different conditions. 
One had participants count all of the objects that fit the criteria within the volume, whereas the other had them count them while they were in a particular subgroup. 

This study showed that the halo \gls{virt} could give users a more intuitive view of a volume.
The results from this study seem to indicate that this effect will keep being useful even with complex datasets.
The ability to accurately perceive what is inside of the volume is diminished when using Stippling and Hatching \glspl{virt}.
However, this shows that it can improve a user's understanding of a volume while \gls{X-ray Vision} is utilized. 


\subsection{Depth Perception}
%R.4 Can Volumetric Illustrative Effects improve the perceived depth reported on virtual objects in \gls{ost} \gls{ar}?
Depth perception plays an important role when interacting naturally with data as the user needs to be able to perceive their location within it. 
To be able to ensure at what range depth perception is possible, depth perception of \gls{dvr} on immersive stereoscopic \glspl{hmd}, and if there is a way that \glspl{virt} can provide an even better quality of depth perception. 
This led to the following question proposed: "Can Volumetric Illustrative Effects improve the perceived depth reported on virtual objects in \gls{ost} \gls{ar}?"
The following experiment in this dissertation looked at how \glspl{virt} affects depth perception. 

Depth perception using immersive stereoscopic \gls{hmd} had not been researched to this point. 
Similar research had shown that the results on immersive stereoscopic \glspl{hmd} had far greater results, leading to our study needing to focus on at what given threshold on these devices what depth detectable and what impact \glspl{virt} provide. 
A psycho-physical experiment was utilized, consisting of a \gls{twofc} questionnaire.
By using this type of questionnaire, the lower limit where a user could decide on depth perception when using \gls{dvr} on an \gls{ost} \gls{ar} display was discovered.

This study showed that the effect on \gls{virt} was very small (< 2mm), showing that while they may not have been beneficial to \gls{dvr}, they were not detrimental.
However, both the Hatching and Stippling \glspl{virt} allowed participants to answer faster, indicating that the sense of depth was clearer when using \glspl{virt}.
However, it was discovered that the Halo \gls{virt} seemed to reduce participants' ability to determine the depth within a volume and that users took longer to answer the questionnaire when it was available.

\section{Future Work}
This Dissertation has covered a lot of ground when it comes to working to find ways to integrate \gls{X-ray Vision} applications utilizing \gls{dvr} on \gls{ost} \gls{ar} devices.
However, this is likely just the start of this work on all these topics. 
This section first looks at the related work that can still be done in \gls{ar} enabled \gls{X-ray Vision}, \gls{dvr} on \gls{ost} \gls{ar} displays, and then an example of work that will be done for future work. 

\subsection{Augmented Reality Enabled X-ray Vision}
\gls{ar} \gls{X-ray Vision} is becoming a well-explored field of research.
There is a reasonable understanding of how \glspl{X-ray Visualization} work, especially on mobile devices~\cite{Dey2014, Santos2016}. Previous literature showed what techniques were required to provide a user with the perceived ability to look through an object in AR~\cite{Ghasemi2018}. Several open research challenges with XRV seek to find methods to improve its presentation's precision, accuracy, and depth perception. 

Recent research in the space has focused on XRV performance and our natural vision. This includes challenges like improving depth and spatial perception~\cite{Gruenefeld2020, Martin-Gomez2021} and the creation of new interaction techniques so people are better able to use XRV to support everyday work tasks and recreational activities~\cite{Wang2022}.

Research on how to visualize other types of non-camera data, such as radar and sonar, will likely be important in the future. 
This thesis has primarily looked at ways to visualize non-photo data like \gls{mri} and \gls{ct} scans behind \glspl{X-ray Visualization}. There are methods for viewing 3D Sonar and Radar~\cite{Gilliam1996, VanSon2018, Macdonald1997} that can be used to view things through walls and the surroundings of objects and could be translated to interfaces for summaries and other naval vessels. 
Moreover, the recent findings of being able to visualize the interference from WIFI signals to allow for \gls{X-ray Vision} become a household utility~\cite{Adib2013}.

The systemic literature review in \autoref{chap:Background} noted that there had been quite a lack of work on \gls{X-ray Vision} effects. This dissertation has tried to integrate the impact of having more than one item in the visible scene (Auxiliary Augmentation). Still, it may be fair to expect that using a combination of \glspl{X-ray Visualization} and other depth cues may provide much stronger effects for \gls{ost} \gls{ar} displays. 
One that this thesis did not investigate in this thesis was vergence-based \gls{ar}; this requires much better eye tracking where it is possible to track the vergence and accommodation of the user's eyes, allowing users to control the \gls{ar} space naturally. 

\subsection{Direct Volume Rendering Displayed Using Ocular See Through Augmented Reality}
This work used synthetic data to create generalizable results across many volume data forms. They are designed to look like \gls{ct} or MRI scans, but there is a real chance that a sparse dataset like an angiogram might produce different results.
Collecting this data may require using deep learning to generate a set of ventricles within a space that conforms to a set of given parameters~\cite{8885576}.
However, more work is still required to ensure that these models can create models to a set of parameters to allow for a controlled study. 

This thesis also did not investigate the effects of applying these effects to medical data to see if there was any real change. 
In the future, I would prefer to use a set of participants with a more diverse skill set. 
While this would be difficult to make into a controlled study, stable diffusion may offer a method to provide large enough datasets to enable a more controlled study than was previously possible~\cite{10049010}. 

The work in this thesis marks the beginning of work in this field, and there is a plethora of studies possible with this technology, including:
Introducing static noise~\cite{Ratcliff2010}, 4D objects~\cite{Langner2008}, and more complicated data would all be interesting areas for exploration in this space and could greatly impact the ways it is possible to visualize real-world data.% I would like to cite this
Different styles of studies, like density observations or trying to analyze small imperfections using these visualizations, could also make for interesting research moving forward~\cite{Laha2016}. 
While also looking into rendering these effects on different displays to more intuitive methods of viewing this data~\cite{Geng2013, Xiong2021}.

% Some work attempted to create a volumetric X-ray vision system that blurs and lowers the foreground's occlusion slightly depending on depth out to provide a depth cue that did not use occlusion. A short pilot seems to indicate this does work for an X-ray vision technique. 

% This work, however, presented several issues, like our need to have various points of focus within an object and a clear indication of where the user is looking. A large limitation of this system is that it would require a fast and accurate enough eye tracking system to be able to determine the point of focus a user is looking at in 3D and adapt to this. This is problematic since, in order for the user to view an area, their real accommodation needs to be faked. 

% I have more information regarding this, but overall, this work was sidelined due to the large number of issues that would first need to be solved before bringing it to market. 

A major limitation of this work is that the volumes that are used are solid and have properties similar to those of a cell or a medical scan. There is a real chance that a sparse dataset like an angiogram might produce different results.
The acquisition of larger datasets for research use could be generated using deep learning to create a set of ventricles within a space that conforms to a set of given parameters~\cite{8885576}.
However, more work is still required to ensure that these models can create models to a set of parameters to allow for a controlled study. 

More research investigating the effects of applying \glspl{virt} to medical data is still required to investigate if there was any real change between the artificial noisy spheres and real data. 
This could be arranged moving forward by recruiting participants with a more diverse skill set. 
This would be difficult to make into a controlled study, so an empirical evaluation may be necessary. 
If a controlled study is required, stable diffusion may offer a method to provide large enough datasets to be able to enable a more controlled study than was previously possible~\cite{10049010}. 

This paper discusses the limitations of viewing volumetric data in augmented reality without utilizing real-world counterparts to determine the accuracy possible when viewing volumetric data on an \gls{ost} \gls{ar} device. 
The next step that will be needed to be taken for this research would be to start working with studies like blind reaching and perceptual matching tasks to determine further what accuracy is possible when interacting with these volume renderings in the real world~\cite{Jamiy2019}.

Introducing static noise~\cite{Ratcliff2010}, 4D objects~\cite{Langner2008}, and more complicated data would all be interesting areas for exploration in this space. 
Different styles of studies, like density observations or trying to analyze small imperfections using these visualizations, could also make for interesting research moving forward~\cite{Laha2016}. 
While also looking into rendering these effects on different displays to more intuitive methods of viewing this data~\cite{Geng2013, Xiong2021}.
A plethora of research can still be done in this space.

\subsection{Further Evaluations}
To continue the research proposed in this thesis, I propose investigating if it is possible to incorporate using eye gazes to select aspects of an \gls{X-ray Visualization} while also distorting the binocular distortion, making the foreground and background lighter to create a similar effect as focusing on an object located in a fog. 
This would enable the system to adapt the volumetric instances to the face they are looking at and lock them to the correct position of the user. This system would also need to be able to estimate the users' bones. Further work in this type of collaboration is essential. 

This is based on previous research based on Zannoli et al.~\cite{Zannoli2016}, who looked at the benefits of blurring the foreground plane to enhance the viewer's depth perception.
This technique resembles the effect of Accommodation and Convergence, so it is possible to improve depth perception further by mimicking the effect.
By utilizing the equations laid out in their research, I hypothesize it is possible to extend the work done by Kitajima et al.~\cite{Kitajima2015} to create a vergence-based visualization and by utilizing similar interaction as mentioned in Jing et al.~\cite{Jing2021} research it should be possible to develop \glspl{virt} further to take advantage of the weakness of \gls{X-ray Vision}.

This type of \gls{X-ray Vision} would utilize \glspl{virt} when the user was not focused on it but would then dissolve them to allow the user to navigate through the volume.
This utilizes the vergence accommodation definancy (described in \autoref{Chap:VolumetricX-rayVision}) with \gls{ost} \gls{ar} \glspl{hmd}. 

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{ChapterFinal/AccomdicationConvergenceXrayVision.png}
    \caption[A prototype of a new form of accommodation-convergence based X-ray effect designed for \gls{ost} \gls{ar} devices.]{A prototype of a new form of accommodation-convergence based X-ray effect designed for \gls{ost} \gls{ar} devices. This image shows the effect that are focusing on the objects within the x-ray field., whereas the outside will grow more transparent and the inside will allow for a greater focus.}
    \label{fig:AccomdicationConvergenceXrayVision}
\end{figure}

I have developed a prototype of this system, which works within a controlled environment. 
\autoref{fig:AccomdicationConvergenceXrayVision} shows that this system uses the tesselation model and only utilizes a couple of shapes. 
However, early tests have shown some promise using this technique. 
However, this would need to be made to work both with the randomly generating spheres and with \gls{ct} and \gls{mri} data sets. 

Moving forward, this technique should be expanded to work within a volumetric environment, and its impacts should be reviewed by user studies. 
This research will enable us to understand how accommodation convergences can be simulated based on artifacts in space or if they need to be represented more closely with real-life phenomena.
It could also look at verifying the hypothesis initially stated by Zannoli et al.~\cite{Zannoli2016}.

\section{Final Remarks}
This thesis set out to determine how \gls{X-ray Vision} on \gls{ost} \gls{ar} devices should be created and use that information to develop new ones.
The systematic literature review in \autoref{chap:Background} and the user study comparing different \glspl{X-ray Visualization} in \autoref{Chap:X-ray Implemntion}.
This required learning to calibrate a user's sight to the display itself, allowing the visualization to overlap the real world. 
Real-time \gls{dvr} were tested and made for different stereoscopic displays, and then the lessons from comparing different \glspl{X-ray Visualization} was utilized to create the three \glspl{virt} (Halo, Hatching, and Stippling). 
These \glspl{virt} were then tested the participant's ability to determine information from within the volume and the ability to locate a specific location within the volume.
Moving beyond this research, utilizing and combining \glspl{virt} to find a method that enables users to convey information from the volume while still understanding the information regarding the depth perception of the object they are looking inside. 


 \newpage 
    \glsresetall

    \bibliographystyle{IEEEtran}
    \DIFdelbegin %DIFDELCMD < \bibliography{bibliography}
%DIFDELCMD < %%%
\DIFdelend %DIF >  Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\DIFaddbegin \begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname \DIFadd{url@samestyle}\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl
\DIFaddend 

\DIFaddbegin \bibitem{Nguyen2016}
\DIFadd{H.~B. Nguyen, T.~Q. Thai, S.~Saitoh, B.~Wu, Y.~Saitoh, S.~Shimo, H.~Fujitani, H.~Otobe, and N.~Ohno, ``}{\DIFadd{Conductive resins improve charging and resolution of acquired images in electron microscopic volume imaging}}\DIFadd{,'' }\emph{\DIFadd{Scientific Reports}}\DIFadd{, vol.~6, no. March, pp. 1--10, 2016.
}

\bibitem{Goodsell1989}
\DIFadd{D.~S. Goodsell, I.~S. Mian, and A.~J. Olson, ``}{\DIFadd{Rendering volumetric data in molecular systems}}\DIFadd{,'' }\emph{\DIFadd{Journal of Molecular Graphics}}\DIFadd{, vol.~7, no.~1, pp. 41--47, 1989.
}

\bibitem{Mathiesen2012}
\DIFadd{D.~Mathiesen, T.~Myers, I.~Atkinson, and J.~Trevathan, ``}{\DIFadd{Geological visualisation with augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 2012 15th International Conference on Network-Based Information Systems, NBIS 2012}}\DIFadd{, no. September, pp. 172--179, 2012.
}

\bibitem{Liu2017}
\DIFadd{L.~Liu, D.~Silver, K.~Bemis, D.~Kang, and E.~Curchitser, ``}{\DIFadd{Illustrative Visualization of Mesoscale Ocean Eddies}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~36, no.~3, pp. 447--458, 2017.
}

\bibitem{HibbardL.1986}
\DIFadd{W.~}{\DIFadd{Hibbard L.}}\DIFadd{, ``}{\DIFadd{4-D Display of Meterological Data}}\DIFadd{,'' }\emph{\DIFadd{Interactive 3D Graphics}}\DIFadd{, pp. 23--36, 1986.
}

\bibitem{Oren2020}
\BIBentryALTinterwordspacing
\DIFadd{O.~Oren, B.~J. Gersh, and D.~L. Bhatt, ``}{\DIFadd{Artificial intelligence in medical imaging: switching from radiographic pathological data to clinically meaningful endpoints}}\DIFadd{,'' }\emph{\DIFadd{The Lancet Digital Health}}\DIFadd{, vol.~2, no.~9, pp. e486--e488, 2020. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/S2589-7500(20)30160-6}
\BIBentrySTDinterwordspacing

\bibitem{Hosny2018}
\BIBentryALTinterwordspacing
\DIFadd{A.~Hosny, C.~Parmar, J.~Quackenbush, L.~H. Schwartz, and H.~J. W.~L. Aerts, ``}\BIBforeignlanguage{eng}{{Artificial intelligence in radiology}}\DIFadd{,'' }\emph{\BIBforeignlanguage{eng}{Nature reviews. Cancer}}\DIFadd{, vol.~18, no.~8, pp. 500--510, aug 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.ncbi.nlm.nih.gov/pubmed/29777175 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6268174/}
\BIBentrySTDinterwordspacing

\bibitem{Jurgaitis2008}
\DIFadd{J.~Jurgaitis, M.~Pa}{\DIFadd{\v{s}}}\DIFadd{konis, J.~Pivoriunas, I.~Martinaityte, A.~Ju}{\DIFadd{\v{s}}}\DIFadd{ka, R.~Jurgaitiene, A.~Samuilis, I.~Volf, M.~Sch}{\DIFadd{\"{o}}}\DIFadd{binger, P.~Schemmer, T.~W. Kraus, and K.~Strupas, ``}{\DIFadd{The comparison of 2-dimensional with 3-dimensional hepatic visualization in the clinical hepatic anatomy education}}\DIFadd{,'' }\emph{\DIFadd{Medicina}}\DIFadd{, vol.~44, no.~6, pp. 428--438, 2008.
}

\bibitem{Mandalika2018}
\BIBentryALTinterwordspacing
\DIFadd{V.~B.~H. Mandalika, A.~I. Chernoglazov, M.~Billinghurst, C.~Bartneck, M.~A. Hurrell, N.~Ruiter, A.~P. Butler, and P.~H. Butler, ``}{\DIFadd{A Hybrid 2D/3D User Interface for Radiological Diagnosis}}\DIFadd{,'' }\emph{\DIFadd{Journal of Digital Imaging}}\DIFadd{, vol.~31, no.~1, pp. 56--73, 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1007/s10278-017-0002-6}
\BIBentrySTDinterwordspacing

\bibitem{Mast2019}
\DIFadd{M.~Mast, I.~Kaup, S.~Kr}{\DIFadd{\"{u}}}\DIFadd{ger, C.~Ullrich, R.~Schneider, and S.~Bay, ``}{\DIFadd{Exploring the Benefits of Holographic Mixed Reality for Preoperative Planning with 3D Medical Images}}\DIFadd{,'' }\emph{\DIFadd{Mensch und Computer 2019 - Workshopband}}\DIFadd{, pp. 590--594, 2019.
}

\bibitem{Dicken2005}
\DIFadd{V.~Dicken, J.~M. Kuhnigk, L.~Bornemann, S.~Zidowitz, S.~Krass, and H.~O. Peitgen, ``}{\DIFadd{Novel CT data analysis and visualization techniques for risk assessment and planning of thoracic surgery in oncology patients}}\DIFadd{,'' }\emph{\DIFadd{International Congress Series}}\DIFadd{, vol. 1281, pp. 783--787, 2005.
}

\bibitem{Rieder2009}
\DIFadd{C.~Rieder, M.~Schwier, A.~Weihusen, S.~Zidowitz, and H.-O. Peitgen, ``}{\DIFadd{Visualization of risk structures for interactive planning of image guided radiofrequency ablation of liver tumors}}\DIFadd{,'' }\emph{\DIFadd{Medical Imaging 2009: Visualization, Image-Guided Procedures, and Modeling}}\DIFadd{, vol. 7261, p. 726134, 2009.
}

\bibitem{Cheung2021}
\DIFadd{C.~C. Cheung, S.~M. Bridges, and G.~L. Tipoe, ``}{\DIFadd{Why is Anatomy Difficult to Learn? The Implications for Undergraduate Medical Curricula}}\DIFadd{,'' }\emph{\DIFadd{Anatomical Sciences Education}}\DIFadd{, vol.~14, no.~6, pp. 752--763, 2021.
}

\bibitem{Abbey2021}
\DIFadd{C.~K. Abbey, M.~A. Lago, and M.~P. Eckstein, ``}{\DIFadd{Comparative observer effects in 2D and 3D localization tasks}}\DIFadd{,'' }\emph{\DIFadd{Journal of Medical Imaging}}\DIFadd{, vol.~8, no.~04, pp. 1--17, 2021.
}

\bibitem{Zhou2022}
\DIFadd{L.~Zhou, M.~Fan, C.~Hansen, C.~R. Johnson, and D.~Weiskopf, ``}{\DIFadd{A Review of Three-Dimensional Medical Image Visualization}}\DIFadd{,'' }\emph{\DIFadd{Health Data Science}}\DIFadd{, vol. 2022, 2022.
}

\bibitem{McIntire2012}
\DIFadd{J.~P. McIntire, P.~R. Havig, and E.~E. Geiselman, ``}{\DIFadd{What is 3D good for? A review of human performance on stereoscopic 3D displays}}\DIFadd{,'' }\emph{\DIFadd{Head- and Helmet-Mounted Displays XVII; and Display Technologies and Applications for Defense, Security, and Avionics VI}}\DIFadd{, vol. 8383, no. February, p. 83830X, 2012.
}

\bibitem{Ahlberg2007}
\BIBentryALTinterwordspacing
\DIFadd{G.~Ahlberg, L.~Enochsson, A.~G. Gallagher, L.~Hedman, C.~Hogman, D.~A. }{\DIFadd{McClusky III}}\DIFadd{, S.~Ramel, C.~D. Smith, and D.~Arvidsson, ``}{\DIFadd{Proficiency-based virtual reality training significantly reduces the error rate for residents during their first 10 laparoscopic cholecystectomies}}\DIFadd{,'' }\emph{\DIFadd{The American Journal of Surgery}}\DIFadd{, vol. 193, no.~6, pp. 797--804, jun 2007. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1016/j.amjsurg.2006.06.050}
\BIBentrySTDinterwordspacing

\bibitem{Zhang2016a}
\DIFadd{G.~Zhang, X.-j. Zhou, C.-z. Zhu, Q.~Dong, and L.~Su, ``}{\DIFadd{Usefulness of Three-dimensional(3D) simulation software in hepatectomy for pediatric hepatoblastoma}}\DIFadd{,'' }\emph{\DIFadd{Surgical Oncology}}\DIFadd{, vol.~25, 2016.
}

\bibitem{Akpan2019}
\DIFadd{I.~J. Akpan and M.~Shanker, ``}{\DIFadd{A comparative evaluation of the effectiveness of virtual reality, 3D visualization and 2D visual interactive simulation: an exploratory meta-analysis}}\DIFadd{,'' }\emph{\DIFadd{Simulation}}\DIFadd{, vol.~95, no.~2, pp. 145--170, 2019.
}

\bibitem{Vetter2002}
\DIFadd{M.~Vetter, P.~Hassenpflug, M.~Thorn, C.~Cardenas, L.~Grenacher, G.~M. Richter, W.~Lamade, C.~Herfarth, and H.-P. Meinzer, ``}{\DIFadd{Superiority of autostereoscopic visualization for image-guided navigation in liver surgery}}\DIFadd{,'' }\emph{\DIFadd{Medical Imaging 2002: Visualization, Image-Guided Procedures, and Display}}\DIFadd{, vol. 4681, no. May 2002, pp. 196--203, 2002.
}

\bibitem{Merino2018}
\DIFadd{L.~Merino, A.~Bergel, and O.~Nierstrasz, ``}{\DIFadd{Overcoming Issues of 3D Software Visualization through Immersive Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 6th IEEE Working Conference on Software Visualization, VISSOFT 2018}}\DIFadd{, pp. 54--64, 2018.
}

\bibitem{Cecotti2021}
\DIFadd{H.~Cecotti, M.~Callaghan, B.~Foucher, and S.~Joslain, ``}{\DIFadd{Serious Game for Medical Imaging in Fully Immersive Virtual Reality}}\DIFadd{,'' }\emph{\DIFadd{TALE 2021 - IEEE International Conference on Engineering, Technology and Education, Proceedings}}\DIFadd{, pp. 615--621, 2021.
}

\bibitem{Asadi2024}
\BIBentryALTinterwordspacing
\DIFadd{Z.~Asadi, M.~Asadi, N.~Kazemipour, }{\DIFadd{\'{E}}}\DIFadd{.~L}{\DIFadd{\'{e}}}\DIFadd{ger, and M.~Kersten-Oertel, ``}{\DIFadd{A decade of progress: bringing mixed reality image-guided surgery systems in the operating room}}\DIFadd{,'' }\emph{\DIFadd{Computer Assisted Surgery}}\DIFadd{, vol.~29, no.~1, pp.~--, 2024. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1080/24699322.2024.2355897}
\BIBentrySTDinterwordspacing

\bibitem{Jha2021}
\BIBentryALTinterwordspacing
\DIFadd{G.~Jha, L.~shm Sharma, and S.~Gupta, }\emph{{\DIFadd{Future of Augmented Reality in Healthcare Department}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Springer Singapore, 2021, vol. 203 LNNS. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1007/978-981-16-0733-2_47}
\BIBentrySTDinterwordspacing

\bibitem{Beams2022}
\BIBentryALTinterwordspacing
\DIFadd{R.~Beams, E.~Brown, W.~C. Cheng, J.~S. Joyner, A.~S. Kim, K.~Kontson, D.~Amiras, T.~Baeuerle, W.~Greenleaf, R.~J. Grossmann, A.~Gupta, C.~Hamilton, H.~Hua, T.~T. Huynh, C.~Leuze, S.~B. Murthi, J.~Penczek, J.~Silva, B.~Spiegel, A.~Varshney, and A.~Badano, ``}{\DIFadd{Evaluation Challenges for the Application of Extended Reality Devices in Medicine}}\DIFadd{,'' }\emph{\DIFadd{Journal of Digital Imaging}}\DIFadd{, vol.~35, no.~5, pp. 1409--1418, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s10278-022-00622-x}
\BIBentrySTDinterwordspacing

\bibitem{Rosen2011}
\DIFadd{J.~Rosen, B.~Hannaford, and R.~M. Satava, ``}{\DIFadd{Surgical robotics: Systems applications and visions}}\DIFadd{,'' }\emph{\DIFadd{Surgical Robotics: Systems Applications and Visions}}\DIFadd{, pp. 1--819, 2011.
}

\bibitem{Bichlmeier2007}
\DIFadd{C.~Bichlmeier, T.~Sielhorst, S.~M. Heining, and N.~Navab, ``}{\DIFadd{Improving depth perception in medical AR a virtual vision panel to the inside of the patient}}\DIFadd{,'' }\emph{\DIFadd{Informatik aktuell}}\DIFadd{, pp. 217--221, 2007.
}

\bibitem{Sielhorst2006}
\DIFadd{T.~Sielhorst, C.~Bichlmeier, S.~M. Heining, and N.~Navab, ``}{\DIFadd{Depth perception - A major issue in medical AR: Evaluation study by twenty surgeons}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 4190 LNCS, pp. 364--372, 2006.
}

\bibitem{Geng2013}
\DIFadd{J.~Geng, ``}{\DIFadd{Three-dimensional display technologies}}\DIFadd{,'' }\emph{\DIFadd{Advance Optical Photonics}}\DIFadd{, pp. 456--535, 2013.
}

\bibitem{Xiong2021}
\DIFadd{J.~Xiong, E.~L. Hsiang, Z.~He, T.~Zhan, and S.~T. Wu, ``}{\DIFadd{Augmented reality and virtual reality displays: emerging technologies and future perspectives}}\DIFadd{,'' }\emph{\DIFadd{Light: Science and Applications}}\DIFadd{, vol.~10, no.~1, pp. 1--30, 2021.
}

\bibitem{Jamiy2019}
\DIFadd{F.~E. Jamiy and R.~Marsh, ``}{\DIFadd{Distance Estimation In Virtual Reality And Augmented Reality: A Survey}}\DIFadd{,'' in }\emph{\DIFadd{2019 IEEE International Conference on Electro Information Technology (EIT)}}\DIFadd{, 2019, pp. 63--68.
}

\bibitem{Rosales2019}
\DIFadd{C.~S. Rosales, G.~Pointon, H.~Adams, J.~Stefanucci, S.~Creem-Regehr, W.~B. Thompson, and B.~Bodenheimer, ``}{\DIFadd{Distance judgments to on- and off-ground objects in augmented reality}}\DIFadd{,'' }\emph{\DIFadd{26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings}}\DIFadd{, pp. 237--243, 2019.
}

\bibitem{Al-Kalbani2019}
\DIFadd{M.~Al-Kalbani, M.~Frutos-Pascual, and I.~Williams, ``}{\DIFadd{Virtual object grasping in augmented reality: Drop shadows for improved interaction}}\DIFadd{,'' }\emph{\DIFadd{2019 11th International Conference on Virtual Worlds and Games for Serious Applications, VS-Games 2019 - Proceedings}}\DIFadd{, p. 1DUUMY, 2019.
}

\bibitem{Armbruster2008}
\DIFadd{C.~Armbr}{\DIFadd{\"{u}}}\DIFadd{ster, M.~Wolter, T.~Kuhlen, W.~Spijkers, and B.~Fimm, ``}{\DIFadd{Depth perception in virtual reality: Distance estimations in peri- and extrapersonal space}}\DIFadd{,'' }\emph{\DIFadd{Cyberpsychology and Behavior}}\DIFadd{, vol.~11, no.~1, pp. 9--15, 2008.
}

\bibitem{Krevelen2010}
\BIBentryALTinterwordspacing
\DIFadd{V.~Krevelen, Ric and }{\DIFadd{Poelman Ronald}}\DIFadd{, ``}{\DIFadd{A Survey of Augmented Reality Technologies, Applications, and Limitations}}\DIFadd{,'' }\emph{\DIFadd{International journal of virtual reality}}\DIFadd{, vol.~9, no.~2, pp. 10--20, 2010. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://www.arvika.de/}
\BIBentrySTDinterwordspacing

\bibitem{Martin-Gomez2021}
\DIFadd{A.~Martin-Gomez, J.~Weiss, A.~Keller, U.~Eck, D.~Roth, and N.~Navab, ``}{\DIFadd{The Impact of Focus and Context Visualization Techniques on Depth Perception in Optical See-Through Head-Mounted Displays}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~XX, no.~X, pp. 1--16, 2021.
}

\bibitem{Bajura1992}
\DIFadd{M.~Bajura, H.~Fuchs, and R.~Ohbuchi, ``}{\DIFadd{Merging virtual objects with the real world: seeing ultrasound imagery within the patient}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics (ACM)}}\DIFadd{, vol.~26, no.~2, pp. 203--210, 1992.
}

\bibitem{Avery2009}
\DIFadd{B.~Avery, C.~Sandor, and B.~H. Thomas, ``}{\DIFadd{Improving Spatial Perception for Augmented Reality X-Ray Vision}}\DIFadd{,'' in }\emph{\DIFadd{2009 IEEE Virtual Reality Conference}}\DIFadd{, 2009, pp. 79--82.
}

\bibitem{Kalkofen2013}
\DIFadd{D.~Kalkofen, E.~Veas, S.~Zollmann, M.~Steinberger, and D.~Schmalstieg, ``}{\DIFadd{Adaptive ghosted views for Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013}}\DIFadd{, vol.~1, no.~c, pp. 1--9, 2013.
}

\bibitem{Parsons2021}
\DIFadd{D.~Parsons and K.~Maccallum, ``}{\DIFadd{Current perspectives on augmented reality in medical education: Applications, affordances and limitations}}\DIFadd{,'' }\emph{\DIFadd{Advances in Medical Education and Practice}}\DIFadd{, vol.~12, pp. 77--91, 2021.
}

\bibitem{Kaufman1999}
\DIFadd{A.~E. Kaufman, ``}{\DIFadd{Introduction to volume graphics}}\DIFadd{,'' }\emph{\DIFadd{Siggraph}}\DIFadd{, vol.~99, no. Section 3, pp. 24--47, 1999.
}

\bibitem{Kasprzak2019}
\BIBentryALTinterwordspacing
\DIFadd{J.~D. Kasprzak, J.~Pawlowski, J.~Z. Peruga, J.~Kaminski, and P.~Lipiec, ``}{\DIFadd{First-in-man experience with real-time holographic mixed reality display of three-dimensional echocardiography during structural intervention: balloon mitral commissurotomy}}\DIFadd{,'' }\emph{\DIFadd{European Heart Journal}}\DIFadd{, 2019-04. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1093/eurheartj/ehz127}
\BIBentrySTDinterwordspacing

\bibitem{Pratt2018}
\DIFadd{P.~Pratt, M.~Ives, G.~Lawton, J.~Simmons, N.~Radev, L.~Spyropoulou, and D.~Amiras, ``}{\DIFadd{Through the HoloLens™ looking glass: augmented reality for extremity reconstruction surgery using 3D vascular models with perforating vessels}}\DIFadd{,'' }\emph{\DIFadd{European Radiology Experimental}}\DIFadd{, vol.~2, no.~1, pp. 0--6, 2018.
}

\bibitem{Hanna2018}
\DIFadd{M.~G. Hanna, I.~Ahmed, J.~Nine, S.~Prajapati, and L.~Pantanowitz, ``}{\DIFadd{Augmented reality technology using microsoft hololens in anatomic pathology}}\DIFadd{,'' }\emph{\DIFadd{Archives of Pathology and Laboratory Medicine}}\DIFadd{, vol. 142, no.~5, pp. 638--644, 2018.
}

\bibitem{Garcia-Vazquez2020}
\DIFadd{V.~Garci}{\DIFadd{\'{a}}}\DIFadd{-V}{\DIFadd{\'{a}}}\DIFadd{zquez, F.~}{\DIFadd{Von Haxthausen}}\DIFadd{, S.~J}{\DIFadd{\"{a}}}\DIFadd{ckle, C.~Schumann, I.~Kuhlemann, J.~Bouchagiar, A.~C. H}{\DIFadd{\"{o}}}\DIFadd{fer, F.~Matysiak, G.~H}{\DIFadd{\"{u}}}\DIFadd{ttmann, J.~P. Goltz, M.~Kleemann, F.~Ernst, and M.~Horn, ``}{\DIFadd{Navigation and visualisation with HoloLens in endovascular aortic repair}}\DIFadd{,'' }\emph{\DIFadd{Innovative Surgical Sciences}}\DIFadd{, vol.~3, no.~3, pp. 167--177, 2020.
}

\bibitem{Unger2019}
\BIBentryALTinterwordspacing
\DIFadd{M.~Unger, D.~Black, N.~M. Fischer, T.~Neumuth, and B.~Glaser, ``}{\DIFadd{Design and evaluation of an eye tracking support system for the scrub nurse}}\DIFadd{,'' }\emph{\DIFadd{The International Journal of Medical Robotics and Computer Assisted Surgery}}\DIFadd{, vol.~15, no.~1, p. e1954, 2019. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://onlinelibrary.wiley.com/doi/abs/10.1002/rcs.1954}
\BIBentrySTDinterwordspacing

\bibitem{Mewes2018}
\BIBentryALTinterwordspacing
\DIFadd{A.~Mewes, F.~Heinrich, B.~Hensen, F.~Wacker, K.~Lawonn, and C.~Hansen, ``}\BIBforeignlanguage{eng}{{Concepts for augmented reality visualisation to support needle guidance inside the MRI}}\DIFadd{,'' }\emph{\BIBforeignlanguage{eng}{Healthcare technology letters}}\DIFadd{, vol.~5, no.~5, pp. 172--176, sep 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.ncbi.nlm.nih.gov/pubmed/30464849 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6222244/}
\BIBentrySTDinterwordspacing

\bibitem{Agten2018}
\DIFadd{C.~A. Agten, C.~Dennler, A.~B. Rosskopf, L.~Jaberg, C.~W. Pfirrmann, and M.~Farshad, ``}{\DIFadd{Augmented Reality-Guided Lumbar Facet Joint Injections}}\DIFadd{,'' }\emph{\DIFadd{Investigative Radiology}}\DIFadd{, vol.~53, no.~8, pp. 495--498, 2018.
}

\bibitem{Li2019a}
\BIBentryALTinterwordspacing
\DIFadd{J.~Li, Q.~Li, X.~Dai, J.~Li, and X.~Zhang, ``}\BIBforeignlanguage{eng}{{Does pre-scanning training improve the image quality of children receiving magnetic resonance imaging?: A meta-analysis of current studies}}\DIFadd{,'' }\emph{\BIBforeignlanguage{eng}{Medicine}}\DIFadd{, vol.~98, no.~5, pp. e14\,323--e14\,323, 2019-02. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.ncbi.nlm.nih.gov/pubmed/30702613 https://www.ncbi.nlm.nih.gov/pmc/PMC6380694/}
\BIBentrySTDinterwordspacing

\bibitem{Si2018}
\DIFadd{W.~Si, X.~Liao, Y.~Qian, and Q.~Wang, ``}{\DIFadd{Mixed Reality Guided Radiofrequency Needle Placement: A Pilot Study}}\DIFadd{,'' }\emph{\DIFadd{IEEE Access}}\DIFadd{, vol.~6, pp. 31\,493--31\,502, 2018.
}

\bibitem{Blum2012}
\DIFadd{T.~Blum, R.~Stauder, E.~Euler, and N.~Navab, ``}{\DIFadd{Superman-like X-ray vision: Towards brain-computer interfaces for medical augmented reality}}\DIFadd{,'' }\emph{\DIFadd{ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers}}\DIFadd{, pp. 271--272, 2012.
}

\bibitem{Booij2019}
\BIBentryALTinterwordspacing
\DIFadd{R.~Booij, R.~P.~J. Budde, M.~L. Dijkshoorn, and M.~van Straten, ``}{\DIFadd{Accuracy of automated patient positioning in CT using a 3D camera for body contour detection}}\DIFadd{,'' }\emph{\DIFadd{European Radiology}}\DIFadd{, vol.~29, no.~4, pp. 2079--2088, apr 2019. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s00330-018-5745-z}
\BIBentrySTDinterwordspacing

\bibitem{Kania2014}
\DIFadd{M.~Kania, H.~Rix, M.~Fereniec, H.~Zavala-Fernandez, D.~Janusek, T.~Mroczka, G.~Stix, and R.~Maniewski, ``}{\DIFadd{The effect of precordial lead displacement on ECG morphology}}\DIFadd{,'' }\emph{\DIFadd{Medical and Biological Engineering and Computing}}\DIFadd{, vol.~52, no.~2, pp. 109--119, 2014.
}

\bibitem{Hadjiantoni2021}
\DIFadd{A.~Hadjiantoni, K.~Oak, S.~Mengi, J.~Konya, and T.~Ungvari, ``}{\DIFadd{Is the Correct Anatomical Placement of the Electrocardiogram (ECG) Electrodes Essential to Diagnosis in the Clinical Setting: A Systematic Review}}\DIFadd{,'' }\emph{\DIFadd{Cardiology and Cardiovascular Medicine}}\DIFadd{, vol.~05, no.~02, pp. 182--200, 2021.
}

\bibitem{Petri2018}
\DIFadd{K.~Petri, K.~Witte, N.~Bandow, P.~Emmermacher, S.~Masik, M.~Dannenberg, S.~Salb, L.~Zhang, and G.~Brunnett, }\emph{{\DIFadd{Development of an Autonomous Character in Karate Kumite}}}\DIFadd{, 2018, vol. 663, no. Iacss.
}

\bibitem{Wang2017a}
\BIBentryALTinterwordspacing
\DIFadd{R.~Wang, Z.~Geng, Z.~Zhang, R.~Pei, and X.~Meng, ``}{\DIFadd{Autostereoscopic augmented reality visualization for depth perception in endoscopic surgery}}\DIFadd{,'' }\emph{\DIFadd{Displays}}\DIFadd{, vol.~48, pp. 50--60, 2017. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.displa.2017.03.003}
\BIBentrySTDinterwordspacing

\bibitem{Santos2015}
\DIFadd{M.~E.~C. Santos, M.~Terawaki, T.~Taketomi, G.~Yamamoto, and H.~Kato, ``}{\DIFadd{Development of handheld augmented reality X-Ray for K-12 settings}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Educational Technology}}\DIFadd{, no. 9783662444467, pp. 199--219, 2015.
}

\bibitem{Kanodia2005}
\DIFadd{R.~L. Kanodia, L.~Linsen, and B.~Hamann, ``}{\DIFadd{Multiple transparent material-enriched isosurfaces}}\DIFadd{,'' }\emph{\DIFadd{13th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2005, WSCG'2005 - In Co-operation with EUROGRAPHICS, Full Papers}}\DIFadd{, no. January, pp. 23--30, 2005.
}

\bibitem{Guo2012}
\DIFadd{H.~Guo, X.~Yuan, J.~Liu, G.~Shan, X.~Chi, and F.~Sun, ``}{\DIFadd{Interference microscopy volume illustration for biomedical data}}\DIFadd{,'' }\emph{\DIFadd{IEEE Pacific Visualization Symposium 2012, PacificVis 2012 - Proceedings}}\DIFadd{, vol. Ill, pp. 177--184, 2012.
}

\bibitem{Vishton1995}
\BIBentryALTinterwordspacing
\DIFadd{J.~C. Vishton and P.M., ``}{\DIFadd{chapter Perceiving Layout and Knowing Distances : The Integration, Relative Potency, and Contextual Use of Different Information about Depth}}\DIFadd{,'' }\emph{\DIFadd{Perception of Space and Motion}}\DIFadd{, vol.~22, no.~5, pp. 69--117, 1995. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.22.5.1299}
\BIBentrySTDinterwordspacing

\bibitem{alma9911190913502466}
\DIFadd{G.~T. Fechner and W.~M. Wundt, }\emph{{\DIFadd{Elemente der Psychophysik. Erster Theil / von Gustav Theodor Fechner ; }[\DIFadd{herausgeber: W. Wundt}]\DIFadd{.}}}\DIFadd{, 2nd~ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Leipzig: Breitkopf \& Hartel, 1889.
}

\bibitem{Boring1942}
\DIFadd{E.~G. Boring, }\emph{{\DIFadd{Sensation and perception in the history of experimental psychology.}}}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Oxford, England: Appleton-Century, 1942.
}

\bibitem{French2022}
\BIBentryALTinterwordspacing
\DIFadd{R.~L. French and G.~C. Deangelis, ``}{\DIFadd{Scene‑relative object motion biases depth percepts}}\DIFadd{,'' }\emph{\DIFadd{Scientific Reports}}\DIFadd{, pp. 1--17, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1038/s41598-022-23219-4}
\BIBentrySTDinterwordspacing

\bibitem{Lee1980}
\BIBentryALTinterwordspacing
\DIFadd{D.~N. Lee, H.~Kalmus, H.~C. Longuet-Higgins, and N.~S. Sutherland, ``The optic flow field: the foundation of vision,'' }\emph{\DIFadd{Philosophical Transactions of the Royal Society of London. B, Biological Sciences}}\DIFadd{, vol. 290, no. 1038, pp. 169--179, 1980. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1980.0089}
\BIBentrySTDinterwordspacing

\bibitem{Shojiro1991}
\DIFadd{S.~Nagata, }\emph{\DIFadd{How to reinforce perception of depth in single two-dimensional pictures - A comparative study of various cues for depth perception}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax {\DIFadd{National Aeronautics and Space Administration (NASA)}}\DIFadd{, 01 1991, pp. 527--545.
}

\bibitem{Zannoli2016}
\DIFadd{M.~Zannoli, G.~D. Love, R.~Narain, and M.~S. Banks, ``}{\DIFadd{Blur and the perception of depth at occlusions}}\DIFadd{,'' }\emph{\DIFadd{Journal of Vision}}\DIFadd{, vol.~16, no.~6, pp. 1--25, 2016.
}

\bibitem{Berkeley1948}
\DIFadd{G.~Berkeley, ``}{\DIFadd{An essay toward a new theory of vision, 1709.}}\DIFadd{'' in }\emph{\DIFadd{Readings in the history of psychology.}}\DIFadd{, ser. Century psychology series.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{East Norwalk, CT, US: Appleton-Century-Crofts, 1948, pp. 69--80.
}

\bibitem{Watson1992}
\DIFadd{J.~S. Watson, M.~S. Banks, C.~von Hofsten, and C.~S. Royden, ``}{\DIFadd{Gravity as a monocular cue for perception of absolute distance and/or absolute size.}}\DIFadd{'' }\emph{\DIFadd{Perception}}\DIFadd{, vol.~21, no.~1, pp. 69--76, 1992.
}

\bibitem{Ping2020a}
\DIFadd{J.~Ping, B.~H. Thomas, J.~Baumeister, J.~Guo, D.~Weng, and Y.~Liu, ``}{\DIFadd{Effects of shading model and opacity on depth perception in optical see-through augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Journal of the Society for Information Display}}\DIFadd{, no. May, pp. 1--13, 2020.
}

\bibitem{Aoi2020}
\BIBentryALTinterwordspacing
\DIFadd{D.~Aoi, K.~Hasegawa, L.~Li, Y.~Sakano, and S.~Tanaka, }\emph{{\DIFadd{Improving depth perception using multiple iso-surfaces for transparent stereoscopic visualization of medical volume data}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Springer Singapore, 2020, vol. 192. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1007/978-981-15-5852-8_6}
\BIBentrySTDinterwordspacing

\bibitem{Cutting1997}
\DIFadd{J.~E. Cutting, ``}{\DIFadd{How the eye measures reality and virtual reality}}\DIFadd{,'' }\emph{\DIFadd{Virtual Reality}}\DIFadd{, vol.~29, no.~1, pp. 27--36, 1997.
}

\bibitem{Siegel2000}
\DIFadd{M.~Siegel and S.~Nagata, ``}{\DIFadd{Just enough reality: comfortable 3-D viewing via microstereopsis}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Circuits and Systems for Video Technology}}\DIFadd{, vol.~10, no.~3, pp. 387--396, 2000.
}

\bibitem{Lawonn2018}
\DIFadd{K.~Lawonn, I.~Viola, B.~Preim, and T.~Isenberg, ``}{\DIFadd{A Survey of Surface-Based Illustrative Rendering for Visualization}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~37, no.~6, pp. 205--234, 2018.
}

\bibitem{gray1877anatomy}
\DIFadd{H.~Gray, }\emph{\DIFadd{Anatomy}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Chrysalis Books plc, 1877.
}

\bibitem{Interrante1995}
\DIFadd{V.~Interrante, H.~Fuchs, and S.~Pizer, ``}{\DIFadd{Enhancing transparent skin surfaces with ridge and valley lines}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE Visualization Conference}}\DIFadd{, pp. 52--59, 1995.
}

\bibitem{Interrante1997}
\DIFadd{V.~Interrante, H.~Fuchs, and S.~M. Pizer, ``}{\DIFadd{Conveying the 3D shape of smoothly curving transparent surfaces via texture}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~3, no.~2, pp. 98--117, 1997.
}

\bibitem{Interrante1997a}
\DIFadd{V.~Interrante, ``}{\DIFadd{Illustrating surface shape in volume data via principal direction-driven 3D line integral convolution}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1997}}\DIFadd{, vol.~D, pp. 109--116, 1997.
}

\bibitem{Hertzmann2000}
\DIFadd{A.~Hertzmann and D.~Zorin, ``}{\DIFadd{Illustrating smooth surfaces}}\DIFadd{,'' }\emph{\DIFadd{SIGGRAPH 2000 - Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques}}\DIFadd{, no. Section 5, pp. 517--526, 2000.
}

\bibitem{Praun2001}
\BIBentryALTinterwordspacing
\DIFadd{E.~Praun, H.~Hoppe, M.~Webb, and A.~Finkelstein, ``}{\DIFadd{Real-Time Hatching}}\DIFadd{,'' }\emph{\DIFadd{SIGGRAPH 2001}}\DIFadd{, vol.~1, p. 581, 2001. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://blendernpr.org/cross-hatch-shader/ https://doi.org/10.1145/383259.383328}
\BIBentrySTDinterwordspacing

\bibitem{Pelt2008}
\DIFadd{R.~V. Pelt, A.~Vilanova, and H.~V.~D. Wetering, ``}{\DIFadd{GPU-based Particle Systems for Illustrative Volume Rendering}}\DIFadd{,'' }\emph{\DIFadd{Volume- and Point-based Graphics}}\DIFadd{, vol.~vi, pp. 2--9, 2008.
}

\bibitem{Lawonn2013}
\DIFadd{K.~Lawonn, T.~Moench, and B.~Preim, ``}{\DIFadd{Computer Graphics Forum - 2013 - Lawonn - Streamlines for Illustrative Real‐Time Rendering}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~32, no.~3, pp. 312--330, 2013.
}

\bibitem{Gerl2012}
\DIFadd{M.~Gerl and T.~Isenberg, ``}{\DIFadd{Interactive Example-based Hatching}}\DIFadd{,'' }\emph{\DIFadd{Computers \& Graphics}}\DIFadd{, 2012.
}

\bibitem{Lawonn2017}
\BIBentryALTinterwordspacing
\DIFadd{K.~Lawonn, M.~Luz, and C.~Hansen, ``}{\DIFadd{Improving spatial perception of vascular models using supporting anchors and illustrative visualization}}\DIFadd{,'' }\emph{\DIFadd{Computers and Graphics (Pergamon)}}\DIFadd{, vol.~63, pp. 37--49, 2017. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.cag.2017.02.002}
\BIBentrySTDinterwordspacing

\bibitem{Lu2002}
\BIBentryALTinterwordspacing
\DIFadd{A.~Lu, J.~Taylor, M.~Hartner, D.~S. Ebert, and C.~D. Hansen, ``}{\DIFadd{Hardware-Accelerated Interactive Illustrative Stipple Drawing of Polygonal Objects}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of Vision, Modeling, and Visualization 2002 (VMV 2002, November 20--22, 2002, Erlangen, Germany)}}\DIFadd{, no. June 2014, pp. 61--68, 2002. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://www.ecn.purdue.edu/purpl/level2/papers/stipple_VMV_2002.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Pastor2004}
\DIFadd{O.~M. Pastor and T.~Strotthote, ``}{\DIFadd{Graph-based point relaxation for 3D stippling}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the Fifth Mexican International Conference in Computer Science, ENC 2004}}\DIFadd{, pp. 141--150, 2004.
}

\bibitem{Lu20}
\DIFadd{A.~Lu, C.~J. Morris, D.~S. Ebert, P.~Rheingans, and C.~Hansen, ``}{\DIFadd{Non-photorealistic volume rendering using stippling techniques}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE Visualization Conference}}\DIFadd{, no. May 2013, pp. 211--218, 2002.
}

\bibitem{MeruviaPastor2002}
\DIFadd{O.~E. }{\DIFadd{Meruvia Pastor}} \DIFadd{and T.~Strothotte, ``}{\DIFadd{Frame-Coherent Stippling}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of Eurographics 2002, Short Presentations}}\DIFadd{, pp. 145--152, 2002.
}

\bibitem{Ma2018}
\DIFadd{L.~Ma, J.~Guo, D.~M. Yan, H.~Sun, and Y.~Chen, ``}{\DIFadd{Instant Stippling on 3D Scenes}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~37, no.~7, pp. 255--266, 2018.
}

\bibitem{Bui2015}
\DIFadd{M.~T. Bui, J.~Kim, and Y.~Lee, ``}{\DIFadd{3D-look shading from contours and hatching strokes}}\DIFadd{,'' }\emph{\DIFadd{Computers and Graphics (Pergamon)}}\DIFadd{, vol.~51, pp. 167--176, 2015.
}

\bibitem{Svakhine2003}
\DIFadd{N.~A. Svakhine and D.~S. Ebert, ``}{\DIFadd{Interactive volume illustration and feature halos}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - Pacific Conference on Computer Graphics and Applications}}\DIFadd{, vol. 2003-Janua, pp. 347--354, 2003.
}

\bibitem{Lum2002}
\BIBentryALTinterwordspacing
\DIFadd{E.~B. Lum and K.-L. Ma, ``}{\DIFadd{Hardware-Accelerated Parallel Non-Photorealistic Volume Rendering}}\DIFadd{,'' in }\emph{\DIFadd{Association for Computing Machinery}}\DIFadd{, 2002, pp. 67--74. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1145/508530.508542}
\BIBentrySTDinterwordspacing

\bibitem{Lawonn2015}
\DIFadd{K.~Lawonn, ``}{\DIFadd{Feature Lines for Illustrating Medical Surface Models: Mathematical Background Feature Lines for Illustrating Medical Surface}}\DIFadd{,'' }\emph{\DIFadd{Visualization in Medicine and Life Sciences}}\DIFadd{, no. January, 2015.
}

\bibitem{Ozgur2017}
\DIFadd{E.~Ozgur, A.~Lafont, and A.~Bartoli, ``}{\DIFadd{Visualizing In-Organ Tumors in Augmented Monocular Laparoscopy}}\DIFadd{,'' }\emph{\DIFadd{Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017}}\DIFadd{, pp. 46--51, 2017.
}

\bibitem{Rheingans2001}
\DIFadd{P.~Rheingans and D.~Ebert, ``}{\DIFadd{Volume illustration: Nonphotorealistic rendering of volume models}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~7, no.~3, pp. 253--264, 2001.
}

\bibitem{Bruckner2007}
\DIFadd{S.~Bruckner and M.~}{\DIFadd{Eduard Gr}{\DIFadd{\"{o}}}\DIFadd{ller}}\DIFadd{, ``}{\DIFadd{Enhancing depth-perception with flexible volumetric halos}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~13, no.~6, pp. 1344--1351, 2007.
}

\bibitem{Bruckner2006}
\DIFadd{S.~Bruckner, S.~Grimm, A.~Kanitsar, and M.~E. Gr}{\DIFadd{\"{o}}}\DIFadd{ller, ``}{\DIFadd{Illustrative context-preserving exploration of volume data}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~12, no.~6, pp. 1559--1569, 2006.
}

\bibitem{Lorensen:1987:MCA}
\DIFadd{W.~E. Lorensen and H.~E. Cline, ``Marching cubes: A high resolution }{\DIFadd{3D}} \DIFadd{surface construction algorithm,'' }\emph{\DIFadd{SIGGRAPH Computer Graphics}}\DIFadd{, vol.~21, no.~4, pp. 163--169, Aug. 1987.
}

\bibitem{Drebin1988}
\DIFadd{R.~A. Drebin, L.~Carpenter, and P.~Hanrahan, ``}{\DIFadd{Volume rendering}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 15th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1988}}\DIFadd{, no. August 1988, pp. 65--74, 1988.
}

\bibitem{Keppel1975}
\DIFadd{E.~Keppel, ``Approximating complex surfaces by triangulation of contour lines,'' }\emph{\DIFadd{IBM Journal of Research and Development}}\DIFadd{, vol.~19, no.~1, pp. 2--11, 1975.
}

\bibitem{Fuchs1977}
\BIBentryALTinterwordspacing
\DIFadd{H.~Fuchs, Z.~M. Kedem, and S.~P. Uselton, ``Optimal surface reconstruction from planar contours,'' }\emph{\DIFadd{Commun. ACM}}\DIFadd{, vol.~20, no.~10, p. 693–702, oct 1977. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1145/359842.359846}
\BIBentrySTDinterwordspacing

\bibitem{Christiansen1978}
\DIFadd{H.~N. Christiansen and T.~W. Sederberg, ``}{\DIFadd{Conversion of complex contour line definitions into polygonal element mosaics}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1978}}\DIFadd{, pp. 187--192, 1978.
}

\bibitem{Herman1981}
\DIFadd{G.~Herman and J.~Udupa, ``}{\DIFadd{Display of three-dimensional discrete surfaces}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the Society of Photo-Optical Instrumentation Engineers}}\DIFadd{, vol. 283, pp. 90--97, 1981.
}

\bibitem{MEAGHER1982129}
\BIBentryALTinterwordspacing
\DIFadd{D.~Meagher, ``}{\DIFadd{Geometric modeling using octree encoding}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics and Image Processing}}\DIFadd{, vol.~19, no.~2, pp. 129--147, 1982. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.sciencedirect.com/science/article/pii/0146664X82901046}
\BIBentrySTDinterwordspacing

\bibitem{Wunsche2003}
\DIFadd{B.~C. W}{\DIFadd{\"{u}}}\DIFadd{nsche, ``}{\DIFadd{A toolkit for visualizing biomedical data sets}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 1st International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia, GRAPHITE '03}}\DIFadd{, no. June, 2003.
}

\bibitem{Liu2010}
\DIFadd{B.~Liu, B.~W}{\DIFadd{\"{u}}}\DIFadd{nsche, and T.~Ropinski, ``}{\DIFadd{Visualization by example: A constructive visual component-based interface for direct volume rendering}}\DIFadd{,'' }\emph{\DIFadd{GRAPP 2010 - Proceedings of the International Conference on Computer Graphics Theory and Applications}}\DIFadd{, pp. 254--259, 2010.
}

\bibitem{Douglas–Peucker1973}
\BIBentryALTinterwordspacing
\DIFadd{D.~H. DOUGLAS and T.~K. PEUCKER, ``Algorithms for the reduction of the number of points required to represent a digitized line or its caricature,'' }\emph{\DIFadd{Cartographica}}\DIFadd{, vol.~10, no.~2, pp. 112--122, 1973. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.3138/FM57-6770-U75U-7727}
\BIBentrySTDinterwordspacing

\bibitem{Meissner2000}
\BIBentryALTinterwordspacing
\DIFadd{M.~Mei}{\DIFadd{\ss}}\DIFadd{ner, H.~Pfister, R.~Westermann, and C.~M. Wittenbrink, ``}{\DIFadd{Volume Visualization and Volume Rendering Techniques}}\DIFadd{,'' }\emph{\DIFadd{Eurographics}}\DIFadd{, vol.~Vi, no. June, 2000. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://ismusicmake.googlecode.com/svn-hi/trunk/rc_Paper/10.1.1.93.2014.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Farrell1983}
\DIFadd{E.~J. Farrell, ``Color display and interactive interpretation of three-dimensional data,'' }\emph{\DIFadd{IBM Journal of Research and Development}}\DIFadd{, vol.~27, no.~4, pp. 356--366, 1983.
}

\bibitem{Ney1990}
\DIFadd{D.~R. Ney, R.~A. Drebin, and D.~Magid, ``}{\DIFadd{Volumetric Rendering of Computed Tomography Data: Principles and Techniques}}\DIFadd{,'' }\emph{\DIFadd{IEEE Computer Graphics and Applications}}\DIFadd{, vol.~10, no.~2, pp. 24--32, 1990.
}

\bibitem{Kaufman2000}
\BIBentryALTinterwordspacing
\DIFadd{A.~E. Kaufman, }\emph{{\DIFadd{Volume Visualization in Medicine}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Academic Press, 2000, vol.~Vi. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/B978-012077790-7/50050-3}
\BIBentrySTDinterwordspacing

\bibitem{Engel2001}
\DIFadd{K.~Engel, M.~Kraus, and T.~Ertl, ``}{\DIFadd{High-quality pre-integrated volume rendering using hardware-accelerated pixel shading}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM SIGGRAPH Conference on Computer Graphics}}\DIFadd{, no. WORKSHOP, pp. 9--16, 2001.
}

\bibitem{MacDougall2016}
\BIBentryALTinterwordspacing
\DIFadd{P.~J. MacDougall, C.~E. Henze, and A.~Volkov, ``}{\DIFadd{Volume-rendering on a 3D hyperwall: A molecular visualization platform for research, education and outreach}}\DIFadd{,'' }\emph{\DIFadd{Journal of Molecular Graphics and Modelling}}\DIFadd{, vol.~70, pp. 1--6, 2016. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.jmgm.2016.09.002}
\BIBentrySTDinterwordspacing

\bibitem{Riley2003}
\DIFadd{K.~Riley, D.~Ebert, C.~Hansen, and J.~Levit, ``}{\DIFadd{Visually Accurate Multi-Field Weather Visualization}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE Visualization Conference}}\DIFadd{, pp. 279--286, 2003.
}

\bibitem{Wang2018}
\DIFadd{Y.~Wang, S.~Zhang, B.~Wan, W.~He, and X.~Bai, ``}{\DIFadd{Point cloud and visual feature-based tracking method for an augmented reality-aided mechanical assembly system}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Advanced Manufacturing Technology}}\DIFadd{, 2018.
}

\bibitem{Okuyan2014}
\DIFadd{E.~Okuyan, U.~G}{\DIFadd{\"{u}}}\DIFadd{d}{\DIFadd{\"{u}}}\DIFadd{kbay, C.~Bulutay, and K.~H. Heinig, ``}{\DIFadd{MaterialVis: Material visualization tool using direct volume and surface rendering techniques}}\DIFadd{,'' }\emph{\DIFadd{Journal of Molecular Graphics and Modelling}}\DIFadd{, vol.~50, pp. 50--60, 2014.
}

\bibitem{Groger2022}
\BIBentryALTinterwordspacing
\DIFadd{B.~Gr}{\DIFadd{\"{o}}}\DIFadd{ger, D.~K}{\DIFadd{\"{o}}}\DIFadd{hler, J.~Vorderbr}{\DIFadd{\"{u}}}\DIFadd{ggen, J.~Troschitz, R.~Kupfer, G.~Meschut, and M.~Gude, ``}{\DIFadd{Computed tomography investigation of the material structure in clinch joints in aluminium fibre-reinforced thermoplastic sheets}}\DIFadd{,'' }\emph{\DIFadd{Production Engineering}}\DIFadd{, vol.~16, no. 2-3, pp. 203--212, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s11740-021-01091-x}
\BIBentrySTDinterwordspacing

\bibitem{Grottel2012}
\DIFadd{S.~Grottel, M.~Krone, K.~Scharnowski, and T.~Ertl, ``}{\DIFadd{Object-space ambient occlusion for molecular dynamics}}\DIFadd{,'' }\emph{\DIFadd{IEEE Pacific Visualization Symposium 2012, PacificVis 2012 - Proceedings}}\DIFadd{, pp. 209--216, 2012.
}

\bibitem{Nguyen2022}
\BIBentryALTinterwordspacing
\DIFadd{N.~Nguyen, F.~Liang, D.~Engel, C.~Bohak, P.~Wonka, T.~Ropinski, and I.~Viola, ``}{\DIFadd{Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization}}\DIFadd{,'' }\emph{\DIFadd{ArXiv}}\DIFadd{, no.~1, pp. 1--22, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://arxiv.org/abs/2205.04464}
\BIBentrySTDinterwordspacing

\bibitem{Baker2007}
\DIFadd{G.~S. Baker, T.~E. Jordan, and J.~Pardy, ``}{\DIFadd{An introduction to ground penetrating radar (GPR)}}\DIFadd{,'' }\emph{\DIFadd{Special Paper of the Geological Society of America}}\DIFadd{, vol. 432, pp. 1--18, 2007.
}

\bibitem{Zehner2021}
\DIFadd{B.~Zehner, ``}{\DIFadd{On the visualization of 3D geological models and their uncertainty}}\DIFadd{,'' }\emph{\DIFadd{Zeitschrift der Deutschen Gesellschaft fur Geowissenschaften}}\DIFadd{, vol. 172, no.~1, pp. 83--98, 2021.
}

\bibitem{Hui1993}
\DIFadd{Y.~Hui and W.~Ng, ``}{\DIFadd{3D cursors for volume rendering applications}}\DIFadd{,'' }\emph{\DIFadd{IEEE Access}}\DIFadd{, pp. 1243--1245, 1993.
}

\bibitem{Kersten2006}
\DIFadd{M.~A. Kersten, A.~J. Stewart, N.~Troje, and R.~Ellis, ``}{\DIFadd{Enhancing depth perception in translucent volumes}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~12, no.~5, pp. 1117--1123, 2006.
}

\bibitem{Perlin1989}
\BIBentryALTinterwordspacing
\DIFadd{K.~Perlin and E.~M. Hoffert, ``}{\DIFadd{Hypertexture}}\DIFadd{,'' }\emph{\DIFadd{ACM SIGGRAPH Computer Graphics}}\DIFadd{, vol.~23, no.~3, pp. 253--262, 1989. }[\DIFadd{Online}]\DIFadd{. Available: }\url{htps://doi.org/10.1145/74334.74359}
\BIBentrySTDinterwordspacing

\bibitem{Kersten-Oertel2014}
\DIFadd{M.~Kersten-Oertel, S.~J.~S. Chen, and D.~L. Collins, ``}{\DIFadd{An evaluation of depth enhancing perceptual cues for vascular volume visualization in neurosurgery}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~20, no.~3, pp. 391--403, 2014.
}

\bibitem{Corcoran2012}
\DIFadd{A.~Corcoran and J.~Dingliana, ``}{\DIFadd{Real-time illumination for two-level volume rendering}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 7431 LNCS, no. PART 1, pp. 544--555, 2012.
}

\bibitem{Laha2013}
\DIFadd{B.~Laha, D.~A. Bowman, and J.~D. Schiffbauer, ``}{\DIFadd{Validation of the MR Simulation Approach for Evaluating the Effects of Immersion on Visual Analysis of Volume Data}}\DIFadd{,'' }\emph{\DIFadd{IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS}}\DIFadd{, vol.~9, no.~4, pp. 529 -- 538, 2013.
}

\bibitem{Laha2012}
\DIFadd{B.~Laha, K.~Sensharma, J.~D. Schiffbauer, and D.~A. Bowman, ``}{\DIFadd{Effects of immersion on visual analysis of volume data}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~18, no.~4, pp. 597--606, 2012.
}

\bibitem{Laha2012a}
\DIFadd{B.~Laha and D.~A. Bowman, ``}{\DIFadd{Identifying the Benefits of Immersion in Virtual Reality for Volume Data Visualization}}\DIFadd{,'' }\emph{\DIFadd{Immersive Visualization Revisited Workshop of the IEEE VR conference}}\DIFadd{, vol.~D, no. March, pp. 1--2, 2012.
}

\bibitem{Laha2014}
\DIFadd{B.~Laha, D.~A. Bowman, and J.~J. Socha, ``}{\DIFadd{Effects of VR system fidelity on analyzing isosurface visualization of volume datasets}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~20, no.~4, pp. 513--522, 2014.
}

\bibitem{Laha2016}
\DIFadd{B.~Laha, D.~A. Bowman, D.~H. Laidlaw, and J.~J. Socha, ``}{\DIFadd{A classification of user tasks in visual analysis of volume data}}\DIFadd{,'' }\emph{\DIFadd{2015 IEEE Scientific Visualization Conference, SciVis 2015 - Proceedings}}\DIFadd{, vol.~D, pp. 1--8, 2016.
}

\bibitem{Shen2014}
\DIFadd{E.~Shen, S.~Li, X.~Cai, L.~Zeng, and W.~Wang, ``}{\DIFadd{Sketch-based interactive visualization: a survey}}\DIFadd{,'' }\emph{\DIFadd{Journal of Visualization}}\DIFadd{, vol.~17, no.~4, pp. 275--294, 2014.
}

\bibitem{Feng20152548}
\DIFadd{X.-M. Feng, L.-D. Wu, R.-H. Yu, and C.~Yang, ``}{\DIFadd{Enhanced depth perception grid-projection algorithm for direct volume rendering}}\DIFadd{,'' }\emph{\DIFadd{Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology}}\DIFadd{, vol.~37, no.~11, pp. 2548 -- 2554, 2015.
}

\bibitem{Grosset2013}
\DIFadd{A.~V. Grosset, M.~Schott, G.~P. Bonneau, and C.~D. Hansen, ``}{\DIFadd{Evaluation of depth of field for depth perception in DVR}}\DIFadd{,'' }\emph{\DIFadd{IEEE Pacific Visualization Symposium}}\DIFadd{, pp. 81--88, 2013.
}

\bibitem{Roberts2016}
\DIFadd{J.~P. Roberts, T.~R. Fisher, M.~J. Trowbridge, and C.~Bent, ``}{\DIFadd{A design thinking framework for healthcare management and innovation}}\DIFadd{,'' }\emph{\DIFadd{Healthcare}}\DIFadd{, vol.~4, no.~1, pp. 11--14, 2016.
}

\bibitem{Englund2016}
\DIFadd{R.~Englund and T.~Ropinski, ``}{\DIFadd{Evaluating the perception of semi-transparent structures in direct volume rendering techniques}}\DIFadd{,'' }\emph{\DIFadd{SA 2016 - SIGGRAPH ASIA 2016 Symposium on Visualization}}\DIFadd{, 2016.
}

\bibitem{Englund2018}
\DIFadd{------, ``}{\DIFadd{Quantitative and Qualitative Analysis of the Perception of Semi-Transparent Structures in Direct Volume Rendering}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~37, no.~6, pp. 174--187, 2018.
}

\bibitem{Milgram1994}
\DIFadd{P.~Milgram and F.~Kishimo, ``}{\DIFadd{A taxonomy of mixed reality}}\DIFadd{,'' }\emph{\DIFadd{IEICE Transactions on Information and Systems}}\DIFadd{, vol.~77, no.~12, pp. 1321--1329, 1994.
}

\bibitem{Sutherland1968}
\DIFadd{I.~E. Sutherland, ``}{\DIFadd{A Head-Mounted Three Dimentional Display}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the AFIPS Fall Joint Computer Conference}}\DIFadd{, pp. 295--302, 1968.
}

\bibitem{Baus2014}
\DIFadd{O.~Baus and S.~Bouchard, ``}{\DIFadd{Moving from virtual reality exposure-based therapy to augmented reality exposure-based therapy: A review}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Human Neuroscience}}\DIFadd{, vol.~8, no. MAR, pp. 1--15, 2014.
}

\bibitem{Pizer1986}
\DIFadd{S.~M. Pizer, ``}{\DIFadd{Systems for 3D Display in Medical Imaging}}\DIFadd{,'' }\emph{\DIFadd{Pictorial Information Systems in Medicine}}\DIFadd{, pp. 235--249, 1986.
}

\bibitem{Wang1990}
\DIFadd{J.~fang Wang, V.~Chi, and H.~Fuchs, ``}{\DIFadd{Real-time optical 3D tracker for head-mounted display systems}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics (ACM)}}\DIFadd{, vol.~24, no.~2, pp. 205--215, 1990.
}

\bibitem{Mann1997}
\DIFadd{S.~Mann, ``}{\DIFadd{Wearable computing: A first step toward personal imaging}}\DIFadd{,'' }\emph{\DIFadd{Computer}}\DIFadd{, vol.~30, no.~2, pp. 25--32, 1997.
}

\bibitem{Park2021}
\DIFadd{S.~Park, S.~Bokijonov, and Y.~Choi, ``}{\DIFadd{Review of microsoft hololens applications over the past five years}}\DIFadd{,'' }\emph{\DIFadd{Applied Sciences (Switzerland)}}\DIFadd{, vol.~11, no.~16, 2021.
}

\bibitem{Marto2022}
\DIFadd{A.~Marto and A.~Gon}{\DIFadd{\c{c}}}\DIFadd{alves, ``}{\DIFadd{Augmented Reality Games and Presence: A Systematic Review}}\DIFadd{,'' }\emph{\DIFadd{Journal of Imaging}}\DIFadd{, vol.~8, no.~4, 2022.
}

\bibitem{Kim2018}
\DIFadd{K.~Kim, M.~Billinghurst, G.~Bruder, H.~B. Duh, and G.~F. Welch, ``}{\DIFadd{Revisiting Trends in Augmented Reality Research: A Review of the 2nd Decade of ISMAR (2008–2017)}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~24, no.~11, pp. 2947--2962, 2018.
}

\bibitem{Hoang2017}
\DIFadd{T.~Hoang, M.~Reinoso, Z.~Joukhadar, F.~Vetere, and D.~Kelly, ``}{\DIFadd{Augmented Studio: Projection Mapping on Moving Body for Physiotherapy Education}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}}\DIFadd{, pp. 1419--1430, 2017.
}

\bibitem{Knodel2018}
\BIBentryALTinterwordspacing
\DIFadd{M.~M. Knodel, B.~Lemke, M.~Lampe, M.~Hoffer, C.~Gillmann, M.~Uder, J.~Hillenga}{\DIFadd{\ss}}\DIFadd{, G.~Wittum, and T.~B}{\DIFadd{\"{a}}}\DIFadd{uerle, ``}{\DIFadd{Virtual reality in advanced medical immersive imaging: a workflow for introducing virtual reality as a supporting tool in medical imaging}}\DIFadd{,'' }\emph{\DIFadd{Computing and Visualization in Science}}\DIFadd{, vol.~18, no.~6, pp. 203--212, 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s00791-018-0292-3}
\BIBentrySTDinterwordspacing

\bibitem{Ding2022}
\DIFadd{X.~Ding and Z.~Li, ``}{\DIFadd{A review of the application of virtual reality technology in higher education based on Web of Science literature data as an example}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Education}}\DIFadd{, vol.~7, 2022.
}

\bibitem{Xie2021}
\DIFadd{B.~Xie, H.~Liu, R.~Alghofaili, Y.~Zhang, Y.~Jiang, F.~D. Lobo, C.~Li, W.~Li, H.~Huang, M.~Akdere, C.~Mousas, and L.~F. Yu, ``}{\DIFadd{A Review on Virtual Reality Skill Training Applications}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Virtual Reality}}\DIFadd{, vol.~2, no. April, pp. 1--19, 2021.
}

\bibitem{Lee2008}
\DIFadd{E.~A.~L. Lee and K.~W. Wong, ``}{\DIFadd{A review of using virtual reality for learning}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 5080 LNCS, pp. 231--241, 2008.
}

\bibitem{Zhan2020}
\BIBentryALTinterwordspacing
\DIFadd{T.~Zhan, K.~Yin, J.~Xiong, Z.~He, and S.~T. Wu, ``}{\DIFadd{Augmented Reality and Virtual Reality Displays: Perspectives and Challenges}}\DIFadd{,'' }\emph{\DIFadd{iScience}}\DIFadd{, vol.~23, no.~8, p. 101397, 2020. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1016/j.isci.2020.101397}
\BIBentrySTDinterwordspacing

\bibitem{microsofthololens}
\BIBentryALTinterwordspacing
\DIFadd{``Microsoft hololens: Mixed reality technology for business.'' }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.microsoft.com/en-us/hololens}
\BIBentrySTDinterwordspacing

\bibitem{Karambakhsh2019}
\DIFadd{A.~Karambakhsh, A.~Kamel, B.~Sheng, P.~Li, P.~Yang, and D.~D. Feng, ``}{\DIFadd{Deep gesture interaction for augmented anatomy learning}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Information Management}}\DIFadd{, vol.~45, no. March, pp. 328--336, 2019.
}

\bibitem{Kim2022}
\DIFadd{J.~Kim, J.~I. Hwang, and J.~Lee, ``}{\DIFadd{VR Color Picker: Three-Dimensional Color Selection Interfaces}}\DIFadd{,'' }\emph{\DIFadd{IEEE Access}}\DIFadd{, vol.~10, pp. 65\,809--65\,824, 2022.
}

\bibitem{Cordeil2017}
\DIFadd{M.~Cordeil, A.~Cunningham, T.~Dwyer, B.~H. Thomas, and K.~Marriott, ``}{\DIFadd{ImAxes: Immersive axes as embodied affordances for interactive multivariate data visualisation}}\DIFadd{,'' }\emph{\DIFadd{UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}}\DIFadd{, pp. 71--83, 2017.
}

\bibitem{Tanagho2012}
\DIFadd{Y.~S. Tanagho, G.~L. Andriole, A.~G. Paradis, K.~M. Madison, G.~S. Sandhu, J.~E. Varela, and B.~M. Benway, ``}{\DIFadd{2D versus 3D visualization: Impact on laparoscopic proficiency using the fundamentals of laparoscopic surgery skill set}}\DIFadd{,'' }\emph{\DIFadd{Journal of Laparoendoscopic and Advanced Surgical Techniques}}\DIFadd{, vol.~22, no.~9, pp. 865--870, 2012.
}

\bibitem{Mujber2004}
\DIFadd{T.~S. Mujber, T.~Szecsi, and M.~S. Hashmi, ``}{\DIFadd{Virtual reality applications in manufacturing process simulation}}\DIFadd{,'' }\emph{\DIFadd{Journal of Materials Processing Technology}}\DIFadd{, vol. 155-156, no. 1-3, pp. 1834--1838, 2004.
}

\bibitem{Qu2010}
\BIBentryALTinterwordspacing
\DIFadd{H.~Qu, Q.~Zhu, M.~Guo, and Z.~Lu, ``}{\DIFadd{Simulation of carbon-based model for virtual plants as complex adaptive system}}\DIFadd{,'' }\emph{\DIFadd{Simulation Modelling Practice and Theory}}\DIFadd{, vol.~18, no.~6, pp. 677--695, 2010. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.simpat.2010.01.004}
\BIBentrySTDinterwordspacing

\bibitem{Guo2019}
\DIFadd{N.~Guo, T.~Wang, B.~Yang, L.~Hu, H.~Liu, and Y.~Wang, ``}{\DIFadd{An online calibration method for Microsoft Hololens}}\DIFadd{,'' }\emph{\DIFadd{IEEE Access}}\DIFadd{, vol.~7, pp. 101\,795--101\,803, 2019.
}

\bibitem{Andrews2021}
\DIFadd{C.~M. Andrews, A.~B. Henry, I.~M. Soriano, M.~K. Southworth, and J.~R. Silva, ``}{\DIFadd{Registration Techniques for Clinical Applications of Three-Dimensional Augmented Reality Devices}}\DIFadd{,'' }\emph{\DIFadd{IEEE Journal of Translational Engineering in Health and Medicine}}\DIFadd{, vol.~9, no. November 2020, 2021.
}

\bibitem{Rodrigues2017}
\DIFadd{D.~G. Rodrigues, A.~Jain, S.~R. Rick, S.~Liu, P.~Suresh, and N.~Weibel, ``}{\DIFadd{Exploring Mixed Reality in specialized surgical environments}}\DIFadd{,'' }\emph{\DIFadd{Conference on Human Factors in Computing Systems - Proceedings}}\DIFadd{, vol. Part F1276, pp. 2591--2598, 2017.
}

\bibitem{Zari2023}
\DIFadd{G.~Zari, S.~Condino, F.~Cutolo, and V.~Ferrari, ``}{\DIFadd{Magic Leap 1 versus Microsoft HoloLens 2 for the Visualization of 3D Content Obtained from Radiological Images}}\DIFadd{,'' }\emph{\DIFadd{Sensors}}\DIFadd{, vol.~23, no.~6, p. 3040, 2023.
}

\bibitem{Mejias2025}
\BIBentryALTinterwordspacing
\DIFadd{D.~Mej}{\DIFadd{\'{i}}}\DIFadd{as, I.~Yeregui, R.~Viola, M.~Fern}{\DIFadd{\'{a}}}\DIFadd{ndez, and M.~Montagud, ``}{\DIFadd{Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols}}\DIFadd{,'' 2025. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://arxiv.org/abs/2507.00623}
\BIBentrySTDinterwordspacing

\bibitem{Gao2025}
\DIFadd{S.~Gao, J.~Liu, Q.~Jiang, F.~Sinclair, W.~Sentosa, B.~Godfrey, and S.~Adve, ``}{\DIFadd{XRgo: Design and Evaluation of Rendering Offload for Low-Power Extended Reality Devices}}\DIFadd{,'' }\emph{\DIFadd{MMSys 2025 - Proceedings of the 16th ACM Multimedia Systems Conference}}\DIFadd{, pp. 124--135, 2025.
}

\bibitem{Pelanis2020}
\BIBentryALTinterwordspacing
\DIFadd{E.~Pelanis, R.~P. Kumar, D.~L. Aghayan, R.~Palomar, }{\DIFadd{\AA}}\DIFadd{.~A. Fretland, H.~Brun, O.~J. Elle, and B.~Edwin, ``}{\DIFadd{Use of mixed reality for improved spatial understanding of liver anatomy}}\DIFadd{,'' }\emph{\DIFadd{Minimally Invasive Therapy and Allied Technologies}}\DIFadd{, vol.~29, no.~3, pp. 154--160, 2020. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1080/13645706.2019.1616558}
\BIBentrySTDinterwordspacing

\bibitem{stadlinger2021cinematic}
\DIFadd{B.~Stadlinger, H.~Essig, P.~Schumann, H.~van Waes, S.~Valdec, and S.~Winklhofer, ``Cinematic rendering in der digitalen volumentomografie: Fotorealistische 3d-rekonstruktion dentaler und maxillofazialer pathologien,'' }\emph{\DIFadd{SWISS DENTAL JOURNAL SSO--Science and Clinical Topics}}\DIFadd{, vol. 131, no.~2, pp. 133--139, 2021.
}

\bibitem{Stadlinger2019}
\DIFadd{B.~Stadlinger, S.~Valdec, L.~Wacht, H.~Essig, and S.~Winklhofer, ``3d-cinematic rendering for dental and maxillofacial imaging,'' }\emph{\DIFadd{Dentomaxillofacial Radiology}}\DIFadd{, vol.~49, p. 20190249, 07 2019.
}

\bibitem{Steffen2022}
\BIBentryALTinterwordspacing
\DIFadd{T.~Steffen, S.~Winklhofer, F.~Starz, D.~Wiedemeier, U.~Ahmadli, and B.~Stadlinger, ``}{\DIFadd{Three-dimensional perception of cinematic rendering versus conventional volume rendering using CT and CBCT data of the facial skeleton}}\DIFadd{,'' }\emph{\DIFadd{Annals of Anatomy}}\DIFadd{, vol. 241, p. 151905, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1016/j.aanat.2022.151905}
\BIBentrySTDinterwordspacing

\bibitem{Heinrich2021}
\DIFadd{F.~Heinrich, L.~Schwenderling, M.~Streuber, K.~Bornemann, K.~Lawonn, and C.~Hansen, ``}{\DIFadd{Effects of Surface Visualizations on Depth Perception in Projective Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 2021 IEEE International Conference on Human-Machine Systems, ICHMS 2021}}\DIFadd{, pp. 0--5, 2021.
}

\bibitem{Furmanski2002}
\DIFadd{C.~Furmanski, R.~Azuma, and M.~Daily, ``}{\DIFadd{Augmented-reality visualizations guided by cognition: Perceptual heuristics for combining visible and obscured information}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - International Symposium on Mixed and Augmented Reality, ISMAR 2002}}\DIFadd{, pp. 215--224, 2002.
}

\bibitem{Ghasemi2018}
\DIFadd{S.~Ghasemi, ``}{\DIFadd{An Investigation of Using Random Dot Patterns to Achieve X-Ray Vision for Near-Field Applications of Stereoscopic Video Based Augmented Reality Displays}}\DIFadd{,'' }\emph{\DIFadd{MIT Press}}\DIFadd{, 2018.
}

\bibitem{Sandor2010}
\DIFadd{C.~Sandor, A.~Cunningham, A.~Dey, and V.~V. Mattila, ``}{\DIFadd{An augmented reality X-ray system based on visual saliency}}\DIFadd{,'' }\emph{\DIFadd{9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings}}\DIFadd{, pp. 27--36, 2010.
}

\bibitem{Fischer2023}
\DIFadd{M.~Fischer, J.~Rosenberg, C.~Leuze, B.~Hargreaves, and B.~Daniel, ``}{\DIFadd{The Impact of Occlusion on Depth Perception at Arm ' s Length}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~PP, pp. 1--9, 2023.
}

\bibitem{Li2016}
\DIFadd{H.~Li, R.~R. Corey, U.~Giudice, and N.~A. Giudice, ``}{\DIFadd{Assessment of visualization interfaces for assisting the development of multi-level cognitive maps}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 9744, no.~1, pp. 308--321, 2016.
}

\bibitem{Dey2012}
\DIFadd{A.~Dey, G.~Jarvis, C.~Sandor, and G.~Reitmayr, ``}{\DIFadd{Tablet versus phone: Depth perception in handheld augmented reality}}\DIFadd{,'' }\emph{\DIFadd{ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers}}\DIFadd{, no. November, pp. 187--196, 2012.
}

\bibitem{Wang2022}
\DIFadd{Z.~Wang, Y.~Zhao, and F.~Lu, ``}{\DIFadd{Gaze-Vergence-Controlled See-Through Vision in Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~28, no.~11, pp. 3843--3853, 2022.
}

\bibitem{Wang2023}
\DIFadd{J.~Wang, R.~Shi, W.~Zheng, W.~Xie, D.~Kao, and H.~N. Liang, ``}{\DIFadd{Effect of Frame Rate on User Experience, Performance, and Simulator Sickness in Virtual Reality}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~29, no.~5, pp. 2478--2488, 2023.
}

\bibitem{Liao2023}
\DIFadd{S.~Liao, Y.~Zhou, and V.~Popescu, ``}{\DIFadd{AR Interfaces for Disocclusion - A Comparative Study}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2023 IEEE Conference Virtual Reality and 3D User Interfaces, VR 2023}}\DIFadd{, pp. 530--540, 2023.
}

\bibitem{Cote2018}
\DIFadd{S.~C}{\DIFadd{\^{o}}}\DIFadd{t}{\DIFadd{\'{e}}} \DIFadd{and A.~Mercier, ``}{\DIFadd{Augmentation of Road Surfaces with Subsurface Utility Model Projections}}\DIFadd{,'' in }\emph{\DIFadd{2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}}\DIFadd{, 2018, pp. 535--536.
}

\bibitem{Eren2013}
\DIFadd{M.~T. Eren, M.~Cansoy, and S.~Balcisoy, ``}{\DIFadd{Multi-view augmented reality for underground exploration}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE Virtual Reality}}\DIFadd{, pp. 117--118, 2013.
}

\bibitem{Muthalif2022}
\DIFadd{M.~Z. Muthalif, D.~Shojaei, and K.~Khoshelham, ``}{\DIFadd{Resolving Perceptual Challenges of Visualizing Underground Utilities in Mixed Reality}}\DIFadd{,'' }\emph{\DIFadd{International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives}}\DIFadd{, vol.~48, no. 4/W4-2022, pp. 101--108, 2022.
}

\bibitem{Gruenefeld2020}
\DIFadd{U.~Gruenefeld, Y.~Br}{\DIFadd{\"{u}}}\DIFadd{ck, and S.~Boll, ``}{\DIFadd{Behind the Scenes: Comparing X-Ray Visualization Techniques in Head-mounted Optical See-through Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{ACM International Conference Proceeding Series}}\DIFadd{, pp. 179--185, 2020.
}

\bibitem{Kyto2013}
\DIFadd{M.~Kyt}{\DIFadd{\"{o}}}\DIFadd{, A.~M}{\DIFadd{\"{a}}}\DIFadd{kinen, J.~H}{\DIFadd{\"{a}}}\DIFadd{kkinen, and P.~Oittinen, ``}{\DIFadd{Improving relative depth judgments in augmented reality with auxiliary augmentations}}\DIFadd{,'' }\emph{\DIFadd{ACM Transactions on Applied Perception}}\DIFadd{, vol.~10, no.~1, 2013.
}

\bibitem{Kitajima2015}
\DIFadd{Y.~Kitajima, S.~Ikeda, and K.~Sato, ``}{[\DIFadd{POSTER}] \DIFadd{Vergence-based AR X-ray vision}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015}}\DIFadd{, pp. 188--189, 2015.
}

\bibitem{Dunn2018}
\DIFadd{D.~Dunn, Q.~Dong, H.~Fuchs, and P.~Chakravarthula, ``}{\DIFadd{Mitigating vergence-accommodation conflict for near-eye displays via deformable beamsplitters}}\DIFadd{,'' p. 104, 2018.
}

\bibitem{Kameda2004}
\DIFadd{Y.~Kameda, T.~Takemasa, and Y.~Ohta, ``}{\DIFadd{Outdoor see-through vision utilizing surveillance cameras}}\DIFadd{,'' }\emph{\DIFadd{ISMAR 2004: Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality}}\DIFadd{, no. Ismar, pp. 151--160, 2004.
}

\bibitem{Ohta2010}
\DIFadd{Y.~Ohta, Y.~Kameda, I.~Kitahara, M.~Hayashi, and S.~Yamazaki, ``}{\DIFadd{See-through vision: A visual augmentation method for sensing-web}}\DIFadd{,'' }\emph{\DIFadd{Communications in Computer and Information Science}}\DIFadd{, vol. 81 PART 2, pp. 690--699, 2010.
}

\bibitem{Tsuda2005}
\DIFadd{T.~Tsuda, H.~Yamamoto, Y.~Kameda, and Y.~Ohta, ``}{\DIFadd{Visualization methods for outdoor see-through vision}}\DIFadd{,'' }\emph{\DIFadd{ACM International Conference Proceeding Series}}\DIFadd{, vol. 157, pp. 62--69, 2005.
}

\bibitem{Aaskov2019}
\DIFadd{J.~Aaskov, G.~N. Kawchuk, K.~D. Hamaluik, P.~Boulanger, and J.~Hartvigsen, ``}{\DIFadd{X-ray vision: The accuracy and repeatability of a technology that allows clinicians to see spinal X-rays superimposed on a person's back}}\DIFadd{,'' }\emph{\DIFadd{PeerJ}}\DIFadd{, vol. 2019, no.~2, pp. 1--11, 2019.
}

\bibitem{DePaolis2019}
\DIFadd{L.~T. }{\DIFadd{De Paolis}} \DIFadd{and V.~}{\DIFadd{De Luca}}\DIFadd{, ``}{\DIFadd{Augmented visualization with depth perception cues to improve the surgeon's performance in minimally invasive surgery}}\DIFadd{,'' }\emph{\DIFadd{Medical and Biological Engineering and Computing}}\DIFadd{, vol.~57, no.~5, pp. 995--1013, 2019.
}

\bibitem{Erat2013}
\DIFadd{O.~Erat, O.~Pauly, S.~Weidert, P.~Thaller, E.~Euler, W.~Mutschler, N.~Navab, and P.~Fallavollita, ``}{\DIFadd{How a surgeon becomes superman by visualization of intelligently fused multi-modalities}}\DIFadd{,'' }\emph{\DIFadd{Medical Imaging 2013: Image-Guided Procedures, Robotic Interventions, and Modeling}}\DIFadd{, vol. 8671, no. March 2013, p. 86710L, 2013.
}

\bibitem{Pauly2012}
\DIFadd{O.~Pauly, A.~Katouzian, A.~Eslami, P.~Fallavollita, and N.~Navab, ``}{\DIFadd{Supervised classification for customized intraoperative augmented reality visualization}}\DIFadd{,'' }\emph{\DIFadd{ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers}}\DIFadd{, pp. 311--312, 2012.
}

\bibitem{Feiner1992}
\DIFadd{S.~K. Feiner and D.~D. Seligmann, ``}{\DIFadd{Cutaways and ghosting: satisfying visibility constraints in dynamic 3D illustrations}}\DIFadd{,'' }\emph{\DIFadd{The Visual Computer}}\DIFadd{, vol.~8, no. 5-6, pp. 292--302, 1992.
}

\bibitem{Habert2015}
\DIFadd{S.~Habert, J.~Gardiazabal, P.~Fallavollita, and N.~Navab, ``}{\DIFadd{RGBDX: First design and experimental validation of a mirror-based RGBD X-ray imaging system}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015}}\DIFadd{, pp. 13--18, 2015.
}

\bibitem{Phillips2021}
\DIFadd{N.~Phillips, F.~A. Khan, B.~Kruse, C.~Bethel, and J.~E. Swan, ``}{\DIFadd{An X-ray vision system for situation awareness in action space}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021}}\DIFadd{, pp. 593--594, 2021.
}

\bibitem{Avveduto2017}
\DIFadd{G.~Avveduto, F.~Tecchia, and H.~Fuchs, ``}{\DIFadd{Real-world occlusion in optical see-through AR displays}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM Symposium on Virtual Reality Software and Technology}}\DIFadd{, vol. Part F1319, 2017.
}

\bibitem{Heinrich2019}
\DIFadd{F.~Heinrich, F.~Joeres, K.~Lawonn, and C.~Hansen, }\emph{{\DIFadd{Comparison of Projective Augmented Reality Concepts to Support Medical Needle Insertion}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{IEEE, jan 2019.
}

\bibitem{Heinrich2022}
\DIFadd{F.~Heinrich, L.~Schwenderling, F.~Joeres, and C.~Hansen, ``}{\DIFadd{2D versus 3D: A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2022}}\DIFadd{, pp. 260--269, 2022.
}

\bibitem{Avery2008}
\DIFadd{B.~Avery, B.~H. Thomas, and W.~Piekarski, ``}{\DIFadd{User evaluation of see-through vision for mobile outdoor augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 7th IEEE International Symposium on Mixed and Augmented Reality 2008, ISMAR 2008}}\DIFadd{, pp. 69--72, 2008.
}

\bibitem{Lerotic2007}
\DIFadd{M.~Lerotic, A.~J. Chung, G.~Mylonas, and G.~Z. Yang, ``}{\DIFadd{Pq-space based non-photorealistic rendering for augmented reality}}\DIFadd{,'' in }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 4792 LNCS, no. PART 2, 2007, pp. 102--109.
}

\bibitem{Erat2018}
\DIFadd{O.~Erat, W.~A. Isop, D.~Kalkofen, and D.~Schmalstieg, ``}{\DIFadd{Drone-Augmented human vision: Exocentric control for drones exploring hidden areas}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~24, no.~4, pp. 1437--1446, 2018.
}

\bibitem{Zollmann2012}
\DIFadd{S.~Zollmann, G.~Schall, S.~Junghanns, and G.~Reitmayr, ``}{\DIFadd{Comprehensible and interactive visualizations of GIS data in augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 7431 LNCS, no. PART 1, pp. 675--685, 2012.
}

\bibitem{Guo2022}
\BIBentryALTinterwordspacing
\DIFadd{H.-J. Guo and B.~Prabhakaran, ``}{\DIFadd{HoloLens 2 Technical Evaluation as Mixed Reality Guide}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science}}\DIFadd{, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://arxiv.org/abs/2207.09554}
\BIBentrySTDinterwordspacing

\bibitem{Otsuki2015}
\DIFadd{M.~Otsuki, Y.~Kamioka, Y.~Kitai, M.~Kanzaki, H.~Kuzuoka, and H.~Uchiyama, ``}{\DIFadd{Please show me inside: Improving the depth perception using virtual mask in stereoscopic AR}}\DIFadd{,'' }\emph{\DIFadd{SIGGRAPH Asia 2015 Emerging Technologies, SA 2015}}\DIFadd{, no. February 2018, pp. 2--5, 2015.
}

\bibitem{Otsuki2016}
\DIFadd{M.~Otsuki and K.~Hideaki, ``}{\DIFadd{Effect of translucent random dot mask on depth perception in stereo AR environment}}\DIFadd{,'' }\emph{\DIFadd{Research report Human-computer interaction (HCI) (Research Report Human-Computer Interaction (HCI))}}\DIFadd{, vol. 2016, no.~2, pp. 1--8, 2016.
}

\bibitem{Becher2021}
\DIFadd{C.~Becher, S.~Bottecchia, and P.~Desbarats, ``}{\DIFadd{Projection Grid Cues: An Efficient Way to Perceive the Depths of Underground Objects in Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 12932 LNCS, pp. 611--630, 2021.
}

\bibitem{Johnson2014}
\DIFadd{A.~S. Johnson, J.~Sanchez, A.~French, and Y.~Sun, ``}\BIBforeignlanguage{eng}{{Unobtrusive augmentation of critical hidden structures in laparoscopy.}}\DIFadd{'' }\emph{\BIBforeignlanguage{eng}{Studies in health technology and informatics}}\DIFadd{, vol. 196, pp. 185--191, 2014.
}

\bibitem{Chen2020}
\DIFadd{W.~Chen, X.~Luo, Z.~Liang, C.~Li, M.~Wu, Y.~Gao, and X.~Jia, ``}{\DIFadd{A unified framework for depth prediction from a single image and binocular stereo matching}}\DIFadd{,'' }\emph{\DIFadd{Remote Sensing}}\DIFadd{, vol.~12, no.~3, pp. 1--13, 2020.
}

\bibitem{Otsuki2017}
\BIBentryALTinterwordspacing
\DIFadd{M.~Otsuki, P.~Milgram, and R.~Chellali, ``}{\DIFadd{Use of Random Dot Patterns in Achieving X-Ray Vision for Near-Field Applications of Stereoscopic Video-Based Augmented Reality Displays}}\DIFadd{,'' }\emph{\DIFadd{Presence: Teleoperators }{\DIFadd{\&}} \DIFadd{Virtual Environments}}\DIFadd{, vol.~26, no.~1, pp. 42--65, 2017. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://www.mitpressjournals.org/doi/pdf/10.1162/PRES\_a\_00135}
\BIBentrySTDinterwordspacing

\bibitem{Rompapas2014}
\DIFadd{D.~C. Rompapas, N.~Sorokin, A.~I.~W. L}{\DIFadd{\"{u}}}\DIFadd{bke, T.~Taketomi, G.~Yamamoto, C.~Sandor, and H.~Kato, ``}{\DIFadd{Dynamic augmented reality X-ray on google glass}}\DIFadd{,'' }\emph{\DIFadd{SIGGRAPH Asia 2014 Mobile Graphics and Interactive Applications, SA 2014}}\DIFadd{, p. 2010, 2014.
}

\bibitem{Kalkofen2007}
\DIFadd{D.~Kalkofen, E.~Mendez, and D.~Schmalstieg, ``}{\DIFadd{Focus and Context Visualization for Medical Augmented Reality Focus and Context Visualization for Medical Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{ISMAR '07: Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality}}\DIFadd{, vol.~6, pp. 1--10, 2007.
}

\bibitem{Chen2010}
\DIFadd{J.~Chen, X.~Granier, and N.~Lin, ``}{\DIFadd{On-Line Visualization of Underground Structures using Context Features}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST}}\DIFadd{, pp. 167--170, 2010.
}

\bibitem{Dey2014}
\BIBentryALTinterwordspacing
\DIFadd{A.~Dey and C.~Sandor, ``}{\DIFadd{Lessons learned: Evaluating visualizations for occluded objects in handheld augmented reality}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Human Computer Studies}}\DIFadd{, vol.~72, no. 10-11, pp. 704--716, 2014. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.ijhcs.2014.04.001}
\BIBentrySTDinterwordspacing

\bibitem{Cong2019}
\DIFadd{R.~Cong, J.~Lei, H.~Fu, M.~M. Cheng, W.~Lin, and Q.~Huang, ``}{\DIFadd{Review of visual saliency detection with comprehensive information}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Circuits and Systems for Video Technology}}\DIFadd{, vol.~29, no.~10, pp. 2941--2959, 2019.
}

\bibitem{1238308}
\DIFadd{Ren and Malik, ``}{\DIFadd{Learning a classification model for segmentation}}\DIFadd{,'' in }\emph{\DIFadd{Proceedings Ninth IEEE International Conference on Computer Vision}}\DIFadd{, 2003, pp. 10--17 vol.1.
}

\bibitem{Zollmann2010}
\DIFadd{S.~Zollmann, D.~Kalkofen, E.~Mendez, and G.~Reitmayr, ``}{\DIFadd{Image-based ghostings for single layer occlusions in augmented reality}}\DIFadd{,'' }\emph{\DIFadd{9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings}}\DIFadd{, pp. 19--26, 2010.
}

\bibitem{Zollmann2014}
\DIFadd{S.~Zollmann, R.~Grasset, G.~Reitmayr, and T.~Langlotz, ``}{\DIFadd{Image-based X-ray visualization techniques for spatial understanding in outdoor augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the 26th Australian Computer-Human Interaction Conference, OzCHI 2014}}\DIFadd{, pp. 194--203, 2014.
}

\bibitem{Santos2016}
\BIBentryALTinterwordspacing
\DIFadd{M.~E.~C. Santos, I.~}{\DIFadd{de Souza Almeida}}\DIFadd{, G.~Yamamoto, T.~Taketomi, C.~Sandor, and H.~Kato, ``}{\DIFadd{Exploring legibility of augmented reality X-ray}}\DIFadd{,'' }\emph{\DIFadd{Multimedia Tools and Applications}}\DIFadd{, vol.~75, no.~16, pp. 9563--9585, 2016. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1007/s11042-015-2954-1}
\BIBentrySTDinterwordspacing

\bibitem{Feiner1995}
\BIBentryALTinterwordspacing
\DIFadd{S.~K. Feiner, A.~C. Webster, T.~E. Krueger, B.~MacIntyre, and E.~J. Keller, ``}{\DIFadd{Architectural Anatomy}}\DIFadd{,'' }\emph{\DIFadd{Presence: Teleoper. Virtual Environ.}}\DIFadd{, vol.~4, no.~3, pp. 318--325, 1995-01. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1162/pres.1995.4.3.318}
\BIBentrySTDinterwordspacing

\bibitem{Eren2018}
\DIFadd{M.~T. Eren and S.~Balcisoy, ``}{\DIFadd{Evaluation of X-ray visualization techniques for vertical depth judgments in underground exploration}}\DIFadd{,'' }\emph{\DIFadd{Visual Computer}}\DIFadd{, vol.~34, no.~3, pp. 405--416, 2018.
}

\bibitem{Dey2011}
\DIFadd{A.~Dey, G.~Jarvis, C.~Sandor, A.~Wibowo, and M.~Ville-Veikko, ``}{\DIFadd{An Evaluation of Augmented Reality X-Ray Vision for Outdoor Navigation}}\DIFadd{,'' }\emph{\DIFadd{In Proceedings of International Conference on Artificial Reality and Telexistence}}\DIFadd{, pp. 28--32, 2011.
}

\bibitem{Maia2016}
\DIFadd{L.~F. Maia, W.~Viana, and F.~Trinta, ``}{\DIFadd{A real-time X-ray mobile application using augmented reality and google street view}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST}}\DIFadd{, vol. 02-04-Nove, pp. 111--119, 2016.
}

\bibitem{Yamamoto2014}
\DIFadd{G.~Yamamoto and A.~W. L, ``}{\DIFadd{A See-through Vision with Handheld}}\DIFadd{,'' }\emph{\DIFadd{Distributed, Ambient, and Pervasive Interactions}}\DIFadd{, pp. 392--399, 2014.
}

\bibitem{Guo2023}
\DIFadd{H.~J. Guo, J.~Z. Bakdash, L.~R. Marusich, O.~E. Ashtiani, and B.~Prabhakaran, ``}{\DIFadd{User Evaluation of Dynamic X-Ray Vision in Mixed Reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023}}\DIFadd{, pp. 851--852, 2023.
}

\bibitem{Phillips2020}
\BIBentryALTinterwordspacing
\DIFadd{N.~Phillips, B.~Kruse, F.~A. Khan, and J.~E.~S. Ii, }\emph{{\DIFadd{Window for Law Enforcement Operations}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Springer International Publishing, 2020. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1007/978-3-030-49695-1_40}
\BIBentrySTDinterwordspacing

\bibitem{AlJanabi2020}
\BIBentryALTinterwordspacing
\DIFadd{H.~F. }{\DIFadd{Al Janabi}}\DIFadd{, A.~Aydin, S.~Palaneer, N.~Macchione, A.~Al-Jabir, M.~S. Khan, P.~Dasgupta, and K.~Ahmed, ``}{\DIFadd{Effectiveness of the HoloLens mixed-reality headset in minimally invasive surgery: a simulation-based feasibility study}}\DIFadd{,'' }\emph{\DIFadd{Surgical Endoscopy}}\DIFadd{, vol.~34, no.~3, pp. 1143--1149, 2020. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s00464-019-06862-3}
\BIBentrySTDinterwordspacing

\bibitem{Kalia2019}
\DIFadd{M.~Kalia, N.~Navab, and T.~Salcudean, ``}{\DIFadd{A real-time interactive augmented reality depth estimation technique for surgical robotics}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE International Conference on Robotics and Automation}}\DIFadd{, vol. 2019-May, pp. 8291--8297, 2019.
}

\bibitem{MartinGomez2021}
\DIFadd{A.~Martin-Gomez, J.~Weiss, A.~Keller, U.~Eck, D.~Roth, and N.~Navab, ``}{\DIFadd{The Impact of Focus and Context Visualization Techniques on Depth Perception in Optical See-Through Head-Mounted Displays}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~XX, no.~X, pp. 1--16, 2021.
}

\bibitem{Nguyen2013}
\DIFadd{H.~M. Nguyen, B.~W}{\DIFadd{\"{u}}}\DIFadd{nsche, P.~Delmas, E.~Zhang, C.~Lutteroth, and W.~}{\DIFadd{Van Der Mark}}\DIFadd{, ``}{\DIFadd{High-definition texture reconstruction for 3D image-based modeling}}\DIFadd{,'' }\emph{\DIFadd{21st International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2013 - Full Papers Proceedings}}\DIFadd{, pp. 39--48, 2013.
}

\bibitem{Pereira2019}
\DIFadd{M.~Pereira, D.~Orfeo, W.~Ezequelle, D.~Burns, T.~Xia, and D.~R. Huston, ``}{\DIFadd{Photogrammetry and augmented reality for underground infrastructure sensing, mapping and assessment}}\DIFadd{,'' }\emph{\DIFadd{International Conference on Smart Infrastructure and Construction 2019, ICSIC 2019: Driving Data-Informed Decision-Making}}\DIFadd{, vol. 2019, pp. 169--175, 2019.
}

\bibitem{Jamiy2019b}
\DIFadd{F.~E. Jamiy and R.~Marsh, ``}{\DIFadd{Survey on depth perception in head mounted displays: distance estimation in virtual reality, augmented reality, and mixed reality}}\DIFadd{,'' }\emph{\DIFadd{IET Image Processing}}\DIFadd{, vol.~13, no.~5, pp. 707--712, 2019.
}

\bibitem{Fan1996}
\BIBentryALTinterwordspacing
\DIFadd{W.~C. Fan, B.~Brown, and M.~K. Yap, ``}{\DIFadd{A new stereotest: the double two rod test.}}\DIFadd{'' }\emph{\DIFadd{Ophthalmic \& physiological optics : the journal of the British College of Ophthalmic Opticians (Optometrists)}}\DIFadd{, vol.~16, no.~3, pp. 196--202, may 1996. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.ptonline.com/articles/how-to-get-better-mfi-results http://www.ncbi.nlm.nih.gov/pubmed/8977882}
\BIBentrySTDinterwordspacing

\bibitem{Ellis1998}
\BIBentryALTinterwordspacing
\DIFadd{S.~R. Ellis and B.~M. Menges, ``}{\DIFadd{Localization of Virtual Objects in the Near Visual Field}}\DIFadd{,'' }\emph{\DIFadd{Human Factors}}\DIFadd{, vol.~40, no.~3, pp. 415--431, sep 1998. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1518/001872098779591278}
\BIBentrySTDinterwordspacing

\bibitem{McCanless2000}
\DIFadd{J.~W. McCanless, S.~R. Ellis, and B.~D. Adelstein, ``}{\DIFadd{Localization of a Time-Delayed, Monocular Virtual Object Superimposed on a Real Environment}}\DIFadd{,'' }\emph{\DIFadd{Presence: Teleoperators and Virtual Environments}}\DIFadd{, vol.~9, no.~1, pp. 15--24, 2000.
}

\bibitem{Rolland2002}
\DIFadd{J.~P. Rolland, C.~Meyer, K.~Arthur, and E.~Rinalducci, ``}{\DIFadd{Method of Adjustments versus Method of Constant St imuli in the Quantification of Accuracy and Precision of Rendered Depth in Head-Mounted Displays}}\DIFadd{,'' }\emph{\DIFadd{Technology}}\DIFadd{, vol.~11, no.~6, 2002.
}

\bibitem{Mather2004}
\DIFadd{G.~Mather and D.~R. Smith, ``}{\DIFadd{Combining depth cues: Effects upon accuracy and speed of performance in a depth-ordering task}}\DIFadd{,'' }\emph{\DIFadd{Vision Research}}\DIFadd{, vol.~44, no.~6, pp. 557--562, 2004.
}

\bibitem{Wither2005}
\DIFadd{J.~Wither and T.~H}{\DIFadd{\"{o}}}\DIFadd{llerer, ``}{\DIFadd{Pictorial depth cues for outdoor augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - International Symposium on Wearable Computers, ISWC}}\DIFadd{, vol. 2005, pp. 92--99, 2005.
}

\bibitem{Murgia2009}
\DIFadd{A.~Murgia and P.~M. Sharkey, ``}{\DIFadd{Estimation of Distances in Virtual Environments Using Size Constancy}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Virtual Reality}}\DIFadd{, vol.~8, no.~1, pp. 67--74, 2009.
}

\bibitem{Swan2007}
\DIFadd{J.~E. Swan, A.~Jones, E.~Kolstad, M.~A. Livingston, and H.~S. Smallman, ``}{\DIFadd{Egocentric depth judgments in optical, see-through augmented reality}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~13, no.~3, pp. 429--442, 2007.
}

\bibitem{Jones2011}
\DIFadd{J.~A. Jones, J.~E. Swan, G.~Singh, and S.~R. Ellis, ``}{\DIFadd{Peripheral visual information and its effect on distance judgments in virtual and augmented environments}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - APGV 2011: ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization}}\DIFadd{, pp. 29--36, 2011.
}

\bibitem{Jones2008}
\DIFadd{A.~Jones, J.~E. Swan, G.~Singh, and E.~Kolstad, ``}{\DIFadd{The effects of virtual reality, augmented reality, and motion parallax on egocentric depth perception}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE Virtual Reality}}\DIFadd{, no. August, pp. 267--268, 2008.
}

\bibitem{Singh2011}
\DIFadd{G.~Singh, J.~E. Swan, J.~A. Jones, and S.~R. Ellis, ``}{\DIFadd{Depth judgment tasks and environments in near-field augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE Virtual Reality}}\DIFadd{, no. March, pp. 241--242, 2011.
}

\bibitem{Singh2012a}
\DIFadd{------, ``}{\DIFadd{Depth judgments by reaching and matching in near-field augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE Virtual Reality}}\DIFadd{, pp. 165--166, 2012.
}

\bibitem{Swan2015}
\DIFadd{J.~E. Swan, G.~Singh, and S.~R. Ellis, ``}{\DIFadd{Matching and Reaching Depth Judgments with Real and Augmented Reality Targets}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~21, no.~11, pp. 1289--1298, 2015.
}

\bibitem{Medeiros2016}
\BIBentryALTinterwordspacing
\DIFadd{D.~Medeiros, M.~Sousa, D.~Mendes, A.~Raposo, and J.~Jorge, ``}{\DIFadd{Perceiving depth: Optical versus video see-through}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST}}\DIFadd{, vol. 02-04-Nove, pp. 237--240, 2016. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://dl.acm.org/doi/10.1145/2993369.2993388{\#}d9578050e1}
\BIBentrySTDinterwordspacing

\bibitem{Cheng2022}
\DIFadd{D.~Cheng, Q.~Hou, Y.~Li, T.~Zhang, D.~Li, Y.~Huang, Y.~Liu, Q.~Wang, W.~Hou, T.~Yang, Z.~Feng, and Y.~Wang, ``}{\DIFadd{Optical design and pupil swim analysis of a compact, large EPD and immersive VR head mounted display}}\DIFadd{,'' }\emph{\DIFadd{Optics Express}}\DIFadd{, vol.~30, no.~5, p. 6584, 2022.
}

\bibitem{Singh2018}
\DIFadd{G.~Singh, S.~R. Ellis, and J.~E. Swan, ``}{\DIFadd{The Effect of Focal Distance, Age, and Brightness on Near-Field Augmented Reality Depth Matching}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~26, no.~2, pp. 1385--1398, 2018.
}

\bibitem{Hua2014}
\DIFadd{C.~Hua and J.~E. Swan, ``}{\DIFadd{The effect of an occluder on near field depth matching in optical see-through augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE Virtual Reality}}\DIFadd{, pp. 81--82, 2014.
}

\bibitem{Edwards2004}
\DIFadd{P.~J. Edwards, L.~G. Johnson, D.~J. Hawkes, M.~R. Fenlon, A.~J. Strong, and M.~J. Gleeson, ``}{\DIFadd{Clinical experience and perception in stereo augmented reality surgical navigation}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 3150, pp. 369--376, 2004.
}

\bibitem{Whitlock2018}
\DIFadd{M.~Whitlock, E.~Harnner, J.~R. Brubaker, S.~Kane, and D.~A. Szafir, ``}{\DIFadd{Interacting with Distant Objects in Augmented Reality}}\DIFadd{,'' }\emph{\DIFadd{25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings}}\DIFadd{, pp. 41--48, 2018.
}

\bibitem{Adams2022}
\DIFadd{H.~Adams, J.~Stefanucci, S.~Creem-Regehr, and B.~Bodenheimer, ``}{\DIFadd{Depth Perception in Augmented Reality: The Effects of Display, Shadow, and Position}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2022}}\DIFadd{, pp. 792--801, 2022.
}

\bibitem{Ping2020}
\DIFadd{J.~Ping, D.~Weng, Y.~Liu, and Y.~Wang, ``}{\DIFadd{Depth perception in shuffleboard: Depth cues effect on depth perception in virtual and augmented reality system}}\DIFadd{,'' }\emph{\DIFadd{Journal of the Society for Information Display}}\DIFadd{, vol.~28, no.~2, pp. 164--176, 2020.
}

\bibitem{Fischer2020a}
\DIFadd{M.~Fischer, C.~Leuze, S.~Perkins, J.~Rosenberg, B.~Daniel, and A.~Martin-Gomez, ``}{\DIFadd{Evaluation of Different Visualization Techniques for Perception-Based Alignment in Medical AR}}\DIFadd{,'' }\emph{\DIFadd{Adjunct Proceedings of the 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2020}}\DIFadd{, pp. 45--50, 2020.
}

\bibitem{Berning2014a}
\DIFadd{M.~Berning, D.~Kleinert, T.~Riedel, and M.~Beigl, ``}{\DIFadd{A study of depth perception in hand-held augmented reality using autostereoscopic displays}}\DIFadd{,'' }\emph{\DIFadd{ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings}}\DIFadd{, pp. 93--98, 2014.
}

\bibitem{Gabbard2014}
\DIFadd{J.~L. Gabbard, G.~M. Fitch, and H.~Kim, ``}{\DIFadd{Behind the glass: Driver challenges and opportunities for AR automotive applications}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE}}\DIFadd{, vol. 102, no.~2, pp. 124--136, 2014.
}

\bibitem{Rolland2000}
\DIFadd{J.~P. Rolland and H.~Fuchs, ``}{\DIFadd{Optical Versus Video See-Through Head-Mounted Displays in Medical Visualization}}\DIFadd{,'' }\emph{\DIFadd{Presence: Teleoperators and Virtual Environments}}\DIFadd{, vol.~9, no.~3, pp. 287--309, 2000.
}

\bibitem{Karlsson2005}
\DIFadd{N.~Karlsson, E.~}{\DIFadd{Di Bernardo}}\DIFadd{, J.~Ostrowski, L.~Goncalves, P.~Pirjanian, and M.~E. Munich, ``}{\DIFadd{The vSLAM algorithm for robust localization and mapping}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE International Conference on Robotics and Automation}}\DIFadd{, vol. 2005, no. April, pp. 24--29, 2005.
}

\bibitem{Klein2007}
\DIFadd{G.~Klein and D.~Murray, ``}{\DIFadd{Parallel tracking and mapping for small AR workspaces}}\DIFadd{,'' }\emph{\DIFadd{2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR}}\DIFadd{, pp. 225--234, 2007.
}

\bibitem{Livingston2003}
\DIFadd{M.~A. Livingston, J.~E. Swan, J.~L. Gabbard, T.~H. Hollerer, D.~Hix, S.~J. Julier, Y.~Baillot, and D.~Brown, ``}{\DIFadd{Resolving multiple occluded layers in augmented reality}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2nd IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR 2003}}\DIFadd{, pp. 56--65, 2003.
}

\bibitem{Webster1996}
\DIFadd{A.~Webster, S.~Feiner, B.~MacIntyre, W.~Massie, and T.~Krueger, ``}{\DIFadd{Augmented reality in architectural construction, inspection, and renovation}}\DIFadd{,'' }\emph{\DIFadd{Computing in Civil Engineering (New York)}}\DIFadd{, no. September 2000, pp. 913--919, 1996.
}

\bibitem{Hettinga2018}
\DIFadd{G.~J. Hettinga, P.~J. Barendrecht, and J.~Kosinka, ``}{\DIFadd{A comparison of GPU tessellation strategies for multisided patches}}\DIFadd{,'' }\emph{\DIFadd{European Association for Computer Graphics - 39th Annual Conference, EUROGRAPHICS 2018 - Short Papers}}\DIFadd{, no. April, pp. 45--48, 2018.
}

\bibitem{flick_2017}
\DIFadd{J.~Flick, ``Tessellation,'' https://catlikecoding.com/unity/tutorials/advanced-rendering/tessellation/, Nov 2017, accessed: 2020-10-06.
}

\bibitem{Tian2009}
\DIFadd{M.~Tian, S.~Wan, and L.~Yue, ``}{\DIFadd{A color saliency model for salient objects detection in natural scenes}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 5916 LNCS, pp. 240--250, 2009.
}

\bibitem{Valensi1945}
\DIFadd{G.~Valensi, ``}{\DIFadd{System of television in colors}}\DIFadd{,'' pp. 0--9, 1945.
}

\bibitem{Saravanan2016}
\DIFadd{G.~Saravanan, G.~Yamuna, and S.~Nandhini, ``}{\DIFadd{Real time implementation of RGB to HSV/HSI/HSL and its reverse color space models}}\DIFadd{,'' }\emph{\DIFadd{International Conference on Communication and Signal Processing, ICCSP 2016}}\DIFadd{, pp. 462--466, 2016.
}

\bibitem{Li2012}
\DIFadd{I.~K.~Y. Li, E.~M. Peek, B.~C. Wunshe, and C.~Lutteroth, ``}{\DIFadd{Enhancing 3D Applicaitons Using Stereoscipic 3D and Motion Parallax}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the Thirteenth Australasian User Interface Conference}}\DIFadd{, vol. 340, pp. 59--68, 2012.
}

\bibitem{Peek2014}
\DIFadd{E.~Peek, B.~W}{\DIFadd{\"{u}}}\DIFadd{nsche, and C.~Lutteroth, ``}{\DIFadd{Using integrated GPUs to perform image warping for HMDs}}\DIFadd{,'' }\emph{\DIFadd{ACM International Conference Proceeding Series}}\DIFadd{, vol. 19-21-November-2014, pp. 172--177, 2014.
}

\bibitem{Hamadouche2018}
\BIBentryALTinterwordspacing
\DIFadd{I.~Hamadouche, ``}{\DIFadd{AUGMENTED REALITY X-RAY VISION ON OPTICAL SEE-THROUGH HEAD-MOUNTED}}\DIFadd{,'' Ph.D. dissertation, University of Oulu, 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://pdfs.semanticscholar.org/de04/a705f11ebdb9a7e913e5dd86914d346a9444.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Brown1966}
\DIFadd{D.~Brown, ``}{\DIFadd{Decentering Distortion of Lenses - The Prism Effect Encountered in Metric Cameras can be Overcome Through Analytic Calibration}}\DIFadd{,'' }\emph{\DIFadd{Photometric Engineering}}\DIFadd{, vol.~32, no.~3, pp. 444--462, 1966.
}

\bibitem{Zhang1998}
{\DIFadd{Zhengyou Zhang}}\DIFadd{, ``}{\DIFadd{A Flexible New Technique for Camera Calibration Zhengyou}}\DIFadd{,'' }\emph{\DIFadd{Technical Report MSR TR-98-71 Microsoft}}\DIFadd{, no. last updated on Aug. 13, 2008, 1998.
}

\bibitem{DeVilliers2008}
\DIFadd{J.~P. de~Villiers, F.~W. Leuschner, and R.~Geldenhuys, ``}{\DIFadd{Centi-pixel accurate real-time inverse distortion correction}}\DIFadd{,'' }\emph{\DIFadd{Optomechatronic Technologies 2008}}\DIFadd{, vol. 7266, no.~1, p. 726611, 2008.
}

\bibitem{Kang2001}
\DIFadd{S.~B. Kang, ``}{\DIFadd{Radial distortion snakes}}\DIFadd{,'' }\emph{\DIFadd{IEICE Transactions on Information and Systems}}\DIFadd{, vol. E84-D, no.~12, pp. 1603--1611, 2001.
}

\bibitem{Liu2025}
\DIFadd{Q.~Liu, X.~Sun, and Y.~Peng, ``}{\DIFadd{A Distortion Image Correction Method for Wide-Angle Cameras Based on Track Visual Detection}}\DIFadd{,'' }\emph{\DIFadd{Photonics}}\DIFadd{, vol.~12, no.~8, p. 767, 2025.
}

\bibitem{Brown1971}
\DIFadd{D.~C. Brown, ``}{\DIFadd{Close-Range Camera Calibration}}\DIFadd{,'' }\emph{\DIFadd{Photogramm. Eng}}\DIFadd{, vol.~8, no.~37, pp. 855--866, 1971.
}

\bibitem{Gruen2020}
\DIFadd{R.~Gruen, E.~Ofek, A.~Steed, R.~Gal, M.~Sinclair, and M.~Gonzalez-Franco, ``}{\DIFadd{Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020}}\DIFadd{, pp. 791--799, 2020.
}

\bibitem{VanWaveren2016}
\DIFadd{J.~M. }{\DIFadd{Van Waveren}}\DIFadd{, ``}{\DIFadd{The asynchronous time warp for virtual reality on consumer hardware}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST}}\DIFadd{, vol. 02-04-Nove, pp. 37--46, 2016.
}

\bibitem{Landvoigt2017}
\BIBentryALTinterwordspacing
\DIFadd{M.~Landvoigt and J.~Cardoso, ``}{\DIFadd{Analysis about publications on Facebook pages: finding of important characteristics}}\DIFadd{,'' }\emph{\DIFadd{Sage Research Methods Cases}}\DIFadd{, no. February, p.~8, 2017. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://www.kdd.org/kdd2016/papers/files/Paper_799.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Martinez-Conde2013}
\DIFadd{S.~Martinez-Conde, J.~Otero-Millan, and S.~L. MacKnik, ``}{\DIFadd{The impact of microsaccades on vision: Towards a unified theory of saccadic function}}\DIFadd{,'' }\emph{\DIFadd{Nature Reviews Neuroscience}}\DIFadd{, vol.~14, no.~2, pp. 83--96, 2013.
}

\bibitem{Casiez2012}
\DIFadd{G.~Casiez, N.~Roussel, and D.~Vogel, ``}{\DIFadd{1 Euro filter: A simple speed-based low-pass filter for noisy input in interactive systems}}\DIFadd{,'' }\emph{\DIFadd{Conference on Human Factors in Computing Systems - Proceedings}}\DIFadd{, pp. 2527--2530, 2012.
}

\bibitem{Wilson2002}
\DIFadd{M.~Wilson, ``}{\DIFadd{Six views of embodied cognition.}}\DIFadd{'' }\emph{\DIFadd{Psychometric Bulletin \& Review}}\DIFadd{, vol.~9, no.~4, pp. 625--636, 2002.
}

\bibitem{Kyto2014}
\DIFadd{M.~Kyt}{\DIFadd{\"{o}}}\DIFadd{, A.~M}{\DIFadd{\"{a}}}\DIFadd{kinen, T.~Tossavainen, and P.~Oittinen, ``}{\DIFadd{Stereoscopic depth perception in video see-through augmented reality within action space}}\DIFadd{,'' }\emph{\DIFadd{Journal of Electronic Imaging}}\DIFadd{, vol.~23, no.~1, p. 11006, 2014.
}

\bibitem{Bruce2009}
\DIFadd{N.~D. Bruce and J.~K. Tsotsos, ``}{\DIFadd{Saliency, attention and visual search: An information theoretic approach}}\DIFadd{,'' }\emph{\DIFadd{Journal of Vision}}\DIFadd{, vol.~9, no.~3, pp. 1--24, 2009.
}

\bibitem{VanDyck2021}
\DIFadd{L.~E. van Dyck, R.~Kwitt, S.~J. Denzler, and W.~R. Gruber, ``}{\DIFadd{Comparing Object Recognition in Humans and Deep Convolutional Neural Networks—An Eye Tracking Study}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Neuroscience}}\DIFadd{, vol.~15, no. October, pp. 1--15, 2021.
}

\bibitem{Paas1992}
\DIFadd{F.~G. Paas, ``}{\DIFadd{Training Strategies for Attaining Transfer of Problem-Solving Skill in Statistics: A Cognitive-Load Approach}}\DIFadd{,'' }\emph{\DIFadd{Journal of Educational Psychology}}\DIFadd{, vol.~84, no.~4, pp. 429--434, 1992.
}

\bibitem{Bimber2004}
\DIFadd{O.~Bimber and R.~Raskar, }\emph{{\DIFadd{Spatial Augmented Reality Merging Real and Virtual Worlds}}}\DIFadd{, 1st~ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{A K Peters Ltd., 2004.
}

\bibitem{Brooke1996SUSA}
\DIFadd{J.~B. Brooke, ``Brooke1996susa,'' in }\emph{\DIFadd{Usability Evaluation In Industry}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{CRC Press, 1996.
}

\bibitem{Meteyard2020}
\BIBentryALTinterwordspacing
\DIFadd{L.~Meteyard and R.~A. Davies, ``}{\DIFadd{Best practice guidance for linear mixed-effects models in psychological science}}\DIFadd{,'' }\emph{\DIFadd{Journal of Memory and Language}}\DIFadd{, vol. 112, no. January, p. 104092, 2020. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1016/j.jml.2020.104092}
\BIBentrySTDinterwordspacing

\bibitem{Bell2019}
\BIBentryALTinterwordspacing
\DIFadd{A.~Bell, M.~Fairbrother, and K.~Jones, ``}{\DIFadd{Fixed and random effects models: making an informed choice}}\DIFadd{,'' }\emph{\DIFadd{Quality and Quantity}}\DIFadd{, vol.~53, no.~2, pp. 1051--1074, 2019. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s11135-018-0802-x}
\BIBentrySTDinterwordspacing

\bibitem{Singmann2019}
\DIFadd{H.~Singmann and D.~Kellen, }\emph{{\DIFadd{An Introduction to Mixed Models for Experimental Psychology}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Tailor and Frances, 2019, no. October.
}

\bibitem{Li2012a}
\DIFadd{Y.~Li and J.~Baron, ``}{\DIFadd{Behavioral research data analysis with R}}\DIFadd{,'' }\emph{\DIFadd{Behavioral Research Data Analysis with R}}\DIFadd{, no. 1973, pp. 1--245, 2012.
}

\bibitem{Models2006}
\DIFadd{L.~M.-e. Models and B.~Concepts, ``}{\DIFadd{Nonlinear Mixed-effects Models: Basic Concepts and Motivating Examples}}\DIFadd{,'' }\emph{\DIFadd{Mixed-Effects Models in S and S-PLUS}}\DIFadd{, pp. 273--304, 2006.
}

\bibitem{Kaptein2016}
\BIBentryALTinterwordspacing
\DIFadd{M.~Kaptein, ``}{\DIFadd{Using Generalized Linear (Mixed) Models in HCI}}\DIFadd{,'' in }\emph{\DIFadd{Modern Statistical Methods for HCI}}\DIFadd{, J.~Robertson and M.~Kaptein, Eds.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Cham: Springer International Publishing, 2016, pp. 251--274. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/978-3-319-26633-6_11}
\BIBentrySTDinterwordspacing

\bibitem{Cameron2005}
\DIFadd{A.~Cameron and P.~Trivedi, }\emph{\DIFadd{Microeconometrics: Methods and Applications}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Cambridge University Press, 05 2005.
}

\bibitem{Matyash2021}
\DIFadd{I.~Matyash, R.~Kutzner, T.~Neumuth, and M.~Rockstroh, ``}{\DIFadd{Accuracy measurement of HoloLens2 IMUs in medical environments}}\DIFadd{,'' }\emph{\DIFadd{Current Directions in Biomedical Engineering}}\DIFadd{, vol.~7, no.~2, pp. 633--636, 2021.
}

\bibitem{Gamage2021}
\DIFadd{N.~M. Gamage, D.~Ishtaweera, M.~Weigel, and A.~Withana, }\emph{{\DIFadd{So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Association for Computing Machinery, 2021, vol.~1, no.~1.
}

\bibitem{Lee2021}
\DIFadd{D.~Lee, M.~Choi, and J.~Lee, ``}{\DIFadd{Prediction of head movement in 360-degree videos using attention model}}\DIFadd{,'' }\emph{\DIFadd{Sensors}}\DIFadd{, vol.~21, no.~11, pp. 1--22, 2021.
}

\bibitem{Avila1994}
\DIFadd{R.~Avila, T.~He, L.~Hong, A.~Kaufman, H.~Pfister, C.~Silva, L.~Sobierajski, and S.~Wang, ``}{\DIFadd{VolVis}}\DIFadd{: A diversified system for volume research and development,'' in }\emph{\DIFadd{Proceedings Visualization '94}}\DIFadd{, oct 1994, pp. 31--38.
}

\bibitem{Adib2013}
\DIFadd{F.~Adib and D.~Katabi, ``}{\DIFadd{See through walls with WiFi!}}\DIFadd{'' }\emph{\DIFadd{SIGCOMM 2013 - Proceedings of the ACM SIGCOMM 2013 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication}}\DIFadd{, pp. 75--86, 2013.
}

\bibitem{Khlebnikov2013}
\DIFadd{R.~Khlebnikov, B.~Kainz, M.~Steinberger, and D.~Schmalstieg, ``}{\DIFadd{Noise-based volume rendering for the visualization of multivariate volumetric data}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~19, no.~12, pp. 2926--2935, 2013.
}

\bibitem{Maloca2018}
\DIFadd{P.~M. Maloca, J.~E.~R. de~Carvalho, T.~Heeren, P.~W. Hasler, F.~Mushtaq, M.~Mon-Williams, H.~P. Scholl, K.~Balaskas, C.~Egan, A.~Tufail, L.~Witthauer, and P.~C. Cattin, ``}{\DIFadd{High-performance virtual reality volume rendering of original optical coherence tomography point-cloud data enhanced with real-time ray casting}}\DIFadd{,'' }\emph{\DIFadd{Translational Vision Science and Technology}}\DIFadd{, vol.~7, no.~4, 2018.
}

\bibitem{VanSon2018}
\DIFadd{R.~}{\DIFadd{Van Son}}\DIFadd{, S.~W. Jaw, J.~Yan, H.~S. Khoo, W.~K. Loo, S.~N. Teo, and G.~Schrotter, ``}{\DIFadd{A framework for reliable three-dimensional underground utility mapping for urban planning}}\DIFadd{,'' }\emph{\DIFadd{International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives}}\DIFadd{, vol.~42, no. 4/W10, pp. 209--214, 2018.
}

\bibitem{Alhazmi2018}
\DIFadd{A.~Alhazmi, ``}{\DIFadd{3D Volume Visualization in Medical Application}}\DIFadd{,'' }\emph{\DIFadd{Virtual Reality \& Intelligent Hardware}}\DIFadd{, vol.~Vi, pp. 58--64, 2018.
}

\bibitem{Zhang2011}
\DIFadd{Q.~Zhang, R.~Eagleson, and T.~M. Peters, ``}{\DIFadd{Volume visualization: A technical overview with a focus on medical applications}}\DIFadd{,'' }\emph{\DIFadd{Journal of Digital Imaging}}\DIFadd{, vol.~24, no.~4, pp. 640--664, 2011.
}

\bibitem{Vicente17}
\BIBentryALTinterwordspacing
\DIFadd{M.~A. Vicente, J.~M}{\DIFadd{\'{i}}}\DIFadd{nguez, and D.~C. Gonz}{\DIFadd{\'{a}}}\DIFadd{lez, ``}{\DIFadd{The Use of Computed Tomography to Explore the Microstructure of Materials in Civil Engineering: From Rocks to Concrete}}\DIFadd{,'' in }\emph{\DIFadd{Computed Tomography}}\DIFadd{, A.~M. Halefoglu, Ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Rijeka: IntechOpen, 2017, ch.~10. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.5772/intechopen.69245}
\BIBentrySTDinterwordspacing

\bibitem{Brath2015}
\DIFadd{R.~Brath, ``}{\DIFadd{3D InfoVis is here to stay: Deal with it}}\DIFadd{,'' }\emph{\DIFadd{2014 IEEE VIS International Workshop on 3DVis, 3DVis 2014}}\DIFadd{, pp. 25--31, 2015.
}

\bibitem{Vernon2002}
\DIFadd{T.~Vernon and D.~Peckham, ``}{\DIFadd{The benefits of 3D modelling and animation in medical teaching}}\DIFadd{,'' }\emph{\DIFadd{Journal of Visual Communication in Medicine}}\DIFadd{, vol.~25, no.~4, pp. 142--148, 2002.
}

\bibitem{Tang2019}
\DIFadd{K.~S. Tang, D.~L. Cheng, E.~Mi, and P.~B. Greenberg, ``}{\DIFadd{Augmented reality in medical education: a systematic review}}\DIFadd{,'' }\emph{\DIFadd{Canadian Medical Education Journal}}\DIFadd{, vol.~11, no.~1, pp. 81--96, 2019.
}

\bibitem{Thomas2015}
\DIFadd{B.~H. Thomas, M.~Marner, R.~T. Smith, N.~A.~M. Elsayed, S.~}{\DIFadd{Von Itzstein}}\DIFadd{, K.~Klein, M.~Adcock, P.~Eades, A.~Irlitti, J.~Zucco, T.~Simon, J.~Baumeister, and T.~Suthers, ``}{\DIFadd{Spatial augmented reality - A tool for 3D data visualization}}\DIFadd{,'' }\emph{\DIFadd{2014 IEEE VIS International Workshop on 3DVis, 3DVis 2014}}\DIFadd{, no. September 2016, pp. 45--50, 2015.
}

\bibitem{Fischer2020}
\DIFadd{R.~Fischer, K.~C. Chang, R.~Weller, and G.~Zachmann, ``}{\DIFadd{Volumetric Medical Data Visualization for Collaborative VR Environments}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 12499 LNCS, no. March 2021, pp. 178--191, 2020.
}

\bibitem{Joshi2013}
\DIFadd{A.~Joshi, S.~Dustin, K.~Vives, D.~Spencer, L.~Staib, and P.~Xenophon, ``}{\DIFadd{Novel Interaction Techniques for Neurosurgical Planning and Stereotactic Navigation}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visual Computing Graphics}}\DIFadd{, vol.~23, no.~1, pp. 1--7, 2013.
}

\bibitem{DePaolis2018}
\BIBentryALTinterwordspacing
\DIFadd{L.~T. }{\DIFadd{De Paolis}} \DIFadd{and F.~Ricciardi, ``}{\DIFadd{Augmented visualisation in the treatment of the liver tumours with radiofrequency ablation}}\DIFadd{,'' }\emph{\DIFadd{Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization}}\DIFadd{, vol.~6, no.~4, pp. 396--404, 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1080/21681163.2017.1287598}
\BIBentrySTDinterwordspacing

\bibitem{Kaufman2005}
\BIBentryALTinterwordspacing
\DIFadd{A.~KAUFMAN and K.~MUELLER, ``Overview of volume rendering,'' in }\emph{\DIFadd{Visualization Handbook}}\DIFadd{, C.~D. Hansen and C.~R. Johnson, Eds.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Burlington: Butterworth-Heinemann, 2005, pp. 127--174. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.sciencedirect.com/science/article/pii/B9780123875822500095}
\BIBentrySTDinterwordspacing

\bibitem{Joshi2009}
\DIFadd{A.~Joshi, J.~Caban, P.~Rheingans, and L.~Sparling, ``}{\DIFadd{Case Study on Visualizing Hurricanes Using Illustration-Inspired Techniques}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~15, no.~5, pp. 709--718, 2009.
}

\bibitem{DenOtter2024}
\DIFadd{T.~D. DenOtter and J.~Schubert, ``}\BIBforeignlanguage{eng}{{Hounsfield Unit.}}\DIFadd{'' in }\emph{\BIBforeignlanguage{eng}{StatPearls}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{StatPearls Publishing, jan 2024.
}

\bibitem{Rinck2024}
\BIBentryALTinterwordspacing
\DIFadd{P.~A. Rinck, }\emph{{\DIFadd{Magnetic Resonance in Medicine}}}\DIFadd{, 14th~ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{The Basic Textbook of the European Magnetic Resonance Forum, 2024. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://www.magnetic-resonance.org/}
\BIBentrySTDinterwordspacing

\bibitem{Langner2008}
\DIFadd{U.~W. Langner and P.~J. Keall, ``}{\DIFadd{Prospective displacement and velocity-based cine 4D CT}}\DIFadd{,'' }\emph{\DIFadd{Medical Physics}}\DIFadd{, vol.~35, no.~10, pp. 4501--4512, 2008.
}

\bibitem{Gill2015}
\DIFadd{G.~Gill and R.~R. Beichel, ``}{\DIFadd{Lung Segmentation in 4D CT Volumes Based on Robust Active Shape Model Matching}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Biomedical Imaging}}\DIFadd{, vol. 2015, 2015.
}

\bibitem{Baoquan2016}
\BIBentryALTinterwordspacing
\DIFadd{B.~Liu, G.~J. Clapworthy, F.~Dong, and E.~Wu, ``}{\DIFadd{Parallel Marching Blocks: A Practical Isosurfacing Algorithm for Large Data on Many-Core Architectures}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~35, no.~3, pp. 211--220, 2016. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12897}
\BIBentrySTDinterwordspacing

\bibitem{Lorensen}
\DIFadd{W.~Lorensen and H.~}{\DIFadd{E. Cline}}\DIFadd{, ``}{\DIFadd{Marching Cubes: A High Resolution 3D Surface Construction Algorithm}}\DIFadd{,'' }\emph{\DIFadd{ACM SIGGRAPH Computer Graphics}}\DIFadd{, vol.~21, pp. 163--, 1987.
}

\bibitem{Newman2006}
\DIFadd{T.~S. Newman and H.~Yi, ``}{\DIFadd{A survey of the marching cubes algorithm}}\DIFadd{,'' }\emph{\DIFadd{Computers and Graphics (Pergamon)}}\DIFadd{, vol.~30, no.~5, pp. 854--879, 2006.
}

\bibitem{Dai2021}
\BIBentryALTinterwordspacing
\DIFadd{H.~Dai, Y.~Tao, X.~He, and H.~Lin, ``}{\DIFadd{IsoExplorer: an isosurface-driven framework for 3D shape analysis of biomedical volume data}}\DIFadd{,'' }\emph{\DIFadd{Journal of Visualization}}\DIFadd{, vol.~24, no.~6, pp. 1253--1266, 2021. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1007/s12650-021-00770-2}
\BIBentrySTDinterwordspacing

\bibitem{Eid2017}
\DIFadd{M.~Eid, C.~N. }{\DIFadd{De Cecco}}\DIFadd{, J.~W. Nance, D.~Caruso, M.~H. Albrecht, A.~J. Spandorfer, D.~}{\DIFadd{De Santis}}\DIFadd{, A.~Varga-Szemes, and U.~}{\DIFadd{Joseph Schoepf}}\DIFadd{, ``}{\DIFadd{Cinematic rendering in CT: A novel, lifelike 3D visualization technique}}\DIFadd{,'' }\emph{\DIFadd{American Journal of Roentgenology}}\DIFadd{, vol. 209, no.~2, pp. 370--379, 2017.
}

\bibitem{Li2003}
\DIFadd{W.~Li, K.~Mueller, and A.~Kaufman, ``}{\DIFadd{Empty Space Skipping and Occlusion Clipping for Texture-based Volume Rendering}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE Visualization Conference}}\DIFadd{, vol. 4400, no. Cvc, pp. 317--324, 2003.
}

\bibitem{Morrical2019}
\DIFadd{N.~Morrical, W.~Usher, I.~Wald, and V.~Pascucci, ``}{\DIFadd{Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes Using Hardware Accelerated Ray Tracing}}\DIFadd{,'' }\emph{\DIFadd{2019 IEEE Visualization Conference, VIS 2019}}\DIFadd{, vol.~1, pp. 256--260, 2019.
}

\bibitem{Hadwiger2018}
\DIFadd{M.~Hadwiger, A.~K. Al-Awami, J.~Beyer, M.~Agus, and H.~Pfister, ``}{\DIFadd{SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~24, no.~1, pp. 974--983, 2018.
}

\bibitem{Deakin2019}
\DIFadd{L.~Deakin and M.~Knackstedt, ``}{\DIFadd{Accelerated volume rendering with Chebyshev distance maps}}\DIFadd{,'' }\emph{\DIFadd{SIGGRAPH Asia 2019 Technical Briefs, SA 2019}}\DIFadd{, pp. 25--28, 2019.
}

\bibitem{Jung2022}
\DIFadd{H.~Jung, Y.~Jung, and J.~Kim, ``}{\DIFadd{Understanding the Capabilities of the HoloLens 1 and 2 in a Mixed Reality Environment for Direct Volume Rendering with a Ray-casting Algorithm}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022}}\DIFadd{, pp. 698--699, 2022.
}

\bibitem{Cetinsaya2020}
\BIBentryALTinterwordspacing
\DIFadd{B.~Cetinsaya, C.~Neumann, and D.~Reiners, ``}{\DIFadd{Using Direct Volume Rendering for Augmented Reality in Resource-constrained Platforms}}\DIFadd{,'' }\emph{\DIFadd{2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}}\DIFadd{, pp. 768--769, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://ieeexplore.ieee.org/document/9757418}
\BIBentrySTDinterwordspacing

\bibitem{Chandrasekhar1950}
\BIBentryALTinterwordspacing
\DIFadd{S.~Chandrasekhar, ``}{\DIFadd{Radiative transfer. By S. Chandrasekhar. London (Oxford University Press) 1950. 8vo. Pp. 393, 35 figures. 35s}}\DIFadd{,'' }\emph{\DIFadd{Quarterly Journal of the Royal Meteorological Society}}\DIFadd{, vol.~76, no. 330, p. 498, 1950. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49707633016}
\BIBentrySTDinterwordspacing

\bibitem{Fong2017}
\DIFadd{J.~Fong, M.~Wrenninge, C.~Kulla, and R.~Habel, ``}{\DIFadd{Production volume rendering SIGGRAPH 2017 course}}\DIFadd{,'' }\emph{\DIFadd{ACM SIGGRAPH 2017 Courses, SIGGRAPH 2017}}\DIFadd{, 2017.
}

\bibitem{TUKORA2020}
\DIFadd{B.~TUKORA, ``}{\DIFadd{Effective volume rendering on mobile and standalone vr headsets by means of a hybrid method}}\DIFadd{,'' }\emph{\DIFadd{Pollack Periodica}}\DIFadd{, vol.~15, no.~2, pp. 3--12, 2020.
}

\bibitem{Ropinski2010}
\DIFadd{T.~Ropinski, C.~D}{\DIFadd{\"{o}}}\DIFadd{ring, and C.~Rezk-Salama, ``}{\DIFadd{Interactive volumetric lighting simulating scattering and shadowing}}\DIFadd{,'' }\emph{\DIFadd{IEEE Pacific Visualization Symposium 2010, PacificVis 2010 - Proceedings}}\DIFadd{, pp. 169--176, 2010.
}

\bibitem{VanDamme2021}
\DIFadd{M.~}{\DIFadd{Van Damme}}\DIFadd{, L.~Vanderstraeten, J.~}{\DIFadd{De Nardis}}\DIFadd{, J.~Haegeman, and F.~Verstraete, ``}{\DIFadd{Real-time scattering of interacting quasiparticles in quantum spin chains}}\DIFadd{,'' }\emph{\DIFadd{Physical Review Research}}\DIFadd{, vol.~3, no.~1, 2021.
}

\bibitem{Li2010c}
\DIFadd{B.~Li, L.~Tian, and S.~Ou, ``}{\DIFadd{An optical model for translucent volume rendering and its implementation using the preintegrated shear-warp algorithm}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Biomedical Imaging}}\DIFadd{, vol. 2010, 2010.
}

\bibitem{Kniss2003}
\DIFadd{J.~Kniss, S.~Premoze, C.~Hansen, P.~Shirley, and A.~McPherson, ``}{\DIFadd{A model for volume lighting and modeling}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~9, no.~2, pp. 150--162, 2003.
}

\bibitem{Li2010d}
\DIFadd{B.~Li, L.~Tian, and S.~Ou, ``}{\DIFadd{An optical model for translucent volume rendering and its implementation using the preintegrated shear-warp algorithm}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Biomedical Imaging}}\DIFadd{, vol. 2010, 2010.
}

\bibitem{Jabonski2016}
\DIFadd{S.~Jab}{\DIFadd{\l}}\DIFadd{o}{\DIFadd{\'{n}}}\DIFadd{ski and T.~Martyn, ``}{\DIFadd{Real-time voxel rendering algorithm based on Screen Space Billboard Voxel Buffer with Sparse Lookup Textures}}\DIFadd{,'' }\emph{\DIFadd{24th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2016 - Full Papers Proceedings}}\DIFadd{, pp. 27--36, 2016.
}

\bibitem{Tomandl2001}
\DIFadd{B.~F. Tomandl, P.~Hastreiter, C.~Rezk-Salama, K.~Engel, T.~Ertl, W.~J. Huk, R.~Naraghi, O.~Ganslandt, C.~Nimsky, and K.~E. Eberhardt, ``}{\DIFadd{Local and remote visualization techniques for interactive direct volume rendering in neuroradiology}}\DIFadd{,'' }\emph{\DIFadd{Radiographics}}\DIFadd{, vol.~21, no.~6, pp. 1561--1572, 2001.
}

\bibitem{Matsui2004}
\DIFadd{M.~Matsui, F.~Ino, and K.~Hagihara, ``}{\DIFadd{Parallel volume rendering with early ray termination for visualizing large-scale datasets}}\DIFadd{,'' }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 3358, pp. 245--256, 2004.
}

\bibitem{GPU_Gems2}
\DIFadd{M.~Pharr and R.~Fernando, }\emph{\DIFadd{GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation (Gpu Gems)}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Addison-Wesley Professional, 2005.
}

\bibitem{Heide2013}
\BIBentryALTinterwordspacing
\DIFadd{F.~Heide, G.~Wetzstein, R.~Raskar, and W.~Heidrich, ``Adaptive image synthesis for compressive displays,'' }\emph{\DIFadd{ACM Trans. Graph.}}\DIFadd{, vol.~32, no.~4, Jul. 2013. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1145/2461912.2461925}
\BIBentrySTDinterwordspacing

\bibitem{Schenke2005}
\BIBentryALTinterwordspacing
\DIFadd{S.~Schenke, C.~W. Burkhard, and J.~Denzler, ``}{\DIFadd{GPU-Based Volume Segmentation}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of IVCNZ 2005}}\DIFadd{, vol.~11, no. 0959-4388 LA - eng PT - Journal Article PT - Review PT - Review, Tutorial, pp. 59--65, 2005. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.3023&rep=rep1&type=pdf}
\BIBentrySTDinterwordspacing

\bibitem{Kraft2020}
\BIBentryALTinterwordspacing
\DIFadd{V.~Kraft, F.~Link, A.~Schenk, and C.~Schumann, ``}{\DIFadd{Adaptive Illumination Sampling for Direct Volume Rendering}}\DIFadd{,'' in }\emph{\DIFadd{Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)}}\DIFadd{, vol. 12221 LNCS.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Springer International Publishing, 2020, pp. 107--118. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1007/978-3-030-61864-3_10}
\BIBentrySTDinterwordspacing

\bibitem{Zellmann2019}
\BIBentryALTinterwordspacing
\DIFadd{S.~Zellmann, ``}{\DIFadd{Comparing Hierarchical Data Structures for Sparse Volume Rendering with Empty Space Skipping}}\DIFadd{,'' }\emph{\DIFadd{ArXiv}}\DIFadd{, 2019. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://arxiv.org/abs/1912.09596}
\BIBentrySTDinterwordspacing

\bibitem{Nakagawa2023}
\DIFadd{S.~Nakagawa and Y.~Watanabe, ``}{\DIFadd{High-Frame-Rate Projection with Thousands of Frames Per Second Based on the Multi-Bit Superimposition Method}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 2023 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2023}}\DIFadd{, pp. 741--750, 2023.
}

\bibitem{Ni2011}
\DIFadd{T.~Ni, A.~K. Karlson, and D.~Wigdor, ``}{\DIFadd{AnatOnMe: Facilitating doctor-patient communication using a projection-based handheld device}}\DIFadd{,'' }\emph{\DIFadd{Conference on Human Factors in Computing Systems - Proceedings}}\DIFadd{, pp. 3333--3342, 2011.
}

\bibitem{Abildgaard2010}
\DIFadd{A.~Abildgaard, A.~K. Witwit, J.~S. Karlsen, E.~A. Jacobsen, B.~Tenn}{\DIFadd{\o}}\DIFadd{e, G.~Ringstad, and P.~Due-T}{\DIFadd{\o}}\DIFadd{nnessen, ``}{\DIFadd{An autostereoscopic 3D display can improve visualization of 3D models from intracranial MR angiography}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Computer Assisted Radiology and Surgery}}\DIFadd{, vol.~5, no.~5, pp. 549--554, 2010.
}

\bibitem{Zhao2022}
\BIBentryALTinterwordspacing
\DIFadd{C.~Zhao, A.~S. Kim, R.~Beams, and A.~Badano, ``}{\DIFadd{Spatiotemporal image quality of virtual reality head mounted displays}}\DIFadd{,'' }\emph{\DIFadd{Scientific Reports}}\DIFadd{, vol.~12, no.~1, pp. 1--13, 2022. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1038/s41598-022-24345-9}
\BIBentrySTDinterwordspacing

\bibitem{Erickson2020}
\DIFadd{A.~Erickson, K.~Kim, G.~Bruder, and G.~F. Welch, ``}{\DIFadd{Exploring the Limitations of Environment Lighting on Optical See-Through Head-Mounted Displays}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - SUI 2020: ACM Symposium on Spatial User Interaction}}\DIFadd{, no. October, 2020.
}

\bibitem{Lee2020}
\DIFadd{K.-K. Lee, J.-W. Kim, J.-H. Ryu, and J.-O. Kim, ``}{\DIFadd{Ambient light robust gamut mapping for optical see-through displays}}\DIFadd{,'' }\emph{\DIFadd{Optics Express}}\DIFadd{, vol.~28, no.~10, p. 15392, 2020.
}

\bibitem{Ashtiani2023}
\DIFadd{O.~Ashtiani, H.~J. Guo, and B.~Prabhakaran, ``}{\DIFadd{Impact of motion cues, color, and luminance on depth perception in optical see-through AR displays}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Virtual Reality}}\DIFadd{, vol.~4, no. December, pp. 1--11, 2023.
}

\bibitem{Pisanpeeti2017}
\DIFadd{A.~P. Pisanpeeti and E.~Dinet, ``}{\DIFadd{Transparent objects: Influence of shape and color on depth perception}}\DIFadd{,'' }\emph{\DIFadd{ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings}}\DIFadd{, pp. 1867--1871, 2017.
}

\bibitem{Gerl2006}
\BIBentryALTinterwordspacing
\DIFadd{M.~Gerl, ``}{\DIFadd{Volume Hatching for Illustrative Visualization}}\DIFadd{,'' }\emph{\DIFadd{TU Wien Library}}\DIFadd{, no. November 2006, 2006. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://www.cg.tuwien.ac.at/research/publications/2006/gerl-2006-vhi/gerl-2006-vhi-pdf.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Zheng2013}
\DIFadd{L.~Zheng, Y.~Wu, and K.~L. Ma, ``}{\DIFadd{Perceptually-based depth-ordering enhancement for direct volume rendering}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~19, no.~3, pp. 446--459, 2013.
}

\bibitem{Diaz2008}
\DIFadd{J.~D\'iaz, H.~Yela, and P.-P. V}{\DIFadd{\'{a}}}\DIFadd{zquez, ``}{\DIFadd{Vicinity }{\DIFadd{O}}\DIFadd{cclusion }{\DIFadd{M}}\DIFadd{aps: }{\DIFadd{E}}\DIFadd{nhanced depth perception of volumetric models}}\DIFadd{,'' }\emph{\DIFadd{Proccedings of Computer Graphics International}}\DIFadd{, no. January 2008, pp. 56--63, 2008.
}

\bibitem{Piringer2004}
\DIFadd{H.~Piringer, R.~Kosara, and H.~Hauser, ``}{\DIFadd{Interactive Focus + Context Visualization with Linked 2D / 3D Scatterplots}}\DIFadd{,'' }\emph{\DIFadd{Proceedings. Second International Conference on Coordinated and Multiple Views in Exploratory Visualization}}\DIFadd{, 2004.
}

\bibitem{Marriott}
\DIFadd{K.~Marriott, F.~Schreiber, T.~Dwyer, K.~Klein, N.~Henry, R.~Takayuki, W.~Stuerzlinger, B.~H. Thomas, and D.~Hutchison, }\emph{{\DIFadd{Immersive Analytics 123}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Springer, 2018.
}

\bibitem{Baumeister2015}
\DIFadd{J.~Baumeister, M.~R. Marner, R.~T. Smith, M.~Kohler, B.~H. Thomas, t.~I. I. S. o.~M. 2015, and A.~R. W. J. . S.~. October, ``}{\DIFadd{Visual subliminal cues for spatial augmented reality}}\DIFadd{,'' in }\emph{\DIFadd{Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality Workshops, ISMARW 2015}}\DIFadd{, no. 7344747.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{IEEE, 2015, pp. 4--11.
}

\bibitem{Joshi2008}
\DIFadd{A.~Joshi, X.~Qian, D.~P. Dione, K.~R. Bulsara, C.~K. Breuer, A.~J. Sinusas, and X.~Papademetris, ``}{\DIFadd{Effective visualization of complex vascular structures using a non-parametric vessel detection method}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~14, no.~6, pp. 1603--1610, 2008.
}

\bibitem{Kim2008}
\DIFadd{D.~Kim, M.~Son, Y.~Lee, H.~Kang, and S.~Lee, ``}{\DIFadd{Feature-guided image stippling}}\DIFadd{,'' }\emph{\DIFadd{Computer Graphics Forum}}\DIFadd{, vol.~27, no.~4, pp. 1209--1216, 2008.
}

\bibitem{Preim2005}
\DIFadd{B.~Preim, C.~Tietjen, and C.~D}{\DIFadd{\"{o}}}\DIFadd{rge, ``}{\DIFadd{NPR, Focussing and Emphasis in Medical Visualizations}}\DIFadd{,'' }\emph{\DIFadd{Simulation und Visualisierung 2005}}\DIFadd{, no. January, pp. 139--152, 2005.
}

\bibitem{Salah2006}
\DIFadd{Z.~Salah, D.~Bartz, W.~Stra}{\DIFadd{\ss}}\DIFadd{er, and M.~Tatagiba, ``}{\DIFadd{Expressive anatomical illustrations based on scanned patient data}}\DIFadd{,'' }\emph{\DIFadd{ArXiv}}\DIFadd{, pp. 1--11, 2006.
}

\bibitem{Maciejewski2008}
\DIFadd{R.~Maciejewski, T.~Isenberg, W.~M. Andrews, D.~S. Ebert, M.~C. Sousa, and W.~Chen, ``}{\DIFadd{Measuring stipple aesthetics in hand-drawn and computer-generated images}}\DIFadd{,'' }\emph{\DIFadd{IEEE Computer Graphics and Applications}}\DIFadd{, vol.~28, no.~2, pp. 62--74, 2008.
}

\bibitem{Martin2017}
\DIFadd{D.~Mart}{\DIFadd{\'{i}}}\DIFadd{n, G.~Arroyo, A.~Rodr}{\DIFadd{\'{i}}}\DIFadd{guez, and T.~Isenberg, ``}{\DIFadd{A survey of digital stippling}}\DIFadd{,'' }\emph{\DIFadd{Computers and Graphics (Pergamon)}}\DIFadd{, vol.~67, no. October, pp. 24--44, 2017.
}

\bibitem{Ulichney1988}
\DIFadd{R.~A. Ulichney, ``}{\DIFadd{Dithering with Blue Noise}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE}}\DIFadd{, vol.~76, no.~1, pp. 56--79, 1988.
}

\bibitem{Lloyd1982}
\DIFadd{S.~P. Lloyd, ``}{\DIFadd{Least Squares Quantization in PCM}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Information Theory}}\DIFadd{, vol.~28, no.~2, pp. 129--137, 1982.
}

\bibitem{Heitz2018}
\DIFadd{E.~Heitz and F.~Neyret, ``}{\DIFadd{High-Performance By-Example Noise using a Histogram-Preserving Blending Operator}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the ACM on Computer Graphics and Interactive Techniques}}\DIFadd{, vol.~1, no.~2, pp. 1--25, 2018.
}

\bibitem{Salvetti2020}
\DIFadd{I.~A. Salvetti and C.~A. Gran, ``}{\DIFadd{Non-Photorealistic Rendering: Cross Hatching}}\DIFadd{,'' Ph.D. dissertation, Universitat Politecnica De Catalunya, 2020.
}

\bibitem{Yuan2005}
\BIBentryALTinterwordspacing
\DIFadd{X.~Yuan, M.~X. Nguyen, N.~Zhang, and B.~Chen, ``}{\DIFadd{Stippling and Silhouettes Rendering in Geometry-Image Space}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of Eurographics Symposium on Rendering 2005 (EGSR'05, June 29--July 1, 2005, Konstanz, Germany)}}\DIFadd{, no. May, pp. 193--200, 2005. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://www-users.cs.umn.edu/$\sim$xyuan/research/publication/egsr05.htm}
\BIBentrySTDinterwordspacing

\bibitem{Philbrick2019}
\DIFadd{G.~Philbrick and C.~S. Kaplan, ``}{\DIFadd{Defining Hatching in Art}}\DIFadd{,'' }\emph{\DIFadd{EXPRESSIVE 2019 - ACM/EG Expressive Symposium}}\DIFadd{, pp. 111--121, 2019.
}

\bibitem{Chen2011}
\DIFadd{J.~Chen, G.~Turk, and B.~MacIntyre, ``}{\DIFadd{Painterly rendering with coherence for augmented reality}}\DIFadd{,'' }\emph{\DIFadd{ISVRI 2011 - IEEE International Symposium on Virtual Reality Innovations 2011, Proceedings}}\DIFadd{, pp. 103--110, 2011.
}

\bibitem{Chen2012}
\DIFadd{------, ``}{\DIFadd{A non-photorealistic rendering framework with temporal coherence for augmented reality}}\DIFadd{,'' }\emph{\DIFadd{ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers}}\DIFadd{, pp. 151--160, 2012.
}

\bibitem{Frech1960}
\DIFadd{T.~E. French and T.~E. French, }\emph{{\DIFadd{A manual of engineering drawing for students \& draftsmen}}}\DIFadd{.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Cornell University, 1960.
}

\bibitem{Cocosco1997BrainWebOI}
\BIBentryALTinterwordspacing
\DIFadd{C.~A. Cocosco, V.~Kollokian, R.~K.-S. Kwan, and A.~C. Evans, ``Brainweb: Online interface to a 3d mri simulated brain database,'' }\emph{\DIFadd{NeuroImage}}\DIFadd{, 1997. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://api.semanticscholar.org/CorpusID:14864257}
\BIBentrySTDinterwordspacing

\bibitem{Ackerman1998}
\DIFadd{M.~J. Ackerman, ``}{\DIFadd{The Visible Human Project}}\DIFadd{,'' }\emph{\DIFadd{Proceedings of the IEEE}}\DIFadd{, vol.~86, no.~3, pp. 504--511, 1998.
}

\bibitem{neghip}
\DIFadd{R.~Avila, T.~He, L.~Hong, A.~Kaufman, H.~Pfister, C.~Silva, L.~Sobierajski, and S.~Wang, ``}{\DIFadd{VolVis}}\DIFadd{: A diversified system for volume research and development,'' in }\emph{\DIFadd{Proceedings Visualization '94}}\DIFadd{, oct 1994, pp. 31--38.
}

\bibitem{dns}
\DIFadd{M.~Lee and R.~D. Moser, ``Direct numerical simulation of turbulent channel flow up to ${R}e_\tau \approx 5200$,'' }\emph{\DIFadd{Journal of Fluid Mechanics}}\DIFadd{, vol. 774, pp. 395--415, jul 2015.
}

\bibitem{jicf_q}
\DIFadd{R.~W. Grout, A.~Gruber, H.~Kolla, P.-T. Bremer, J.~C. Bennett, A.~Gyulassy, and J.~H. Chen, ``A direct numerical simulation study of turbulence and flame structure in transverse jets analysed in jet-trajectory based coordinates,'' }\emph{\DIFadd{Journal of Fluid Mechanics}}\DIFadd{, vol. 706, pp. 351--383, 2012.
}

\bibitem{Bossek2018}
\DIFadd{J.~Bossek, ``}{\DIFadd{grapherator: A Modular Multi-Step Graph Generator}}\DIFadd{,'' }\emph{\DIFadd{The Journal of Open Source Software}}\DIFadd{, vol.~3, no.~22, p. 528, 2018.
}

\bibitem{Haghighi2017}
\DIFadd{S.~Haghighi, ``}{\DIFadd{Pyrgg: Python Random Graph Generator}}\DIFadd{,'' }\emph{\DIFadd{The Journal of Open Source Software}}\DIFadd{, vol.~2, no.~17, p. 331, 2017.
}

\bibitem{Meyer_and_Nagler_and_Hogan_2021}
\BIBentryALTinterwordspacing
\DIFadd{D.~Meyer, T.~Nagler, and R.~J. Hogan, ``Copula-based synthetic data augmentation for machine-learning emulators,'' }\emph{\DIFadd{Geoscientific Model Development}}\DIFadd{, vol.~14, no.~8, pp. 5205--5215, 2021. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.5194/gmd-14-5205-2021}
\BIBentrySTDinterwordspacing

\bibitem{Patki2016}
\DIFadd{N.~Patki, R.~Wedge, and K.~Veeramachaneni, ``}{\DIFadd{The synthetic data vault}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - 3rd IEEE International Conference on Data Science and Advanced Analytics, DSAA 2016}}\DIFadd{, pp. 399--410, 2016.
}

\bibitem{Fabian2015}
\BIBentryALTinterwordspacing
\DIFadd{B.~Fabian, T.~Ermakova, and P.~Junghanns, ``}{\DIFadd{Collaborative and secure sharing of healthcare data in multi-clouds}}\DIFadd{,'' }\emph{\DIFadd{Information Systems}}\DIFadd{, vol.~48, no. October 2022, pp. 132--150, 2015. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.is.2014.05.004}
\BIBentrySTDinterwordspacing

\bibitem{Gillmann2021}
\DIFadd{C.~Gillmann, N.~N. Smit, E.~Groller, B.~Preim, A.~Vilanova, and T.~Wischgoll, ``}{\DIFadd{Ten Open Challenges in Medical Visualization}}\DIFadd{,'' }\emph{\DIFadd{IEEE Computer Graphics and Applications}}\DIFadd{, vol.~41, no.~5, pp. 7--15, 2021.
}

\bibitem{Nature2023}
\BIBentryALTinterwordspacing
\DIFadd{Nature, ``}{\DIFadd{Data sharing is the future}}\DIFadd{,'' }\emph{\DIFadd{Nature Methods}}\DIFadd{, vol.~20, no.~4, p. 471, 2023. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1038/s41592-023-01865-4}
\BIBentrySTDinterwordspacing

\bibitem{Goodfellow2020}
\DIFadd{I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair, A.~Courville, and Y.~Bengio, ``}{\DIFadd{Generative adversarial networks}}\DIFadd{,'' }\emph{\DIFadd{Communications of the ACM}}\DIFadd{, vol.~63, no.~11, pp. 139--144, 2020.
}

\bibitem{Dhariwal2021}
\DIFadd{P.~Dhariwal and A.~Nichol, ``}{\DIFadd{Diffusion Models Beat GANs on Image Synthesis}}\DIFadd{,'' }\emph{\DIFadd{Advances in Neural Information Processing Systems}}\DIFadd{, vol.~11, pp. 8780--8794, 2021.
}

\bibitem{Pinaya2022}
\DIFadd{W.~H.~L. Pinaya, P.-d. Tudosiu, J.~Dafflon, P.~F. Da, V.~Fernandez, P.~Nachev, S.~Ourselin, and J.~M. Cardoso, ``}{\DIFadd{Brain Imaging Generation with Latent Diffusion Models}}\DIFadd{,'' }\emph{\DIFadd{arXiv}}\DIFadd{, pp. 1--10, 2022.
}

\bibitem{Ren2021}
\DIFadd{Z.~Ren, S.~X. Yu, and D.~Whitney, ``}{\DIFadd{Controllable medical image generation via generative adversarial networks}}\DIFadd{,'' }\emph{\DIFadd{IS and T International Symposium on Electronic Imaging Science and Technology}}\DIFadd{, vol. 2021, no.~11, 2021.
}

\bibitem{Togo2019}
\DIFadd{R.~Togo, T.~Ogawa, and M.~Haseyama, ``}{\DIFadd{Synthetic gastritis image generation via loss function-based conditional pggan}}\DIFadd{,'' }\emph{\DIFadd{IEEE Access}}\DIFadd{, vol.~7, pp. 87\,448--87\,457, 2019.
}

\bibitem{Nguyen2023}
\DIFadd{L.~X. Nguyen, P.~S. Aung, H.~Q. Le, S.~B. Park, and C.~S. Hong, ``}{\DIFadd{A New Chapter for Medical Image Generation: The Stable Diffusion Method}}\DIFadd{,'' }\emph{\DIFadd{International Conference on Information Networking}}\DIFadd{, vol. 2023-January, pp. 483--486, 2023.
}

\bibitem{Tomic2023}
\DIFadd{H.~Tomic, A.~C. Costa, A.~Bjerk}{\DIFadd{\'{e}}}\DIFadd{n, M.~A. Vieira, S.~Zackrisson, A.~Tingberg, P.~Timberg, M.~Dustler, and P.~R. Bakic, ``}{\DIFadd{Simulation of breast lesions based upon fractal Perlin noise}}\DIFadd{,'' }\emph{\DIFadd{Physica Medica}}\DIFadd{, vol. 114, no. July, 2023.
}

\bibitem{Lawson2024}
\BIBentryALTinterwordspacing
\DIFadd{B.~A. Lawson, C.~Drovandi, P.~Burrage, A.~Bueno-Orovio, R.~W. dos Santos, B.~Rodriguez, K.~Mengersen, and K.~Burrage, ``}{\DIFadd{Perlin noise generation of physiologically realistic cardiac fibrosis}}\DIFadd{,'' }\emph{\DIFadd{Medical Image Analysis}}\DIFadd{, vol.~98, no. October 2023, p. 103240, 2024. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1016/j.media.2024.103240}
\BIBentrySTDinterwordspacing

\bibitem{Svakhine2009}
\DIFadd{N.~A. Svakhine, D.~S. Ebert, and W.~M. Andrews, ``}{\DIFadd{Illustration-inspired depth enhanced volumetric medical visualization}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~15, no.~1, pp. 77--86, 2009.
}

\bibitem{Lin2021}
\DIFadd{D.~Lin, C.~Wyman, and C.~Yuksel, ``}{\DIFadd{Fast volume rendering with spatiotemporal reservoir resampling}}\DIFadd{,'' }\emph{\DIFadd{ACM Transactions on Graphics}}\DIFadd{, vol.~40, no.~6, pp. 1--18, 2021.
}

\bibitem{Gonzalez2010}
{\DIFadd{\'{A}}}\DIFadd{.~Gonz}{\DIFadd{\'{a}}}\DIFadd{lez, ``}{\DIFadd{Measurement of Areas on a Sphere Using Fibonacci and Latitude-Longitude Lattices}}\DIFadd{,'' }\emph{\DIFadd{Mathematical Geosciences}}\DIFadd{, vol.~42, no.~1, pp. 49--64, 2010.
}

\bibitem{Alex2020}
\DIFadd{M.~Alex, D.~Lottridge, J.~Lee, S.~Marks, and B.~W}{\DIFadd{\"{u}}}\DIFadd{ensche, ``}{\DIFadd{Discrete versus Continuous Colour Pickers Impact Colour Selection in Virtual Reality Art-Making}}\DIFadd{,'' }\emph{\DIFadd{ACM International Conference Proceeding Series}}\DIFadd{, no. Figure 2, pp. 158--169, 2020.
}

\bibitem{Coskun2022}
{\DIFadd{\"{O}}}\DIFadd{.~Coşkun, G.~}{\DIFadd{Nteli Chatzioglou}}\DIFadd{, and A.~}{\DIFadd{\"{O}}}\DIFadd{zt}{\DIFadd{\"{u}}}\DIFadd{rk, ``}{\DIFadd{Henry Gray (1827–1861): the great author of the most widely used resource in medical education}}\DIFadd{,'' }\emph{\DIFadd{Child's Nervous System}}\DIFadd{, vol.~38, no.~8, pp. 1421--1423, 2022.
}

\bibitem{Wijayanto2023}
\DIFadd{I.~A. Wijayanto, S.~V. Babu, C.~C. Pagano, and J.~H. Chuang, ``}{\DIFadd{Comparing the Effects of Visual Realism on Size Perception in VR versus Real World Viewing through Physical and Verbal Judgments}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~29, no.~5, pp. 2721--2731, 2023.
}

\bibitem{Busking2007}
\BIBentryALTinterwordspacing
\DIFadd{S.~Busking, A.~Vilanova, and J.~J. van Wijk, ``}{\DIFadd{VolumeFlies: Illustrative Volume Visualization using Particles}}\DIFadd{,'' }\emph{\DIFadd{Thirteenth Annual Conference of the Advanced School for Computing and Imaging, Proceedings}}\DIFadd{, vol.~Vi, no. September, pp. 51--58, 2007. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://graphics.tudelft.nl/$\sim$stef/publications/Busking2007-ASCI.pdf}
\BIBentrySTDinterwordspacing

\bibitem{Blum2012a}
\DIFadd{T.~Blum, V.~Kleeberger, C.~Bichlmeier, and N.~Navab, ``}{\DIFadd{Mirracle: Augmented reality in-situ visualization of human anatomy using a magic mirror}}\DIFadd{,'' }\emph{\DIFadd{Proceedings - IEEE Virtual Reality}}\DIFadd{, pp. 169--170, 2012.
}

\bibitem{JunYoungChoi2018}
{\DIFadd{JunYoung Choi}}\DIFadd{, }{\DIFadd{Hae Jin Jeong}}\DIFadd{, and }{\DIFadd{Jeong Wonki}}\DIFadd{, ``}{\DIFadd{Improvement Depth Perception of Volume Rendering using Virtual Reality}}\DIFadd{,'' pp. 29--40, 2018.
}

\bibitem{Munzner2014}
\BIBentryALTinterwordspacing
\DIFadd{T.~Munzner, ``}{\DIFadd{Why: Task Abstraction}}\DIFadd{,'' in }\emph{\DIFadd{Visual Analysis and Design}}\DIFadd{, 1st~ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{New York, NY, USA: CRC Press, 2014, ch.~3, pp. 43--62. }[\DIFadd{Online}]\DIFadd{. Available: }\url{https://doi.org/10.1201/b17511}
\BIBentrySTDinterwordspacing

\bibitem{Raab2019}
\DIFadd{M.~Raab and D.~Ara}{\DIFadd{\'{u}}}\DIFadd{jo, ``}{\DIFadd{Embodied cognition with and without mental representations: The case of embodied choices in sports}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Microbiology}}\DIFadd{, vol.~10, no. AUG, 2019.
}

\bibitem{Wilson2013}
\DIFadd{A.~D. Wilson and S.~Golonka, ``}{\DIFadd{Embodied Cognition is Not What you Think it is}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Psychology}}\DIFadd{, vol.~4, no. February, pp. 1--13, 2013.
}

\bibitem{Martin-Gomez2019}
\DIFadd{A.~Martin-Gomez, U.~Eck, and N.~Navab, ``}{\DIFadd{Visualization techniques for precise alignment in VR: A comparative study}}\DIFadd{,'' }\emph{\DIFadd{26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings}}\DIFadd{, pp. 735--741, 2019.
}

\bibitem{Paas2003}
\DIFadd{F.~Paas, J.~E. Tuovinen, H.~Tabbers, and P.~W. }{\DIFadd{Van Gerven}}\DIFadd{, ``}{\DIFadd{Cognitive load measurement as a means to advance cognitive load theory}}\DIFadd{,'' }\emph{\DIFadd{Educational Psychologist}}\DIFadd{, vol.~38, no.~1, pp. 63--71, 2003.
}

\bibitem{Lewis2018}
\DIFadd{J.~R. Lewis, ``}{\DIFadd{The System Usability Scale: Past, Present, and Future}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Human-Computer Interaction}}\DIFadd{, vol.~34, no.~7, p. 577, 2018.
}

\bibitem{Yoghourdjian2018}
\DIFadd{V.~Yoghourdjian, D.~Archambault, S.~Diehl, T.~Dwyer, K.~Klein, H.~C. Purchase, and H.~Y. Wu, ``}{\DIFadd{Exploring the limits of complexity: A survey of empirical studies on graph visualisation}}\DIFadd{,'' }\emph{\DIFadd{Visual Informatics}}\DIFadd{, vol.~2, no.~4, pp. 264--282, 2018.
}

\bibitem{Jang2017}
\BIBentryALTinterwordspacing
\DIFadd{S.~Jang, J.~M. Vitale, R.~W. Jyung, and J.~B. Black, ``}{\DIFadd{Direct manipulation is better than passive viewing for learning anatomy in a three-dimensional virtual reality environment}}\DIFadd{,'' }\emph{\DIFadd{Computers and Education}}\DIFadd{, vol. 106, pp. 150--165, 2017. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1016/j.compedu.2016.12.009}
\BIBentrySTDinterwordspacing

\bibitem{sep-embodied-cognition}
\DIFadd{L.~Shapiro and S.~Spaulding, ``}{\DIFadd{Embodied Cognition}}\DIFadd{,'' in }\emph{\DIFadd{The }{\DIFadd{Stanford}} \DIFadd{Encyclopedia of Philosophy}}\DIFadd{, E.~N. Zalta, Ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Metaphysics Research Lab, Stanford University, 2021.
}

\bibitem{Kalkofen2009}
\DIFadd{D.~Kalkofen, E.~Mendez, and D.~Schmalstieg, ``}{\DIFadd{Comprehensible visualization for augmented reality}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~15, no.~2, pp. 193--204, 2009.
}

\bibitem{Hansen2010}
\DIFadd{C.~Hansen, J.~Wieferich, F.~Ritter, C.~Rieder, and H.~O. Peitgen, ``}{\DIFadd{Illustrative visualization of 3D planning models for augmented reality in liver surgery}}\DIFadd{,'' }\emph{\DIFadd{International Journal of Computer Assisted Radiology and Surgery}}\DIFadd{, vol.~5, no.~2, pp. 133--141, 2010.
}

\bibitem{Lowonn2013}
\DIFadd{K.~Lowonn, R.~Gasteiger, and B.~Preim, ``}{\DIFadd{Qualitative Evaluation of Feature Lines on Anatomical Surfaces}}\DIFadd{,'' in }\emph{\DIFadd{Bildverarbeitung f}{\DIFadd{\"{u}}}\DIFadd{r die Medizin}}\DIFadd{, 2013, no. May 2014, pp. 187--193.
}

\bibitem{Nagata1983}
\DIFadd{S.~Nagata, ``}{\DIFadd{How To Reinforce Perception of Depth in Single Two-Dimensional Pictures.}}\DIFadd{'' }\emph{\DIFadd{Proceedings of the SID}}\DIFadd{, vol.~25, no.~3, pp. 239--246, 1983.
}

\bibitem{Chen2018}
\BIBentryALTinterwordspacing
\DIFadd{Z.~Chen, R.~N. Denison, D.~Whitney, and G.~W. Maus, ``}{\DIFadd{Illusory occlusion affects stereoscopic depth perception}}\DIFadd{,'' }\emph{\DIFadd{Scientific Reports}}\DIFadd{, vol.~8, no.~1, pp. 1--9, 2018. }[\DIFadd{Online}]\DIFadd{. Available: }\url{http://dx.doi.org/10.1038/s41598-018-23548-3}
\BIBentrySTDinterwordspacing

\bibitem{Vinson1967}
\DIFadd{J.~C. Vinson, }\emph{{\DIFadd{Thomas Nast. Political Cartoonist}}}\DIFadd{, 1st~ed.}\hskip \DIFadd{1em plus 0.5em minus 0.4em}\relax \DIFadd{Atlanta: Athens: University of Georgia Press, 1967.
}

\bibitem{Philbrick2022}
\DIFadd{G.~Philbrick and C.~S. Kaplan, ``}{\DIFadd{A Primitive for Manual Hatching}}\DIFadd{,'' }\emph{\DIFadd{ACM Transactions on Graphics}}\DIFadd{, vol.~41, no.~2, 2022.
}

\bibitem{Ritter2006}
\DIFadd{F.~Ritter, C.~Hansen, V.~Dicken, O.~Konrad, B.~Preim, and H.~O. Peitgen, ``}{\DIFadd{Real-time illustration of vascular structures}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, vol.~12, no.~5, pp. 877--884, 2006.
}

\bibitem{Krajancich2020}
\DIFadd{B.~Krajancich, P.~Kellnhofer, and G.~Wetzstein, ``}{\DIFadd{Optimizing depth perception in virtual and augmented reality through gaze-contingent stereo rendering}}\DIFadd{,'' }\emph{\DIFadd{ACM Transactions on Graphics}}\DIFadd{, vol.~39, no.~6, 2020.
}

\bibitem{Adams2021}
\DIFadd{H.~Adams, J.~Stefanucci, S.~H. Creem-Regehr, G.~Pointon, W.~B. Thompson, and B.~Bodenheimer, ``}{\DIFadd{Shedding Light on Cast Shadows: An Investigation of Perceived Ground Contact in AR and VR}}\DIFadd{,'' }\emph{\DIFadd{IEEE Transactions on Visualization and Computer Graphics}}\DIFadd{, no. July, 2021.
}

\bibitem{Harvey1986}
\DIFadd{L.~Harvey, ``Efficient estimation of sensory thresholds,'' }\emph{\DIFadd{Behavior Research Methods, Instruments, \& Computers}}\DIFadd{, vol.~18, pp. 623--632, 11 1986.
}

\bibitem{Gilliam1996}
\DIFadd{X.~Gilliam, K.~Manross, and M.~Gamel, ``}{\DIFadd{Visualization of Radar Data in Three-Dimensions}}\DIFadd{,'' }\emph{\DIFadd{Wind Engineering}}\DIFadd{, 1996.
}

\bibitem{Macdonald1997}
\DIFadd{S.~D. Macdonald, P.~J. Fields, and M.~D. Stroup, ``}{\DIFadd{REAL-TIME 3D SONAR MODELING AND VISUALIZATION}}\DIFadd{,'' Ph.D. dissertation, Navel Postgraduate School, Monterey, California, 1997.
}

\bibitem{8885576}
\DIFadd{M.~Danu, C.-I. Nita, A.~Vizitiu, C.~Suciu, and L.~M. Itu, ``Deep learning based generation of synthetic blood vessel surfaces,'' in }\emph{\DIFadd{2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)}}\DIFadd{, 2019, pp. 662--667.
}

\bibitem{10049010}
\DIFadd{L.~X. Nguyen, P.~Sone~Aung, H.~Q. Le, S.-B. Park, and C.~S. Hong, ``A new chapter for medical image generation: The stable diffusion method,'' in }\emph{\DIFadd{2023 International Conference on Information Networking (ICOIN)}}\DIFadd{, 2023, pp. 483--486.
}

\bibitem{Ratcliff2010}
\DIFadd{R.~Ratcliff and P.~L. Smith, ``}{\DIFadd{Perceptual Discrimination in Static and Dynamic Noise: The Temporal Relation Between Perceptual Encoding and Decision Making}}\DIFadd{,'' }\emph{\DIFadd{Journal of Experimental Psychology: General}}\DIFadd{, vol. 139, no.~1, pp. 70--94, 2010.
}

\bibitem{Jing2021}
\DIFadd{A.~Jing, K.~May, G.~Lee, and M.~Billinghurst, ``}{\DIFadd{Eye See What You See: Exploring How Bi-Directional Augmented Reality Gaze Visualisation Influences Co-Located Symmetric Collaboration}}\DIFadd{,'' }\emph{\DIFadd{Frontiers in Virtual Reality}}\DIFadd{, vol.~2, no. June, pp. 1--17, 2021.
}

\bibitem{Booij2022}
\DIFadd{R.~Booij, M.~van Straten, A.~Wimmer, and R.~P. Budde, ``}{\DIFadd{Influence of breathing state on the accuracy of automated patient positioning in thoracic CT using a 3D camera for body contour detection}}\DIFadd{,'' }\emph{\DIFadd{European Radiology}}\DIFadd{, vol.~32, no.~1, pp. 442--447, 2022.
}

\end{thebibliography}


    \DIFaddend \glsresetall

	\listoffigures
	\listoftables
    %\listofalgorithms

    
    \glsresetall

    \appendix
     \newpage \chapter{Holographic Overlay System Details} \label{App:HolographicOverlaySystemDetails}
% Add a paragraph to introduce this

% The DICOM image 
The \gls{dicom} data generated by the \gls{ct} scanner was displayed using a real-time rendered iso-surface to represent 3D models by compiling the \gls{dicom} data from real-time using marching cubes~\cite{Lorensen} from \gls{dicom} information generated by fo-DICOM~\footnote{https://github.com/fo-dicom/fo-dicom}.
Running marching cubes, even in parallel blocks, was deemed too slow and imprecise.
This was then encased within an avatar of the patient for reviewing processes or could be viewed.

% The placing of electrodes
Later iterations of this project aimed at helping to communicate tasks for physiotherapists by communicating with them where electrodes should be placed on the patient. 
Predefined locations were based on previous data to simulate the concept of predefined instructions and the system's body type using the later work of Booij et al.~\cite {Booij2022}, where a more accurate model of the body could be formed to counteract issues like the patient's breathing.
This allowed us to correct the difference between different body types in different patients. 

% The final system
The final part of this system looked at making the \gls{ct} \gls{dicom} data more interactive. 
A two-way clipping plane that would show the current slice with adjustable contrast was included.
These six clipping planes were placed on the scan's vertical, horizontal, and depth axes.
These would each have their own data set that could be displayed, communicating what slice was being looked at in either slice/pixel range, percentage through the volume, or center meters through the volume. 
This system also came with several different methods of interacting with it, depending on the type of task required by the radiologist.
The final version of the product can be seen in \autoref{fig:HolographicOverlaySystem}. 

% The Direct Volume Rendering Section
This system was then extended to the present version of this project and moved over to having a \gls{dvr} system attached. 
\gls{dvr} works by directly rendering the data from the \gls{dicom} files. This allows for far greater data flexibility, rendering the visualization with almost no performance overheads. 
The trade-off for using \gls{dvr} is that it requires a much higher level of performance overall, requiring us to use a wireless theater using holographic remoting to a nearby \gls{pc} to run the system. 
This was an acceptable trade-off as this system to have both performed more reliably, delivered a smoother frame rate, improved the quality of graphics, and allowed for much faster network connectivity. 

% add a paragraph to end this

\chapter{X-ray Vision Literature Review Methodology} \label{app:XRayLitReviewMethodology}
\DIFdelbegin \DIFdel{This literature review was inspired by the PRIMA 2020 SRC (Page et al. 2021) protocols checklist. It utilized five databases: 
ACM Digital Library~}\footnote{%DIFDELCMD < \url{https://dl.acm.org/}%%%
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{, 
IEEE Xplore~}\footnote{%DIFDELCMD < \url{https://ieeexplore.ieee.org/Xplore/home.jsp}%%%
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{, 
Pub Med~}\footnote{%DIFDELCMD < \url{https://pubmed.ncbi.nlm.nih.gov/}%%%
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{, 
Web of Science~}\footnote{%DIFDELCMD < \url{https://www.webofscience.com/wos/}%%%
}%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{, 
and Scopus~}\footnote{%DIFDELCMD < \url{https://www.scopus.com/}%%%
} 
%DIFAUXCMD
\addtocounter{footnote}{-1}%DIFAUXCMD
\DIFdel{using the search terms:
}\textit{\DIFdel{“("X-ray vision" OR "X-ray visualization" OR ("occlusion" AND "perception") OR (visualization AND x-ray) OR "see-through vision" OR "ghosted views" OR "augmented reality x-ray") AND ("AR" OR "Augmented Reality" OR "Mixed Reality")”}}
%DIFAUXCMD
\DIFdel{These terms were filtered by the paper’s Abstract, Title, and Keywords. To find grey literature, papers cited by more than five papers were also considered for entry into this database (2 papers found and accepted). For a paper to be accepted, it needed to meet the eligibility criteria outlined below. The process and results of this review as depicted in }%DIFDELCMD < \autoref{fig:X-LItReviewMethodology}
%DIFDELCMD < %%%
\DIFdelend %DIF >  This literature review was inspired by the PRIMA 2020 SRC (Page et al. 2021) protocols checklist. It utilized five databases: 
%DIF >  ACM Digital Library~\footnote{\url{https://dl.acm.org/}}, 
%DIF >  IEEE Xplore~\footnote{\url{https://ieeexplore.ieee.org/Xplore/home.jsp}}, 
%DIF >  Pub Med~\footnote{\url{https://pubmed.ncbi.nlm.nih.gov/}}, 
%DIF >  Web of Science~\footnote{\url{https://www.webofscience.com/wos/}}, 
%DIF >  and Scopus~\footnote{\url{https://www.scopus.com/}} 
%DIF >  using the search terms:
%DIF >  \textit{“("X-ray vision" OR "X-ray visualization" OR ("occlusion" AND "perception") OR (visualization AND x-ray) OR "see-through vision" OR "ghosted views" OR "augmented reality x-ray") AND ("AR" OR "Augmented Reality" OR "Mixed Reality")”}
%DIF >  These terms were filtered by the paper’s Abstract, Title, and Keywords. To find grey literature, papers cited by more than five papers were also considered for entry into this database (2 papers found and accepted). For a paper to be accepted, it needed to meet the eligibility criteria outlined below. The process and results of this review as depicted in \autoref{fig:X-LItReviewMethodology}

\DIFdelbegin %DIFDELCMD < \begin{figure}
%DIFDELCMD <     \centering
%DIFDELCMD <     \includegraphics[width=\textwidth]{Chapter2/Images/LItReviewMethodology.pdf}
%DIFDELCMD <     %%%
%DIFDELCMD < \caption{%
{%DIFAUXCMD
\DIFdelFL{The protocol utilized for this literature review with the matching results. }}
    %DIFAUXCMD
%DIFDELCMD < \label{fig:X-LItReviewMethodology}
%DIFDELCMD < \end{figure}
%DIFDELCMD < %%%
\DIFdelend %DIF >  \begin{figure}
%DIF >      \centering
%DIF >      \includegraphics[width=\textwidth]{Chapter2/Images/LItReviewMethodology.pdf}
%DIF >      \caption{The protocol utilized for this literature review with the matching results. }
%DIF >      \label{fig:X-LItReviewMethodology}
%DIF >  \end{figure}

\DIFdelbegin \subsubsection{\DIFdel{Eligibility Criteria}}
%DIFAUXCMD
\addtocounter{subsubsection}{-1}%DIFAUXCMD
\DIFdel{All papers in this review needed to meet the following criteria:
}%DIFDELCMD < \begin{itemize}
\begin{itemize}%DIFAUXCMD
%DIFDELCMD <     \item %%%
\item%DIFAUXCMD
\DIFdel{X-ray vision (XRV) needed to be explored in the research, placing virtual objects behind real-world objects.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Mixed Reality Technologies other than VR must have been used.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Papers that simply superimposed data over the real world and papers that did not focus on making the content appear on the other side of or within the real-world object were not included.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{Studies included in multiple papers were only recorded once all papers that highlighted the same research had been included in this review, as the content between both articles provides different amounts of information.
    }%DIFDELCMD < \item %%%
\item%DIFAUXCMD
\DIFdel{If the focus on XRV was light compared to other study elements and two or more reviewers agreed on it, then the work would not be included in favor of more focused research. 
}
\end{itemize}%DIFAUXCMD
%DIFDELCMD < \end{itemize}
%DIFDELCMD < %%%
\DIFdelend %DIF >  \subsubsection{Eligibility Criteria}
%DIF >  All papers in this review needed to meet the following criteria:
%DIF >  \begin{itemize}
%DIF >      \item X-ray vision (XRV) needed to be explored in the research, placing virtual objects behind real-world objects.
%DIF >      \item Mixed Reality Technologies other than VR must have been used.
%DIF >      \item Papers that simply superimposed data over the real world and papers that did not focus on making the content appear on the other side of or within the real-world object were not included.
%DIF >      \item Studies included in multiple papers were only recorded once all papers that highlighted the same research had been included in this review, as the content between both articles provides different amounts of information.
%DIF >      \item If the focus on XRV was light compared to other study elements and two or more reviewers agreed on it, then the work would not be included in favor of more focused research. 
%DIF >  \end{itemize}

\DIFdelbegin \DIFdel{These criteria ensured that the X-ray visualization was more than superimposing a medical image onto a human subject. Any paper that did not utilize }%DIFDELCMD < \gls{ar} %%%
\DIFdel{was removed. This meant that some documents that utilized Virtual Reality technologies to test }%DIFDELCMD < \gls{ar} %%%
\DIFdel{in a completely virtual environment were not included.
}\DIFdelend %DIF >  These criteria ensured that the X-ray visualization was more than superimposing a medical image onto a human subject. Any paper that did not utilize \gls{ar} was removed. This meant that some documents that utilized Virtual Reality technologies to test \gls{ar} in a completely virtual environment were not included.

\chapter{Class Diagram for Random Generating Volumes} \label{app:RandomGerneratingVolumesClassDiagram}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Chapter4/Images/DefaultClassDiagram.png}
    \caption{The Class Diagram showing the main dependencies of the Random Generating Volumes System.}
    \label{fig:RandomGerneratingVolumesClassDiagram}
\end{figure}


\chapter{Chapter 5: Counting Everything Post Hoc}  \label{apendix:ErrorWhenCountingEverything}

% latex table generated in R 4.2.3 by xtable 1.8-4 package
% Fri May 12 14:44:16 2023

\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{llllrrr}
  \hline

LHS VIRT &  \begin{tabular}{@{}c@{}}LHS Amount\\Of Objects\end{tabular} & RHS VIRT & \begin{tabular}{@{}c@{}}RHS Amount\\Of Objects\end{tabular}  & estimate& t.ratio & p.value \\ 
  \hline
Halo/Outline  &  14  &  Hatching  &  14 & -2.76573 & -8.68107 & 0.00000 \\ 
  Halo/Outline  &  14  &  No Effect  &  14 & -1.04346 & -3.28904 & 0.11078 \\ 
  Halo/Outline  &  14  &  Stippling  &  14 & -2.10695 & -6.64124 & 0.00000 \\ 
  Halo/Outline  &  14  &  Halo/Outline  &  16 & -0.04346 & -0.13697 & 1.00000 \\ 
  Halo/Outline  &  14  &  Hatching  &  16 & -3.13869 & -9.89337 & 0.00000 \\ 
  Halo/Outline  &  14  &  No Effect  &  16 & -1.23393 & -3.88944 & 0.01542 \\ 
  Halo/Outline  &  14  &  Stippling  &  16 & -2.18631 & -6.89140 & 0.00000 \\ 
  Halo/Outline  &  14  &  Halo/Outline  &  18 & -0.39266 & -1.23770 & 0.99960 \\ 
  Halo/Outline  &  14  &  Hatching  &  18 & -3.93234 & -12.39501 & 0.00000 \\ 
  Halo/Outline  &  14  &  No Effect  &  18 & -1.37679 & -4.33973 & 0.00254 \\ 
  Halo/Outline  &  14  &  Stippling  &  18 & -2.78712 & -8.97394 & 0.00000 \\ 
  Halo/Outline  &  14  &  Halo/Outline  &  20 & -0.55139 & -1.73802 & 0.97364 \\ 
  Halo/Outline  &  14  &  Hatching  &  20 & -5.02791 & -15.96877 & 0.00000 \\ 
  Halo/Outline  &  14  &  No Effect  &  20 & -1.82812 & -5.78551 & 0.00000 \\ 
  Halo/Outline  &  14  &  Stippling  &  20 & -3.34638 & -10.50360 & 0.00000 \\ 
  Halo/Outline  &  14  &  Halo/Outline  &  22 & -0.61488 & -1.93816 & 0.92538 \\ 
  Halo/Outline  &  14  &  Hatching  &  22 & -5.20219 & -16.39764 & 0.00000 \\ 
  Halo/Outline  &  14  &  No Effect  &  22 & -2.36092 & -7.44176 & 0.00000 \\ 
  Halo/Outline  &  14  &  Stippling  &  22 & -4.10695 & -12.94537 & 0.00000 \\
  Hatching  &  14  &  No Effect  &  14 & 1.72228 & 5.38580 & 0.00002 \\ 
  Hatching  &  14  &  Stippling  &  14 & 0.65878 & 2.06011 & 0.87628 \\ 
  Hatching  &  14  &  Halo/Outline  &  16 & 2.72228 & 8.51293 & 0.00000 \\ 
  Hatching  &  14  &  Hatching  &  16 & -0.37296 & -1.16630 & 0.99983 \\ 
  Hatching  &  14  &  No Effect  &  16 & 1.53180 & 4.79015 & 0.00033 \\ 
  Hatching  &  14  &  Stippling  &  16 & 0.57942 & 1.81192 & 0.95998 \\ 
  Hatching  &  14  &  Halo/Outline  &  18 & 2.37307 & 7.42092 & 0.00000 \\ 
  Hatching  &  14  &  Hatching  &  18 & -1.16661 & -3.64816 & 0.03631 \\ 
  Hatching  &  14  &  No Effect  &  18 & 1.38894 & 4.34342 & 0.00250 \\ 
  Hatching  &  14  &  Stippling  &  18 & -0.02139 & -0.06822 & 1.00000 \\ 
  Hatching  &  14  &  Halo/Outline  &  20 & 2.21434 & 6.92455 & 0.00000 \\ 
  Hatching  &  14  &  Hatching  &  20 & -2.26217 & -7.12720 & 0.00000 \\ 
  Hatching  &  14  &  No Effect  &  20 & 0.93761 & 2.94296 & 0.26525 \\ 
  Hatching  &  14  &  Stippling  &  20 & -0.58065 & -1.80864 & 0.96068 \\ 
  Hatching  &  14  &  Halo/Outline  &  22 & 2.15085 & 6.72600 & 0.00000 \\ 
  Hatching  &  14  &  Hatching  &  22 & -2.43645 & -7.61913 & 0.00000 \\  
       \hline
\end{tabular}
\end{table}

\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{llllrrr}
  \hline
LHS VIRT & \begin{tabular}{@{}c@{}}RHS Amount\\Of Objects\end{tabular}  & RHS VIRT & \begin{tabular}{@{}c@{}}RHS Amount\\Of Objects\end{tabular}  & estimate & t.ratio & p.value \\ 
  \hline 
  Hatching  &  14  &  No Effect  &  22 & 0.40482 & 1.26592 & 0.99946 \\ 
  Hatching  &  14  &  Stippling  &  22 & -1.34122 & -4.19417 & 0.00468 \\
  No Effect  &  14  &  Stippling  &  14 & -1.06349 & -3.33926 & 0.09595 \\ 
  No Effect  &  14  &  Halo/Outline  &  16 & 1.00000 & 3.13990 & 0.16561 \\ 
  No Effect  &  14  &  Hatching  &  16 & -2.09524 & -6.57885 & 0.00000 \\ 
  No Effect  &  14  &  No Effect  &  16 & -0.19048 & -0.59808 & 1.00000 \\ 
  No Effect  &  14  &  Stippling  &  16 & -1.14286 & -3.58846 & 0.04431 \\ 
  No Effect  &  14  &  Halo/Outline  &  18 & 0.65079 & 2.04343 & 0.88396 \\ 
  No Effect  &  14  &  Hatching  &  18 & -2.88889 & -9.07083 & 0.00000 \\ 
  No Effect  &  14  &  No Effect  &  18 & -0.33333 & -1.04663 & 0.99997 \\ 
  No Effect  &  14  &  Stippling  &  18 & -1.74367 & -5.58808 & 0.00001 \\ 
  No Effect  &  14  &  Halo/Outline  &  20 & 0.49206 & 1.54503 & 0.99286 \\ 
  No Effect  &  14  &  Hatching  &  20 & -3.98445 & -12.60529 & 0.00000 \\ 
  No Effect  &  14  &  No Effect  &  20 & -0.78467 & -2.47333 & 0.60495 \\ 
  No Effect  &  14  &  Stippling  &  20 & -2.30292 & -7.20155 & 0.00000 \\ 
  No Effect  &  14  &  Halo/Outline  &  22 & 0.42857 & 1.34567 & 0.99875 \\ 
  No Effect  &  14  &  Hatching  &  22 & -4.15873 & -13.05801 & 0.00000 \\ 
  No Effect  &  14  &  No Effect  &  22 & -1.31746 & -4.13670 & 0.00592 \\ 
  No Effect  &  14  &  Stippling  &  22 & -3.06349 & -9.61907 & 0.00000 \\ 
  Stippling  &  14  &  Halo/Outline  &  16 & 2.06349 & 6.47917 & 0.00000 \\ 
  Stippling  &  14  &  Hatching  &  16 & -1.03175 & -3.23958 & 0.12711 \\ 
  Stippling  &  14  &  No Effect  &  16 & 0.87302 & 2.74119 & 0.39865 \\ 
  Stippling  &  14  &  Stippling  &  16 & -0.07937 & -0.24920 & 1.00000 \\ 
  Stippling  &  14  &  Halo/Outline  &  18 & 1.71429 & 5.38269 & 0.00002 \\ 
  Stippling  &  14  &  Hatching  &  18 & -1.82540 & -5.73157 & 0.00000 \\ 
  Stippling  &  14  &  No Effect  &  18 & 0.73016 & 2.29263 & 0.73955 \\ 
  Stippling  &  14  &  Stippling  &  18 & -0.68018 & -2.17982 & 0.81246 \\ 
  Stippling  &  14  &  Halo/Outline  &  20 & 1.55556 & 4.88429 & 0.00021 \\ 
  Stippling  &  14  &  Hatching  &  20 & -2.92096 & -9.24080 & 0.00000 \\ 
  Stippling  &  14  &  No Effect  &  20 & 0.27882 & 0.87887 & 1.00000 \\ 
  Stippling  &  14  &  Stippling  &  20 & -1.23943 & -3.87587 & 0.01622 \\ 
  Stippling  &  14  &  Halo/Outline  &  22 & 1.49206 & 4.68494 & 0.00054 \\ 
  Stippling  &  14  &  Hatching  &  22 & -3.09524 & -9.71875 & 0.00000 \\ 
  Stippling  &  14  &  No Effect  &  22 & -0.25397 & -0.79744 & 1.00000 \\ 
  Stippling  &  14  &  Stippling  &  22 & -2.00000 & -6.27981 & 0.00000 \\ 
  Halo/Outline  &  16  &  Hatching  &  16 & -3.09524 & -9.71875 & 0.00000 \\ 
  Halo/Outline  &  16  &  No Effect  &  16 & -1.19048 & -3.73798 & 0.02665 \\ 
  Halo/Outline  &  16  &  Stippling  &  16 & -2.14286 & -6.72837 & 0.00000 \\ 
  Halo/Outline  &  16  &  Halo/Outline  &  18 & -0.34921 & -1.09647 & 0.99993 \\ 
  Halo/Outline  &  16  &  Hatching  &  18 & -3.88889 & -12.21074 & 0.00000 \\ 
  Halo/Outline  &  16  &  No Effect  &  18 & -1.33333 & -4.18654 & 0.00483 \\ 
  Halo/Outline  &  16  &  Stippling  &  18 & -2.74367 & -8.79286 & 0.00000 \\ 
  Halo/Outline  &  16  &  Halo/Outline  &  20 & -0.50794 & -1.59487 & 0.98967 \\ 
  Halo/Outline  &  16  &  Hatching  &  20 & -4.98445 & -15.76891 & 0.00000 \\ 
  Halo/Outline  &  16  &  No Effect  &  20 & -1.78467 & -5.62540 & 0.00000 \\ 
  Halo/Outline  &  16  &  Stippling  &  20 & -3.30292 & -10.32869 & 0.00000 \\ 
  Halo/Outline  &  16  &  Halo/Outline  &  22 & -0.57143 & -1.79423 & 0.96365 \\ 
  Halo/Outline  &  16  &  Hatching  &  22 & -5.15873 & -16.19792 & 0.00000 \\ 
  Halo/Outline  &  16  &  No Effect  &  22 & -2.31746 & -7.27660 & 0.00000 \\ 
  Halo/Outline  &  16  &  Stippling  &  22 & -4.06349 & -12.75897 & 0.00000 \\ 
  Hatching  &  16  &  No Effect  &  16 & 1.90476 & 5.98077 & 0.00000 \\ 
  Hatching  &  16  &  Stippling  &  16 & 0.95238 & 2.99038 & 0.23834 \\ 
\hline
\end{tabular}
\end{table}


\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{llllrrr}
\hline
LHS VIRT & \begin{tabular}{@{}c@{}}LHS Amount\\Of Objects\end{tabular} & RHS VIRT & \begin{tabular}{@{}c@{}}RHS Amount\\Of Objects\end{tabular} & estimate & t.ratio & p.value \\ 
  \hline 
  Hatching  &  16  &  Halo/Outline  &  18 & 2.74603 & 8.62228 & 0.00000 \\ 
  Hatching  &  16  &  Hatching  &  18 & -0.79365 & -2.49199 & 0.59037 \\ 
  Hatching  &  16  &  No Effect  &  18 & 1.76190 & 5.53221 & 0.00001 \\ 
  Hatching  &  16  &  Stippling  &  18 & 0.35157 & 1.12671 & 0.99990 \\ 
  Hatching  &  16  &  Halo/Outline  &  20 & 2.58730 & 8.12388 & 0.00000 \\ 
  Hatching  &  16  &  Hatching  &  20 & -1.88921 & -5.97675 & 0.00000 \\ 
  Hatching  &  16  &  No Effect  &  20 & 1.31057 & 4.13100 & 0.00605 \\ 
  Hatching  &  16  &  Stippling  &  20 & -0.20768 & -0.64946 & 1.00000 \\ 
  Hatching  &  16  &  Halo/Outline  &  22 & 2.52381 & 7.92452 & 0.00000 \\ 
  Hatching  &  16  &  Hatching  &  22 & -2.06349 & -6.47917 & 0.00000 \\ 
  Hatching  &  16  &  No Effect  &  22 & 0.77778 & 2.44215 & 0.62916 \\ 
  Hatching  &  16  &  Stippling  &  22 & -0.96825 & -3.04022 & 0.21205 \\ 
  No Effect  &  16  &  Stippling  &  16 & -0.95238 & -2.99038 & 0.23834 \\ 
  No Effect  &  16  &  Halo/Outline  &  18 & 0.84127 & 2.64151 & 0.47344 \\ 
  No Effect  &  16  &  Hatching  &  18 & -2.69841 & -8.47276 & 0.00000 \\ 
  No Effect  &  16  &  No Effect  &  18 & -0.14286 & -0.44856 & 1.00000 \\ 
  No Effect  &  16  &  Stippling  &  18 & -1.55319 & -4.97764 & 0.00013 \\ 
  No Effect  &  16  &  Halo/Outline  &  20 & 0.68254 & 2.14311 & 0.83363 \\ 
  No Effect  &  16  &  Hatching  &  20 & -3.79397 & -12.00269 & 0.00000 \\ 
  No Effect  &  16  &  No Effect  &  20 & -0.59419 & -1.87294 & 0.94517 \\ 
  No Effect  &  16  &  Stippling  &  20 & -2.11245 & -6.60591 & 0.00000 \\ 
  No Effect  &  16  &  Halo/Outline  &  22 & 0.61905 & 1.94375 & 0.92347 \\ 
  No Effect  &  16  &  Hatching  &  22 & -3.96825 & -12.45994 & 0.00000 \\ 
  No Effect  &  16  &  No Effect  &  22 & -1.12698 & -3.53862 & 0.05211 \\ 
  No Effect  &  16  &  Stippling  &  22 & -2.87302 & -9.02099 & 0.00000 \\ 
  Stippling  &  16  &  Halo/Outline  &  18 & 1.79365 & 5.63189 & 0.00000 \\ 
  Stippling  &  16  &  Hatching  &  18 & -1.74603 & -5.48237 & 0.00001 \\ 
  Stippling  &  16  &  No Effect  &  18 & 0.80952 & 2.54183 & 0.55123 \\ 
  Stippling  &  16  &  Stippling  &  18 & -0.60081 & -1.92547 & 0.92957 \\ 
  Stippling  &  16  &  Halo/Outline  &  20 & 1.63492 & 5.13349 & 0.00006 \\ 
  Stippling  &  16  &  Hatching  &  20 & -2.84159 & -8.98972 & 0.00000 \\ 
  Stippling  &  16  &  No Effect  &  20 & 0.35819 & 1.12903 & 0.99990 \\ 
  Stippling  &  16  &  Stippling  &  20 & -1.16006 & -3.62768 & 0.03890 \\ 
  Stippling  &  16  &  Halo/Outline  &  22 & 1.57143 & 4.93413 & 0.00016 \\ 
  Stippling  &  16  &  Hatching  &  22 & -3.01587 & -9.46955 & 0.00000 \\ 
  Stippling  &  16  &  No Effect  &  22 & -0.17460 & -0.54824 & 1.00000 \\ 
  Stippling  &  16  &  Stippling  &  22 & -1.92063 & -6.03061 & 0.00000 \\ 
  Halo/Outline  &  18  &  Hatching  &  18 & -3.53968 & -11.11426 & 0.00000 \\ 
  Halo/Outline  &  18  &  No Effect  &  18 & -0.98413 & -3.09006 & 0.18781 \\ 
  Halo/Outline  &  18  &  Stippling  &  18 & -2.39446 & -7.67373 & 0.00000 \\ 
  Halo/Outline  &  18  &  Halo/Outline  &  20 & -0.15873 & -0.49840 & 1.00000 \\ 
  Halo/Outline  &  18  &  Hatching  &  20 & -4.63524 & -14.66415 & 0.00000 \\ 
  Halo/Outline  &  18  &  No Effect  &  20 & -1.43546 & -4.52468 & 0.00113 \\ 
  Halo/Outline  &  18  &  Stippling  &  20 & -2.95372 & -9.23667 & 0.00000 \\ 
  Halo/Outline  &  18  &  Halo/Outline  &  22 & -0.22222 & -0.69776 & 1.00000 \\ 
  Halo/Outline  &  18  &  Hatching  &  22 & -4.80952 & -15.10144 & 0.00000 \\ 
  Halo/Outline  &  18  &  No Effect  &  22 & -1.96825 & -6.18013 & 0.00000 \\ 
  Halo/Outline  &  18  &  Stippling  &  22 & -3.71429 & -11.66250 & 0.00000 \\ 
  Hatching  &  18  &  No Effect  &  18 & 2.55556 & 8.02420 & 0.00000 \\ 
  Hatching  &  18  &  Stippling  &  18 & 1.14522 & 3.67018 & 0.03369 \\ 
  Hatching  &  18  &  Halo/Outline  &  20 & 3.38095 & 10.61586 & 0.00000 \\ 
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\footnotesize
\centering
\begin{tabular}{llllrrr}
\hline
LHS VIRT & \begin{tabular}{@{}c@{}}LHS Amount\\Of Objects\end{tabular} & RHS VIRT & \begin{tabular}{@{}c@{}}RHS Amount\\Of Objects\end{tabular}  & estimate & t.ratio & p.value \\ 
\hline
  Hatching  &  18  &  Hatching  &  20 & -1.09556 & -3.46594 & 0.06557 \\ 
  Hatching  &  18  &  No Effect  &  20 & 2.10422 & 6.63264 & 0.00000 \\ 
  Hatching  &  18  &  Stippling  &  20 & 0.58597 & 1.83240 & 0.95539 \\ 
  Hatching  &  18  &  Halo/Outline  &  22 & 3.31746 & 10.41651 & 0.00000 \\ 
  Hatching  &  18  &  Hatching  &  22 & -1.26984 & -3.98718 & 0.01066 \\ 
  Hatching  &  18  &  No Effect  &  22 & 1.57143 & 4.93413 & 0.00016 \\ 
  Hatching  &  18  &  Stippling  &  22 & -0.17460 & -0.54824 & 1.00000 \\ 
  No Effect  &  18  &  Stippling  &  18 & -1.41033 & -4.51982 & 0.00116 \\ 
  No Effect  &  18  &  Halo/Outline  &  20 & 0.82540 & 2.59167 & 0.51213 \\ 
  No Effect  &  18  &  Hatching  &  20 & -3.65112 & -11.55075 & 0.00000 \\ 
  No Effect  &  18  &  No Effect  &  20 & -0.45134 & -1.42264 & 0.99741 \\ 
  No Effect  &  18  &  Stippling  &  20 & -1.96959 & -6.15917 & 0.00000 \\ 
  No Effect  &  18  &  Halo/Outline  &  22 & 0.76190 & 2.39231 & 0.66721 \\ 
  No Effect  &  18  &  Hatching  &  22 & -3.82540 & -12.01138 & 0.00000 \\ 
  No Effect  &  18  &  No Effect  &  22 & -0.98413 & -3.09006 & 0.18781 \\ 
  No Effect  &  18  &  Stippling  &  22 & -2.73016 & -8.57244 & 0.00000 \\ 
  Stippling  &  18  &  Halo/Outline  &  20 & 2.23573 & 7.16503 & 0.00000 \\ 
  Stippling  &  18  &  Hatching  &  20 & -2.24078 & -7.23731 & 0.00000 \\ 
  Stippling  &  18  &  No Effect  &  20 & 0.95900 & 3.08777 & 0.18888 \\ 
  Stippling  &  18  &  Stippling  &  20 & -0.55925 & -1.78354 & 0.96574 \\ 
  Stippling  &  18  &  Halo/Outline  &  22 & 2.17224 & 6.96155 & 0.00000 \\ 
  Stippling  &  18  &  Hatching  &  22 & -2.41506 & -7.73975 & 0.00000 \\ 
  Stippling  &  18  &  No Effect  &  22 & 0.42621 & 1.36590 & 0.99848 \\ 
  Stippling  &  18  &  Stippling  &  22 & -1.31982 & -4.22975 & 0.00404 \\ 
  Halo/Outline  &  20  &  Hatching  &  20 & -4.47651 & -14.16199 & 0.00000 \\ 
  Halo/Outline  &  20  &  No Effect  &  20 & -1.27673 & -4.02435 & 0.00923 \\ 
  Halo/Outline  &  20  &  Stippling  &  20 & -2.79499 & -8.74030 & 0.00000 \\ 
  Halo/Outline  &  20  &  Halo/Outline  &  22 & -0.06349 & -0.19936 & 1.00000 \\ 
  Halo/Outline  &  20  &  Hatching  &  22 & -4.65079 & -14.60304 & 0.00000 \\ 
  Halo/Outline  &  20  &  No Effect  &  22 & -1.80952 & -5.68173 & 0.00000 \\ 
  Halo/Outline  &  20  &  Stippling  &  22 & -3.55556 & -11.16410 & 0.00000 \\ 
  Hatching  &  20  &  No Effect  &  20 & 3.19978 & 10.16259 & 0.00000 \\ 
  Hatching  &  20  &  Stippling  &  20 & 1.68153 & 5.29782 & 0.00003 \\ 
  Hatching  &  20  &  Halo/Outline  &  22 & 4.41302 & 13.96112 & 0.00000 \\ 
  Hatching  &  20  &  Hatching  &  22 & -0.17428 & -0.55135 & 1.00000 \\ 
  Hatching  &  20  &  No Effect  &  22 & 2.66699 & 8.43734 & 0.00000 \\ 
  Hatching  &  20  &  Stippling  &  22 & 0.92096 & 2.91356 & 0.28284 \\ 
  No Effect  &  20  &  Stippling  &  20 & -1.51825 & -4.76549 & 0.00037 \\ 
  No Effect  &  20  &  Halo/Outline  &  22 & 1.21324 & 3.82422 & 0.01960 \\ 
  No Effect  &  20  &  Hatching  &  22 & -3.37406 & -10.63526 & 0.00000 \\ 
  No Effect  &  20  &  No Effect  &  22 & -0.53279 & -1.67939 & 0.98165 \\ 
  No Effect  &  20  &  Stippling  &  22 & -2.27882 & -7.18300 & 0.00000 \\ 
  Stippling  &  20  &  Halo/Outline  &  22 & 2.73149 & 8.54175 & 0.00000 \\ 
  Stippling  &  20  &  Hatching  &  22 & -1.85581 & -5.80337 & 0.00000 \\ 
  Stippling  &  20  &  No Effect  &  22 & 0.98546 & 3.08167 & 0.19175 \\ 
  Stippling  &  20  &  Stippling  &  22 & -0.76057 & -2.37841 & 0.67764 \\ 
  Halo/Outline  &  22  &  Hatching  &  22 & -4.58730 & -14.40369 & 0.00000 \\ 
  Halo/Outline  &  22  &  No Effect  &  22 & -1.74603 & -5.48237 & 0.00001 \\ 
  Halo/Outline  &  22  &  Stippling  &  22 & -3.49206 & -10.96474 & 0.00000 \\ 
  Hatching  &  22  &  No Effect  &  22 & 2.84127 & 8.92131 & 0.00000 \\ 
  Hatching  &  22  &  Stippling  &  22 & 1.09524 & 3.43894 & 0.07126 \\ 
  No Effect  &  22  &  Stippling  &  22 & -1.74603 & -5.48237 & 0.00001 \\ 
   \hline
\end{tabular}
\end{table}

\chapter{Chapter 3's Participant Comments on X-Ray Vision} \label{app:Chapter3Comments}
This appendix displays users' comments on the study in \autoref{Chap:X-ray Implemntion}. Results like space and N/A have been omitted from all of the sections in this appendix and have been anonymized. 

\section{Favorite Visualizations and Why}
\begin{itemize}
  \item Favorite: \textbf{Wire Frame}, Why: Consistent lines and i could see all objects clearly. 
  \item Favorite: \textbf{Random Dot}, Why: I could use it as a grid to assist in placing the shape.  
  \item Favorite: \textbf{None}, Why: Easy to see the pointer and object
  \item Favorite: \textbf{Saliency}, Why: It was easier to position the objects accurately as opposed to the others.
  \item Favorite: \textbf{None}, Why: They are pretty much all the same to me minus none does not have a bothersome outline  
  \item Favorite: \textbf{Wire Frame}, Why: It created a better frame of reference in my opinion
  \item Favorite: \textbf{Random Dot}, Why: Didn't freak out as much as edge based and saliency and  gave me a good idea of what I was looking at
  \item Favorite: \textbf{Wire Frame}, Why: Helped me to place the object with more confident.
  \item Favorite: \textbf{None}, Why: No obstacles to obstruct vision 
  \item Favorite: \textbf{Saliency}, Why: Because it was fun to try to place the object with certain sections blacked out
  \item Favorite: \textbf{Wire Frame}, Why: It helped to have corners I could use for reference. I thought it might be better though if the mesh was more of a grid.
  \item Favorite: \textbf{Wire Frame}, Why: I felt like it gave me a better  understanding  of the location of objects
  \item Favorite: \textbf{Random Dot}, Why: Gave a good impression of the outer edge of the box, the cubes matching the shape helped distinguish edges and the gave good reference points for placement
  \item Favorite: \textbf{None}, Why: The back plane in the none condition gave me enough cue and least cluttered view.
  \item Favorite: \textbf{Edge based}, Why: Gave me a reference to where I was pointing (I felt like I was being guided more). Other than that none provided the least amount of visual clutter. 
  \item Favorite: \textbf{Random Dot}, Why: Not too sure why. It did not occlude my view. I felt that it just easier to use than the others.
  \item Favorite: \textbf{Edge based}, Why: I feel that the wireframe and random dot conditions felt the most natural due to the uniform nature of the pattern designs. The edge-based and saliency patterns were a little bit too confusing visually and I felt that I couldn't judge the depth as well with these patterns for some reason.
  \item Favorite: \textbf{None}, Why: There was no distraction lines, etc. so it was much easier to place the object.
  \item Favorite: \textbf{Edge based}, Why: Edge base, as it has some points of references, but not enough to hide the position of the objects.
  \item Favorite: \textbf{Wire Frame}, Why: This one gave the best 3d visual reference to tell where I was placing the object compared to the others.
\end{itemize}

\section{General Comments}
\begin{enumerate}
  \item Very interesting.  I think having some form of guidance helps a lot compared to none. Saliency was very frustrating though as I couldn't see the objects from the views I wanted to.
  \item Saliency was the hardest one because my sight of the shapes were often blocked. 
  \item Everything's are interesting and satisfied. But saliency bit hard to recognise object
  \item Cool experiment! It would be good if the box position was higher around the waist or chest for ease of use.
  \item I personally don't fine myself using any of the vis as I think the box and a hexagon  or ngon is enough to judge the xyz positions  
  \item Edge based and saliency made the task harder than with no visualisation
  \item Very interesting experience, looking forward to more AR research!
  \item I found even the slight delay from moving the headset disorientating (especially when holding the orb thing)
  \item Wireframe or none are close, with none the back edge is clearer and the front of the box is like the wire frame 
  \item If there were no back plane in the none condition then I would have chosen the wire frame condition as my preference.
  \item I think a grid pattern could be interesting. 
  \item The tracking was cutting out when I was standing in front of the cube so I was limited to manipulating the object from one specific angle. The precision was not great using the fishing reel manipulation and was abit jittery.
  \item I was not always sure if I could see the back faces of the cubic workspace when working in every condition, but I think that the system could benefit from a brighter or easier-to-see backface edge design. This way I could relate the placement of objects relative to not only the faces positioned toward me, but relative to the Cuba's rear sides/edges as well.
  \item Controller issues, but that probably can be resolved at a later stage.
  \item I really enjoyed participating in the study and getting to use the hololens technology.
\end{enumerate}


\chapter{Feedback from Chapter 5 Examing the Effect of Perception on VIRTs} \label{app:Chapter5Comments}
This section of the appendix displays the comments users had on the study in \autoref{Chap:X-ray Implemntion}. Results like empty space and N/A have been omitted from all of the sections in this appendix and have been anonymized. 

\section{Things That Were Liked and Disliked About VIRTs}
\subsection{Halo}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item The outline effect was much clearer than the no effect as it made counting the nodes easier. 
  \item The objects are easyly to see
  \item it have the outline to idicated the obh=ject which allows me to identiflied where is the object and how many so i won't miss the object now
  \item outline makes it very easy to count green shapes
  \item It was very easy to count the green objects because of the white outline.
  \item Objects towards the back of the frame were clearly highlighted, whereas in other conditions they were dim and sometimes impossible to see from the opposite side
  \item it outline the objects, i can just sit and see the objects easily without moving around 
  \item The halo effect added a lot of clarity to each object, which made it obvious an object existed even if it was very small. Combined with the inherant colour changes of the object made it quite clear to me.
  \item it's easy to use and because it's 3D interface, I can check in different angle
  \item I could easily idenity all shapes.
  \item The outline effect made it easier to identify and differentiate betwen the blue circles.
  \item Simple and effective. Easy to read, smooth.
  \item There was a good amount of clarity to identify the green things
  \item The shapes of the objects are very clear
  \item Very easy to count green objects (counting total condition)
  \item It made it easier to count and view objects that were overlapping with others
  \item outline makes things a lot clearer
  \item the outline made it way easier to spot the blobs
  \item it was easy to notice the shape of the object so I could quickly count the numbers.
  \item It's easy and clear to count
  \item with just outline it was much easier to count the green objects. 
  \item It is easy to locate all the green objects (in total). It is easy to seperate the green objects because there is a boundary around each of them. The appearnce of the material changes slightly with respect to the viewpoint.
  \item I needed to move my head less often
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item Some of the objects are not easy to see
  \item it is still hard to tell if the onejct was inside other object or not , because of the colour and sometimes , other object block the sign of those object behind it includet he outline.
  \item Dont know
  \item no 
  \item Sometimes on very small objects the halo effect was much stronger than the base colour of the object, making it sometimes difficult to determine the object's colour. 
  \item Sometimes it was hard to identify the depth of a shape. The outline made them look a bit flat.
  \item Good visualisation, but it requires alot of concentration to keep track of the blue circles while counting. (especially as the number of blue circles in the visualisation is high).
  \item -
  \item It is hard to recognize whether the white dots in the back are in the boundary of blue object
  \item Little bit hard to find green objects when it placed behind blue cells.
  \item I didnt have any dislikes
  \item sometimes made me too confident so I had to recount
  \item none
  \item it was sometimes hard to detect when the object was too small.
  \item It is a little hard to see how deep is the green object inside the blue objects. The green object has to be far away from the blue object, in order to decide that it is definetly not inside. But when the green object is just infront of the blue object, I felt it is hard to say if it is inside it or outside it. I would prefered that the material of the object could tell more about its depth or 3D structure.
\end{itemize}
\subsection{Hatching}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item Some of the objects are easy to see
  \item It have a web to identiflied the border of the whole red object so I won't missunderstand the background outside of the object as the green object
  \item easy to catch the edges of shapes
  \item It highlight the boundary of the objects, that help me to find out the objects, but not helping much. 
  \item The alternating cross-hatching density (direction?) occasionally made it clear where there were two separate objects. 
  \item It is visually the most unique.
  \item It was still kind of useful for identifying the green objects I think but it would be hard for me to use long-term.
  \item The shape of blue object is very easy to recognize
  \item This condition is easy to count total green objects.
  \item I didn't have any likes for this visulization
  \item Sometimes when overlapping crosshatching helped
  \item it helps to notice depth information at some degree.
  \item The opacity of the color of the object seems to change as a function of distance, which helped in identifying the objects. The strides (or lines) on the object assisted in separating the objects (I could tell the nearby objects were disconncted rather than perceiving them as a one big object)
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item This visualization was least preferred as the nodes were not easy to count at all. It was straineous to count them. 
  \item Some objects are difficults
  \item the web strongly interupt my adjustment on green obejct's position adn existance.
  \item still difficult to make out distant shapes, slightly worse performance compared to no effect
  \item the billboard view of the objects, which make me difficult to see the side of the red objects and count the object number. Also, the cross-line could be slightly transparent, and the inside object cross-line could be more obvious. 
  \item The Hatching was very heavy and often felt like it was occluding the already sometimes faint colours of distant/deep objects. The effect helped to show the blue objects but did not help represent the green ones. Additionally, the effect felt relative to my view rather than the world, meaning looking around the visualisation was not very natural and a bit confusing. 
  \item it's hard to count the number because the cross hatching make me confuse
  \item I really struggled to tell anything about and when I moved my head, because the visual changed, I couldn't be certain that I hadn't already counted a shape. I had no real idea what any shapes actually looked like. If the grid didn't move with my head, maybe it would have been easier. This was the only visualization that gave me eye strain.
  \item The grid patterns was starting to give me a headache. I had to try to keep my head still to avoid physical discomfort.
  \item To my eyes it seemed glitchy (but maybe that's how it's supposed to be). Slightly laggy unfrotunately but overall really really hard to use.
  \item The objects were much harder to identify due to their shapes and their lack of clarity
  \item It is really lagging and makes me feel sick, the green object in the depth faded in the blue object. 
  \item Difficult to distinguish when green objects are in blue cells or placing close to other objects
  \item The visualisation made the objects seem very custered and made me confused during counting objects that were within close proximity to each other
  \item Crosshatching the big red blob itself made it way too cluttered. I think if only the green and blue were I would have had an easier time and maybe even rated it highly. But with the red one crosshatched there is just too much going on
  \item Hatching made it difficult to see into objects
  \item the lines made it distracting to see the blobs, i had difficulties in determining for both within blue blob and whole volume
  \item I had to rotate my view to find out whether the green objects were overlapped or not.
  \item it's not a little clear to see
  \item Some of the overlapping blue objects were hard to figure out.
  \item The opacity of the objects in the furthest point (too far away) was fading that I couldn't decisively determine if it was an object or not. I wish I could rotate the scene to look behind the big blue objects.
  \item It seemed to me that I had too much information
\end{itemize}
\subsection{No VIRT}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item It was clear and easy to differentiate the colors of the artifacts
  \item it is simple enough for everyone to use it,
  \item It's simple and clean. Transparencies are a good way to perceive depth
  \item It was easy to figure out which green objects were inisde the blue shapes and which weren't
  \item Straightforward, interesting colours
  \item 3d view of the objects
  \item Without additional effects the visualisation was very clear to view. I think this made aspects of the visualisation more comfortable to parse.
  \item It was easy to see green shapes through green shapes (as in they looked off color and it help identify some hiding behind others).
  \item Everything felt clearer on the screen, so think I performed better at keeping track when counting green objects. Because of this I felt like the task was easier, despite depth probably being harder to interpret. Additionally, I probably took my time abit longer and used more head movement to carefully interpret the data with this one compared to the grid visualisation (because the grid was causing physical discomfort when I moved too much).
  \item Simple, smooth
  \item It was easier to identify each object individually, there was nothing reducing clarity
  \item The green objects are obviously in the blue object
  \item Low complexity and relatively easy to learn.
  \item Didnt have any particular likes for the visualizations
  \item No clutter, generally easy to see.
  \item There were less distractions which made it easier for me to spot the green blobs for both within the blue volumes and whole volume
  \item somehow seems natural for me.
  \item In some cases it was easier to identify number of green objects due to color shades.
  \item It is relatively easy to identidy the little green objects (in total) as the space is rather empty of everything but colors. The opacity or color seems to change relative to the depth and compostion of the object.
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item Most of the objects are difficult to see and count
  \item it is hard to identified which one is green, and sometimes blue objects were blocking the green one, causing me need more time and attention to identify, sometimes I even missed it 
  \item Don't know
  \item I couldn't see the green objects far in the back
  \item Colours can bleed together, it can be difficult to determine blue pockets towards the back of the volume
  \item Sometimes, when the objects are far behind the objects, they can't really see them clearly. 
  \item Very small or far back green objects became difficult to see without significant head movement as their coloring blended a lot with the surrounding volume. Also, looking through green objects produced an "xray" effect, which was inconsistent with the external visualisation. 
  \item This visualization do not have an outline. Sometimes it's hard to count the number, and the 3D object is not too clear.
  \item I really struggled with being confident that I was identifying all the shapes near the back. Also some shapes would kind of blend together and I'd have to move my head a lot to see them clearer.
  \item I felt that the depth was probably harder to interpret compared to some of the other visualizations (specifically the visualization where objects were outlined \& also the staple one).
  \item A bit hard to count with it. Sometimes, green thingies hide behind other green thingies, and this visualization does not help in realizing that.
  \item THe green objects in the depth are hardly recognized
  \item It is Difficult to percept depth, so I couldn't recognize whether green objects are in the blue or not.
  \item It felt like i had to strain my eyes too much in order to try and correclty count objects as it was hard to determine where one object started and another ended if they were overlapping
  \item Sometimes confused if a green blob is half in a blue one or fully in one
  \item after a while, it did made me a bit dizzy looking at it
  \item It was difficult to figure out whether the green object is inside or behind the blue object.
  \item some of the opposite sided objects are hard to identify if they are single or double (one over other)
  \item It is harder to identify the relation between the blue and the green objects. In other words, it is hard to say if the green object is inside or outside the blue object.
  \item I might have missed some objects due to occlusion and thickness of red and blue 'fluids'
\end{itemize}
\subsection{Stippling}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item Almost all the objects are easy to see and count
  \item because it has the dot to identify the size of the object so I tell it's position once I spotted it 
  \item LIttle dots help see green elelemts
  \item easier to spot the green objects because of the white spots on them
  \item More legible than the others so far
  \item The dots on the objects helped me to differenciate different objects 
  \item The Stippling seemed to serve to add texture to the objects, which helped with perceiving object depth, and also identifying the shape of objects, which was important due to some green blobs overlapping in various ways. 
  \item it's clear
  \item It was easy on the eyes. Seemed sort of magical.
  \item I feel like the task would probably be harder without the visualisation because it overall made the green objects easier to identify.
  \item I felt more confident answering. It feels like the green thingies are more visible.
  \item With the dots on the objects, it is easy to recognize the shapes of them. 
  \item It has low mental load and low complexity.
  \item It was easy to notice objects that were behind others
  \item Sometimes the white dots help difference between overlapping ones
  \item the green blobs were easy to count
  \item glitering surfaces were helpful for depth perception.
  \item It is easy to locate the objects in total. The material is helping alot I felt that the reflectance changes based on the viewpoint and the dotts on the surface of the object are helping as well. It is easy to differentiate between the green objects (even when many green objects are too close, I can tell they are not one object but three that are too close). It is easy to tell whether the green object is inside or outside the blue object.
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item It was not clear as compared to the outline effect. At times, I doubted if I had seen the node properly. 
  \item it is still confusing about the green object is within or not of the blue object because sometime the dot mixed up together and looks like half is within and half is outside
  \item When having several shapes it gets confusing.
  \item Slight lag, but forgiveable
  \item sometimes. the objects are way behind other objects, then i need to move around to see it. 
  \item The effect was very noisy and was uncomfortable at first due to the amount of detail in the scene. 
  \item I found it hard to identify which dots where for each shape.
  \item The main issue was that sometimes when green objects were close together it was difficult for me to identify whether it was a single or multiple objects \& as a result had to make a guess sometimes since it was unclear to me.
  \item Slightly laggy
  \item Like the cross-hatching, it lowered the clarity and made it harder to identify between objects
  \item The white dots sometimes confuse me about the boundaries. 
  \item Difficult to distinguish when objects are grouped close.
  \item Felt clustered
  \item Still wasn't too clear overall sometimes, though. can't describe much but from training I'm guessing outline is going to be my fav
  \item I wasn't really sure if the number I put in is right because I'm counting all the blobs that touch the blue space
  \item sometimes it distracts me for counting the number.
  \item due to too much noise (dots), it was creating confusion.
  \item My depth perception of point clouds was not as good as with NOTHING qand OUTLINE conditions
\end{itemize}

\chapter{Feedback from Chapter 6 Examining the limits of Depth Perception} \label{app:Chapter6Comments}
This Appendix focuses of the written feedback that was collected during the study form \autoref{chap:DepthPerception}. Results like empty space and N/A have been omitted from all of the sections in this appendix and have been anonymized. 

\section{Things That Were Liked and Disliked About VIRTs}
This section contains a list of all of the user comments related to things participants liked and disliked about the \glspl{virt} can be found here.

\subsection{Halo}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item some objects are obvious to see the differences 
  \item it is simple, the outlines do help to see depth
  \item Different colors make identifying objects' boundaries an easy task. 
  \item This visualization method has low complexity, easy to learn.
  \item The visulisation was easy to comprehend and I think I could easily figure out which blue orb was closer
  \item it's easy to see the areas and how close they are. 
  \item It was easy to understand straight away
  \item I have no opinion
  \item The shape of the object is very clear.
  \item Halo helps readability and distinguishing between "particles"
  \item the outline helped with the depth a bit
  \item I felt confident that I could guage depth using the fadedness of the lines.
  \item Simpe, easy to read, easy to compare colours
  \item The scene was very clear and comfortable to view. The outlines defined the objects even when small and deep within the green objects.
  \item the outline made it easier to see the different layers and to determine how close the blob was to me
  \item it gives me a shape information which is helpful for depth perception.
  \item It was good for counting
  \item This the most preferred visualization because its clear and easier to compare both objects to decide depth perception
  \item I think that the white lines are good to recognize.
  \item The curve of the edge can support visibility
  \item it was well prepared 
  \item there's no much visual distraction that could happen in this visualization
  \item it was easy to tell objects apart
  \item outlines made it easier to see the red and green objects but so much for the blue objects.
  \item Not too complicated
  \item contouring helps with colorblindness
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item it is really hard to tell which one is closer in ar, especially it's limit ur movement  and graphics were rendered in x ray 
  \item Some object are not obvius ti see the distane
  \item Some of the pairs felt the same
  \item Hard to distinguish depth level.
  \item sometimes, when the two objects are in the similar distance to me, then i need to move around to be able to see it. 
  \item I found it difficult to interpret depth at times.
  \item It is hard to tell the depth of the objects and also hard to compare the depth of them. 
  \item I am not convinced the halo helps distinguishing depth levels but I guess my results will speak for themselves...
  \item Sometimes the headsets resolution made the lines look to pixel-y.
  \item Difficult to get a lot of contrast between shapes
  \item I don't think the outlines helped with the depth aspect very much.
  \item some of the outlines looked identical that it made it hard to determine which one was closer so i could only guess
  \item sometimes the outline reminds me a face, so it seems distracting.
  \item It was a lot harder in this experiment to determine which was closer. It made the usual method of how much green  is in front of the blue more difficult to determine I felt
  \item Probably less compatability when using Holo2 Lens with another glassess
  \item some of them were hard to distinguish whether right one is closer or the left one.
  \item it's similar to the basic "No Effect" visualization therefore had to rely on the color difference and other hint to tell the difference
  \item it was hard to gauge the distance of the objects
  \item Blue objects sometimes was hard to see it as 3D object
  \item Hard to tell the distance
  \item the depth perception was not enhanced as much than with the dots or wireframe visu
\end{itemize}
\subsection{Hatching}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item That easy to see the objects' different
  \item Its the best one for describing shapes and it does help for depth perception a lot
  \item Defining the blue object made it easier to localize it
  \item This method is suitable to recognize 3D shape of tissue.
  \item It was very easy to figure out which blue object was closest because of the grid.
  \item it has the linein the objects which makes little bit easier to see the distance of the blue object 
  \item it's more clear than outline one
  \item Having the grid patterns made it easier to interpret depth than the 4th condition (standard visualisation).
  \item The cross hatching gives a good reference of comparing depth
  \item I think the parallel lines helped me "read" the depth
  \item Its easy to get an idea of how high  an object is.
  \item I could tell the shape of each blue thing really quickly and it made me much more confident in my answer.
  \item High contrast with crosshatching, nade discerning shapes easy
  \item The cross hatching gave a sense of the topology of the object which I think helped when comparing the blue objects against each other. 
  \item at first i thought it was easy to use as i would choose the grids that "pop out"more as the ones that are closer 
  \item it gives me some texture information of the object.
  \item The hatching effect allows me to differentiate depth perception of the objects clearer
  \item It looks intutive and quick to use
  \item The hashing made it easier to tell which was closer as it provided more 3d space definition. 
  \item this was a bit easier to distinguese between compare to past 2 tests.
  \item I could predict and estimate the depth based on the curve "angle" of the wireframe
  \item it was easy to tell objects apart
  \item Fun and vivid.
  \item I could aprehend the shapes better
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item i don't like the x ray render texture on it 
  \item It can be a bit confusing. It would be awesome to be able to turn it on and off.
  \item Sometimes difficult to see inside tissue(blue) because of occlusion of visualization effects.
  \item no. 
  \item Due the the hatching on the outer objects, it added additional challenge to identifying the inner objects.
  \item It's slightly uncomfortable on the eyes, but didn't have any impact on my performance. However, I think if I had to use it for a longer duration (i.e. an hour) then it would start to probably impact my performance.
  \item It is lagging sometimes. 
  \item The angles of the lines move with the viewer's position, which is distracting
  \item It is visually overloading. Just a lot going on.
  \item The crosshatching made comparing depth more difficult. I ended up relying on comparing colour
  \item While I got used to the view-dependent effect, I think I would like it more if the grid effect stayed static on the object like a wireframe. There was also some artefecting when looking at the objects too close. 
  \item afterwards as i progressed through this visualisation, the grids that used to assist me made it more difficult to determine how close it is to me, i had to focus really hard and stare at it a lot to determine the smallest details to guess which one is closer.
  \item I felt like it oclude my eyesight sometimes.
  \item It is somehow frozen automatically
  \item The red blob would sometimes have hashing in front of the green/blue and cluttered it a bit. Not often though.
  \item visualizations were difficult to see or observe when there's too many wires on the wireframe
  \item didnt have any dislikes
  \item Compared to stippling the lines more heavier which made it bit more difficult to see,
  \item Nothing that I can think of. My favorite so far.
  \item it was a bit too much
\end{itemize}
\subsection{No VIRT}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item n/a nothing good, just easy to use , nothing else then that
  \item The colour is obvius
  \item it is the cleanest but hard to tell depth
  \item This method does not cause mental effort and eye fatigue.
  \item simple and clean 
  \item The shape of the objects is very clear. 
  \item Simple, no distracting elements
  \item It was easier to tell the difference when the difference was smaller
  \item I could usually tell straight away which one was closer.
  \item Clean visuals made colour comparisons easy
  \item This was the clearest way to view the scene, and I found it emphasized the colour and shading of each object making it easier to use that as a cue for depth. I am unsure if that was more accurate or not however. 
  \item it was quite easy to determine which bob was closer to me, at first I just kept staring at both of them to see which one was closer, but afterwards i noticed that it would be easier if i just stared between the two blobs, it makes it way obvious
  \item it is intuitive and quite easier to understand.
  \item the vagueness of the edge can be easily observed
  \item Less clutter so I could focus on the green blob and how much I thought was closer to be compared to the blue blob
  \item it was nice to look for the objects in 3rd dimentions. 
  \item its simple
  \item didnt have any likes
  \item compared to the other ones, this one didnt have any lines or dots which made it a bit easier to look at.
  \item It's simpler
  \item it was simple
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item i feel the xray texture is very disrupting 
  \item Some of the objests are really difficult to see the distance
  \item It is very difficult to asses depth withoute any guides
  \item I found it harder to perceive the blue object. I think the scene looked more planar (like clouds, there is depth that is hard to see) and my brain could not differentiate between the objects.
  \item Some of images impossible to percept depth.
  \item In most cases both the visualisations loked the same thus, it was difficult to figure out which one was the closest.
  \item somethings the transparent of the color confuses me, it make me feels like with high opacity is closer but sometimes it is not. 
  \item It was harder to visualise depth when  compared to other visualisations
  \item it's not clear
  \item I think that having the stippling/outline effects (cond 1/2) helped more with understanding depth.
  \item It hard to compare the depth of two objects. 
  \item Harder to guess depth without depth cues
  \item When I couldn't tell straight away, no ammount of moving my head made me feel any more confident in my guess.
  \item Difficult to discern relative depths between the two fields
  \item Without additional effects, I sometimes found it hard to determine the shape of objects when further behind the green containers as they were very faint. 
  \item at first when i kept staring atthe blobs it was hard to differenciate and the quality i was seeing was a bit low but afterwards once i got used to it, it was fine.
  \item I wasn't sure to find correct answer for some difficult questions. Not much visual information to decide the answer.
  \item This visualization was hard to use because the depth could not be differentiated. 
  \item It is difficult to judge the distance when the volume of both blue object is almost same with similar shade.
  \item When the blue blobs were close together is was almost impossible to tell without having some form of hashing or outline etc
  \item there isn't much clue or information that I could use to help with the depth perception
  \item found it hard to see object in other objets and to tell distances of objects
  \item At the same time, not having any white indicators made it hard to recongnize the object as 3D object. Sometimes it would be seen as a hole in the green object.
  \item Hard to tell the difference
  \item it was not obvious to determine which was closer
\end{itemize}
\subsection{Stippling}
\subsubsection{I liked this visualization because.}
\begin{itemize}
  \item the dot did helped me to didentiflied whch one is closer to me, and the system is easy to use 
  \item The colour is obvius to see
  \item It felt easier than the outline/halo condition. The dots help see parallax when moving to the sides and this makes easier to determine depth.
  \item It is much easier to determine how far is the blue object inside the green one.
  \item This method seems easy to learn and not too much affect to original image.
  \item The dots made it easy to understand the depth of the blue orbs
  \item it gives me more feeling of how blue object close. 
  \item Changes in the pattern could be discerned by distance
  \item the white dot is very helpful
  \item No idea why but I felt that depth was easier to interpret than the first condition (halo/outline).
  \item The dots on the surface give me a good reference to compare the depth
  \item Simple to understand and I feel like it gave me a good idea of the depth of the objects
  \item It was easy to visualize vertex points in 3d space
  \item I could almost always tell straight away which was the right one. I had a good idea of the shape of each blue thing.
  \item The white dots offered a point of contrast for comparison
  \item The "static" nature of the dots added extra detail that I felt made it easier to judge depth and shape. 
  \item the dots made it a bit easier to determine how close the blobs were to me
  \item it gives me clearer depth information.
  \item sometimes the bubbles helped determine one was closer
  \item Similar to hatching, the Stippling was easy to visualize depth of the objects
  \item The white dots help to understand the depth.
  \item The stips may be visually easy to count
  \item in this test it was way easier to identify the distance between both the blue objects. I was able to identify by the color density of the white dots and size of the white dots.
  \item the dots were useful for telling the depth perception
  \item didnt have any particular likes for the visualization 
  \item It looked very cool
  \item I think visualising things helps me to better understand things
  \item the paralax was easier to see
\end{itemize}
\subsubsection{I disliked this visualization because.}
\begin{itemize}
  \item the x ray texture is really disrupting 
  \item Some of the objects are difficult to see the differents
  \item The dots could get in the way of other info., but very effective.
  \item Little bit difficult to percept depth.
  \item sometimes, when the distances between objects and my eyes are similar, then it will be little bit diffcult to make decision 
  \item Depth is still hard for me to interpret.
  \item I like it
  \item The dots were a bit inconsistent
  \item Sometimes the dots looked a little cut off.
  \item The varying appearance of the dots (as in where they appeared on the green) obfuscated comparison to an extent
  \item While the dots were static, there was some artefacting during head movement that was slightly confusing. 
  \item i had to keep moving my head sideways to determine which one was closer and due to that it made me a bit dizzy in the process
  \item it was sometimes hard to discriminate shapes.
  \item But at the same time, sometimes they didn't feel as impactful. I mainly relied on how much green was layered over the blue
  \item It is less supportive than the glid or line.
  \item sometimes a cluster of dots could bring negative effect than helping
  \item it was sometimes hard to view overlapping objects 
  \item It was a bit hard to see things because it was bit blurry sometimes. It took a while for my eyes to adjust but it was fine afterwards. Looking at it for a long time might hurt my eyes a bit.
  \item Eyes gets exhausted in a while.
  \item the dots density didn't seem consistent
\end{itemize}

\section{Validation for Answers Given in the Post-Study Questionaire}
\subsection{What Made The Visualization Easy to use?}
\begin{itemize}
  \item The clarity of the nodes and visual was clear
  \item I can see all the objrcts obviusly
  \item it shows me the border/size i can tell where is it located at the volumn without interupt by colour
  \item The outline helped count green shapes and wasnt too confusing
  \item The white outline in the Halo condition made it really easy to spot the green objects
  \item Clear outlines, seperating the shapes from the volumes
  \item the outlines of the objects helped me to differenciate different objects even they are far away from me, i still can use the outline to see it. 
  \item In terms of use, the halo effect was the most logical and quick to understand. It did not make the scene more complicated and helped add definition. I would put no effect ahead of stippling as it is also simple and not overwhelming, even though the stippling was advantagous (see below). 
  \item the outline is clear
  \item I could pick out shapes easier with the outline and stippling. I could easily see shapes at the back aswell.
  \item The readability of the visualisations.
  \item No technical knowledge required. Task can be accomplished without learning any skill, it's just a matter of persistence really.

No effect and halo are the easiest, they feel comfortable and are aligned with our vision of the world every day. Stippling is not what we are used to be is still quite easy to use after using it for a while.
  \item The halo and no effect were easier as they did not hinder the visual differences between the object, hence improving clarity and allowing me to identify each individual.
  \item The clear boundary of the objects makes me easy to recognize the shapes of them
  \item When visualization method can assist to ditinguish which one is closer.
  \item The outlines allowed me to easily tell overlapping objects apart from each other
  \item Outline made it obvious how many green blobs there were from various angles 
  \item simplicity, lightlights blobs behind others (halo)
  \item outline/halo easily slows me which are the blobs and all i had to do was count
  \item The outlines were helpful to discriminate the object so i could easity count the number.
  \item It's easy to figure out the shape of green object.
  \item It was easier to count green objects on the no effect because there was no noice at all and I was able to identify objcts based on color density.
  \item the material of the objects and the ability to deduce the 3D shape.
  \item Reduced the amount of head movement to attain the same level of certainty before vlidating the result
\end{itemize}
\subsection{What Made The Visualization Difficult to use?}
\begin{itemize}
  \item The complexity of the image displayed did not make counting easy
  \item Almost all the objects are difficult to see and count
  \item the cros hatch become an interuption while i trying to determine the postion of that object, sometimes i try to find out is there any object behind other object, the cross hatch block my sign and cause confusion
  \item The cross Hatching was extremely confusing with a lot of shapes
  \item The grid in the cross hatching condition were covering the green objects making them dificult to spot
  \item No outlines, making the shapes difficult to discerne against long views of the volume
  \item too much overlapping with each other, and the far, the color is too transparent, which makes the objects are even harder to see it. 
  \item The cross-hatching visualisation made the task much harder as the effect was very dense and moved with my view, which wasn't "realistic" and made understanding the visualisation much harder. I would say any of the dense effects added "more" to parse for the user. 
  \item too much elements and hard to count
  \item Cross hatching made it hard to see shapes.
  \item Stippling \& Cross Hatching made it more difficult for me to identify whether close objects together was a single object or actually separate multiple objects. Cross-hatching was by far the worst because it was causing me physical discomfort when I moved my head too much.
  \item Mainly occlusion. When the green thingies are deep inside the volume, or when they are behind each others, they can be very hard to see.
The stippling helps a lot with that but caused (for me) a slight discomfort at the beginning. It goes away after a while.
Cross-hatching feels very weird and uncomfortable. I could not get used to it. On top of that, it was hard to see the added value. Counting was the hardest with it, because of what I mentioned before but also because sometimes it would "merge" green thigies depending on the angle of view.
  \item The stippling and cross hatching were present on all objects and made it harder to identify between each object, specifically with the cross hatching blocking the identification of objects beind it.
  \item The objects in the depth will faded in the backgruond color. 
  \item Some visualization method have too much visual effect.
  \item Lack of guidance and very clustered
  \item Crosshatching helped with overlapping but it was really noisy. I think if the red blob itself wasn't crosshatched it would be a better. It did help with overlapping blobs sometimes though
  \item Clutter, some vis techniques just added too much clutter
  \item there were many distractions in cross hatching, there were so many lines that it made it hard to see if there was a lightly coloured blob or even any blobs at all, so i had to count, and keep asking myself, "does that count as a blob?" and just guess
  \item The visualiztion seems to much so it made harder to perceive depth information.
  \item The boundary is blurred.
  \item cross hatching and stippling were a bit hard to work with because there too many lines or dots on the ojects which were creating confusions sometimes.
  \item I think Cross Hatching made the scene complicated so it was hard to count because there was alot of things (lines, specularity). The No effects lacks depth information so it is harder to locate the realtive depth of objects and also it is hard to seperate nearby objets.
  \item The depth perception needed "parala' help with me moving my head
\end{itemize}
\subsection{Why Do You Think You Performed Better With These Visualization?}
\begin{itemize}
  \item The outline effect was best because it was clear to count the nodes in both cases
  \item Because I can count the objects
  \item becuse i can see the border of the object clerly so i can dtermine it's postion
  \item I felt the most confident with Outlines and the No effect.
  \item In the Halo condition it was really easy to locate the green objects that's I think I could accurately count the number of the green objects
  \item Easy identification of small shapes. Clear identification of shapes within the volume
  \item i can clearly see and differenciating different objects. 
  \item Both the halo and stippling effects helped me perform better becuase they added additional information to the objects which assisted where objects where occuluded or deep. The halo effect felt better to use as the white colouring seemed stronger with depth, whereas the stippling still required me to move around a lot more to detirmine overlapping objects apart.
  \item the visulization is clear
  \item I could keep keep track of shapes with the outline and stippling.
  \item I think no effect was best because it was overall easier on me to mentally keep track of the green objects when counting. I felt like I made alot more mistakes (For example - forgetting whether I had already counted an object and having to start over) in comparison to the other visualisations. I believe Outline was the best for depth-perception.
  \item I think halo was good but not as effective in terms of helping counting the green thingies than the stippling one. Halo just provides a contour, so you it highlights the separate green thingies really well, but stippling's random dots sometime reveals occluded green thigies I might not have seen otherwise.
As much as I hated the cross hatching, I still think it helped me count compared to the no effect condition.
  \item Halo and no effect as I believe it was easier to identify and hence count each object
  \item I can finish the counting faster. It is easier for me recognize the shapes and locations of the objects. 
  \item The Outlines \textbackslash Halo visualization is most comfort interms of mental effort and esay to count total number of green objects.
  \item Felt it was easier to count
  \item Outline made me more confident. No effect there was less noise so I could focus.
  \item Same as above
  \item I was more confident when the blobs were specifically outlined, it made counting much easier
  \item The outlines highlights the object's surface so I could answer faster with confidence.
  \item The boundaries were clear and easy to count
  \item with outlines I was easily able to count the number of green objects there was not much going on I was able to recognise if the object is inside the blue object or outside.
  \item I think I had no difficulty with the two questions under the stiplling.
  \item The easier the task seems to me, the better is my percieved performance
\end{itemize}
\subsection{Why Do You Think You Performed Worse With These visualization?}
\begin{itemize}
  \item The clarity was poor
  \item I cannot see the objects
  \item becasue the cross hatch cause interuption hwile i trying to locate the object , especially hwne the object is behind other object 
  \item I got mostly confused with these stippling and cross hatching
  \item The grid in the cross hatching condition made it really difficult to locate the green objects thus, I feel i couldn't count accurately
  \item Difficult to discerne shapes, worse performance made it harder to pay attention too
  \item cannot easily differenciating the objects which might cause double counting or miss counting 
  \item The cross hatching effect was view dependent which did not feel natural and I don't think helped determine object depth or positions. This caused me to want to look closer, but this would cause the effect to become heavier in my mind. 
  \item it's hard to count 
  \item I had more trouble remembering which shapes I had already counted with "no effect" and cross hatching.
  \item I think I performed reasonably poor with stippling/cross-hatching due to readability and mental discomfort.
  \item I believe that no effect makes it really difficult distinguish two green thingies that are close together, but also two green thingies occluding each others. Cross hatching causes lots of headache; feels like there was too much information that was barely useful in the end. The "merging" effect I mention in the previous question also makes it difficult to distinguish between two green thigies that are too close together.
  \item Stippling and cross hatching as I found it harder to identify these and sometimes felt like I was guessing 
  \item It is hard to tell the location and the shapes of the objects, I need more time to double confirm. 
  \item Too difficult to count grouped objects and hard to percept depth.
  \item Felt like i struggled with them due to clustered infromation
  \item Crosshatch was too noisy and made me less focused. Stippling to a lesser extent but it wasn't that bad. I just feel I could focus more on the no effect and outline.
  \item Same as above
  \item i couldn't determine if what i saw was classified as a green blob making it very hard to count
  \item It was quite demanding for me to count the number in the cross hatching condition.
  \item It was hard tounderstand if the green object was one or several attached to it.
  \item cross hatching has too many lines which was confusing me on green objects whihc are at the back of the whole volume. 
  \item I needed more cues to deduce the depth. Parallax was not always enough.
  \item The easier the task seems to me, the better is my percieved performance
\end{itemize}

\section{General Comments}
\begin{itemize}
  \item The display kept dropping in and out hence it made counting harder as I would have to recount again. 
  \item Some of the effects are easy to count
  \item Thank you
  \item Very cool tech, good work
  \item the outline one is the best makes everyting easier. Would prefer to increase the transparency of the cross hatching line for the red objects and reduce the transparency of the green or blue cross hatching, would be better. Also would be better to remove the billboard functions for the cross hatching, I would prefer the objects are staying not follow my eyes to rotate, so i can see the objects from different views. 
  \item I think FPS performance is a factor as with the heavier effects (i.e. crosshatching) the object would jump and jitter a little more, which when combined with the view dependent effect was a bit confusing. 
  \item I never really had a good strategy to do this task, for most of  the task I was just trying to mentally keep track of what objects I had already counted. I would start from a random object and then go to the next closest object etc and keep track that way. Early on in the study I tried to go in order from top to bottom or furthest to closest but this strategy wasn't working working well for me
  \item Not that I can think of right now.
  \item The cross harching one is really lagging and make me feel sick. 
  \item Mainly just crosshatching looks like it could be great as it helps detect overlaps. But it's very noisy with crosshatching the red blob (I assume it is anyway). That or there is a lot of other blobs inside the red blob and it just gets too cluttered with crosshatching for me.
  \item counting outloud and pointing made it easier for me to count, cause when i move my head, the headset moves along which made me dizzy, pointing keeps me focused and eases the dizziness
  \item Other than "No effect", the visulization was not stable when I move my head quickly. It may affect the ease of use (it may also get people dizzy) but I personally tried to ignore it.
  \item I was able to se clearly thgough the green volumes with the No effect condition, which helped me spotting the background objects
\end{itemize}

\section{Validation for Answers Given in the Post-Study Questionaire}
During the post hoc comparison, participants were asked a series of questions stating why they selected some answers over others. In terms of what was easy vs what what they believed they did better with.

\subsection{What Made The Visualization Easy to use?}
\begin{itemize}
  \item Because the distance and colour are obvius
  \item For some reason the dots immediately made sense and helped a lot with depth perception.
  \item Stippling method does not much affect to original image and easy to recognize which one is closer.
  \item the distances between the lines are easier for me to judge, i used that as the refences to make the decision. 
  \item the white dot
  \item It has clear references to compare the depth of them. 
  \item Stippling is quite discreet but yet does it a good job (IMO) at indicating the depth. Halo also seemed to help for some reason (might be placebo) but the reason why I preffered it over the others is because it's just simple to read and appealing.
  \item Had points where its easy to tell the depth
  \item Stippling and cross hatching let me tell the shape of the blue thing very quickly, but stippling was more consistent when I moved my head.
  \item colour contrast as a depth indicator
  \item I think the stippling and outline effects were the easiest to use as they were quick to understand and didn't experience artefacting when moving my head. 
  \item Stippling and CH allowed for a better understand of front and back
  \item the visualizations were consistent which made it easier for me to use
  \item The glittering surface made me easier to perceive depth information.
  \item The outline/halo effect was clearer to differentiate the depth of different objects
  \item The dots and lines are helpful.
  \item The Halo/Outline can quickly help detect the distance by its quantities
  \item Hatching allowed for an indication when the blue blobs were close together so I found it easier
  \item with stippling it was very easy to identify the closest blue object due to color density of white dots and size of the dots in both the objects. I was also able to identify the shape of both of the blue objects due to different orientation of white dots. 
  \item The dots in stippling helped me to better understand the closest object easier than the other three visualizations
  \item the objects visualization made it more understanding when judging distances
  \item No effect was easy to see but hard to understand the 3D object.
  \item Seems more vivid and 3D
  \item I was primarily focusing on the blurryness of the blue object, the stipling and cros hatching helped to emphasize on the object paralax
\end{itemize}
\subsection{What Made These visualization Difficult to use?}
\begin{itemize}
  \item The distance is difficult to identify
  \item Because there are no reference points. trying to figure out depth of semitransparent objects is always hard.
  \item If visual effect is too much used, it might cause mental load and difficult to percept depth since occlusion of outer tissue's visualization effect.
  \item In many cases, i have to move around to be able to see which object is closer. 
  \item there is no outline and hard to check the depth of each object
  \item It does not have clear references to compare the depth 
  \item Cross Hatching, if it's supposed to work like I think it's supposed to, is in theory a great idea. However it is very hard to distinguish between small differences in depth, on top of being very distracting because it changes based on the viewer's location.
No effect was definitely the hardest because appart from the decrease in saturation depending on the depth and the perspective, there is nothing to indicate depth.
  \item Covering the objects made it difficult to compare depth. I think if there is too much going on then it makes it harder not easier.
  \item If I couldn't tell early on which shape was closer, no effect and halo (in part) did not help much when I moved my head.
  \item obfuscation of depth from additional components, ie, crosshatching
  \item The cross hatching was the most confusing simply because of the way it changes based on head position, wheras a static wireframe would have been easier.
  \item None cause an optical allusion that made the red and green parts of the vis move, Halo provided not depth understanding
  \item because i have no understanding as to how to differenciate the closeness of blob with these visualizations, so i had to get used to it in the beginning 
  \item I don't have much visual information so I had to back and forth to decide.
  \item No effect was the only least preferred visualization as it was harder to identify the object's distances
  \item It's difficult to understand the depth because the boundaries are blurred.
  \item There is little highlighted marks so that it might require users to come up with other strategy to identify the distance like the volume/brightness/blurriness etc.
  \item Outline did not improve how close they were and it felt like the green blob was harder to determine how close it was to me. No effect itself I could focus a lot on it though but it was difficult to determine which was closest when they were closer together.
  \item No Effect was a bit difficult to use because I felt like both the images are dull and blur. so with that it was a bit hard to identify which one is close even after head movement.
  \item there isn't too much visualization or effect to support my task
  \item was hard to tell distances and see overlapping objects
  \item For Cross Hatching and Stippling, it was a bit difficult to see but made it easier to recognize the bkue objects as 3D objects.
  \item Too flat
  \item I had little to no depth information
\end{itemize}
\subsection{Why Do You Think You Performed Better With These visualization?}
\begin{itemize}
  \item Becsue I can see the distance and all the colour of the object
  \item Cross hatching does help a lot when you take a bit of time to move your head to the sides. i guess this was my best score.
  \item Easy to understand purpose of visualization.
  \item not sure, based on my guess
  \item the white dot can help to check the object's depth
  \item It has clear references, the shape of the objects are clear. 
  \item Same reason as before
  \item The halo did help a bit but it was still a bit harder than no effect.
  \item Stippling was less annoying than cross hatching so I felt more confident with my answer quicker.
  \item practice, high colour contrast for comparison
  \item The stippling effect felt like it enhanced my ability to understand the 3D depth of the obejcts and wasn't too noisy. I'm not sure if I actually did well with no effect, but it was easier to focus on just the colour of the object with no other white visualisation effect. 
  \item See above
  \item No effect had the least "distractions" although the other visualizations were to assist me in making a decision, i also didnt have to move much for no effect while i had to keep moving my head position for the others which made it dizzy for me
  \item I think I relatively easily decide the answer in the stippling condition.
  \item Personally, I felt more confident in judging the depth of the objects
  \item I think that it's easy to understand.
  \item The Outlines/Halo condition ease the most consideration workload when making selections
  \item Hatching I could tell the close together ones easier. No effect I could focus on the green blob size so I think I got a majority of the easier ones correct without second guessing. But was hard to tell the close together ones
  \item It was much eaier to identify the shape and size of the blue objects with stippling effect. I was also able to identify the distance from me with the help of color density. I did not have to move much I was able to identify by sitting still. some of them required me movement because some of them looked kinda similar (90% similar).
  \item There are many attributes from this visualization that helped me with the prediction "color, size, the dots"
  \item was easier to tell objects apart
  \item  I used light reflections to recognize the objects but even then a bit of white visualization was necessary to see it as 3D object. But too much visualization made it hard to see.
  \item I am more confident because it is easier to tell the distance.
  \item same as previous question
\end{itemize}
\subsection{Why Do You Think You Performed Worse With These visualization?}
\begin{itemize}
  \item I coudnt see the distance of the object
  \item For the 'no effect' condition I was mostly guessing. So I imagine this was by worst score.
  \item I just had to rely on brightness and contrast of tissue, it might not indicate its depth.
  \item the objects are not easy to see it clearly for the distance
  \item it's not clear
  \item Very hard to compare the depth of the objects. Doesn't have reference in the environment
  \item Same reason as before
  \item Too much going on (as well as covering the objects)
  \item I think I felt like I was guessing too much with no effect.
  \item obfuscation of depth and difficulty comparing colour
  \item The cross hatching was a close second/third but I think due to the movement I never felt fullly confident with it. However it did help the most with understanding the shape of the objects. Finally I think the outline was the worst because even though it was very clear to understand, the outline made it harder to focus on the colour and felt like it exaggerated depth rather than make it feel clearer. 
  \item See above
  \item i found it rather distracting and i had to think more to determine, and staring at the strippling and crosshatching made me a bit dizzy and had to keep reminding myself "which is closer"
  \item It was most difficult when I had to choose the answer in the no effect condition.
  \item The no effect was essentially harder to use as there was no way to identify the depth of the objects
  \item There were times when I was a little unsure.
  \item the cross Hatching hightlighted the edge but hinder the judgement of distance
  \item Outline just felt difficult to determine in 3d space, made it more 2d style so I feel I got a lot incorrect.
  \item I found no effect the worst because I was not able to focus on the blue object. it was blurry most of the time. I had to move to much to identify the real closest blue objects but I may have failed on some of them.
  \item There are too much distractions in the cross hatching "or wireframe"
  \item was hard to tell ojects apart and gauge distances
  \item Having no virtualization made it harder to see.
  \item Had a few guess.
  \item same as previous question
\end{itemize}


\section{General Comments}
The following comments were gathered from users as a method for them to provide general feedback about the study or to say whatever they wanted to say. 
\begin{itemize}
  \item If I can face to a white wall of the background will be better
  \item Thank you!
  \item One idea is allocate artificial spot light and shadow. 
  \item I'm not sure if you added it or not, but i used the transparency of the color to help to judge the locations. 
  \item The cross hatching one is bit of lagging. 
  \item Interesting study!
  \item I think if cross hatch was more consistent when I moved my head, I might have liked it more.
  \item fun study
  \item I think my answers here are accurate for a sitting experience wher one cannot move around the visualisation to observe it. If I was able to use more signficant movement to help judge depth, then the outline effect would have been preferable I think. 
  \item The allusion caused by the none condition was interesting, it look although the green and red areas were slowly rotating.
  \item none
  \item What if the objects in each study were the same size? Would that change the participants choices in idenifying the depth of each object?
  \item Probably can add more guidance before the day so that paticipants may have more clear ideas on this research.
  \item the eye can get fatigue especially in the final condition due to the long usage "~1hour" of focus therefore I'm concerned if my performance could be negatively affected in the last condition. In addition, the reflection from the HoloLens 2 made me easy to see the researcher therefore causing distruptions at some point. 
  \item Overall, my eyes felt bit strained starting for a long time but otherwise, it was fine.
\end{itemize}


\chapter{Statements of Authorship}
This section contains the Statement of Authorship clarifying the contributions of the authors of the published works found in this thesis.

\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/AdaptingVSTARX-RayVisionTechniquesToOSTAR.pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/AdaptingVSTARX-RayVisionTechniquesToOSTAR-1.pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/GeneratingPseudoRandomVolumesForVolumetricResearch.pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/GeneratingPseudoRandomVolumesForVolumetricResearch-1.pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/SuperpowersInTheMetaverseAugmentedRealityEnabledXRayVisionInImmersiveEnvironments[1] (2).pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/SuperpowersInTheMetaverseAugmentedRealityEnabledXRayVisionInImmersiveEnvironments[1] (2)[1].pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/VolumetricXrayVisionUsingIllustrativeVisualEffects.pdf}
\includepdf[pages=-,pagecommand{},width=\textwidth]{Appendix/StatementsOfAuthorship/VolumetricXrayVisionUsingIllustrativeVisualEffects-1.pdf}
%\includepdf[pages=-]{myfile.pdf} \newpage 

\end{document}

