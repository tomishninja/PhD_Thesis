@misc{Zhou2011,
abstract = {In automotive manufacturing, the quality of spot welding on car bodies needs to be inspected frequently. Operators often only check different subsets of spots on different car bodies with a predetermined sequence. Currently, spot welding inspections rely on a printed drawing of the testing body, with the inspection points marked on this drawing. Operators have to locate the matching spot on the drawing and the body manually to perform the inspection. The manual inspection process suffers from inefficiencies and potential mistakes. This paper describes a system that projects visual data onto arbitrary surfaces for providing just-in-time information to a user in-situ within a physical work-cell. Spatial Augmented Reality (SAR) is the key technology utilized in our system. SAR facilitates presentation of projected digital Augmented Reality (AR) information on surfaces of car bodies. Four types of digital AR information are projected onto the surfaces of car body parts in structured work environments: 1) Location of spot welds},
address = {US},
annote = {This paper isn't quite in the realm I'm going to be researching however it is close to the feild I'm looking at it just moved in a different direction. I don't think SAR tech would work in a medical space due to the lighting however it is a good implemention of what they are trying to achive.},
author = {Zhou, Jianlong and Lee, Ivan and Thomas, Bruce and Menassa, Roland and Farrant, Anthony and Sansome, Andrew and {ACM New York 10th International Conference on Virtual Reality Continuum and Its Applications in Industry Hong Kong}, China 11-12 December 2011},
booktitle = {Proceedings of the 10th International Conference on Virtual Reality Continuum and Its Applications in Industry},
doi = {10.1145/2087756.2087784},
file = {:C$\backslash$:/Users/Thomas/Downloads/p195-zhou.pdf:pdf},
pages = {195--200},
publisher = {ACM},
title = {{Applying spatial augmented reality to facilitate in-situ support for automotive spot welding inspection}},
url = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA{\&}search{\_}scope=All{\_}Resources{\&}docid=UNISA{\_}ALMA11144077620001831 https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA{\&}search{\_}scope=All{\_}Resources{\&}docid=UNISA{\_}ALMA51119624780},
year = {2011}
}

@book{Heinrich2019,
abstract = {Augmented reality (AR) is a promising tool to improve instrument navigation in needle-based interventions. Limited research has been conducted regarding suitable navigation visualizations. In this work, three navigation concepts based on existing approaches were compared in a user study using a projective AR setup. Each concept was implemented with three different scales for accuracy-to-color mapping and two methods of navigation indicator scaling. Participants were asked to perform simulated needle insertion tasks with each of the resulting 18 prototypes. Insertion angle and insertion depth accuracies were measured and analyzed, as well as task completion time and participants' subjectively perceived task difficulty. Results show a clear ranking of visualization concepts across variables. Less consistent results were obtained for the color and indicator scaling factors. Results suggest that logarithmic indicator scaling achieved better accuracy, but participants perceived it to be more difficult than linear scaling. With specific results for angle and depth accuracy, our study contributes to the future composition of improved navigation support and systems for precise needle insertion or similar applications. Index},
author = {Heinrich, Florian and Joeres, Fabian and Lawonn, Kai and Hansen, Christian},
file = {:C$\backslash$:/Users/z004344b/Downloads/Heinrich2019{\_}Preprint.pdf:pdf},
keywords = {Visualization,augmented reality,evaluation,instrument guidance,medical navigation systems,needle placement},
mendeley-groups = {Needle Intervention},
mendeley-tags = {Visualization,augmented reality,evaluation,medical navigation systems,instrument guidance,needle placement},
month = {jan},
title = {{Comparison of Projective Augmented Reality Concepts to Support Medical Needle Insertion}},
year = {2019},
journal = {IEEE Transactions on Visualization and Computer Graphics},
publisher = {IEEE}
}


@article{Ammenwerth2003,
abstract = {The use of modern information technology (IT) offers tremendous opportunities such as reducing clinical errors and supporting health care professionals in providing care. Evaluation of user satisfaction is often seen as a surrogate for the success of an information systems. We will present the evaluation of a report writing system at the Innsbruck University Medical Center based on a standardized, validated psychometric questionnaire. The results show high reliability and validity of the questionnaire. They also show some interesting differences in user satisfaction between departments, due to differences in working processes and preconditions. Psychometric questionnaires can be seen as a reliable and valid method to measure certain psychological constructs. Their development requires, however, methodological rigour and sufficient time. Psychometric questionnaires allow only a limited interaction between researcher and user, their results may be very dependant on the time of measurement, and their interpretation often needs external knowledge. Those limitations have to be taken into account when preparing evaluation studies.},
author = {Ammenwerth, Elske and Kaiser, Frieda and Wilhelmy, Immanuel and H{\"{o}}fer, Stefan},
doi = {10.3233/978-1-60750-939-4-643},
file = {:C$\backslash$:/Users/z004344b/Downloads/95f814d8e1b71e0653a0b6573fa511f2b216.pdf:pdf},
isbn = {1586033476},
issn = {18798365},
journal = {Studies in Health Technology and Informatics},
keywords = {Medical informatics,evaluation studies,psychometrics,questionnaires,user satisfaction},
mendeley-groups = {3DMenus/DesignThinking/Questionaires,PapersRegardingAcceptanceOfTechInIT},
number = {May},
pages = {643--648},
pmid = {14664060},
title = {{Evaluation of user acceptance of information systems in health care - The value of questionnaires}},
volume = {95},
year = {2003}
}

@article{Kuzhagaliyev2018,
author = {Kuzhagaliyev, Timur and Clancy, Neil and Janatka, Mirek and Tchaka, Kevin and Vasconcelos, Francisco and Clarkson, Matt and Gurusamy, Kurinchi and Hawkes, David and Davidson, Brian and Stoyanov, Danail},
file = {:C$\backslash$:/Users/z004344b/Downloads/1802.03274.pdf:pdf},
mendeley-groups = {Needle Intervention},
month = {feb},
title = {{Augmented Reality needle ablation guidance tool for Irreversible Electroporation in the pancreas}},
year = {2018},
journal = {SPIE Medical Imaging}
}


@article{Zhang2011,
abstract = {With the increasing availability of high-resolution isotropic three- or four-dimensional medical datasets from sources such as magnetic resonance imaging, computed tomography, and ultrasound, volumetric image visualization techniques have increased in importance. Over the past two decades, a number of new algorithms and improvements have been developed for practical clinical image display. More recently, further efficiencies have been attained by designing and implementing volume-rendering algorithms on graphics processing units (GPUs). In this paper, we review volumetric image visualization pipelines, algorithms, and medical applications. We also illustrate our algorithm implementation and evaluation results, and address the advantages and drawbacks of each algorithm in terms of image quality and efficiency. Within the outlined literature review, we have integrated our research results relating to new visualization, classification, enhancement, and multi-modal data dynamic rendering. Finally, we illustrate issues related to modern GPU working pipelines, and their applications in volume visualization domain. Copyright {\textcopyright} 2010 by Society for Imaging Informatics in Medicine.},
author = {Zhang, Qi and Eagleson, Roy and Peters, Terry M.},
doi = {10.1007/s10278-010-9321-6},
file = {:C$\backslash$:/Users/admin/Downloads/10278{\_}2010{\_}Article{\_}9321.pdf:pdf},
issn = {08971889},
journal = {Journal of Digital Imaging},
keywords = {Classification Graphics processing unit (GPU),Fourier transformation,Image processing,Medical imaging,Raycasting,Shading,Shear-warp,Shell rendering,Splatting,Texture mapping,Transfer function,Visualization,Volume rendering},
mendeley-groups = {VoxelAlgorithms},
number = {4},
pages = {640--664},
title = {{Volume visualization: A technical overview with a focus on medical applications}},
volume = {24},
year = {2011}
}


@article{Ropinski2010,
abstract = {In this paper we present a volumetric lighting model, which simulates scattering as well as shadowing in order to generate high quality volume renderings. By approximating light transport in inhomogeneous participating media, we are able to come up with an efficient GPU implementation, in order to achieve the desired effects at interactive frame rates. Moreover, in many cases the frame rates are even higher as those achieved with conventional gradient-based shading. To evaluate the impact of the proposed illumination model on the spatial comprehension of volumetric objects, we have conducted a user study, in which the participants had to perform depth perception tasks. The results of this study show, that depth perception is significantly improved when comparing our illumination model to conventional gradient-based volume shading. Additionally, since our volumetric illumination model is not based on gradient calculation, it is also less sensitive to noise and therefore also applicable to imaging modalities incorporating a higher degree of noise, as for instance magnet resonance tomography or 3D ultrasound. {\textcopyright}2010 IEEE.},
annote = {This is a one of the first papers writen on cinimatic rendering. Its not a bad reference for various approches},
author = {Ropinski, Timo and D{\"{o}}ring, Christian and Rezk-Salama, Christof},
doi = {10.1109/PACIFICVIS.2010.5429594},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ropinski, D{\"{o}}ring, Rezk-Salama - 2010 - Interactive volumetric lighting simulating scattering and shadowing.pdf:pdf},
isbn = {9781424466849},
journal = {IEEE Pacific Visualization Symposium 2010, PacificVis 2010 - Proceedings},
keywords = {I.3.7 [computer graphics]: Three-dimensional graph},
mendeley-groups = {VoxelAlgorithms},
pages = {169--176},
title = {{Interactive volumetric lighting simulating scattering and shadowing}},
year = {2010}
}


@article{Milgram1994,
abstract = {Summary This paper focuses on Mixed Reality (MR) visual displays, a particular subset of Virtual Reality (VR) related technologies that involve the merging of real and virtual worlds somewhere along the "virtuality continuum" which connects completely real environments to completely virtual ones. Probably the best known of these is Augmented Reality (AR), which refers to all cases in which the display of an otherwise real environment is augmented by means of virtual (computer graphic) objects. The converse case on the virtuality continuum is therefore Augmented Virtuality (AV). Six classes of hybrid MR display environments are identified. However, an attempt to distinguish these classes on the basis of whether they are primarily video or computer graphics based, whether the real world is viewed directly or via some electronic display medium, whether the viewer is intended to feel part of the world or on the outside looking in, and whether or not the scale of the display is intended to map orthoscopically onto the real world leads to quite different groupings among the six identified classes, thereby demonstrating the need for an efficient taxonomy, or classification framework, according to which essential differences can be identified. The 'obvious' distinction between the terms "real" and "virtual" is shown to have a number of different aspects, depending on whether one is dealing with real or virtual objects, real or virtual images, and direct or non-direct viewing of these. An (approximately) three dimensional taxonomy is proposed, comprising the following dimensions: Extent of World Knowledge ("how much do we know about the world being displayed?"), Reproduction Fidelity ("how 'realistically' are we able to display it?"), and Extent of Presence Metaphor ("what is the extent of the illusion that the observer is present within that world?").},
author = {Milgram, Paul and Kishimo, Fumio},
file = {:C$\backslash$:/Users/admin/Downloads/A{\_}Taxonomy{\_}of{\_}Mixed{\_}Reality{\_}Visual{\_}Displays.pdf:pdf},
isbn = {0916-8532},
issn = {0916-8532},
journal = {IEICE Transactions on Information and Systems},
keywords = {ar,augmented reality,mixed reality,mr,virtual reality,vr},
mendeley-groups = {Proposals},
number = {12},
pages = {1321--1329},
title = {{A taxonomy of mixed reality}},
volume = {77},
year = {1994}
}


@article{Pratt2018,
abstract = {Precision and planning are key to reconstructive surgery. Augmented reality (AR) can bring the information within preoperative computed tomography angiography (CTA) imaging to life, allowing the surgeon to ‘see through' the patient's skin and appreciate the underlying anatomy without making a single incision. This work has demonstrated that AR can assist the accurate identification, dissection and execution of vascular pedunculated flaps during reconstructive surgery. Separate volumes of osseous, vascular, skin, soft tissue structures and relevant vascular perforators were delineated from preoperative CTA scans to generate three-dimensional images using two complementary segmentation software packages. These were converted to polygonal models and rendered by means of a custom application within the HoloLens™ stereo head-mounted display. Intraoperatively, the models were registered manually to their respective subjects by the operating surgeon using a combination of tracked hand gestures and voice commands; AR was used to aid navigation and accurate dissection. Identification of the subsurface location of vascular perforators through AR overlay was compared to the positions obtained by audible Doppler ultrasound. Through a preliminary HoloLens-assisted case series, the operating surgeon was able to demonstrate precise and efficient localisation of perforating vessels.},
author = {Pratt, Philip and Ives, Matthew and Lawton, Graham and Simmons, Jonathan and Radev, Nasko and Spyropoulou, Liana and Amiras, Dimitri},
doi = {10.1186/s41747-017-0033-2},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pratt et al. - 2018 - Through the HoloLens™ looking glass augmented reality for extremity reconstruction surgery using 3D vascular model.pdf:pdf},
journal = {European Radiology Experimental},
keywords = {3d,Augmented reality,HoloLens,Three-dimensional (3D),augmented reality,computed,hololens,reconstruction,three-dimensional,vascular pedicle flap},
mendeley-groups = {MedicalDataOverlay},
number = {1},
pages = {0--6},
publisher = {European Radiology Experimental},
title = {{Through the HoloLens™ looking glass: augmented reality for extremity reconstruction surgery using 3D vascular models with perforating vessels}},
volume = {2},
year = {2018}
}


@article{Mast2019,
author = {Mast, Marcus and Kaup, Ina and Kr{\"{u}}ger, Sebastian and Ullrich, Christian and Schneider, Robert and Bay, Susanne},
file = {:C$\backslash$:/Users/admin/Downloads/628.pdf:pdf},
keywords = {augmented reality,ct,holographic mixed reality,hololens,imaging,medical,mri,preoperative planning,reconstruction,scanners,th ey may need,to identify the position},
pages = {590--594},
title = {{Exploring the Benefits of Holographic Mixed Reality for Preoperative Planning with 3D Medical Images}},
year = {2019},
journal = {Mensch und Computer 2019 - Workshopband},
}


@article{Eckert2019,
abstract = {BACKGROUND: Augmented reality (AR) is a technology that integrates digital information into the user's real-world environment. It offers a new approach for treatments and education in medicine. AR aids in surgery planning and patient treatment and helps explain complex medical situations to patients and their relatives. OBJECTIVE: This systematic and bibliographic review offers an overview of the development of apps in AR with a medical use case from March 2012 to June 2017. This work can aid as a guide to the literature and categorizes the publications in the field of AR research. METHODS: From March 2012 to June 2017, a total of 1309 publications from PubMed and Scopus databases were manually analyzed and categorized based on a predefined taxonomy. Of the total, 340 duplicates were removed and 631 publications were excluded due to incorrect classification or unavailable technical data. The remaining 338 publications were original research studies on AR. An assessment of the maturity of the projects was conducted on these publications by using the technology readiness level. To provide a comprehensive process of inclusion and exclusion, the authors adopted the Preferred Reporting Items for Systematic Reviews and Meta-Analyses statement. RESULTS: The results showed an increasing trend in the number of publications on AR in medicine. There were no relevant clinical trials on the effect of AR in medicine. Domains that used display technologies seemed to be researched more than other medical fields. The technology readiness level showed that AR technology is following a rough bell curve from levels 4 to 7. Current AR technology is more often applied to treatment scenarios than training scenarios. CONCLUSIONS: This work discusses the applicability and future development of augmented- and mixed-reality technologies such as wearable computers and AR devices. It offers an overview of current technology and a base for researchers interested in developing AR apps in medicine. The field of AR is well researched, and there is a positive trend in its application, but its use is still in the early stages in the field of medicine and it is not widely adopted in clinical practice. Clinical studies proving the effectiveness of applied AR technologies are still lacking.},
author = {Eckert, Martin and Volmerg, Julia S and Friedrich, Christoph M},
doi = {10.2196/10967},
file = {:C$\backslash$:/Users/admin/Downloads/6658230.epub:epub},
issn = {2291-5222},
journal = {JMIR mHealth and uHealth},
keywords = {*Augmented Reality,*medicine,*mixed/augmented reality,*mobile computing,*mobile phone,*systematic review,Bibliometrics,Biomedical/methods/trends,Humans,Technology Assessment},
language = {eng},
month = {apr},
number = {4},
pages = {e10967--e10967},
publisher = {JMIR Publications},
title = {{Augmented Reality in Medicine: Systematic and Bibliographic Review}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/31025950 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6658230/},
volume = {7},
year = {2019}
}

@misc{DeLange2010,
author = {{De Lange}, P and Takata, Y and Kim, H and Liao, H and Kobayashi, E and Ono, M and Kyo, S and Takamoto, S and Ishii, S and Asano, T and Sakuma, I},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15699-1_55},
isbn = {03029743},
pages = {521--530},
title = {{Real-time epicardial excitation time map overlay}},
volume = {6326},
year = {2010}
}


@article{Song2018,
author = {Song, Chanho and Jeon, Sangseo and Lee, Seongpung and Ha, Ho-Gun and Kim, Jonghyun and Hong, Jaesung},
doi = {10.1186/s12938-018-0500-x},
file = {:C$\backslash$:/Users/admin/Downloads/Augmented{\_}reality-based{\_}electrode{\_}guidance{\_}system{\_}.pdf:pdf},
journal = {BioMedical Engineering OnLine},
mendeley-groups = {MedicalDataOverlay},
month = {dec},
title = {{Augmented reality-based electrode guidance system for reliable electroencephalography}},
volume = {17},
year = {2018}
}

@article{Chagas-Neto2016,
abstract = {Abstract Objective: To compare the diagnostic performance of the three-dimensional turbo spin-echo (3D TSE) magnetic resonance imaging (MRI) technique with the performance of the standard two-dimensional turbo spin-echo (2D TSE) protocol at 1.5 T, in the detection of meniscal and ligament tears. Materials and Methods: Thirty-eight patients were imaged twice, first with a standard multiplanar 2D TSE MR technique, and then with a 3D TSE technique, both in the same 1.5 T MRI scanner. The patients underwent knee arthroscopy within the first three days after the MRI. Using arthroscopy as the reference standard, we determined the diagnostic performance and agreement. Results: For detecting anterior cruciate ligament tears, the 3D TSE and routine 2D TSE techniques showed similar values for sensitivity (93{\%} and 93{\%}, respectively) and specificity (80{\%} and 85{\%}, respectively). For detecting medial meniscal tears, the two techniques also had similar sensitivity (85{\%} and 83{\%}, respectively) and specificity (68{\%} and 71{\%}, respectively). In addition, for detecting lateral meniscal tears, the two techniques had similar sensitivity (58{\%} and 54{\%}, respectively) and specificity (82{\%} and 92{\%}, respectively). There was a substantial to almost perfect intraobserver and interobserver agreement when comparing the readings for both techniques. Conclusion: The 3D TSE technique has a diagnostic performance similar to that of the routine 2D TSE protocol for detecting meniscal and anterior cruciate ligament tears at 1.5 T, with the advantage of faster acquisition.Resumo Objetivo: Comparar o desempenho diagn{\'{o}}stico da t{\'{e}}cnica tridimensional turbo spin-eco (3D TSE) de resson{\^{a}}ncia magn{\'{e}}tica (RM) do joelho na detec{\c{c}}{\~{a}}o de rupturas meniscais e ligamentares em compara{\c{c}}{\~{a}}o com o protocolo bidimensional turbo spin-eco (2D TSE). Materiais e M{\'{e}}todos: A sequ{\^{e}}ncia 3D TSE foi adicionada ao protocolo de rotina 2D TSE em 38 pacientes que foram submetidos a cirurgia artrosc{\'{o}}pica do joelho em at{\'{e}} tr{\^{e}}s dias ap{\'{o}}s a realiza{\c{c}}{\~{a}}o da RM. Usando os achados artrosc{\'{o}}picos como refer{\^{e}}ncia padr{\~{a}}o ouro, foram calculados o desempenho diagn{\'{o}}stico e a concord{\^{a}}ncia entre os protocolos. Resultados: A t{\'{e}}cnica 3D TSE e o protocolo 2D TSE apresentaram, respectivamente, sensibilidade (93{\%}/93{\%}) e especificidade (80{\%}/ 85{\%}) semelhantes na detec{\c{c}}{\~{a}}o de rupturas do ligamento cruzado anterior, sensibilidade (85{\%}/83{\%}) e especificidade (68{\%}/71{\%}) semelhantes na detec{\c{c}}{\~{a}}o de rupturas do menisco medial, assim como sensibilidade (58{\%}/54{\%}) e especificidade (82{\%}/92{\%}) semelhantes na detec{\c{c}}{\~{a}}o de rupturas do menisco lateral. A concord{\^{a}}ncia intraobservador entre os dois m{\'{e}}todos foi de substancial a quase perfeita em todos os par{\^{a}}metros avaliados para ambos os leitores. Conclus{\~{a}}o: A t{\'{e}}cnica 3D- SE apresentou desempenho diagn{\'{o}}stico semelhante ao protocolo de rotina 2D TSE na detec{\c{c}}{\~{a}}o de rupturas meniscais e ligamentares em magneto de 1,5 T, com a vantagem de possibilitar uma redu{\c{c}}{\~{a}}o significativa no tempo de aquisi{\c{c}}{\~{a}}o.},
author = {Chagas-Neto, Francisco Abaet{\'{e}} and Nogueira-Barbosa, Marcello Henrique and Lorenzato, M{\'{a}}rio M{\"{u}}ller and Salim, Rodrigo and Kfuri-Junior, Maur{\'{i}}cio and Crema, Michel Daoud},
doi = {10.1590/0100-3984.2015.0042},
file = {:C$\backslash$:/Users/admin/Downloads/rb-49-02-0069.pdf:pdf},
issn = {0100-3984},
journal = {Radiologia Brasileira},
keywords = {2d tse,3d tse,a sequ{\^{e}}ncia 3d tse,anterior cruciate ligament,arthroscopy,comparar o desempenho diagn{\'{o}}stico,da t{\'{e}}cnica tridimensional turbo,de resson{\^{a}}ncia magn{\'{e}}tica,de rotina 2d tse,do,em 38 pacientes que,em compara{\c{c}}{\~{a}}o com o,foi adicionada ao protocolo,foram submetidos a,joelho na detec{\c{c}}{\~{a}}o de,knee,magnetic resonance imaging,materiais e m{\'{e}}todos,menisci,protocolo bidimensional turbo spin-eco,resumo objetivo,rm,rupturas meniscais e ligamentares,spin-eco},
number = {2},
pages = {69--74},
title = {{Diagnostic performance of 3D TSE MRI versus 2D TSE MRI of the knee at 1.5 T, with prompt arthroscopic correlation, in the detection of meniscal and cruciate ligament tears}},
volume = {49},
year = {2016}
}


@article{Nalcaci2010,
abstract = {Objectives: The objective of this study was to assess the reliability of three-dimensional (3D) cephalometric approaches by comparing this method with authenticated traditional twodimensional (2D) cephalometry in angular cephalometric measurements. Methods: CT images and lateral cephalometric radiographs of ten patients (five women, five men) were used in this study. Raw CT data of the patients were converted to 3D images with a 3D simulation program (Mimics 9.0, Leuven, Belgium). Lateral cephalometric radiographs were used manually for 2D measurements. The comparisons of the two methods were made using 14 cephalometric angular measurements. The Wilcoxon matched-pairs signed-ranks test ($\alpha$50.05) was used to determine the difference between the two methods. To assess the intra- and interobserver reproducibility, two sets of recordings made by each observer, in each modality were used. Dahlberg's formula was used to determine the intraobserver reproducibility, and the Wilcoxon matched-pairs signed-rank test ($\alpha$ 5 0.05) was used to assess the interobserver reproducibility. Results: The method errors of both observers ranged from 0.35° to 0.65°. In addition, there were no significant differences between the measurements of the two observers (P {\textgreater} 0.05). However, comparison of 2D and 3D parameters showed significant differences in U1-NA and U1-SN measurements (P {\textless} 0.05). Conclusions: The 3D angular cephalometric analysis is a fairly reliable method, like the traditional 2D cephalometric analysis. Currently, the 3D system is likely to be more suitable for the diagnosis of cases with complex orthodontic anomalies. However, with the decrease in radiation exposure and costs in the future, 3D cephalometrics can be a suitable alternative method to 2D cephalometry. {\textcopyright} 2010 The British Institute of Radiology.},
annote = {States some information on regarding if 3D data is usful or not.},
author = {Nal{\c{c}}aci, Ruhi and {\"{O}}zt{\"{u}}rk, F. and S{\"{o}}k{\"{u}}c{\"{u}}, O.},
doi = {10.1259/dmfr/82724776},
file = {:C$\backslash$:/Users/admin/Downloads/dmf-39-100.pdf:pdf},
issn = {0250832X},
journal = {Dentomaxillofacial Radiology},
keywords = {Cephalometric measurements,Computed tomography,Three-dimensional cephalometry},
number = {2},
pages = {100--106},
title = {{A comparison of two-dimensional radiography and threedimensional computed tomography in angular cephalometric measurements}},
volume = {39},
year = {2010}
}

@article{Vernon2002,
abstract = {Three-dimensional models created using materials such as wax, bronze and ivory, have been used in the teaching of medicine for many centuries. Today, computer technology allows medical illustrators to create virtual three-dimensional medical models. This paper considers the benefits of using still and animated output from computer-generated models in the teaching of medicine, and examines how three-dimensional models are made. {\textcopyright} 2002 Informa UK Ltd All rights reserved.},
author = {Vernon, Tim and Peckham, Daniel},
doi = {10.1080/0140511021000051117},
file = {:C$\backslash$:/Users/admin/Downloads/The benefits of 3D modelling and animation in medical teaching.pdf:pdf},
issn = {17453054},
journal = {Journal of Visual Communication in Medicine},
number = {4},
pages = {142--148},
title = {{The benefits of 3D modelling and animation in medical teaching}},
volume = {25},
year = {2002}
}

@article{Ballantyne2011,
annote = {doi: 10.3109/17453054.2011.605057},
author = {Ballantyne, Lauren},
doi = {10.3109/17453054.2011.605057},
file = {:C$\backslash$:/Users/admin/Downloads/11{\_}02{\_}2020{\_}Comparing .pdf:pdf},
issn = {1745-3054},
journal = {Journal of Visual Communication in Medicine},
month = {sep},
number = {3},
pages = {138--141},
publisher = {Taylor {\&} Francis},
title = {{Comparing 2D and 3D Imaging}},
url = {https://doi.org/10.3109/17453054.2011.605057},
volume = {34},
year = {2011}
}

@article{Marner2014,
annote = {Paper details the different types of User interfaces possibl Z re with Project ed AR Intknows. good pap is to- to know pro ling not reliwent},
author = {Marner, M R and Smith, R T and Walsh, J A and Thomas, B H},
doi = {10.1109/MCG.2014.117},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marner et al. - 2014 - Spatial User Interfaces for Large-Scale Projector-Based Augmented Reality.pdf:pdf},
issn = {0272-1716 VO - 34},
journal = {IEEE Computer Graphics and Applications},
keywords = {Augmented reality,Computer Graphics,Computer interfaces,Context modeling,Three-dimensional displays,User interfaces,User-Computer Interface,Virtual environment,augmented reality,computer graphics,graphics,large-scale projector-based augmented reality,natural passive haptic feedback,projection surface,spatial augmented reality,spatial user interfaces,user collaboration,user interfaces,virtual environment,virtual environments,virtual reality},
mendeley-groups = {papers I have Read},
number = {6},
pages = {74--82},
title = {{Spatial User Interfaces for Large-Scale Projector-Based Augmented Reality}},
volume = {34},
year = {2014}
}


@article{Roberts1986,
author = {Roberts, David W and Strohbehn, John W and Hatch, John F and Murray, William and Kettenberger, Hans},
doi = {10.3171/jns.1986.65.4.0545},
journal = {Journal of Neurosurgery},
language = {English},
number = {4},
pages = {545--549},
publisher = {Journal of Neurosurgery Publishing Group},
title = {{A frameless stereotaxic integration of computerized tomographic imaging and the operating microscope}},
url = {https://thejns.org/view/journals/j-neurosurg/65/4/article-p545.xml},
volume = {65},
year = {1986}
}


@article{Mori2017,
abstract = {In this paper, we review diminished reality (DR) studies that visually remove, hide, and see through real objects from the real world. We systematically analyze and classify publications and present a technology map as a reference for future research. We also discuss future directions, including multimodal diminished reality. We believe that this paper will be useful mainly for students who are interested in DR, beginning DR researchers, and teachers who introduce DR in their classes.},
author = {Mori, Shohei and Ikeda, Sei and Saito, Hideo},
doi = {10.1186/s41074-017-0028-1},
file = {:C$\backslash$:/Users/admin/Downloads/s41074-017-0028-1.pdf:pdf},
issn = {1882-6695},
journal = {IPSJ Transactions on Computer Vision and Applications},
keywords = {augmented reality,camera,diminished reality,image inpainting,image-based rendering,mediated reality,mixed reality,object detection,object recognition,pose estimation,survey},
number = {1},
pages = {1--14},
publisher = {IPSJ Transactions on Computer Vision and Applications},
title = {{A survey of diminished reality: Techniques for visually concealing, eliminating, and seeing through real objects}},
volume = {9},
year = {2017}
}


@inproceedings{Avery2009,
abstract = {Augmented reality x-ray vision allows users to see through walls and view real occluded objects and locations. We present an augmented reality x-ray vision system that employs multiple view modes to support new visualizations that provide depth cues and spatial awareness to users. The edge overlay visualization provides depth cues to make hidden objects appear to be behind walls, rather than floating in front of them. Utilizing this edge overlay, the tunnel cut-out visualization provides details about occluding layers between the user and remote location. Inherent limitations of these visualizations are addressed by our addition of view modes allowing the user to obtain additional detail by zooming in, or an overview of the environment via an overhead exocentric view.},
author = {Avery, B and Sandor, C and Thomas, B H},
booktitle = {2009 IEEE Virtual Reality Conference},
doi = {10.1109/VR.2009.4811002},
file = {:C$\backslash$:/Users/admin/Downloads/04811002.pdf:pdf},
isbn = {2375-5334 VO -},
keywords = {Augmented reality,Computer graphics,Depth Perception,I.3.7 [Computer Graphics]: Three-Dimensional Graph,Image reconstruction,Image-Based Rendering,J.9.e [Mobile Applications]: Wearable computers an,Layout,Machine vision,Outdoor Augmented Reality,Rendering (computer graphics),Virtual reality,Visualization,Wearable Computers,Wearable computers,X-ray imaging},
pages = {79--82},
title = {{Improving Spatial Perception for Augmented Reality X-Ray Vision}},
year = {2009}
}

@misc{Bean2017,
abstract = {A method for managing a content overlay. The method included a processor identifying a first image and a second image from an augmented reality (AR) device. The method further includes identifying a first element of interest within the first image. The method further includes associating a corresponding first AR content overlay for the first element of interest. The method further includes determining one or more differences between the first image and the second image, wherein the second image includes at least the first element of interest. The method further includes modifying a position of at least the first AR content overlay based, at least in part, on the one or more differences between the first image and the second image.},
author = {Bean, Chris R. and Ford, Chandler's and {Chelmsford Green}, Sophie D. and Head, Stephen R. E. and Ford, Chandler's and Smith, Madeleine and Winchester, R. Neil},
file = {:C$\backslash$:/Users/admin/Downloads/US9589372.pdf:pdf},
number = {12},
publisher = {US},
title = {{AUGMENTED REALITY OVERLAYS BASED ON AN OPTICALLY ZOOMED INPUT}},
volume = {1},
year = {2017}
}

@article{Bajura1992,
abstract = {We describe initial results which show 'live' ultrasound echography data visualized within a pregnant human subject. The visualization is achieved by using a small video camera mounted in front of a conventional head-mounted display worn by an observer. The camera's video images are composited with computer-generated ones that contain one or more 2D ultrasound images properly transformed to the observer's current viewing position. As the observer walks around the subject, the ultrasound images appear stationary in 3-space within the subject. This kind of enhancement of the observer's vision may have many other applications, e.g., image guided surgical procedures and on location 3D interactive architecture preview.},
author = {Bajura, Michael and Fuchs, Henry and Ohbuchi, Ryutarou},
doi = {10.1145/142920.134061},
file = {:C$\backslash$:/Users/admin/Downloads/MergVirtObjs92.pdf:pdf},
isbn = {0897914791},
issn = {00978930},
journal = {Computer Graphics (ACM)},
number = {2},
pages = {203--210},
title = {{Merging virtual objects with the real world: seeing ultrasound imagery within the patient}},
volume = {26},
year = {1992}
}


@inproceedings{Meng2013,
abstract = {Education of anatomy is a challenging but crucial element in educating medical professionals, but also for general education of pupils. Our research group has previously developed a prototype of an Augmented Reality (AR) magic mirror which allows intuitive visualization of realistic anatomical information on the user. However, the current overlay is imprecise as the magic mirror depends on the skeleton output from Kinect. These imprecisions affect the quality of education and learning. Hence, together with clinicians we have defined bone landmarks which users can touch easily on their body while standing in front of the sensor. We demonstrate that these landmarks allow the proper deformation of medical data within the magic mirror and onto the human body, resulting in a more precise augmentation.},
author = {Meng, Ma and Fallavollita, P and Blum, T and Eck, U and Sandor, C and Weidert, S and Waschke, J and Navab, N},
booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
doi = {10.1109/ISMAR.2013.6671803},
file = {:C$\backslash$:/Users/admin/Downloads/06671803.pdf:pdf},
isbn = {null VO -},
keywords = {AR magic mirror,Anatomy Learning,Augmented Reality,Augmented reality,Biomedical imaging,Bones,Computed tomography,Education,Kinect,Mirrors,anatomy education,augmented reality,biomedical education,bone,bone landmarks,computer aided instruction,data visualisation,education quality,human body,interactive AR anatomy learning,intuitive visualization,learning quality,medical computing,medical data deformation,realistic anatomical information,sensor,skeleton output},
pages = {277--278},
title = {{Kinect for interactive AR anatomy learning}},
year = {2013}
}

@book{AdamMoserandDarrinM.York2008,
author = {{Adam Moser  and Darrin M. York}, Kevin Range},
booktitle = {Bone},
doi = {10.1038/jid.2014.371},
file = {:C$\backslash$:/Users/admin/Downloads/nihms195421.pdf:pdf},
isbn = {6176321972},
keywords = {epiblast,gfp fusion,histone h2b-,icm,lineage specification,live imaging,mouse blastocyst,pdgfr $\alpha$,primitive endoderm},
number = {1},
pages = {1--7},
pmid = {1000000221},
title = {{基因的改变NIH Public Access}},
volume = {23},
year = {2008}
}

@article{St.John2001,
abstract = {Research on when and how to use three-dimensional (3D) perspective views on flat screens for operational tasks such as air traffic control is complex. We propose a functional distinction between tasks: those that require shape understanding versus those that require precise judgments of relative position. The distortions inherent in 3D displays hamper judging relative positions, whereas the integration of dimensions in 3D displays facilitates shape understanding. We confirmed these hypotheses with two initial experiments involving simple block shapes. The shape-understanding tasks were identification or mental rotation. The relative-position tasks were locating shadows and determining directions and distances between objects. We then extended the results to four experiments involving complex natural terrain. We compare our distinction with the integral/separable task distinction of Haskel and Wickens (1993). Applications for this research include displays for air traffic control, geoplots for military command and control, and potentially, any display of 3D information.},
annote = {This paper is performing a direct comparsion between 2D and 3D displays and peoples understanding of them.

This paper was mainly designed for displaying flight information. 

This paper is quite old but it does get it's point accross.},
author = {{St. John}, M. and Cowen, M. B. and Smallman, H. S. and Oonk, H. M.},
doi = {10.1518/001872001775992534},
file = {:C$\backslash$:/Users/admin/Downloads/001872001775992534.pdf:pdf},
issn = {00187208},
journal = {Human Factors},
mendeley-groups = {3Dvs2DMedicalVis},
number = {1},
pages = {79--98},
title = {{The use of 2D and 3D displays for shape-understanding versus relative-position tasks}},
volume = {43},
year = {2001}
}

@article{Dube2016,
author = {Dube, Adam and Mcewen, Rhonda and Dub{\'{e}}, Adam K},
doi = {10.13140/RG.2.1.5104.0881},
file = {:C$\backslash$:/Users/admin/Downloads/DubeAdam.pdf:pdf},
mendeley-groups = {video deficit effect},
number = {June},
title = {{How do tablet computers mitigate the video deficit effect? Digital Home Numeracy Practice for children with mathematical learning disability View project How do tablet computers mitigate the video deficit effect?}},
url = {https://www.researchgate.net/publication/304467404},
year = {2016}
}

@article{Reiß2019,
abstract = {The video deficit effect (VDE) has been demonstrated in several studies on word learning, self-recognition, and imitation: Younger children (up to 3 years old) solved tasks more easily in a direct interaction with an examiner than when instructed by video (Anderson {\&} Pempek, 2005). Older children might also be susceptible to a VDE, especially with more complex tasks; however, evidence is sparse. Furthermore, to what extent preschoolers' understanding of others' mental states (theory of mind) is impaired by video presentations has not been tested. We tested 174 children of 4 and 5 years of age in a traditional change of location task for false belief understanding (cf. Baron-Cohen, Leslie, {\&} Frith, 1985). Children were presented with the original story, enacted by adult actors, in either a video or a live demonstration. Children watched the events in 2 live conditions, either through a 1-way mirror or directly. Our results indicate a significant VDE for 4- and 5-year-old children regarding the encoding and solution of the false belief task, respectively.},
author = {Rei{\ss}, Mirjam and Kr{\"{u}}ger, Markus and Krist, Horst},
doi = {10.1080/15213269.2017.1412321},
file = {:C$\backslash$:/Users/admin/Downloads/Theory of Mind and the Video Deficit Effect Video Presentation Impairs Children s Encoding and Understanding of False Belief.pdf:pdf},
issn = {15213269},
journal = {Media Psychology},
mendeley-groups = {video deficit effect},
number = {1},
pages = {23--38},
publisher = {Routledge},
title = {{Theory of Mind and the Video Deficit Effect: Video Presentation Impairs Children's Encoding and Understanding of False Belief}},
url = {https://doi.org/10.1080/15213269.2017.1412321},
volume = {22},
year = {2019}
}


@article{Krcmar2010,
abstract = {Two experiments were conducted to test several questions regarding very young children's (6?24 months) learning (i.e., simple action imitation and word learning) from video. Specifically, this study tested the video deficit, which is the tendency for infants and toddlers to learn significantly more effectively from live information than they do when identical information is presented on a screen. First, the video deficit was explored using two different tasks. Overall, the pattern of results was similar for action imitation and word learning. Specifically, the video deficit was present for both simple action imitation and for word learning in the middle cohort, but not present for younger and older children. Second, there was some mitigation of the video deficit from seeing socially meaningful actors for action imitation; however for word learning the effect only approached significance. Third, repetition helped children learn words more effectively, especially for the youngest and oldest cohort; however, repetition did not help for simple task imitation.},
annote = {doi: 10.1080/15213260903562917},
author = {Krcmar, Marina},
doi = {10.1080/15213260903562917},
issn = {1521-3269},
journal = {Media Psychology},
mendeley-groups = {video deficit effect},
month = {mar},
number = {1},
pages = {31--53},
publisher = {Routledge},
title = {{Can Social Meaningfulness and Repeat Exposure Help Infants and Toddlers Overcome the Video Deficit?}},
url = {https://doi.org/10.1080/15213260903562917},
volume = {13},
year = {2010}
}

@article{Barr2010,
author = {Barr, Rachel},
doi = {10.1038/jid.2014.371},
file = {:C$\backslash$:/Users/admin/Downloads/nihms195421.pdf:pdf},
isbn = {6176321972},
journal = {National Instutes of Health},
keywords = {Infants,Representational Flexibility,Television,Transfer of Learning,Video Deficit Effect},
mendeley-groups = {video deficit effect},
pages = {128 -- 154},
pmid = {1000000221},
publisher = {Elsevier},
title = {{Transfer of learning between 2D and 3D sources during infancy: Informing theory and practice}},
year = {2010}
}


@article{Lilija2019,
abstract = {We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.},
annote = {This paper},
author = {Lilija, Klemen and Pohl, Henning and Boring, Sebastian and Hornb{\ae}k, Kasper},
doi = {10.1145/3290605.3300676},
file = {:C$\backslash$:/Users/admin/Downloads/3290605.3300676.pdf:pdf},
isbn = {9781450359702},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Augmented reality,Finger-camera,Manipulation task},
pages = {1--12},
title = {{Augmented reality views for occluded interaction}},
year = {2019}
}

@inproceedings{Cote2018,
abstract = {Subsurface utility work planning would benefit from augmented reality. Unfortunately, the exact pipe location is rarely known, which produces unreliable augmentations. We proposed an augmentation technique that drapes 2D pipe maps onto the road surface and aligns them with corresponding features in the physical world using a pre-captured 3D mesh. Resulting augmentations are more likely to be displayed at the true pipe locations.},
annote = {A industry ready product this is how various parties got around placing pipes underground.

This paper has never been cited but it could be very important to you.},
author = {C{\^{o}}t{\'{e}}, S and Mercier, A},
booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
doi = {10.1109/VR.2018.8446545},
file = {:C$\backslash$:/Users/admin/Downloads/08446545.pdf:pdf},
isbn = {null VO -},
keywords = {2D pipe maps,Applied computing → Physical sciences and engineer,Augmented Reality,Augmented reality,Data visualization,Engineering,Human-centered computing → Human computer interact,Planning,Roads,Subsurface Utilities,Surface topography,Surface treatment,Three-dimensional displays,augmentation technique,augmented reality,cartography,pipe location,pipe locations,precaptured 3D mesh,public utilities,road surface,subsurface utility model projections,subsurface utility work planning,unreliable augmentations},
pages = {535--536},
title = {{Augmentation of Road Surfaces with Subsurface Utility Model Projections}},
year = {2018}
}

@article{VanSon2018,
abstract = {To optimise the use of limited available land, land-scarce cities such as Singapore are increasingly looking towards the underground in search of more space. A good understanding of what already exists underground is essential for the planning of underground spaces. In particular, utility services make up a significant part of what exists underground. To meet planning needs, the Singapore government has initiated efforts towards bringing records of existing utility networks together in a single database and share its contents to support planning, design, and construction of underground developments. However, these records can not be relied on to support these critical processes: They are not guaranteed to represent today's state of the underground, are not accurate or of unknown accuracy, are inconsistently modelled, and may indicate as-design information instead of as-built information. This lack of reliability leads to an increase in cost and a loss in efficiency caused by the need to repeatedly survey to locate existing utility services on-site, and can have potentially disastrous outcomes when an excavation would damage existing services. Technological advances in utility surveying and mapping devices such as Ground Penetrating Radar (GPR) and gyroscopic pipeline mapping devices offer the potential of accurately mapping utilities in three dimensions (3D) at a large scale and high speed. However, a better understanding of the benefits and limitations of these technologies in a practical context is needed, as well as their suitability for mapping to support applications such as urban planning and land administration. The Digital Underground project is a collaboration between Singapore-ETH Centre, Singapore Land Authority and the City of Z{\"{u}}rich that aims to develop a roadmap towards a reliable 3D utility map of Singapore. To enable the development of utility mapping standards and guidelines, the 3D mapping workflow for underground utilities is studied extensively based on market research, literature study, and case studies. This work presents the beginnings of a framework for 3D mapping of underground utilities as one of the initial results of the Digital Underground project as it is in progress. From these experiences, it can be concluded that, together with existing data, data captured using various surveying methods can indeed contribute to the establishment and maintenance of a consolidated and reliable utility map. To this end, a multi-sensor, multi-data 3D mapping workflow is proposed to integrate data captured using different surveying techniques during different moments in the development lifecycle of utilities. Based on this framework, this work also identifies areas for improvement and critical gaps to be bridged that will ultimately form part of the roadmap.},
author = {{Van Son}, R. and Jaw, S. W. and Yan, J. and Khoo, H. S.V. and Loo, W. K.R. and Teo, S. N.S. and Schrotter, G.},
doi = {10.5194/isprs-archives-XLII-4-W10-209-2018},
file = {:C$\backslash$:/Users/admin/Downloads/isprs-archives-XLII-4-W10-209-2018.pdf:pdf},
issn = {16821750},
journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
keywords = {3D Mapping,Guidelines,Mapping Standards,Underground Utility Mapping,Urban Planning},
number = {4/W10},
pages = {209--214},
title = {{A framework for reliable three-dimensional underground utility mapping for urban planning}},
volume = {42},
year = {2018}
}

@article{Sun2019,
abstract = {Facial expressions are an important part of human communication. However, children with autism spectrum disorders (ASD) are often suffering from difficulties of understanding non-verbal cues and form appropriate responses. Traditional approaches including labeling formatted photographs of human facial expressions from a third person's perspective could help them learn and improve such skills. Yet such training systems are often in lack of real time feedback. As Optical See-Through (OST) Augmented Reality (AR) headsets possess the advantages of near-eyes display and better depth alignment between virtual renderings and the environment, we decided on an OST AR approach for the system. In this paper, we present a system designed for OST AR headsets that occludes the subject's facial expressions with an emotion-presenting 3D emoji model. We hope this system could help us understand how children with ASD perceive emotions through standard emotion presenting systems and help them enhance their skills of understanding facial expressions.},
author = {Sun, Ran and Haraldsson, Harald and Zhao, Yuhang and Belongie, Serge},
doi = {10.1109/ISMAR-Adjunct.2019.00052},
file = {:C$\backslash$:/Users/admin/Downloads/08951905.pdf:pdf},
isbn = {9781728147659},
journal = {Adjunct Proceedings of the 2019 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2019},
keywords = {Augmented-Reality,Autism-spectrum-disorders,Emoji,Emotion-understanding,Head-pose-Tracking,Optical-see-Through},
pages = {448--450},
publisher = {IEEE},
title = {{Anon-Emoji: An optical see-Through augmented reality system for children with autism spectrum disorders to promote understanding of facial expressions and emotions}},
year = {2019}
}

@article{Swan2007,
abstract = {A fundamental problem in optical, see-through augmented reality (AR) is characterizing how it affects the perception of spatial layout and depth. This problem is important because AR system developers need to both place graphics in arbitrary spatial relationships with real-world objects, and to know that users will perceive them in the same relationships. Furthermore, AR makes possible enhanced perceptual techniques that have no real-world equivalent, such as x-ray vision, where AR users are supposed to perceive graphics as being located behind opaque surfaces. This paper reviews and discusses protocols for measuring egocentric depth judgments in both virtual and augmented environments, and discusses the well-known problem of depth underestimation in virtual environments. It then describes two experiments that measured egocentric depth judgments in AR. Experiment I used a perceptual matching protocol to measure AR depth judgments at medium and far-field distances of 5 to 45 meters. The experiment studied the effects of upper versus lower visual field location, the x-ray vision condition, and practice on the task. The experimental findings include evidence for a switch in bias, from underestimating to overestimating the distance of AR-presented graphics, at ∼23 meters, as well as a quantification of how much more difficult the x-ray vision condition makes the task. Experiment II used blind walking and verbal report protocols to measure AR depth judgments at distances of 3 to 7 meters. The experiment examined real-world objects, real-world objects seen through the AR display, virtual objects, and combined real and virtual objects. The results give evidence that the egocentric depth of AR objects is underestimated at these distances, but to a lesser degree than has previously been found for most virtual reality environments. The results are consistent with previous studies that have implicated a restricted field-of-view, combined with an inability for observers to scan the ground plane in a near-to-far direction, as explanations for the observed depth underestimation. {\textcopyright} 2007 IEEE.},
author = {Swan, J. Edward and Jones, Adam and Kolstad, Eric and Livingston, Mark A. and Smallman, Harvey S.},
doi = {10.1109/TVCG.2007.1035},
file = {:C$\backslash$:/Users/admin/Downloads/07573294.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Artificial,Augmented,Depth perception,Ergonomics,Evaluation/methodology,Experimentation,Measurement,Optical see-through augmented reality,Performance,Screen design,Virtual realities},
number = {3},
pages = {429--442},
title = {{Egocentric depth judgments in optical, see-through augmented reality}},
volume = {13},
year = {2007}
}

@inproceedings{Jamiy2019,
abstract = {The purpose of Virtual Reality (VR) is to provide a consistent simulation of the realistic world and make interaction between different worlds and objects possible. Perceiving depth and distance correctly in VR is essential, but, many previous work showed an underestimation of distance in Virtual Reality with Head Mounted Displays (HMDs). The present work gives a literature review of the design challenges of such systems, distance perception issue in VR, and study perceptual research in virtual environments. We will give a review of the history of the work and efforts done in visual perception to measure the perceived distance. A particular focus will be on distance estimates methods and techniques in VR and AR developed throughout these work. Depth perception is one of the important elements in virtual reality. The perceived depth is influenced by Head Mounted Displays (HMD) that inevitability decrease the virtual content's depth perception.},
annote = {This paper notes that there have not been any Ocular see though depth guides vs see though ones developed.

Quotes required are highlighted},
author = {Jamiy, F E and Marsh, R},
booktitle = {2019 IEEE International Conference on Electro Information Technology (EIT)},
doi = {10.1109/EIT.2019.8834182},
file = {:C$\backslash$:/Users/admin/Downloads/paperEIT19-Fatima.pdf:pdf},
isbn = {2154-0357 VO -},
keywords = {Augmented Reality,Distance Estimation,Estimation,Head Mounted Displays,Legged locomotion,Perception,Resists,Task analysis,VR,Virtual Reality,Virtual environments,Visualization,augmented reality,distance estimation methods,distance perception issue,head mounted displays,helmet mounted displays,virtual content depth perception,virtual environments,virtual reality,visual perception},
pages = {63--68},
title = {{Distance Estimation In Virtual Reality And Augmented Reality: A Survey}},
year = {2019}
}

@article{Jamiy2019b,
abstract = {Depth perception is one of the important elements in virtual reality (VR). The perceived depth is influenced by the head mounted displays that inevitability decreases the virtual content's depth perception. While several questions within this area are still under research; the main objective of this study is to survey the recently conducted studies on depth perception in VR, augmented reality (AR), and mixed reality (MR). First, depth perception in the human visual system is discussed including the different visual cues involved in depth perception. Second, research performed to understand and confirm depth perception issue is examined. The contributions made to improve depth perception and specifically distance perception will be discussed with their main proposed design key, advantages, and limitations. Most of the contributions were based on using one or two depth cues to improve depth perception in VR, AR, and MR.},
author = {Jamiy, F El and Marsh, R},
doi = {10.1049/iet-ipr.2018.5920},
file = {:C$\backslash$:/Users/admin/Downloads/08689144.pdf:pdf},
issn = {1751-9667 VO - 13},
journal = {IET Image Processing},
keywords = {augmented reality,depth perception,head mounted displays,helmet mounted displays,human visual system,mixed reality,virtual content,virtual reality,visual cues,visual perception},
mendeley-groups = {DepthPerceptionPapers},
number = {5},
pages = {707--712},
title = {{Survey on depth perception in head mounted displays: distance estimation in virtual reality, augmented reality, and mixed reality}},
volume = {13},
year = {2019}
}


@article{Kyto2014,
abstract = {Depth perception is an important component of many augmented reality{\$}\backslash{\$}napplications. It is, however, subject to multiple error sources. In this{\$}\backslash{\$}nstudy, we investigated depth judgments with a stereoscopic video{\$}\backslash{\$}nsee-through head-mounted display for the purpose of designing depth{\$}\backslash{\$}ncueing for systems that operate in an individual's action space. In the{\$}\backslash{\$}nexperiment, we studied the use of binocular disparity and relative size{\$}\backslash{\$}nto improve relative depth judgments of augmented objects above the{\$}\backslash{\$}nground plane. The relative size cue was created by adding auxiliary{\$}\backslash{\$}naugmentations to the scene according to constraints described in the{\$}\backslash{\$}nsection on the underlying theory. The results showed that binocular{\$}\backslash{\$}ndisparity and relative size improved depth judgments over the distance{\$}\backslash{\$}nrange. This indicates that for accurate depth judgments, additional{\$}\backslash{\$}ndepth cues should be used to facilitate stereoscopic perception within{\$}\backslash{\$}nan individual's action space. (C) The Authors. Published by SPIE under a{\$}\backslash{\$}nCreative Commons Attribution 3.0 Unported License. Distribution or{\$}\backslash{\$}nreproduction of this work in whole or in part requires full attribution{\$}\backslash{\$}nof the original publication, including its DOI.},
author = {Kyt{\"{o}}, Mikko and M{\"{a}}kinen, Aleksi and Tossavainen, Timo and Oittinen, Pirkko},
doi = {10.1117/1.jei.23.1.011006},
issn = {1017-9909},
journal = {Journal of Electronic Imaging},
keywords = {1,10,13,2013,2014,5,accepted for publication feb,action space,augmented reality,depth perception,head-mounted display,online mar,paper 13496ss received sep,published,revised manuscript received feb,stereoscopy},
number = {1},
pages = {11006},
title = {{Stereoscopic depth perception in video see-through augmented reality within action space}},
volume = {23},
year = {2014}
}

@article{Kyto2013,
abstract = {Significant depth judgment errors are common in augmented reality. This study presents a visualization approach for improving relative depth judgments in augmented reality. The approach uses auxiliary augmented objects in addition to the main augmentation to support ordinal and interval depth judgment tasks. The auxiliary augmentations are positioned spatially near real-world objects, and the location of the main augmentation can be deduced based on the relative depth cues between the augmented objects. In the experimental part, the visualization approach was tested in the "X-ray" visualization case with a video see-through system. Two relative depth cues, in addition to motion parallax, were used between graphical objects: relative size and binocular disparity. The results show that the presence of auxiliary objects significantly reduced errors in depth judgment. Errors in judging the ordinal location with respect to a wall (front, at, or behind) and judging depth intervals were reduced. In addition to reduced errors, the presence of auxiliary augmentation increased the confidence in depth judgments, and it was subjectively preferred. The visualization approach did not have an effect on the viewing time. {\textcopyright} 2013 ACM.},
author = {Kyt{\"{o}}, Mikko and M{\"{a}}kinen, Aleksi and H{\"{a}}kkinen, Jukka and Oittinen, Pirkko},
doi = {10.1145/2422105.2422111},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kyt{\"{o}} et al. - 2013 - Improving relative depth judgments in augmented reality with auxiliary augmentations.pdf:pdf},
issn = {15443558},
journal = {ACM Transactions on Applied Perception},
keywords = {Design,Experimentation,Human factors,Performance},
number = {1},
title = {{Improving relative depth judgments in augmented reality with auxiliary augmentations}},
volume = {10},
year = {2013}
}


@article{Vishton1995,
abstract = {J.E. Cutting and P.M. Vishton. Perception of Space and Motion, chapter Perceiving Layout and Knowing Distances : The Integration, Relative Potency, and Contextual Use of Different Information about Depth, pages 69–117. Academic Press, New-York, USA, 1995. 2, 9, 11},
author = {Vishton, J.E. Cutting and P.M.},
doi = {http://dx.doi.org/10.1016/B978-012240530-3/50005-5},
file = {:C$\backslash$:/Users/admin/Downloads/78.pdf:pdf},
isbn = {978-0-12-240530-3},
issn = {1939-1277},
journal = {Perception of Space and Motion},
number = {5},
pages = {69--117},
title = {{chapter Perceiving Layout and Knowing Distances : The Integration, Relative Potency, and Contextual Use of Different Information about Depth}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.22.5.1299},
volume = {22},
year = {1995}
}

@article{Feldstein2019,
annote = {doi: 10.1177/0301006619861892},
author = {Feldstein, Ilja T},
doi = {10.1177/0301006619861892},
file = {:C$\backslash$:/Users/admin/Downloads/0301006619861892.pdf:pdf},
issn = {0301-0066},
journal = {Perception},
month = {jul},
number = {9},
pages = {769--795},
publisher = {SAGE Publications Ltd STM},
title = {{Impending Collision Judgment from an Egocentric Perspective in Real and Virtual Environments: A Review}},
url = {https://doi.org/10.1177/0301006619861892},
volume = {48},
year = {2019}
}

@article{Watson1992,
abstract = {When the motion of an object is influenced by gravity (eg free fall, pendulum, wave motion), that influence may provide a cue to computing the absolute distance and/or size of the object. Formal analysis supports the claim that the distance and size of moving objects are generally computable with reference to the gravitational component of motion. Informal evidence from judgments of realism in films is consistent with this gravity-cue hypothesis.},
author = {Watson, J. S. and Banks, M. S. and von Hofsten, C. and Royden, C. S.},
doi = {10.1068/p210069},
file = {:C$\backslash$:/Users/admin/Downloads/53watsonBanksvHR1992.pdf:pdf},
issn = {03010066},
journal = {Perception},
number = {1},
pages = {69--76},
title = {{Gravity as a monocular cue for perception of absolute distance and/or absolute size.}},
volume = {21},
year = {1992}
}


@article{Medeiros2016,
abstract = {Head-Mounted Displays (HMDs) and similar 3D visualization devices are becoming ubiquitous. Going a step forward, HMD seethrough systems bring virtual objects to real world settings, allowing augmented reality to be used in complex engineering scenarios. Of these, optical and video see-through systems differ on how the real world is captured by the device. To provide a seamless integration of real and virtual imagery, the absolute depth and size of both virtual and real objects should match appropriately. However, these technologies are still in their early stages, each featuring different strengths and weaknesses which affect the user experience. In this work we compare optical to video see-through systems, focusing on depth perception via exocentric and egocentric methods. Our study pairs Meta Glasses, an off-the-shelf optical see-through, to a modified Oculus Rift setup with attached video-cameras, for video see-through. Results show that, with the current hardware available, the video see-through configuration provides better overall results.These experiments and our results can help interaction designers for both virtual and augmented reality conditions.},
author = {Medeiros, Daniel and Sousa, Maur{\'{i}}cio and Mendes, Daniel and Raposo, Alberto and Jorge, Joaquim},
doi = {10.1145/2993369.2993388},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Medeiros et al. - 2016 - Perceiving depth Optical versus video see-through.pdf:pdf},
isbn = {9781450344913},
journal = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
keywords = {Augmented reality,Depth perception,See-through system,User evaluation},
pages = {237--240},
title = {{Perceiving depth: Optical versus video see-through}},
url = {https://dl.acm.org/doi/10.1145/2993369.2993388{\#}d9578050e1},
volume = {02-04-Nove},
year = {2016}
}

@article{Diaz2017,
abstract = {Augmented reality technologies allow people to view and interact with virtual objects that appear alongside physical objects in the real world. For augmented reality applications to be effective, users must be able to accurately perceive the intended real world location of virtual objects. However, when creating augmented reality applications, developers are faced with a variety of design decisions that may affect user perceptions regarding the real world depth of virtual objects. In this paper, we conducted two experiments using a perceptual matching task to understand how shading, cast shadows, aerial perspective, texture, dimensionality (i.e., 2D vs. 3D shapes) and billboarding affected participant perceptions of virtual object depth relative to real world targets. The results of these studies quantify trade-offs across virtual object designs to inform the development of applications that take advantage of users' visual abilities to better blend the physical and virtual world.},
annote = {Paper that looks into depth illisions in AR and compares different techniques. 

This is quite an important paper},
author = {Diaz, Catherine and Walker, Michael and Szafir, Danielle Albers and Szafir, Daniel},
doi = {10.1109/ISMAR.2017.28},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Diaz et al. - 2017 - Designing for depth perceptions in augmented reality.pdf:pdf},
isbn = {9781538629437},
journal = {Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2017},
mendeley-groups = {DepthPerceptionPapers},
pages = {111--122},
title = {{Designing for depth perceptions in augmented reality}},
year = {2017}
}

@article{Siegel2017,
abstract = {Distances tend to be underperceived in virtual environments (VEs) by up to 50{\%}, whereas distances tend to be perceived accurately in the real world. Previous work has shown that allowing participants to interact with the VE while receiving continual visual feedback can reduce this underperception. Judgments of virtual object size have been used to measure whether this improvement is due to the rescaling of perceived space, but there is disagreement within the literature as to whether judgments of object size benefit from interaction with feedback. This study contributes to that discussion by employing a more natural measure of object size. We also examined whether any improvement in virtual distance perception was limited to the space used for interaction (1–5 m) or extended beyond (7–11 m). The results indicated that object size judgments do benefit from interaction with the VE, and that this benefit extends to distances beyond the explored space.},
author = {Siegel, Zachary D. and Kelly, Jonathan W.},
doi = {10.3758/s13414-016-1243-z},
file = {:C$\backslash$:/Users/admin/Downloads/s13414-016-1243-z.pdf:pdf},
issn = {1943393X},
journal = {Attention, Perception, and Psychophysics},
keywords = {Spatial cognition,Visual perception},
number = {1},
pages = {39--44},
publisher = {Attention, Perception, {\&} Psychophysics},
title = {{Walking through a virtual environment improves perceived size within and beyond the walked space}},
url = {http://dx.doi.org/10.3758/s13414-016-1243-z},
volume = {79},
year = {2017}
}


@article{Kelly2013,
abstract = {Egocentric distances in virtual environments are commonly underperceived by up to 50 {\%} of the intended distance. However, a brief period of interaction in which participants walk through the virtual environment while receiving visual feedback can dramatically improve distance judgments. Two experiments were designed to explore whether the increase in postinteraction distance judgments is due to perception-action recalibration or the rescaling of perceived space. Perception-action recalibration as a result of walking interaction should only affect action-specific distance judgments, whereas rescaling of perceived space should affect all distance judgments based on the rescaled percept. Participants made blind-walking distance judgments and verbal size judgments in response to objects in a virtual environment before and after interacting with the environment through either walking (Experiment 1) or reaching (Experiment 2). Size judgments were used to infer perceived distance under the assumption of size-distance invariance, and these served as an implicit measure of perceived distance. Preinteraction walking and size-based distance judgments indicated an underperception of egocentric distance, whereas postinteraction walking and size-based distance judgments both increased as a result of the walking interaction, indicating that walking through the virtual environment with continuous visual feedback caused rescaling of the perceived space. However, interaction with the virtual environment through reaching had no effect on either type of distance judgment, indicating that physical translation through the virtual environment may be necessary for a rescaling of perceived space. Furthermore, the size-based distance and walking distance judgments were highly correlated, even across changes in perceived distance, providing support for the size-distance invariance hypothesis. {\textcopyright} 2013 Psychonomic Society, Inc.},
author = {Kelly, Jonathan W. and Donaldson, Lisa S. and Sjolund, Lori A. and Freiberg, Jacob B.},
doi = {10.3758/s13414-013-0503-4},
file = {:C$\backslash$:/Users/admin/Downloads/s13414-013-0503-4.pdf:pdf},
issn = {19433921},
journal = {Attention, Perception, and Psychophysics},
keywords = {Action,Distance perception,Virtual reality,Visual perception},
number = {7},
pages = {1473--1485},
title = {{More than just perception-action recalibration: Walking through a virtual environment causes rescaling of perceived space}},
volume = {75},
year = {2013}
}

@article{Rosales2019,
abstract = {Augmented reality (AR) technologies have the potential to provide individuals with unique training and visualizations, but the effectiveness of these applications may be influenced by users' perceptions of the distance to AR objects. Perceived distances to AR objects may be biased if these objects do not appear to make contact with the ground plane. The current work compared distance judgments of AR targets presented on the ground versus off the ground when no additional AR depth cues, such as shadows, were available to denote ground contact. We predicted that without additional information for height off the ground, observers would perceive the off-ground objects as placed on the ground, but at farther distances. Furthermore, this bias should be exaggerated when targets were viewed with one eye rather than two. In our experiment, participants judged the absolute egocentric distance to various cubes presented on or off the ground with an action-based measure, blind walking. We found that observers walked farther for off-ground AR objects and that this effect was exaggerated when participants viewed off-ground objects with monocular vision compared to binocular vision. However, we also found that the restriction of binocular cues influenced participants' distance judgments for on-ground AR objects. Our results suggest that distances to off-ground AR objects are perceived differently than on-ground AR objects and that the elimination of binocular cues further influences how users perceive these distances.},
author = {Rosales, Carlos Salas and Pointon, Grant and Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah and Thompson, William B. and Bodenheimer, Bobby},
doi = {10.1109/VR.2019.8798095},
file = {:C$\backslash$:/Users/admin/Downloads/08798095.pdf:pdf},
isbn = {9781728113777},
journal = {26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings},
keywords = {Augmented reality,Depth cues,Distance perception,Virtual environments},
mendeley-groups = {DepthPerceptionPapers},
pages = {237--243},
title = {{Distance judgments to on- and off-ground objects in augmented reality}},
year = {2019}
}

@article{Santos2016,
abstract = {Virtual objects can be visualized inside real objects using augmented reality (AR). This visualization is called AR X-ray because it gives the impression of seeing through the real object. In standard AR, virtual information is overlaid on top of the real world. To position a virtual object inside an object, AR X-ray requires partially occluding the virtual object with visually important regions of the real object. In effect, the virtual object becomes less legible compared to when it is completely unoccluded. Legibility is an important consideration for various applications of AR X-ray. In this research, we explored legibility in two implementations of AR X-ray, namely, edge-based and saliency-based. In our first experiment, we explored on the tolerable amounts of occlusion to comfortably distinguish small virtual objects. In our second experiment, we compared edge-based and saliency-based AR X-ray methods when visualizing virtual objects inside various real objects. Moreover, we benchmarked the legibility of these two methods against alpha blending. From our experiments, we observed that users have varied preferences for proper amounts of occlusion cues for both methods. The partial occlusions generated by the edge-based and saliency-based methods need to be adjusted depending on the lighting condition and the texture complexity of the occluding object. In most cases, users identify objects faster with saliency-based AR X-ray than with edge-based AR X-ray. Insights from this research can be directly applied to the development of AR X-ray applications.},
author = {Santos, Marc Ericson C. and {de Souza Almeida}, Igor and Yamamoto, Goshiro and Taketomi, Takafumi and Sandor, Christian and Kato, Hirokazu},
doi = {10.1007/s11042-015-2954-1},
file = {:C$\backslash$:/Users/admin/Downloads/Santos2016{\_}Article{\_}ExploringLegibilityOfAugmented.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Augmented reality,Augmented reality X-ray,Empirical study,Legibility,Visualization},
mendeley-groups = {DepthPerceptionPapers,X-RayVision},
number = {16},
pages = {9563--9585},
publisher = {Multimedia Tools and Applications},
title = {{Exploring legibility of augmented reality X-ray}},
url = {http://dx.doi.org/10.1007/s11042-015-2954-1},
volume = {75},
year = {2016}
}

@phdthesis{Hamadouche2018,
abstract = {In this thesis, we present the development and evaluation of an augmented realityX-ray system on optical see-through head-mounted displays. Augmented reality X-ray vision allows users to see through solid surfaces such as walls and facades,by augmenting the real view with virtual images representing the hidden objects. Our system is developed based on the optical see-through mixed reality headset Microsoft Hololens. We have developed an X-ray cutout algorithm that uses thegeometric data of the environment and enables seeing through surfaces. We have developed four different visualizations as well based on the algorithm. The first visualization renders simply the X-ray cutout without displaying any information about the occluding surface. The other three visualizations display features extracted from the occluder surface to help the user to get better depth perceptionof the virtual objects. We have used Sobel edge detection to extract the information. The three visualizations differ in the way to render the extracted features. A subjective experiment is conducted to test and evaluate the visualizations and to compare them with each other. The experiment consists of two parts; depthestimation task and a questionnaire. Both the experiment and its results are pre-sented in the thesis.},
author = {Hamadouche, Ilyas},
file = {:C$\backslash$:/Users/admin/Downloads/a705f11ebdb9a7e913e5dd86914d346a9444.pdf:pdf},
keywords = {augmented reality,mixed reality,optical see-through,x-ray vision},
mendeley-groups = {X-RayVision},
number = {June},
pages = {58},
title = {{AUGMENTED REALITY X-RAY VISION ON OPTICAL SEE-THROUGH HEAD-MOUNTED}},
url = {https://pdfs.semanticscholar.org/de04/a705f11ebdb9a7e913e5dd86914d346a9444.pdf},
year = {2018} ,
school = {University of Oulu}
}

@article{Al-Kalbani2019,
abstract = {This paper presents the use of rendered visual cues as drop shadows and their impact on overall usability and accuracy of grasping interactions for Augmented Reality (AR). We report on two conditions; grasping with drop shadows and without drop shadows and analyse a total of 910 grasps. We report on the accuracy of Grasp Aperture (GAp), Grasp Displacement (GDisp), completion time and usability metrics from 30 participants. A comprehensive statistical analysis of the results is presented giving comparisons of the inclusion of drop shadows in AR grasping. Findings showed that the use of drop shadows increases usability of AR grasping while significantly decreasing task completion time. Furthermore drop shadows also significantly improve user's depth estimation of AR object position. However, this study also shows that using drop shadows does not improve user's object size estimation, which remains as a problematic element in grasping AR interaction literature.},
author = {Al-Kalbani, Maadh and Frutos-Pascual, Maite and Williams, Ian},
doi = {10.1109/VS-Games.2019.8864596},
file = {:C$\backslash$:/Users/admin/Downloads/08864596.pdf:pdf},
isbn = {9781538671238},
journal = {2019 11th International Conference on Virtual Worlds and Games for Serious Applications, VS-Games 2019 - Proceedings},
keywords = {Augmented Reality,HCI,Interaction,Serious Games,Usability},
mendeley-groups = {DepthPerceptionPapers},
pages = {1DUUMY},
publisher = {IEEE},
title = {{Virtual object grasping in augmented reality: Drop shadows for improved interaction}},
year = {2019}
}

@article{Armbruster2008,
abstract = {The present study investigated depth perception in virtual environments. Twenty-three participants verbally estimated ten distances between 40 cm and 500 cm in three different virtual environments in two conditions: (1) only one target was presented or (2) ten targets were presented at the same time. Additionally, the presence of a metric aid was varied. A questionnaire assessed subjective ratings about physical complaints (e.g., headache), the experience in the virtual world (e.g., presence), and the experiment itself (self-evaluation of the estimations). Results show that participants underestimate the virtual distances but are able to perceive the distances in the right metric order even when only very simple virtual environments are presented. Furthermore, interindividual differences and intraindividual stabilities can be found among participants, and neither the three different virtual environments nor the metric aid improved depth estimations. Estimation performance is better in peripersonal than in extrapersonal space. In contrast, subjective ratings provide a preferred space: a closed room with visible floor, ceiling, and walls. {\textcopyright} 2008 Mary Ann Liebert, Inc.},
annote = {Some reserach on depth perception in Virtual Reality. Participants were asked to tell the verberly express how far away the virtual objects were and answer three questionaires. 
The first one focused on the users discomfort, The next one looked into presence and awareness and the last one asked questions that were specific to this questionaire. 

Overall each participant made over 120 depth estimations. The most important factors were seen to be 3D depth perception and bifocal effects.

One important note of this that there seems to be large discrepency with estimations over 4 meters away. 

This research belives that depth perception alone is not sufficiant to creating realistic enviroments.},
author = {Armbr{\"{u}}ster, C. and Wolter, M. and Kuhlen, T. and Spijkers, W. and Fimm, B.},
doi = {10.1089/cpb.2007.9935},
file = {:C$\backslash$:/Users/admin/Downloads/armbruster20et20al20200820cyberpsychol20behav.pdf:pdf},
issn = {10949313},
journal = {Cyberpsychology and Behavior},
mendeley-groups = {DepthPerceptionPapers},
number = {1},
pages = {9--15},
title = {{Depth perception in virtual reality: Distance estimations in peri- and extrapersonal space}},
volume = {11},
year = {2008}
}

@article{Ping2020,
abstract = {The virtual reality (VR) and augmented reality (AR) applications have been widely used in a variety of fields},
annote = {Ok so this is an important study to add in

It has a ton of information and it is one of only a few papers that look at AR vs VR},
author = {Ping, Jiamin and Weng, Dongdong and Liu, Yue and Wang, Yongtian},
doi = {10.1002/jsid.840},
file = {:C$\backslash$:/Users/admin/Downloads/jsid.840.pdf:pdf},
issn = {1071-0922},
journal = {Journal of the Society for Information Display},
keywords = {Augmented Reality,Depth Cues,Depth Matching,Depth Perception,Virtual Reality},
mendeley-groups = {DepthPerceptionPapers},
number = {2},
pages = {164--176},
title = {{Depth perception in shuffleboard: Depth cues effect on depth perception in virtual and augmented reality system}},
volume = {28},
year = {2020}
}

@misc{CThru,
    author = "Quake Technologies",
    title = "Quake Technologies - CThru",
    url  = "https://www.qwake.tech/",
    addendum = "(accessed: 01.04.2020)",
    keywords = "FireFighters, Augmented Reality, Startup"
}

@article{Kameda2004,
abstract = {This paper presents a new outdoor mixed-reality system designed for people who carry a camera-attached small handy device in an outdoor scene where a number of surveillance cameras are embedded. We propose a new functionality in outdoor mixed reality that the handy device can display live status of invisible areas hidden by some structures such as buildings, walls, etc. The function is implemented on a camera-attached, small handy subnotebook PC (HPC). The videos of the invisible areas are taken by surveillance cameras and they are precisely overlapped on the video of HPC camera, hence a user can notice objects in the invisible areas and see directly what the objects do. We utilize surveillance cameras for two purposes. (1) They obtain videos of invisible areas. The videos are trimmed and warped so as to impose them into the video of the HPC camera. (2) They are also used for updating textures of calibration markers in order to handle possible texture changes in real outdoor world. We have implemented a preliminary system with four surveillance cameras and proved that our system can visualize invisible areas in real time. {\textcopyright} 2004 IEEE.},
annote = {An example of X-ray for security

Need to read},
author = {Kameda, Yoshinari and Takemasa, Taisuke and Ohta, Yuichi},
doi = {10.1109/ISMAR.2004.45},
file = {:C$\backslash$:/Users/admin/Pictures/ISMAR.2004.45.pdf:pdf},
isbn = {0769521916},
journal = {ISMAR 2004: Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
mendeley-groups = {X-RayVision},
number = {Ismar},
pages = {151--160},
title = {{Outdoor see-through vision utilizing surveillance cameras}},
year = {2004}
}


@article{Livingston2011,
abstract = {Designing a user interface for military situation awareness presents challenges for managing information in a useful and usable manner. We present an integrated set of functions for the presentation of and interaction with information for a mobile augmented reality application for military applications. Our research has concentrated on four areas. We filter information based on relevance to the user (in turn based on location), evaluate methods for presenting information that represents entities occluded from the user's view, enable interaction through a top-down map view metaphor akin to current techniques used in the military, and facilitate collaboration with other mobile users and/or a command center. In addition, we refined the user interface architecture to conform to requirements from subject matter experts. We discuss the lessons learned in our work and directions for future research. {\textcopyright} 2010 Springer-Verlag (outside the USA).},
author = {Livingston, Mark A. and Ai, Zhuming and Karsch, Kevin and Gibson, Gregory O.},
doi = {10.1007/s10055-010-0179-1},
file = {:C$\backslash$:/Users/admin/Downloads/Livingston2011{\_}Article{\_}UserInterfaceDesignForMilitary.pdf:pdf},
isbn = {1005501001791},
issn = {13594338},
journal = {Virtual Reality},
keywords = {Augmented reality,Evaluation,Interaction,Mobile systems,User interface},
number = {2-3},
pages = {175--184},
title = {{User interface design for military AR applications}},
volume = {15},
year = {2011}
}

@article{Sandor2010,
abstract = {In the past, several systems have been presented that enable users to view occluded points of interest using Augmented Reality X-ray visualizations. It is challenging to design a visualization that provides correct occlusions between occluder and occluded objects while maximizing legibility. We have previously published an Augmented Reality X-ray visualization that renders edges of the occluder region over the occluded region to facilitate correct occlusions while providing foreground context. While this approach is simple and works in a wide range of situations, it provides only minimal context of the occluder object. In this paper, we present the background, design, and implementation of our novel visualization technique that aims at providing users with richer context of the occluder object. While our previous visualization only employed one salient feature (edges) to determine which parts of the occluder to display, our novel visualization technique is an initial attempt to explore the design space of employing multiple salient features for this task. The prototype presented in this paper employs three additional salient features: hue, luminosity, and motion. We have conducted two evaluations with human participants to investigate the benefits and limitations of our prototype compared to our previous system. The first evaluation showed that although our novel visualization provides a richer context of the occluder object, it does not impede users to select objects in the occluded area; but, it also indicated problems in our prototype. In the second evaluation, we have investigated these problems through an online survey with systematically varied occluder and occluded scenes, focussing on the qualitative aspects of our visualizations. The results were encouraging, but pointed out that our novel visualization needs a higher level of adaptiveness. {\textcopyright}2010 IEEE.},
author = {Sandor, Christian and Cunningham, Andrew and Dey, Arindam and Mattila, Ville Veikko},
doi = {10.1109/ISMAR.2010.5643547},
file = {:C$\backslash$:/Users/admin/Downloads/paper.pdf:pdf},
isbn = {9781424493449},
journal = {9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings},
keywords = {Augmented reality,Augmented reality X-ray,Evaluation,Saliency,Visualization},
mendeley-groups = {X-RayVision},
pages = {27--36},
title = {{An augmented reality X-ray system based on visual saliency}},
year = {2010}
}

@article{Blum2012,
abstract = {This paper describes first steps towards a Superman-like X-ray vision where a brain-computer interface (BCI) device and a gaze-tracker are used to allow the user controlling the augmented reality (AR) visualization. A BCI device is integrated into two medical AR systems. To assess the potential of this technology first feedback from medical doctors is gathered. While in this pilot study not the full range of available signals but only electromyographic signals are used, the medical doctors provided very positive feedback on the use of BCI for medical AR. {\textcopyright} 2012 IEEE.},
author = {Blum, Tobias and Stauder, Ralf and Euler, Ekkehard and Navab, Nassir},
doi = {10.1109/ISMAR.2012.6402569},
file = {:C$\backslash$:/Users/admin/Downloads/06402569.pdf:pdf},
isbn = {9781467346603},
journal = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
keywords = {H.5.1 [Information Interfaces and Presentation]: A},
mendeley-groups = {X-RayVision},
pages = {271--272},
publisher = {IEEE},
title = {{Superman-like X-ray vision: Towards brain-computer interfaces for medical augmented reality}},
year = {2012}
}

@Article{Sielhorst2006,
  author          = {Sielhorst, Tobias and Bichlmeier, Christoph and Heining, Sandro Michael and Navab, Nassir},
  title           = {{Depth perception - A major issue in medical AR: Evaluation study by twenty surgeons}},
  doi             = {10.1007/11866565_45},
  issn            = {16113349},
  pages           = {364--372},
  volume          = {4190 LNCS},
  abstract        = {The idea of in-situ visualization for surgical procedures has been widely discussed in the community [1, 2, 3, 4]. While the tracking technology offers nowadays a sufficient accuracy and visualization devices have been developed that fit seamlessly into the operational workflow [1, 3], one crucial problem remains, which has been discussed already in the first paper on medical augmented reality [4]. Even though the data is presented at the correct place, the physician often perceives the spatial position of the visualization to be closer or further because of virtual/real overlay. This paper describes and evaluates novel visualization techniques that are designed to overcome misleading depth perception of trivially superimposed virtual images on the real view. We have invited 20 surgeons to evaluate seven different visualization techniques using a head mounted display (HMD). The evaluation has been divided into two parts. In the first part, the depth perception of each kind of visualization is evaluated quantitatively. In the second part, the visualizations are evaluated qualitatively in regard to user friendliness and intuitiveness. This evaluation with a relevant number of surgeons using a state-of-the-art system is meant to guide future research and development on medical augmented reality. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
  annote          = {This paper was one of the first to look at if AR could be used for surgical guidance.},
  file            = {:C$\backslash$:/Users/admin/Downloads/Sielhorst2006{\_}Chapter{\_}DepthPerceptionAMajorIssueInMe.pdf:pdf},
  isbn            = {3540447075},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  mendeley-groups = {DepthPerceptionPapers},
  year            = {2006},
}

@article{Messing2005,
abstract = {Can distance perception be studied using virtual reality (VR) if distances are systematically underestimated in VR head-mounted displays (HMDs)? In an experiment in which a real environment was observed through an HMD, via live video, distances, as measured by visually directed walking, were underestimated even when the perceived environment was known to be real and present. However, the underestimation was linear, which means that higher-order space perception effects might be preserved in VR. This is illustrated in a second experiment, in which the visual horizon was artificially manipulated in a simulated outdoor field presented in immersive VR. As predicted by the claim that angle of declination from the horizon may serve as a strong cue to distance, lowering the horizon line produced “expansive” judgments of distance (power function exponents greater than one) both in verbal and in motor estimates. {\textcopyright} 2005, ACM. All rights reserved.},
author = {Messing, Ross and Durgin, Frank H.},
doi = {10.1145/1077399.1077403},
file = {:C$\backslash$:/Users/admin/Downloads/MessingDurgin2005.pdf:pdf},
issn = {15443965},
journal = {ACM Transactions on Applied Perception},
keywords = {Experimentation,Human Factors,Virtual reality (VR),distance perception,head-mounted displays (HMD),space perception},
mendeley-groups = {DepthPerceptionPapers},
number = {3},
pages = {234--250},
title = {{Distance Perception and the Visual Horizon in Head-Mounted Displays}},
volume = {2},
year = {2005}
}

@article{Aaskov2019,
abstract = {Objective: Since the discovery of ionizing radiation, clinicians have evaluated X-ray images separately from the patient. The objective of this study was to investigate the accuracy and repeatability of a new technology which seeks to resolve this historic limitation by projecting anatomically correct X-ray images on to a person's skin. Methods: A total of 13 participants enrolled in the study, each having a pre-existing anteroposterior lumbar X-ray. Each participant's image was uploaded into the Hololens Mixed reality system which when worn, allowed a single examiner to view a participant's own X-ray superimposed on the participant's back. The projected image was topographically corrected using depth information obtained by the Hololens system then aligned via existing anatomic landmarks. Using this superimposed image, vertebral levels were identified and validated against spinous process locations obtained by ultrasound. This process was repeated 1–5 days later. The projection of each vertebra was deemed to be “on-target” if it fell within the known morphological dimensions of the spinous process for that specific vertebral level. Results: The projection system created on-target projections with respect to individual vertebral levels 73{\%} of the time with no significant difference seen between testing sessions. The average repeatability for all vertebral levels between testing sessions was 77{\%}. Conclusion: These accuracy and repeatability data suggest that the accuracy and repeatability of projecting X-rays directly on to the skin is feasible for identifying underlying anatomy and as such, has potential to place radiological evaluation within the patient context. Future opportunities to improve this procedure will focus on mitigating potential sources of error.},
annote = {This paper superimposes a 2D view of a Xray on a human back using hololens.

Issues with this seemed to be based around the fact that people don't all have the same anatomy and the distortions aren't always correct.},
author = {Aaskov, Jacob and Kawchuk, Gregory N. and Hamaluik, Kenton D. and Boulanger, Pierre and Hartvigsen, Jan},
doi = {10.7717/peerj.6333},
file = {:C$\backslash$:/Users/admin/Downloads/peerj-6333.pdf:pdf},
issn = {21678359},
journal = {PeerJ},
keywords = {Heads up display,Mixed reality,Spine,X-ray},
mendeley-groups = {X-RayVision},
number = {2},
pages = {1--11},
title = {{X-ray vision: The accuracy and repeatability of a technology that allows clinicians to see spinal X-rays superimposed on a person's back}},
volume = {2019},
year = {2019}
}

@article{Tanagho2012,
abstract = {Introduction: We compared the impact of two-dimensional (2D) versus three-dimensional (3D) visualization on both objective and subjective measures of laparoscopic performance using the validated Fundamentals of Laparoscopic Surgery (FLS) skill set. Subjects and Methods: Thirty-three individuals with varying laparoscopic experience completed three essential drills from the FLS skill set (peg transfer, pattern cutting, and suturing/knot tying) in both 2D and 3D. Participants were randomized to begin all tasks in either 2D or 3D. Time to completion and number of attempts required to achieve proficiency were measured for each task. Errors were also noted. Participants completed questionnaires evaluating their experiences with both visual modalities. Results: Across all tasks, greater speed was achieved in 3D versus 2D: peg transfer, 183.4 versus 245.6 seconds (P{\textless}.0001); pattern cutting, 167.7 versus 209.3 seconds (P=.004); and suturing/knot tying, 255.2 versus 329.5 seconds (P=.031). Fewer errors were committed in the peg transfer task in 3D versus 2D (P=.008). Fourteen participants required multiple attempts to achieve proficiency in one or more tasks in 2D, compared with 7 in 3D. Subjective measures of efficiency and accuracy also favored 3D visualization. The advantage of 3D vision persisted independent of participants' level of technical expertise (novice versus intermediate/expert). There were no differences in reported side effects between the two visual modalities. Overall, 87.9{\%} of participants preferred 3D visualization. Conclusions: Three-dimensional vision appears to greatly enhance laparoscopic proficiency based on objective and subjective measures. In our experience, 3D visualization produced no more eye strain, headaches, or other side effects than 2D visualization. Participants overwhelmingly preferred 3D visualization. {\textcopyright} Copyright 2012, Mary Ann Liebert, Inc. 2012.},
annote = {This research showed overwhealming positive results for the research.},
author = {Tanagho, Youssef S. and Andriole, Gerald L. and Paradis, Alethea G. and Madison, Kerry M. and Sandhu, Gurdarshan S. and Varela, J. Esteban and Benway, Brian M.},
doi = {10.1089/lap.2012.0220},
file = {:C$\backslash$:/Users/admin/Downloads/2D versus 3D visualization{\_} Impact on laparoscopic proficiency us.pdf:pdf},
issn = {10926429},
journal = {Journal of Laparoendoscopic and Advanced Surgical Techniques},
mendeley-groups = {3Dvs2DMedicalVis},
number = {9},
pages = {865--870},
title = {{2D versus 3D visualization: Impact on laparoscopic proficiency using the fundamentals of laparoscopic surgery skill set}},
volume = {22},
year = {2012}
}

@article{Booij2019,
abstract = {To assess the accuracy of a 3D camera for body contour detection and patient positioning in CT compared to routine manual positioning by radiographers.},
author = {Booij, Ronald and Budde, Ricardo P J and Dijkshoorn, Marcel L and van Straten, Marcel},
doi = {10.1007/s00330-018-5745-z},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Booij et al. - 2019 - Accuracy of automated patient positioning in CT using a 3D camera for body contour detection.pdf:pdf},
issn = {1432-1084},
journal = {European Radiology},
mendeley-groups = {Directly Related to Project at Siemens},
month = {apr},
number = {4},
pages = {2079--2088},
title = {{Accuracy of automated patient positioning in CT using a 3D camera for body contour detection}},
url = {https://doi.org/10.1007/s00330-018-5745-z},
volume = {29},
year = {2019}
}


@article{Ravishankar2017,
author = {Ma, Kai and Jiangping, Wang and Singh, Vivek and Tamersoy, Birgi and Chang, Yao-Jen and Wimmer, Andreas and Chen, Terrence},
doi = {10.1007/978-3-319-66182-7},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravishankar et al. - 2017 - Learning and Incorporating Shape Models.pdf:pdf},
isbn = {9783319661827},
journal = {Miccai 2017},
mendeley-groups = {Directly Related to Project at Siemens},
number = {2},
pages = {203--211},
title = {{Learning and Incorporating Shape Models}},
year = {2017}
}

@article{Salehi2017,
author = {Salehi, Mehrdad and Prevost, Raphael and Moctezuma, Jos{\'{e}}-Luis and Navab, Nassir and Wein, Wolfgang},
doi = {10.1007/978-3-319-66185-8},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salehi et al. - 2017 - Precise Ultrasound Bone Registration with Learning-Based Segmentation and Speed.pdf:pdf},
isbn = {9783319661858},
journal = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
mendeley-groups = {Directly Related to Project at Siemens},
number = {November},
pages = {682--690},
title = {{Precise Ultrasound Bone Registration with Learning-Based Segmentation and Speed}},
year = {2017}
}

@misc{poser_2019, title={The Premier 3D Rendering and Animation Software}, url={https://www.posersoftware.com/}, journal={Poser}, publisher={Poser}, year={2019}, month={Jan}}


@article{Anguelov2005,
abstract = {We introduce the SCAPE method (Shape Completion and Animation for PEople) - a data-driven method for building a human shape model that spans variation in both subject shape and pose. The method is based on a representation that incorporates both articulated and non-rigid deformations. We learn a pose deformation model that derives the non-rigid surface deformation as a function of the pose of the articulated skeleton. We also learn a separate model of variation based on body shape. Our two models can be combined to produce 3D surface models with realistic muscle deformation for different people in different poses, when neither appear in the training set. We show how the model can be used for shape completion - generating a complete surface mesh given a limited set of markers specifying the target shape. We present applications of shape completion to partial view completion and motion capture animation. In particular, our method is capable of constructing a high-quality animated surface model of a moving person, with realistic muscle deformation, using just a single static scan and a marker motion capture sequence of the person. Copyright {\textcopyright} 2005 by the Association for Computing Machinery, Inc.},
author = {Anguelov, Dragomir and Srinivasan, Praveen and Koller, Daphne and Thrun, Sebastian and Rodgers, Jim and Davis, James},
doi = {10.1145/1073204.1073207},
file = {:C$\backslash$:/Users/admin/Downloads/anguelov.shapecomp.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {Animation,Deformations,Morphing,Synthetic actors},
mendeley-groups = {Directly Related to Project at Siemens},
number = {3},
pages = {408--416},
title = {{SCAPE: Shape Completion and Animation of People}},
volume = {24},
year = {2005}
}

@article{Robinette2002,
abstract = {The Civilian Americana and European Surface Anthropometry Resource (CAESAR) project was a survey of the civilian populations of three countries representing the North Atlantic Treaty Organization (NATO) countries: The United States of America (USA), The Netherlands, and Italy (Robinette et al. 1999, Robinette 2000). One site in Ottawa, Canada was added to the USA sample and it is henceforth referred to as the North American sample. The survey was carried out by the U.S. Air Force, with the of 1) the contractor, Sytronics Inc., 2) The Netherlands Organization for Applied Scientific Research (TNO), 3) the subcontractor D'Appolonia in Italy, and 4) a consortium of companies under the umbrella of the Society of automotive Engineers (SAB).},
author = {Robinette, Kathleen and Blackwell, Sherri and Daanen, Hein and Boehmer, Mark and Fleming, Scott},
file = {:C$\backslash$:/Users/admin/Downloads/Civilian{\_}American{\_}and{\_}European{\_}Surface{\_}Anthropomet.pdf:pdf},
month = {jun},
pages = {74},
title = {{Civilian American and European Surface Anthropometry Resource (CAESAR), Final Report. Volume 1. Summary}},
year = {2002}
}


@Misc{spatial_anchors_azure_2020,
  title     = {Azure Spatial Anchors: Microsoft Azure},
  url       = {https://azure.microsoft.com/en-au/services/spatial-anchors/},
  journal   = {Azure Spatial Anchors | Microsoft Azure},
  publisher = {Microsoft},
  year      = {2020},
}


@misc{vuforia_2020, title={Vuforia Developer Portal}, url={https://developer.vuforia.com/}, journal={Vuforia Developer Portal |}, publisher={Vuforia}, year={2020}}

@article{Besl1992,
abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of 'shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces.{\textgreater}},
author = {Besl, P J and Mckay, Neil D},
doi = {10.1109/34.121791},
file = {:C$\backslash$:/Users/admin/Downloads/00121791.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Computer Science,Convergence,Engineering,Inspection,Iterative Algorithms,Iterative Closest Point Algorithm,Iterative Methods,Motion Estimation,Quaternions,Shape Measurement,Solid Modeling,Testing},
mendeley-groups = {ContouringPapers},
number = {2},
pages = {239--256},
title = {{A method for registration of 3-D shapes}},
volume = {14},
year = {1992}
}

@misc{sccn_2020, title={sccn/labstreaminglayer}, url={https://github.com/sccn/labstreaminglayer}, journal={GitHub - sccn / labstreaminglayer}, publisher={GitHub}, author={Sccn}, year={2020}, month={Apr}}


@article{Kim2018,
annote = {A overview of all the research conducted within the last decade (since 2008) in the AR VR type feilds.},
author = {Kim, K and Billinghurst, M and Bruder, G and Duh, H B and Welch, G F},
doi = {10.1109/TVCG.2018.2868591},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2018 - Revisiting Trends in Augmented Reality Research A Review of the 2nd Decade of ISMAR (2008–2017).pdf:pdf},
issn = {1077-2626 VO - 24},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Augmented reality,Calibration,ISMAR conferences,ISMAR publications,Indexes,Industries,International Symposium on Mixed and Augmented Rea,Market research,Rendering (computer graphics),Sensors,augmented reality,mixed reality,rendering,rendering (computer graphics),survey,trends},
number = {11},
pages = {2947--2962},
title = {{Revisiting Trends in Augmented Reality Research: A Review of the 2nd Decade of ISMAR (2008–2017)}},
volume = {24},
year = {2018}
}


@article{Zhang2012,
abstract = {Recent advances in 3D depth cameras such as Microsoft Kinect sensors (www.xbox.com/en-US/kinect) have created many opportunities for multimedia computing. The Kinect sensor lets the computer directly sense the third dimension (depth) of the players and the environment. It also understands when users talk, knows who they are when they walk up to it, and can interpret their movements and translate them into a format that developers can use to build new experiences. While the Kinect sensor incorporates several advanced sensing hardware, this article focuses on the vision aspect of the Kinect sensor and its impact beyond the gaming industry. {\textcopyright} 2012 IEEE.},
author = {Zhang, Zhengyou},
doi = {10.1109/MMUL.2012.24},
file = {:C$\backslash$:/Users/admin/Downloads/Microsoft20Kinect20Sensor20and20Its20Effect20-20IEEE20MM202012.pdf:pdf},
issn = {1070986X},
journal = {IEEE Multimedia},
keywords = {Microsoft Kinect,computer vision,human-computer interaction,motion capture,multimedia},
mendeley-groups = {AugmentedRealityForRealWorldUse},
number = {2},
pages = {4--10},
title = {{Microsoft kinect sensor and its effect}},
volume = {19},
year = {2012}
}

@article{Bane2004,
abstract = {This paper presents a set of interactive tools designed to give users virtual x-ray vision. These tools address a common problem in depicting occluded infrastructure: either too much information is displayed, confusing users, or too little information is displayed, depriving users of important depth cues. Four tools are presented: the tunnel tool and room selector tool directly augment the user's view of the environment, allowing them to explore the scene in direct, first person view. The room in miniature tool allows the user to select and interact with a room from a third person perspective, allowing users to view the contents of the room from points of view that would normally be difficult or impossible to achieve. The room slicer tool aids users in exploring volumetric data displayed within the room in miniature tool. Used together, the tools presented in this paper can be used to achieve the virtual x-ray vision effect. We test our prototype system in a far-field mobile augmented reality setup, visualizing the interiors of a small set of buildings on the UCSB campus.},
author = {Bane, R and Hollerer, T},
doi = {10.1109/ISMAR.2004.36},
file = {:C$\backslash$:/Users/admin/Downloads/01383060.pdf:pdf},
journal = {ISMAR 2004: Proceedings of the Third IEEE and ACM International Symposium on Mixed and Augmented Reality},
keywords = {Augmented Reality,Buildings,Data Security,Data Visualization,Floors,Layout,Prototypes,Solids,System Testing,Wireless Networks},
mendeley-groups = {X-RayVision},
pages = {231--239},
title = {{Interactive tools for virtual x-ray vision in mobile augmented reality}},
year = {2004}
}


@inproceedings{Coffin2006,
abstract = {We present a technique that allows a user to look beyond occluding objects in arbitrary 3D graphics scenes. In order to control this form of virtual x-ray vision, the user interactively cuts holes into the occluding geometry. The user can rapidly define a cutout shape or choose a standard shape and sweep it over the occluding wall segments to reveal what lies behind them. Holes are rendered in the correct 3D perspective as if they were actually cut into the obstructing geometry, including border regions that give the cutout shape physical depth, simulating penetration of a physical wall that possesses some generic thickness.},
author = {Coffin, C and Hollerer, T},
booktitle = {3D User Interfaces (3DUI'06)},
doi = {10.1109/VR.2006.88},
file = {:C$\backslash$:/Users/admin/Downloads/01647502.pdf:pdf},
isbn = {VO -},
keywords = {Computational modeling,Computer graphics,Computer science,Cut-away view,Information geometry,Layout,Lenses,Rendering (computer graphics),Shape,Switches,User interfaces,interactive cutout,x-ray vision},
mendeley-groups = {X-RayVision},
pages = {25--28},
title = {{Interactive Perspective Cut-away Views for General 3D Scenes}},
year = {2006}
}


@article{Kalkofen2013,
abstract = {In Augmented Reality (AR), ghosted views allow a viewer to explore hidden structure within the real-world environment. A body of previous work has explored which features are suitable to support the structural interplay between occluding and occluded elements. However, the dynamics of AR environments pose serious challenges to the presentation of ghosted views. While a model of the real world may help determine distinctive structural features, changes in appearance or illumination detriment the composition of occluding and occluded structure. In this paper, we present an approach that considers the information value of the scene before and after generating the ghosted view. Hereby, a contrast adjustment of preserved occluding features is calculated, which adaptively varies their visual saliency within the ghosted view visualization. This allows us to not only preserve important features, but to also support their prominence after revealing occluded structure, thus achieving a positive effect on the perception of ghosted views. {\textcopyright} 2013 IEEE.},
author = {Kalkofen, Denis and Veas, Eduardo and Zollmann, Stefanie and Steinberger, Markus and Schmalstieg, Dieter},
doi = {10.1109/ISMAR.2013.6671758},
file = {:C$\backslash$:/Users/admin/Downloads/Adaptive{\_}ghosted{\_}views{\_}for{\_}Augmented{\_}Rea.pdf:pdf},
isbn = {9781479928699},
journal = {2013 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2013},
keywords = {H.5.1 [Information Interfaces and Presentation],Multimedia Information Systems-Artificial, Augmented, and Virtual Realities},
mendeley-groups = {X-RayVision},
number = {c},
pages = {1--9},
title = {{Adaptive ghosted views for Augmented Reality}},
volume = {1},
year = {2013}
}


@article{Uekusa2015,
abstract = {We describe a spatial augmented reality system that enables superimposed projection of an internal image on a real object with color correction. Our system is a projector-camera system, which consists of a camera, a projector, and a PC. At first, we generate a first projection image from the internal image of CG and a camera image of the real object captured by the camera. Next, we project the first projection image on the real object, and again capture an image of the real object with the internal image. At last, we update the projection image with color correction on CIELUV color space and project the image on the real object. This system will be able to visualize the internal structures on various objects easily.},
author = {Uekusa, Naoto and Koike, Takafumi},
doi = {10.1145/2735711.2735810},
file = {:C$\backslash$:/Users/admin/Downloads/2735711.2735810.pdf:pdf},
isbn = {9781450333498},
journal = {ACM International Conference Proceeding Series},
keywords = {Ghosted view,Image processing,Projector-camera system,Spatial augmented reality},
mendeley-groups = {X-RayVision},
pages = {189--190},
title = {{Superimposed projection of ghosted view on real object with color correction}},
volume = {11},
year = {2015}
}

@article{Fukiage2014,
abstract = {There are many situations in which virtual objects are presented half-transparently on a background in real time applications. In such cases, we often want to show the object with constant visibility. However, using the conventional alpha blending, visibility of a blended object substantially varies depending on colors, textures, and structures of the background scene. To overcome this problem, we present a framework for blending images based on a subjective metric of visibility. In our method, a blending parameter is locally and adaptively optimized so that visibility of each location achieves the targeted level. To predict visibility of an object blended by an arbitrary parameter, we utilize one of the error visibility metrics that have been developed for image quality assessment. In this study, we demonstrated that the metric we used can linearly predict visibility of a blended pattern on various texture images, and showed that the proposed blending methods can work in practical situations assuming augmented reality.},
author = {Fukiage, Taiki and Oishi, Takeshi and Ikeuchi, Katsushi},
doi = {10.1109/ISMAR.2014.6948410},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fukiage, Oishi, Ikeuchi - 2014 - Visibility-based blending for real-time applications.pdf:pdf},
isbn = {9781479961849},
journal = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
keywords = {Blending,Human visual system model,Visibility},
mendeley-groups = {X-RayVision},
number = {September},
pages = {63--72},
publisher = {IEEE},
title = {{Visibility-based blending for real-time applications}},
year = {2014}
}


@article{Otsuki2017,
abstract = {Detecting and measuring emotional responses while interacting with virtual reality (VR), and assessing and interpreting their impacts on human engagement and ‘‘immersion,'' are both academically and technologically challenging. While many researchers have, in the past, focused on the affective evaluation of passive environments, such as listening to music or the observation of videos and imagery, virtual realities and related interactive environments have been used in only a small number of research studies as a mean of presenting emotional stimuli. This article reports the first stage (focusing on participants' subjective responses) of a range of experimental investigations supporting the evaluation of emotional responses within a virtual environment, according to a three-dimensional (Valence, Arousal, and Dominance) model of affects, developed in the 1970s and 1980s. To populate this three-dimensional model with participants' emotional responses, an ‘‘affective VR,'' capable of manipulating users' emotions, has been designed and subjectively evaluated. The VR takes the form of a dynamic ‘‘speedboat'' simulation, elements (controllable VR parameters) of which were assessed and selected based on a 35-respondent online survey, coupled with the implementation of an affective power approximation algorithm. A further 68 participants took part in a series of trials, interacting with a number of VR variations, while subjectively rating their emotional responses. The experimental results provide an early level of confidence that this particular affective VR is capable of manipulating individuals' emotional experiences, through the control of its internal parameters. Moreover, the approximation technique proved to be fairly reliable in predicting users' potential emotional responses, in various affective VR settings, prior to actual experiences. Finally, the analysis suggested that the emotional response of the users, with different gender and gaming experiences, could vary, when presented with the same affective VR situation.},
author = {Otsuki, Mai and Milgram, Paul and Chellali, Ryad},
doi = {10.1162/PRES},
file = {:C$\backslash$:/Users/admin/Downloads/PRES{\_}a{\_}00286.pdf:pdf},
isbn = {9780199644469},
issn = {15313263},
journal = {Presence: Teleoperators {\&} Virtual Environments},
mendeley-groups = {X-RayVision},
number = {1},
pages = {42--65},
pmid = {94301451},
title = {{Use of Random Dot Patterns in Achieving X-Ray Vision for Near-Field Applications of Stereoscopic Video-Based Augmented Reality Displays}},
url = {http://www.mitpressjournals.org/doi/pdf/10.1162/PRES\_a\_00135},
volume = {26},
year = {2017}
}

@article{Iwai2006,
abstract = {We propose Limpid Desk which supports document search on a real desktop with virtual transparentizing of the upper layer of a document stack in projection-based mixed reality (MR)environments. In the system, users can visually access a lower layer document without physically removing the upper documents. This is accomplished by projecting a special pattern of light that is calculated to compensate the appearances of the upper layer documents as if they are transparent. In addition, we propose two types of visual effects for users to cognize the progress of the transparentizing of real documents and to recognize the layer number of the virtually exposing document, and excecute psychological tests to confirm the intuitiveness of these effects. This paper also presents an intuitive document search interaction in the proposed system. Copyright 2006 ACM.},
author = {Iwai, Daisuke and Sato, Kosuke},
doi = {10.1145/1180495.1180519},
file = {:C$\backslash$:/Users/admin/Downloads/1180495.1180519.pdf:pdf},
isbn = {1595933212},
journal = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
keywords = {Color reflectance compensation,Projection-based mixed reality,Smart desk,Transparentizing documents},
mendeley-groups = {X-RayVision},
pages = {112--115},
title = {{Limpid desk: See-through access to disorderly desktop in projection-based mixed reality}},
year = {2006}
}

@article{Yasuda2012,
abstract = {This study specifically examines wall see-through visualization for drivers at blind corners to prevent crossing collisions. We believe that realizing the desired effect with the simplest visualization is a key to building practical systems, although previous studies mainly targeted rich visualization as if the wall were actually transparent. We compared several visualization levels using qualitative and quantitative measures based on performance of the driver's collision estimation and the meaning assignment to visual stimuli. The results revealed that displaying only the direction of the obscured vehicle by a small circle is sufficient for collision estimation, although it was perceived as less informative. We also obtained a preliminary result indicating that the meaning assignment performance is significantly lower in a peripheral region of the driver's view. Although both collision estimation and meaning assignment performance are necessary for building an effective system, these results clarify that future studies must specifically examine the meaning assignment performance of the stimuli. {\textcopyright} 2012 IEEE.},
author = {Yasuda, Hiroshi and Ohama, Yoshihiro},
doi = {10.1109/ISMAR.2012.6402600},
file = {:C$\backslash$:/Users/admin/Downloads/06402600.pdf:pdf},
isbn = {9781467346603},
journal = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
keywords = {active safety,advanced driver assistance systems,peripheral vision,wall see-through,x-ray vision},
mendeley-groups = {X-RayVision},
pages = {333--334},
publisher = {IEEE},
title = {{Toward a practical wall see-through system for drivers: How simple can it be?}},
year = {2012}
}

@article{Santos2015,
abstract = {Mobile augmented reality is a next-generation interface for seamless ubiquitous learning. It offers many novel interactions that enable the visualization of digital information on real places and objects. These interactions require user-based testing for suitability in educational settings. One important interaction is augmented reality X-ray—providing an illusion to look inside objects. In this chapter, we implement augmented reality X-ray on a tablet computer by modifying the live video feed with computer graphics. Then, we evaluated our prototype based on the students' perception of depth, legibility, and realism. Results show that augmented reality X-ray hampers legibility. However, it does not have a significant impact on the perception of depth and realism. In our interviews with teachers, we found that augmented reality X-ray is perceived to be useful because it promotes learning by experience. It has the potential to improve both student attention and motivation. However, the teachers require a high-quality lesson plan, and extra training to use augmented reality X-ray in the classroom effectively.},
author = {Santos, Marc Ericson C. and Terawaki, Mitsuaki and Taketomi, Takafumi and Yamamoto, Goshiro and Kato, Hirokazu},
doi = {10.1007/978-3-662-44447-4_11},
file = {:C$\backslash$:/Users/admin/Downloads/Santos2015{\_}Chapter{\_}DevelopmentOfHandheldAugmented.pdf:pdf},
isbn = {9783662444474},
issn = {21964971},
journal = {Lecture Notes in Educational Technology},
keywords = {Augmented reality X-ray,Contextual visualization,Mobile augmented reality,Prototype evaluation,User-based studies},
mendeley-groups = {X-RayVision},
number = {9783662444467},
pages = {199--219},
title = {{Development of handheld augmented reality X-Ray for K-12 settings}},
year = {2015}
}

@misc{pokemongo, title={Pokemon Go}, url={https://pokemongolive.com/en/}, journal={Pokémon GO}, publisher={NIANTIC}}

@misc{microsofthololens, title={Microsoft HoloLens: Mixed Reality Technology for Business}, url={https://www.microsoft.com/en-us/hololens}, journal={Microsoft HoloLens | Mixed Reality Technology for Business}, publisher={Microsoft}}

@article{Tuthill2018,
abstract = {Although familiar to each of us, the sensation of inhabiting a body is ineffable. Traditional senses like vision and hearing monitor the external environment, allowing humans to have shared sensory experiences. But proprioception, the sensation of body position and movement, is fundamentally personal and typically absent from conscious perception. Nonetheless, this ‘sixth sense' remains critical to human experience, a fact that is most apparent when one considers those who have lost it. Take, for example, the case of Ian Waterman who, at the age of 19, suffered a rare autoimmune response to a flu infection that attacked the sensory neurons from his neck down. This infection deprived him of the sense of position, movement and touch in his body. With this loss of feedback came a complete inability to coordinate his movements. While he could compel his muscles to contract, he lost the ability to orchestrate these actions into purposeful behaviors, in essence leaving him immobile, unable to stand, walk, or use his body to interact with the world. Only after years of dedicated training was he able to re-learn to move his body entirely under visual control. In this Primer, John Tuthill and Eiman Azim examine proprioceptive feedback systems in mammals and insects, and argue that functional parallels across species can reveal fundamental principles of sensorimotor control.},
author = {Tuthill, John C. and Azim, Eiman},
doi = {10.1016/j.cub.2018.01.064},
file = {:C$\backslash$:/Users/admin/Downloads/PIIS0960982218300976.pdf:pdf},
issn = {09609822},
journal = {Current Biology},
mendeley-groups = {HapticsRegardingMedicalImaging},
number = {5},
pages = {R194--R203},
title = {{Proprioception}},
volume = {28},
year = {2018}
}

@article{Lappin2006,
abstract = {What properties determine visually perceived space? We discovered that the perceived relative distances of familiar objects in natural settings depended in unexpected ways on the surrounding visual field. Observers bisected egocentric distances in a lobby, in a hallway, and on an open lawn. Three key findings were the following: (1) Perceived midpoints were too far from the observer, which is the opposite of the common foreshortening effect (2) This antiforeshortening constant error depended on the environmental setting-greatest in the lobby and hall but nonsignificant on the lawn. (3) Context also affected distance discrimination; variability was greater in the hall than in the lobby or on the lawn. A second experiment replicated these findings, using a method of constant stimuli. Evidently, both the accuracy and the precision of perceived distance depend on subtle properties of the surrounding environment. Copyright 2006 Psychonomic Society, Inc.},
author = {Lappin, Joseph S. and Shelton, Amy L. and Rieser, John J.},
doi = {10.3758/BF03208759},
file = {:C$\backslash$:/Users/admin/Downloads/Lappin2006{\_}Article{\_}EnvironmentalContextInfluences.pdf:pdf},
issn = {00315117},
journal = {Perception and Psychophysics},
mendeley-groups = {DepthPerceptionPapers},
number = {4},
pages = {571--581},
title = {{Environmental context influences visually perceived distance}},
volume = {68},
year = {2006}
}

@article{DePaolis2019,
abstract = {Minimally invasive techniques, such as laparoscopy and radiofrequency ablation of tumors, bring important advantages in surgery: by minimizing incisions on the patient's body, they can reduce the hospitalization period and the risk of postoperative complications. Unfortunately, they come with drawbacks for surgeons, who have a restricted vision of the operation area through an indirect access and 2D images provided by a camera inserted in the body. Augmented reality provides an “X-ray vision” of the patient anatomy thanks to the visualization of the internal organs of the patient. In this way, surgeons are free from the task of mentally associating the content from CT images to the operative scene. We present a navigation system that supports surgeons in preoperative and intraoperative phases and an augmented reality system that superimposes virtual organs on the patient's body together with depth and distance information. We implemented a combination of visual and audio cues allowing the surgeon to improve the intervention precision and avoid the risk of damaging anatomical structures. The test scenarios proved the good efficacy and accuracy of the system. Moreover, tests in the operating room suggested some modifications to the tracking system to make it more robust with respect to occlusions. [Figure not available: see fulltext.].},
annote = {This paper looks into a system tha is capable of overlaying medical data though a blackbox method in a sudo real life setting.},
author = {{De Paolis}, Lucio Tommaso and {De Luca}, Valerio},
doi = {10.1007/s11517-018-1929-6},
file = {:C$\backslash$:/Users/admin/Downloads/DePaolis-DeLuca2019{\_}Article{\_}AugmentedVisualizationWithDept.pdf:pdf},
issn = {17410444},
journal = {Medical and Biological Engineering and Computing},
keywords = {Augmented reality,Depth perception,Distance information,Image-guided surgery,Minimally invasive surgery},
mendeley-groups = {MedicalDataOverlay},
number = {5},
pages = {995--1013},
publisher = {Medical {\&} Biological Engineering {\&} Computing},
title = {{Augmented visualization with depth perception cues to improve the surgeon's performance in minimally invasive surgery}},
volume = {57},
year = {2019}
}

@article{Guha2017,
abstract = {Augmented reality (AR) superimposes computer-generated virtual objects onto the user's view of the real world. Among medical disciplines, neurosurgery has long been at the forefront of image-guided surgery, and it continues to push the frontiers of AR technology in the operating room. In this systematic review, we explore the history of AR in neurosurgery and examine the literature on current neurosurgical applications of AR. Significant challenges to surgical AR exist, including compounded sources of registration error, impaired depth perception, visual and tactile temporal asynchrony, and operator inattentional blindness. Nevertheless, the ability to accurately display multiple three-dimensional datasets congruently over the area where they are most useful, coupled with future advances in imaging, registration, display technology, and robotic actuation, portend a promising role for AR in the neurosurgical operating room.},
author = {Guha, Daipayan and Alotaibi, Naif M. and Nguyen, Nhu and Gupta, Shaurya and McFaul, Christopher and Yang, Victor X.D.},
doi = {10.1017/cjn.2016.443},
file = {:C$\backslash$:/Users/admin/Downloads/div-class-title-augmented-reality-in-neurosurgery-a-review-of-current-concepts-and-emerging-applications-div.pdf:pdf},
issn = {03171671},
journal = {Canadian Journal of Neurological Sciences},
keywords = {augmented reality,image guidance,neuronavigation,virtual reality},
mendeley-groups = {MedicalDataOverlay},
number = {3},
pages = {235--245},
title = {{Augmented Reality in Neurosurgery: A Review of Current Concepts and Emerging Applications}},
volume = {44},
year = {2017}
}

@article{Jurgaitis2008,
abstract = {Objective. To determine whether 2-dimensional or 3-dimensional hepatic visualization is better for the medical students to be used while studying the clinical hepatic anatomy. Material and methods. Twenty-nine patients who underwent surgical intervention due to focal hepatic pathology at the Department of General Surgery, University of Heidelberg, and at Clinics of Santari{\v{s}}kės, Vilnius University Hospital were included in the retrospective cohort study. Before the surgical intervention, the computed tomography (CT) liver scan and 3-dimensional (3D) hepatic visualization were performed. A total of 58 2-dimensional and 3-dimensional digital liver images, mixed up in random sequence not to follow each other with a specially designed questionnaire, were presented to the students of Faculty of Medicine, Vilnius University. Their aim was to determine tumor-affected liver segments, to plan which liver segments should be resected, and to predict anatomical difficulties for liver resection. Results were compared with the data of real operation. Results. The students achieved better results for tumor localization analyzing 3D liver images vs. CT scans. This was especially evident determining the localization of tumor in segments 5, 6, 7, and 8 (P{\textless}0.05). Furthermore, the results of proposed extent of liver resection have been found to be better with 3D visualization (mean±SD - 0.794±0.175) in comparison with CT scans (mean±SD - 0.670±0.200), (P{\textless}0.001). Conclusions. Computer-generated 3D visualizations of the liver images helped the medical students to determine the tumor localization and to plan the prospective liver resection operations more precisely comparing with 2D visualizations. Computer-generated 3D visualization should be used as a means of studying liver anatomy.},
annote = {So you only skim read this. 

But it found out that students have more luck with reconising a liver that is in 3D},
author = {Jurgaitis, Jonas and Pa{\v{s}}konis, Marius and Pivoriunas, Jonas and Martinaityte, Ieva and Ju{\v{s}}ka, Agnius and Jurgaitiene, Ruta and Samuilis, Arturas and Volf, Ivo and Sch{\"{o}}binger, Maks and Schemmer, Peter and Kraus, Thomas W. and Strupas, Kestutis},
doi = {10.3390/medicina44060056},
file = {:C$\backslash$:/Users/admin/Downloads/medicina-44-00428.pdf:pdf},
issn = {1010660X},
journal = {Medicina},
keywords = {3-dimensional visualization,Clinical hepatic anatomy,Computed tomography,Hepatic resection planning,Surgical education},
mendeley-groups = {3Dvs2DMedicalVis},
number = {6},
pages = {428--438},
title = {{The comparison of 2-dimensional with 3-dimensional hepatic visualization in the clinical hepatic anatomy education}},
volume = {44},
year = {2008}
}

@article{Boulic2010,
abstract = {We are interested in developing real-time applications such as games or virtual prototyping that take advantage of the user full-body input to control a wide range of entities, from a self-similar avatar to any type of animated characters, including virtual humanoids with differing size and proportions. The key issue is, as always in real-time interactions, to identify the key factors that should get computational resources for ensuring the best user interaction efficiency. For this reason we first recall the definition and scope of such essential terms as immersion and presence, while clarifying the confusion existing in the fields of Virtual Reality and Games. This is done in conjunction with a short literature survey relating our interaction efficiency goal to key inspirations and findings from the field of Action Neuroscience. We then briefly describe our full-body real-time postural control with proactive local collision avoidance. The concept of obstacle spherification is introduced both to reduce local minima and to decrease the user cognitive task while interacting in complex environments. Finally we stress the interest of the egocentric environment scaling so that the user egocentric space matches the one of a height-differing controlled avatar. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
author = {Boulic, Ronan and Maupu, Damien and Peinado, Manuel and Raunhardt, Daniel},
doi = {10.1007/978-3-642-16958-8_7},
file = {:C$\backslash$:/Users/admin/Downloads/Boulic2010{\_}Chapter{\_}SpatialAwarenessInFull-BodyImm.pdf:pdf},
isbn = {3642169570},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Spatial awareness,collision avoidance,immersion,presence},
mendeley-groups = {DepthPerceptionPapers},
pages = {59--69},
title = {{Spatial awareness in full-body immersive interactions: Where do we stand?}},
volume = {6459 LNCS},
year = {2010}
}


@article{Lindemann2011,
abstract = {In this paper, we present a user study in which we have investigated the influence of seven state-of-the-art volumetric illumination models on the spatial perception of volume rendered images. Within the study, we have compared gradient-based shading with half angle slicing, directional occlusion shading, multidirectional occlusion shading, shadow volume propagation, spherical harmonic lighting as well as dynamic ambient occlusion. To evaluate these models, users had to solve three tasks relying on correct depth as well as size perception. Our motivation for these three tasks was to find relations between the used illumination model, user accuracy and the elapsed time. In an additional task, users had to subjectively judge the output of the tested models. After first reviewing the models and their features, we will introduce the individual tasks and discuss their results. We discovered statistically significant differences in the testing performance of the techniques. Based on these findings, we have analyzed the models and extracted those features which are possibly relevant for the improved spatial comprehension in a relational task. We believe that a combination of these distinctive features could pave the way for a novel illumination model, which would be optimized based on our findings. {\textcopyright} 2011 IEEE.},
author = {Lindemann, Florian and Ropinski, Timo},
doi = {10.1109/TVCG.2011.161},
file = {:C$\backslash$:/Users/admin/Downloads/06064955.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Volumetric illumination,spatial comprehension,volume rendering},
mendeley-groups = {VoxelAlgorithms},
number = {12},
pages = {1922--1931},
publisher = {IEEE},
title = {{About the influence of illumination models on image comprehension in direct volume rendering}},
volume = {17},
year = {2011}
}


@article{Ooijen2003,
annote = {Notes that depth perception with surface rendering is better.},
author = {Ooijen, P M A Van},
file = {:C$\backslash$:/Users/admin/Downloads/ajr.180.1.1800223.pdf:pdf},
journal = {Computers in Radiology},
mendeley-groups = {VoxelAlgorithms},
number = {January},
pages = {223--226},
title = {{Using Electron Beam CT : Surface}},
year = {2003}
}

@article{Englund2016,
abstract = {Direct volume rendering (DVR) provides the possibility to visualize volumetric data sets as they occur in many scientific disciplines. A key benefit of DVR is that semi-transparency can be facilitated in order to convey the complexity of the visualized data. Unfortunately, semi-transparency introduces new challenges in spatial comprehension of the visualized data, as the ambiguities inherent to semi-transparent representations affect spatial comprehension. Accordingly, many visualization techniques have been introduced to enhance the spatial comprehension of DVR images. In this paper, we conduct a user evaluation in which we compare standard DVR with five visualization techniques which have been proposed to enhance the spatial comprehension of DVR images. In our study, we investigate the perceptual performance of these techniques and compare them against each other to find out which technique is most suitable for different types of data and purposes. In order to do this, a large-scale user study was conducted with 300 participants who completed a number of micro-tasks designed such that the aggregated feedback gives us insight on how well these techniques aid the end user to perceive depth and shape of objects. Within this paper we discuss the tested techniques, present the conducted study and analyze the retrieved results.},
author = {Englund, Rickard and Ropinski, Timo},
doi = {10.1145/3002151.3002164},
file = {:C$\backslash$:/Users/admin/Downloads/3002151.3002164.pdf:pdf},
isbn = {9781450345477},
journal = {SA 2016 - SIGGRAPH ASIA 2016 Symposium on Visualization},
keywords = {Depth perception,Transparency,Volume rendering},
mendeley-groups = {VoxelAlgorithms},
title = {{Evaluating the perception of semi-transparent structures in direct volume rendering techniques}},
year = {2016}
}

@misc{Collins2014,
abstract = {An active research objective in Computer Assisted Intervention (CAI) is to develop guidance systems to aid surgical teams in laparoscopic Minimal Invasive Surgery (MIS) using Augmented Reality (AR). This involves registering and fusing additional data from other modalities and overlaying it onto the laparoscopic video in realtime. We present the first AR-based image guidance system for assisted myoma localisation in uterine laparosurgery. This involves a framework for semi-automatically registering a pre-operative Magnetic Resonance Image (MRI) to the laparoscopic video with a deformable model. Although there has been several previous works involving other organs, this is the first to tackle the uterus. Furthermore, whereas previous works perform registration between one or two laparoscopic images (which come from a stereo laparoscope) we show how to solve the problem using many images (e.g. 20 or more), and show that this can dramatically improve registration. Also unlike previous works, we show how to integrate occluding contours as registration cues. These cues provide powerful registration constraints and should be used wherever possible. We present retrospective qualitative results on a patient with two myomas and quantitative semi-synthetic results. Our multi-image framework is quite general and could be adapted to improve registration in other organs with other modalities such as CT.},
author = {Collins, Toby and Pizarro, Daniel and Bartoli, Adrien and Canis, Michel and Bourdel, Nicolas},
doi = {10.1109/ISMAR.2014.6948434},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collins et al. - 2014 - Computer-Assisted Laparoscopic myomectomy by augmenting the uterus with pre-operative MRI data.pdf:pdf},
keywords = {Deformable Models,Laparoscopes,Magnetic Resonance Imaging,Solid Modeling,Surgery,Three-Dimensional Displays,Transforms},
pages = {243--248},
title = {{Computer-Assisted Laparoscopic myomectomy by augmenting the uterus with pre-operative MRI data}},
url = {http://igt.ip.uca.fr/{~}ab/Publications/Collins{\_}etal{\_}ISMAR14.pdf},
year = {2014}
}

@article{Gao2020,
abstract = {This paper focuses on how virtual objects' shadows as well as differences in alignment between virtual and real lighting influence distance perception in optical see-through (OST) augmented reality (AR). Four hypotheses are proposed: (H1) Participants underestimate distances in OST AR; (H2) Virtual objects' shadows improve distance judgment accuracy in OST AR; (H3) Shadows with different realism levels have different influence on distance perception in OST AR; (H4) Different levels of lighting misalignment between real and virtual lights have different influence on distance perception in OST AR scenes. Two experiments were designed with an OST head mounted display (HMD), the Microsoft HoloLens. Participants had to match the position of a virtual object displayed in the OST-HMD with a real target. Distance judgment accuracy was recorded under the different shadows and lighting conditions. The results validate hypotheses H2 and H4 but surprisingly showed no impact of the shape of virtual shadows on distance judgment accuracy thus rejecting hypothesis H3. Regarding hypothesis H1, we detected a trend toward underestimation; given the high variance of the data, more experiments are needed to confirm this result. Moreover, the study also reveals that perceived distance errors and completion time of trials increase along with targets' distance.},
author = {Gao, Yuan and Peillard, Etienne and Normand, Jean Marie and Moreau, Guillaume and Liu, Yue and Wang, Yongtian},
doi = {10.1002/jsid.832},
file = {:C$\backslash$:/Users/admin/Downloads/Gao{\_}et{\_}al-2020-Journal{\_}of{\_}the{\_}Society{\_}for{\_}Information{\_}Display.pdf:pdf},
issn = {19383657},
journal = {Journal of the Society for Information Display},
keywords = {augmented reality,distance perception,lighting coherence,optical see-through HMD,shadows},
mendeley-groups = {DepthPerceptionPapers},
number = {2},
pages = {117--135},
title = {{Influence of virtual objects' shadows and lighting coherence on distance perception in optical see-through augmented reality}},
url = {https://onlinelibrary.wiley.com/doi/full/10.1002/jsid.832},
volume = {28},
year = {2020}
}

@article{Swan2015,
abstract = {Many compelling augmented reality (AR) applications require users to correctly perceive the location of virtual objects, some with accuracies as tight as 1 mm. However, measuring the perceived depth of AR objects at these accuracies has not yet been demonstrated. In this paper, we address this challenge by employing two different depth judgment methods, perceptual matching and blind reaching, in a series of three experiments, where observers judged the depth of real and AR target objects presented at reaching distances. Our experiments found that observers can accurately match the distance of a real target, but when viewing an AR target through collimating optics, their matches systematically overestimate the distance by 0.5 to 4.0 cm. However, these results can be explained by a model where the collimation causes the eyes' vergence angle to rotate outward by a constant angular amount. These findings give error bounds for using collimating AR displays at reaching distances, and suggest that for these applications, AR displays need to provide an adjustable focus. Our experiments further found that observers initially reach ∼4 cm too short, but reaching accuracy improves with both consistent proprioception and corrective visual feedback, and eventually becomes nearly as accurate as matching.},
annote = {Notes that haptics could aid depth perception.

This paper notes a overestimation of 5mm near distance and 4 cm far distance.},
author = {Swan, J. Edward and Singh, Gurjot and Ellis, Stephen R.},
doi = {10.1109/TVCG.2015.2459895},
file = {:C$\backslash$:/Users/admin/Downloads/07164348.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Depth judgment,accommodation,augmented reality,blind reaching,perceptual matching,vergence},
mendeley-groups = {DepthPerceptionPapers},
number = {11},
pages = {1289--1298},
publisher = {IEEE},
title = {{Matching and Reaching Depth Judgments with Real and Augmented Reality Targets}},
volume = {21},
year = {2015}
}

@article{Ellis1998,
abstract = {We examined errors in the localization of nearby virtual objects presented via see-through helmet-mounted displays as a function of viewing conditions and scene content in four experiments using a total of 38 participants. Monocular, biocular, and stereoscopic presentation of the virtual objects, accommodation (required focus), participants? age, and the position of physical surfaces were examined. Nearby physical surfaces were found to introduce localization errors that differ depending on the other experimental factors. These errors apparently arise from the occlusion of the physical background by the optically superimposed virtual objects, but they are modified by participants? accommodative competence and specific viewing conditions. The apparent physical size and transparency of the virtual objects and physical surfaces, respectively, are influenced by their relative position when superimposed. The design implications of the findings are discussed in a concluding section. Head-mounted displays of virtual objects are currently being evaluated as aids for mechanical assembly and equipment maintenance. Other applications include telesurgery, surgical planning, telerobotics, and visualization aids for robotic programming.},
annote = {doi: 10.1518/001872098779591278},
author = {Ellis, Stephen R and Menges, Brian M},
doi = {10.1518/001872098779591278},
file = {:C$\backslash$:/Users/admin/Downloads/10.1.1.925.9245.pdf:pdf},
issn = {0018-7208},
journal = {Human Factors},
mendeley-groups = {DepthPerceptionPapers},
month = {sep},
number = {3},
pages = {415--431},
publisher = {SAGE Publications Inc},
title = {{Localization of Virtual Objects in the Near Visual Field}},
url = {https://doi.org/10.1518/001872098779591278},
volume = {40},
year = {1998}
}

@article{Rompapas2014,
author = {Rompapas, Damien Constantine and Sorokin, Nicholas and L{\"{u}}bke, Arno In Wolde and Taketomi, Takafumi and Yamamoto, Goshiro and Sandor, Christian and Kato, Hirokazu},
doi = {10.1145/2669062.2669087},
file = {:C$\backslash$:/Users/admin/Downloads/paper{\_}3.pdf:pdf},
isbn = {9781450318914},
journal = {SIGGRAPH Asia 2014 Mobile Graphics and Interactive Applications, SA 2014},
keywords = {Augmented reality,Google glass,X-ray visualization},
mendeley-groups = {X-RayVision},
pages = {2010},
title = {{Dynamic augmented reality X-ray on google glass}},
year = {2014}
}

@article{Schattschneider2010,
abstract = {Summary The transition from the juvenile to the adult phase of shoot development in plants is accompanied by changes in vegetative morphology and an increase in reproductive potential. Here, we describe the regulatory mechanism of this transition. We show that miR156 is necessary and sufficient for the expression of the juvenile phase, and regulates the timing of the juvenile-to-adult transition by coordinating the expression of several pathways that control different aspects of this process. miR156 acts by repressing the expression of functionally distinct SPL transcription factors. miR172 acts downstream of miR156 to promote adult epidermal identity. miR156 regulates the expression of miR172 via SPL9 which, redundantly with SPL10, directly promotes the transcription of miR172b. Thus, like the larval-to-adult transition in Caenorhabditis elegans, the juvenile-to-adult transition in Arabidopsis is mediated by sequentially operating miRNAs. miR156 and miR172 are positively regulated by the transcription factors they target, suggesting that negative feedback loops contribute to the stability of the juvenile and adult phases.},
author = {Schattschneider, Doris},
file = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schattschneider - 2010 - The mathematical side of M. C. Escher.pdf:pdf},
issn = {1088-9477},
journal = {Notices of the American Mathematical Society},
number = {6},
pages = {706--718},
title = {{The mathematical side of M. C. Escher}},
volume = {57},
year = {2010}
}

@article{Ackerman1998,
abstract = {The Visible Human Project data sets are designed to serve as a common reference point for the study of human anatomy, as a set of common public-domain data for testing medical imaging algorithms, and as a testbed and model for the construction of image libraries that can be accessed through networks. The data sets are being applied to a wide range of educational, diagnostic, treatment planning, virtual reality, artistic, mathematical, and industrial uses by more than 800 licensees in 27 countries. But key issues remain in the development of methods to link such image data to text-based data. Standards do not currently exist for such linkages. Basic research is needed in the description and representation of image-based structures and in the connection of image-based structural-anatomical data to text-based functional-physiological data. This is the larger, long-term goal of the Visible Human Project: to link the print library of functional-physiological knowledge with the image...},
author = {Ackerman, M J},
doi = {10.1109/5.662875},
file = {:C$\backslash$:/Users/admin/Downloads/00662875.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {Algorithm Design and Analysis,Biomedical Imaging,Cognitive Science,Computer Displays,Engineering,Human Anatomy,Image Storage,Libraries,Medical Tests,Rendering (Computer Graphics),Two Dimensional Displays},
number = {3},
pages = {504--511},
title = {{The Visible Human Project}},
volume = {86},
year = {1998}
}


@article{Page2021,
author = {Page, Matthew J. and McKenzie, Joanne E. and Bossuyt, Patrick M. and Boutron, Isabelle and Hoffmann, Tammy C. and Mulrow, Cynthia D. and Shamseer, Larissa and Tetzlaff, Jennifer M. and Akl, Elie A. and Brennan, Sue E. and Chou, Roger and Glanville, Julie and Grimshaw, Jeremy M. and Hr{\'{o}}bjartsson, Asbj{\o}rn and Lalu, Manoj M. and Li, Tianjing and Loder, Elizabeth W. and Mayo-Wilson, Evan and McDonald, Steve and McGuinness, Luke A. and Stewart, Lesley A. and Thomas, James and Tricco, Andrea C. and Welch, Vivian A. and Whiting, Penny and Moher, David},
doi = {10.1186/s13643-021-01626-4},
file = {:C\:/Users/adminuser/Downloads/s13643-021-01626-4.pdf:pdf},
isbn = {1364302101},
issn = {20464053},
journal = {Systematic Reviews},
mendeley-groups = {Reviews},
number = {1},
pages = {1--11},
pmid = {33781348},
publisher = {Systematic Reviews},
title = {{The PRISMA 2020 statement: an updated guideline for reporting systematic reviews}},
volume = {10},
year = {2021}
}

@Article{Lorensen,
  author  = {Lorensen, William and {E. Cline}, Harvey},
  title   = {{Marching Cubes: A High Resolution 3D Surface Construction Algorithm}},
  doi     = {10.1145/37401.37422},
  pages   = {163--},
  volume  = {21},
  file    = {:C$\backslash$:/Users/z004344b/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lorensen, E. Cline - 1987 - Marching Cubes A High Resolution 3D Surface Construction Algorithm.pdf:pdf},
  journal = {ACM SIGGRAPH Computer Graphics},
  year    = {1987},
}

@Article{Meulstee2019,
  author   = {Meulstee, Jene W. and Nijsink, Johan and Schreurs, Ruud and Verhamme, Luc M. and Xi, Tong and Delye, Hans H.K. and Borstlap, Wilfred A. and Maal, Thomas J.J.},
  title    = {{Toward Holographic-Guided Surgery}},
  doi      = {10.1177/1553350618799552},
  issn     = {15533514},
  number   = {1},
  pages    = {86--94},
  volume   = {26},
  abstract = {The implementation of augmented reality (AR) in image-guided surgery (IGS) can improve surgical interventions by presenting the image data directly on the patient at the correct position and in the actual orientation. This approach can resolve the switching focus problem, which occurs in conventional IGS systems when the surgeon has to look away from the operation field to consult the image data on a 2-dimensional screen. The Microsoft HoloLens, a head-mounted AR display, was combined with an optical navigation system to create an AR-based IGS system. Experiments were performed on a phantom model to determine the accuracy of the complete system and to evaluate the effect of adding AR. The results demonstrated a mean Euclidean distance of 2.3 mm with a maximum error of 3.5 mm for the complete system. Adding AR visualization to a conventional system increased the mean error by 1.6 mm. The introduction of AR in IGS was promising. The presented system provided a solution for the switching focus problem and created a more intuitive guidance system. With a further reduction in the error and more research to optimize the visualization, many surgical applications could benefit from the advantages of AR guidance.},
  file     = {:C$\backslash$:/Users/admin/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meulstee et al. - 2019 - Toward Holographic-Guided Surgery.pdf:pdf},
  isbn     = {1553350618},
  journal  = {Surgical Innovation},
  keywords = {HoloLens,augmented reality,biomedical engineering,image-guided surgery,maxillofacial surgery,neurosurgery},
  year     = {2019},
}

@Article{HECHT1996,
  author          = {HECHT, HEIKO and KAISER, MARY K. and BANKS, MARTIN S.},
  title           = {{Gravitational Acceleration}},
  number          = {7},
  pages           = {1066 -- 1075},
  volume          = {58},
  abstract        = {When an object's motion is influenced by gravity, as in the rise and fall of a thrown ball, the vertical component of acceleration is roughly constant at 9.8 m/sec-. In principle, an observer could use this information to estimate the absolute size and distance of the object (Saxberg, 1987a; Watson, Banks, von Hofsten, {\&} Royden, 1992).In five experiments, we examined people's ability to utilize the size and distance information provided by gravitational acceleration. Observers viewed computer simulations of an object rising and falling on a trajectory aligned with the gravitational vector. The simulated ob- jects were balls ofdifferent diameters presented across a wide range of simulated distances. Observers were asked to identify the ball that was presented and to estimate its distance. The results showed that observers were much more sensitive to average velocity than to the gravitational acceleration pattern. Likewise, verticality of the motion and visibility of the trajectory's apex had negligible effects on the accuracy of size and distance judgments.},
  file            = {:C$\backslash$:/Users/admin/Downloads/Hecht1996{\_}Article{\_}GravitationalAccelerationAsACu.pdf:pdf},
  journal         = {Perception {\&} Psychophysics},
  mendeley-groups = {DepthPerceptionPapers},
  year            = {1996},
}

@Article{Kindlmann2003,
  author       = {Kindlmann, Gordon and Whitaker, Ross and Tasdizen, Tolga and M{\"{o}}ller, Torsten},
  year         = {2003},
  journal       = {Proceedings of the IEEE Visualization Conference},
  title        = {{Curvature-Based Transfer Functions for Direct Volume Rendering: Methods and Applications}},
  doi          = {10.1109/VISUAL.2003.1250414},
  number       = {May 2014},
  pages        = {513--520},
  abstract     = {Direct volume rendering of scalar fields uses a transfer function to map locally measured data properties to opacities and colors. The domain of the transfer function is typically the one-dimensional space of scalar data values. This paper advances the use of curvature information in multi-dimensional transfer functions, with a methodology for computing high-quality curvature measurements. The proposed methodology combines an implicit formulation of curvature with convolution-based reconstruction of the field. We give concrete guidelines for implementing the methodology, and illustrate the importance of choosing accurate filters for computing derivatives with convolution. Curvature-based transfer functions are shown to extend the expressivity and utility of volume rendering through contributions in three different application areas: non-photorealistic volume rendering, surface smoothing via anisotropic diffusion, and visualization of isosurface uncertainty.},
  file         = {:D\:/Thomas/Downloads/download.pdf:pdf},
  isbn         = {0780381203},
  keywords     = {Convolution-based differentiation,Flowline curvature,Implicit surface curvature,Non-photorealistic rendering,Surface processing,Uncertainty visualization,Volume rendering},
}

@Article{Bertoni2012,
  author       = {Bertoni, Guido and Daemen, Joan and Peeters, Micha{\"{e}}l and {Van Assche}, Gilles},
  year         = {2012},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{Duplexing the sponge: Single-pass authenticated encryption and other applications}},
  doi          = {10.1007/978-3-642-28496-0_19},
  issn         = {03029743},
  pages        = {320--337},
  volume       = {7118 LNCS},
  abstract     = {This paper proposes a novel construction, called duplex, closely related to the sponge construction, that accepts message blocks to be hashed and-at no extra cost-provides digests on the input blocks received so far. It can be proven equivalent to a cascade of sponge functions and hence inherits its security against single-stage generic attacks. The main application proposed here is an authenticated encryption mode based on the duplex construction. This mode is efficient, namely, enciphering and authenticating together require only a single call to the underlying permutation per block, and is readily usable in, e.g., key wrapping. Furthermore, it is the first mode of this kind to be directly based on a permutation instead of a block cipher and to natively support intermediate tags. The duplex construction can be used to efficiently realize other modes, such as a reseedable pseudo-random bit sequence generators and a sponge variant that overwrites part of the state with the input block rather than to XOR it in. {\textcopyright} 2012 Springer-Verlag.},
  file         = {:D\:/Thomas/Downloads/SpongeDuplex.pdf:pdf},
  isbn         = {9783642284953},
  keywords     = {Keccak,authenticated encryption,duplex construction,key wrapping,provable security,pseudo-random bit sequence generator,sponge functions},
}

@Article{Steptoe2014,
  author       = {Steptoe, William and Julier, Simon and Steed, Anthony},
  year         = {2014},
  journal       = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
  title        = {{Presence and discernability in conventional and non-photorealistic immersive augmented reality}},
  doi          = {10.1109/ISMAR.2014.6948430},
  pages        = {213--218},
  abstract     = {Non-photorealistic rendering (NPR) has been shown as a powerful way to enhance both visual coherence and immersion in augmented reality (AR). However, it has only been evaluated in idealized pre-rendered scenarios with handheld AR devices. In this paper we investigate the use of NPR in an immersive, stereoscopic, wide field-of-view head-mounted video see-through AR display. This is a demanding scenario, which introduces many real-world effects including latency, tracking failures, optical artifacts and mismatches in lighting. We present the AR-Rift, a low-cost video see-through AR system using an Oculus Rift and consumer webcams. We investigate the themes of consistency and immersion as measures of psychophysical non-mediation. An experiment measures discernability and presence in three visual modes: conventional (unprocessed video and graphics), stylized (edge-enhancement) and virtualized (edge-enhancement and color extraction). The stylized mode results in chance-level discernability judgments, indicating successful integration of virtual content to form a visually coherent scene. Conventional and virutalized rendering bias judgments towards correct or incorrect respectively. Presence as it may apply to immersive AR, and which, measured both behaviorally and subjectively, is seen to be similarly high over all three conditions.},
  file         = {:D\:/Thomas/Downloads/Presence_and_discernability_in_conventional_and_non-photorealistic_immersive_augmented_reality (1).pdf:pdf},
  isbn         = {9781479961849},
  keywords     = {H.5.1 [Information Interfaces and PresentationMult,[I.3.7] Computer Graphics - Three-Dimensional Grap,augmented,shading,shadowing,texture,virtual realities},
  publisher    = {IEEE},
}

@Article{Lawonn2018,
  author       = {Lawonn, Kai and Viola, Ivan and Preim, Bernhard and Isenberg, Tobias},
  year         = {2018},
  journal       = {Computer Graphics Forum},
  title        = {{A Survey of Surface-Based Illustrative Rendering for Visualization}},
  doi          = {10.1111/cgf.13322},
  issn         = {14678659},
  number       = {6},
  pages        = {205--234},
  volume       = {37},
  abstract     = {In this paper, we survey illustrative rendering techniques for 3D surface models. We first discuss the field of illustrative visualization in general and provide a new definition for this sub-area of visualization. For the remainder of the survey, we then focus on surface-based models. We start by briefly summarizing the differential geometry fundamental to many approaches and discuss additional general requirements for the underlying models and the methods' implementations. We then provide an overview of low-level illustrative rendering techniques including sparse lines, stippling and hatching, and illustrative shading, connecting each of them to practical examples of visualization applications. We also mention evaluation approaches and list various application fields, before we close with a discussion of the state of the art and future work.},
  file         = {:D\:/Thomas/Downloads/cgf.13322.pdf:pdf},
  keywords     = {scientific visualization,visualization},
}

@Article{Lichtenberg2017,
  author       = {Lichtenberg, Nils and Hansen, Christian and Lawonn, Kai},
  year         = {2017},
  journal       = {VCBM 2017 - Eurographics Workshop on Visual Computing for Biology and Medicine},
  title        = {{Concentric circle glyphs for enhanced depth-judgment in vascular models}},
  doi          = {10.2312/vcbm.20171252},
  number       = {September},
  pages        = {179--188},
  abstract     = {Using 3D models of medical data for surgery or treatment planning requires a comprehensive visualization of the data. This is crucial to support the physician in creating a cognitive image of the presented model. Vascular models are complex structures and, thus, the correct spatial interpretation is difficult. We propose view-dependent circle glyphs that enhance depth perception in vascular models. The glyphs are automatically placed on vessel end-points in a balanced manner. For this, we introduce a vessel end-point detection algorithm as a pre-processing step and an extensible, feature-driven glyph filtering strategy. Our glyphs are simple to implement and allow an enhanced and quick judgment of the depth value that they represent. We conduct a qualitative evaluation to compare our approach with two existing approaches, that enhance depth perception with illustrative visualization techniques. The evaluation shows that our glyphs perform better in the general case and decisively outperform the reference techniques when it comes to just noticeable differences.},
  file         = {:D\:/Thomas/Downloads/vcbm-2017.pdf:pdf},
  isbn         = {9783038680369},
}

@Article{Isenberg2013,
  author = {Isenberg, Tobias and Evaluating, Tobias Isenberg and Non-photorealistic, Validating and Paul, Illustrative Rendering and Isenberg, Tobias},
  year   = {2013},
  title  = {{Evaluating and Validating Non-Photorealistic and Illustrative Rendering To cite this version : HAL Id : hal-00781058 Evaluating and Validating Non-Photorealistic and Illustrative Rendering}},
  doi    = {10.1007/978-1-4471-4519-6},
  file   = {:D\:/Thomas/Downloads/Isenberg_2013_EVN.pdf:pdf},
  isbn   = {9781447145189},
}

@Article{Tominski2017,
  author       = {Tominski, C. and Gladisch, S. and Kister, U. and Dachselt, R. and Schumann, H.},
  year         = {2017},
  journal       = {Computer Graphics Forum},
  title        = {{Interactive Lenses for Visualization: An Extended Survey}},
  doi          = {10.1111/cgf.12871},
  issn         = {14678659},
  number       = {6},
  pages        = {173--200},
  volume       = {36},
  abstract     = {The elegance of using virtual interactive lenses to provide alternative visual representations for selected regions of interest is highly valued, especially in the realm of visualization. Today, more than 50 lens techniques are known in the closer context of visualization, far more in related fields. In this paper, we extend our previous survey on interactive lenses for visualization. We propose a definition and a conceptual model of lenses as extensions of the classic visualization pipeline. An extensive review of the literature covers lens techniques for different types of data and different user tasks and also includes the technologies employed to display lenses and to interact with them. We introduce a taxonomy of lenses for visualization and illustrate its utility by dissecting in detail a multi-touch lens for exploring large graph layouts. As a conclusion of our review, we identify challenges and unsolved problems to be addressed in future research.},
  file         = {:D\:/Thomas/Downloads/cgf.12871.pdf:pdf},
  keywords     = {H.5.2 [Information Interfaces and Presentation]: U,Visualization,interaction,magic lenses},
}

@Article{Roettger2003,
  author = {Roettger, Stefan},
  year   = {2003},
  title  = {{Volumetric Methods for the Real Time Display of Natural Gaseous Phenonema}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roettger - 2003 - Volumetric Methods for the Real Time Display of Natural Gaseous Phenonema.pdf:pdf},
}

@Article{Diaz2008,
  author       = {D\'iaz, Jose and Yela, H{\'{e}}ctor and V{\'{a}}zquez, Pere-Pau},
  year         = {2008},
  journal       = {Proccedings of Computer Graphics International},
  title        = {{Vicinity {O}cclusion {M}aps: {E}nhanced depth perception of volumetric models}},
  number       = {January 2008},
  pages        = {56--63},
  abstract     = {Volume models often show high depth complexity. This poses di±culties to the observer in judging the spatial relationships accurately. Illustrators usually use certain techniques such as halos or edge darkening in order to enhance depth perception of certain structures. Halos may be dark or light, and even colored. Halo construction on a volumetric basis impacts rendering performance due to the complexity of the construction process. In this paper we present Vicinity Occlusion Maps: a simple and fast method to compute the light occlusion due to neighboring voxels. Vicinity Occlusion Maps may be used to generate flexible halos around objects or selected structures in order to enhance depth perception or accentuate the presence of some structures in volumetric models at a low cost. The user may freely select the structure that requires the halos to be generated, its color and size, and our proposed application generates those in real time. They may also be used to perform vicinity shading in realtime, or even to combine both effects.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/D'iaz, Yela, V{\'{a}}zquez - 2008 - Vicinity {O}cclusion {M}aps {E}nhanced depth perception of volumetric models.pdf:pdf},
  keywords     = {depth,gpu-based volume ray casting,halo,perception enhancement,vicinity shading},
}

@Article{Elshafei2019,
  author       = {Elshafei, Moustafa and Binder, Johannes and Baecker, Justus and Brunner, Maximilian and Uder, Michael and Weber, Georg F. and Gr{\"{u}}tzmann, Robert and Krautz, Christian},
  year         = {2019},
  journal       = {JAMA Surgery},
  title        = {{Comparison of Cinematic Rendering and Computed Tomography for Speed and Comprehension of Surgical Anatomy}},
  doi          = {10.1001/jamasurg.2019.1168},
  issn         = {21686254},
  number       = {8},
  pages        = {738--744},
  volume       = {154},
  abstract     = {Importance: Three-dimensional (3-D) volume rendering has been shown to improve visualization in general surgery. Cinematic rendering (CR), a novel 3-D visualization technology for postprocessing of computed tomographaphy (CT) images, provides photorealistic images with the potential to improve visualization of anatomic details. Objective: To determine the value of CR for the comprehension of the surgical anatomy. Design, Setting, and Participants: This preclinical, randomized, 2-sequence crossover study was conducted from February to November 1, 2018, at University Hospital of Erlangen, Germany. The 40 patient cases were evaluated by 18 resident and attending surgeons using a prepared set of CT and CR images. The patient cases were randomized to 2 assessment sequences (CR-CT and CT-CR). During each assessment period, participants answered 1 question per case that addressed crucial issues of anatomic understanding, preoperative planning, and intraoperative strategies. After a washout period of 2 weeks, case evaluations were crossed over to the respective second image modality. Main Outcomes and Measures: The primary outcome measure was the correctness of answers. Secondary outcome was the time needed to answer. Results: The mean (SD) interperiod differences for the percentage of correct answers in the CR-CT sequence (8.5\% [7.0\%]) differed significantly from those in the CT-CR sequence (-13.1\% [6.3\%]) (P <.001). The mean (SD) interperiod differences for the time spent to answer the questions in the CR-CT sequence (-18.3 [76.9] seconds) also differed significantly from those in the CT-CR sequence (52.4 [88.5] seconds) (P <.001). Subgroup analysis revealed that residents as well as attending physicians benefitted from CR visualization. Analysis of the case assessment questionnaire showed that CR added significant value to the comprehension of the surgical anatomy (overall mean [SD] score, 4.53 [0.75]). No carryover or period effects were observed. Conclusions and Relevance: The visualization with CR allowed a more correct and faster comprehension of the surgical anatomy compared with conventional CT imaging, independent of level of surgeon experience. Therefore, CR may assist general surgeons with preoperative preparation and intraoperative guidance.},
  file         = {:D\:/Thomas/Downloads/jamasurgery_elshafei_2019_oi_190024.pdf:pdf},
  pmid         = {31141115},
}

@Article{Johnson2017,
  author       = {Johnson, Pamela T. and Schneider, Robert and Lugo-Fagundo, Carolina and Johnson, Michael B. and Fishman, Elliot K.},
  year         = {2017},
  journal       = {American Journal of Roentgenology},
  title        = {{MDCT angiography with 3D rendering: A novel cinematic rendering algorithm for enhanced anatomic detail}},
  doi          = {10.2214/AJR.17.17903},
  issn         = {15463141},
  number       = {2},
  pages        = {309--312},
  volume       = {209},
  abstract     = {OBJECTIVE. The two most widely used postprocessing 3D tools in clinical practice are volume rendering (VR) and maximum intensity projection (MIP). With the use of currentgeneration MDCT, these techniques enable accurate characterization of arterial anatomy and pathology in all anatomic regions. Recently, the VR algorithm has been enhanced by the incorporation of a new lighting model. This new technique-called cinematic rendering-generates photorealistic images with the potential to more accurately depict anatomic detail. CONCLUSION. As an enhancement of the technology championed in VR, cinematic rendering promises to provide additional anatomic detail for MDCT interpretation and display. Future investigations must be conducted to evaluate the diagnostic accuracy of cinematic rendering and determine whether interpretative pitfalls result from its unique lighting model in practice.},
  file         = {:D\:/Thomas/Downloads/pugh2019.pdf:pdf},
  isbn         = {0000000000000},
  keywords     = {3D rendering,Cinematic rendering,Lighting model,Volume rendering},
  pmid         = {28590775},
}

@InBook{Smith2020,
  author    = {Smith, Ross T and Clarke, Thomas J and Mayer, Wolfgang and Cunningham, Andrew and Matthews, Brandon and Zucco, Joanne E},
  booktitle = {Biomedical Visualisation: Volume 8},
  year      = {2020},
  title     = {{Mixed Reality Interaction and Presentation Techniques for Medical Visualisations}},
  doi       = {10.1007/978-3-030-47483-6_7},
  editor    = {Rea, Paul M},
  isbn      = {978-3-030-47483-6},
  location  = {Cham},
  pages     = {123--139},
  publisher = {Springer International Publishing},
  url       = {https://doi.org/10.1007/978-3-030-47483-6_7},
  abstract  = {Mixed, Augmented and Virtual reality technologies are burgeoning with new applications and use cases appearing rapidly. This chapter provides a brief overview of the fundamental display presentation methods; head-worn, hand-held and projector-based displays. We present a summary of visualisation methods that employ these technologies in the medical domain with a diverse range of examples presented including diagnostic and exploration, intervention and clinical, interaction and gestures, and education.},
}

@Article{Sawhney2017,
  author       = {Sawhney, Rohan and Crane, Keenan},
  year         = {2017},
  journal       = {ACM Transactions on Graphics},
  title        = {{Boundary first flatening}},
  doi          = {10.1145/3132705},
  eprint       = {1704.06873},
  eprinttype   = {arXiv},
  issn         = {15577368},
  number       = {1},
  pages        = {1--13},
  url          = {https://arxiv.org/pdf/1704.06873.pdf https://github.com/GeometryCollective/boundary-first-flattening},
  volume       = {37},
  abstract     = {A conformal flattening maps a curved surface to the plane without distorting angles - such maps have become a fundamental building block for problems in geometry processing, numerical simulation, and computational design. Yet existing methods provide little direct control over the shape of the flattened domain, or else demand expensive nonlinear optimization. Boundary first flattening (BFF) is a linear method for conformal parameterization that is faster than traditional linear methods, yet provides control and quality comparable to sophisticated nonlinear schemes. The key insight is that the boundary data for many conformal mapping problems can be efficiently constructed via the Cherrier formula together with a pair of Poincar{\'{e}}-Steklov operators; once the boundary is known, the map can be easily extended over the rest of the domain. Since computation demands only a single factorization of the real Laplace matrix, the amortized cost is about 50× less than any previously published technique for boundary-controlled conformal flattening. As a result, BFF opens the door to real-time editing or fast optimization of high-resolution maps, with direct control over boundary length or angle. We show how this method can be used to construct maps with sharp corners, cone singularities, minimal area distortion, and uniformization over the unit disk; we also demonstrate for the first time how a surface can be conformally flattened directly onto any given target shape.},
  arxivid      = {1704.06873},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawhney, Crane - 2017 - Boundary first flatening.pdf:pdf},
  keywords     = {Conformal geometry,Digital geometry processing,Discrete differential geometry,Surface parameterization},
}

@Article{Zhang2006,
  author = {Zhang, Caixia and Crawfis, Roger and Machiraju, Raghu and Shen, Han-wei and Science, Computer},
  year   = {2006},
  title  = {{Advanced Volume Rendering}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2006 - Advanced Volume Rendering.pdf:pdf},
}

@Article{Bruckner2007,
  author       = {Bruckner, Stefan and {Eduard Gr{\"{o}}ller}, M.},
  year         = {2007},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Enhancing depth-perception with flexible volumetric halos}},
  doi          = {10.1109/TVCG.2007.70555},
  issn         = {10772626},
  number       = {6},
  pages        = {1344--1351},
  volume       = {13},
  abstract     = {Volumetric data commonly has high depth complexity which makes it difficult to judge spatial relationships accurately. There are many different ways to enhance depth perception, such as shading, contours, and shadows. Artists and illustrators frequently employ halos for this purpose. In this technique, regions surrounding the edges of certain structures are darkened or brightened which makes it easier to judge occlusion. Based on this concept, we present a flexible method for enhancing and highlighting structures of interest using GPU-based direct volume rendering. Our approach uses an interactively defined halo transfer function to classify structures of interest based on data value, direction, and position. A feature-preserving spreading algorithm is applied to distribute seed values to neighboring locations, generating a controllably smooth field of halo intensities. These halo intensities are then mapped to colors and opacities using a halo profile function. Our method can be used to annotate features at interactive frame rates. {\textcopyright} 2007 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bruckner, Eduard Gr{\"{o}}ller - 2007 - Enhancing depth-perception with flexible volumetric halos.pdf:pdf;:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bruckner, Eduard Gr{\"{o}}ller - 2007 - Enhancing depth-perception with flexible volumetric halos(2).pdf:pdf},
  keywords     = {Halos,Illustrative visualization,Volume rendering},
  pmid         = {17968083},
}

@Article{Adams2021,
  author       = {Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah H. and Pointon, Grant and Thompson, William B. and Bodenheimer, Bobby},
  year         = {2021},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Shedding Light on Cast Shadows: An Investigation of Perceived Ground Contact in AR and VR}},
  doi          = {10.1109/TVCG.2021.3097978},
  issn         = {19410506},
  number       = {July},
  abstract     = {Virtual objects in augmented reality (AR) often appear to float atop real world surfaces, which makes it difficult to determine where they are positioned in space. This is problematic as many applications for AR require accurate spatial perception. In the current study, we examine how the way we render cast shadows--which act as an important monocular depth cue for creating a sense of contact between an object and the surface beneath it--impacts spatial perception. Over two experiments, we evaluate people's sense of surface contact given both traditional and non-traditional shadow shading methods in optical see-through augmented reality (OST AR), video see-through augmented reality (VST AR), and virtual reality (VR) head-mounted displays. Our results provide evidence that nontraditional shading techniques for rendering shadows in AR displays may enhance the accuracy of one's perception of surface contact. This finding implies a possible tradeoff between photorealism and accuracy of depth perception, especially in OST AR displays. However, it also supports the use of more stylized graphics like non-traditional cast shadows to improve perception and interaction in AR applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adams et al. - 2021 - Shedding Light on Cast Shadows An Investigation of Perceived Ground Contact in AR and VR.pdf:pdf},
  keywords     = {Augmented Reality,Contrast,Ground Contact,Image color analysis,Layout,Lighting,OST AR,Optical imaging,Optical sensors,Perception,Rendering (computer graphics),Shadows,VR,VST AR,Visualization},
}

@Article{Schott2009,
  author       = {Schott, Mathias and Pegoraro, Vincent and Hansen, Charles and Boulanger, K{\'{e}}vin and Bouatouch, Kadi},
  year         = {2009},
  journal       = {Computer Graphics Forum},
  title        = {{A directional occlusion shading model for interactive direct volume rendering}},
  doi          = {10.1111/j.1467-8659.2009.01464.x},
  issn         = {14678659},
  number       = {3},
  pages        = {855--862},
  volume       = {28},
  abstract     = {Volumetric rendering is widely used to examine 3D scalar fields from CT/MRI scanners and numerical simulation datasets. One key aspect of volumetric rendering is the ability to provide perceptual cues to aid in understanding structure contained in the data. While shading models that reproduce natural lighting conditions have been shown to better convey depth information and spatial relationships, they traditionally require considerable (pre)computation. In this paper, a shading model for interactive direct volume rendering is proposed that provides perceptual cues similar to those of ambient occlusion, for both solid and transparent surface-like features. An image space occlusion factor is derived from the radiative transport equation based on a specialized phase function. The method does not rely on any precomputation and thus allows for interactive explorations of volumetric data sets via on-the-fly editing of the shading model parameters or (multi-dimensional) transfer functions while modifications to the volume via clipping planes are incorporated into the resulting occlusion-based shading. {\textcopyright} 2009 The Eurographics Association and Blackwell Publishing Ltd.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schott et al. - 2009 - A directional occlusion shading model for interactive direct volume rendering.pdf:pdf},
}

@Article{Louis2020,
  author   = {Louis, Thibault and Troccaz, Jocelyne and Rochet-Capellan, Am{\'{e}}lie and Hoyek, Nady and B{\'{e}}rard, Fran{\c{c}}ois},
  year     = {2020},
  title    = {{When High Fidelity Matters}},
  doi      = {10.1145/3399715.3399815},
  pages    = {1--9},
  file     = {:D\:/Thomas/Downloads/3399715.3399815.pdf:pdf},
  isbn     = {9781450375351},
  keywords = {acm reference format,display,head mounted,learning task,mental rotation,spatial augmented reality,user study,virtual reality},
}

@Article{Commins2020,
  author       = {Commins, Sean and Duffin, Joseph and Chaves, Keylor and Leahy, Diarmuid and Corcoran, Kevin and Caffrey, Michelle and Keenan, Lisa and Finan, Deirdre and Thornberry, Conor},
  year         = {2020},
  journal       = {Behavior Research Methods},
  title        = {{NavWell: A simplified virtual-reality platform for spatial navigation and memory experiments}},
  doi          = {10.3758/s13428-019-01310-5},
  issn         = {15543528},
  number       = {3},
  pages        = {1189--1207},
  volume       = {52},
  abstract     = {Being able to navigate, recall important locations, and find the way home are critical skills, essential for survival for both humans and animals. These skills can be examined in the laboratory using the Morris water maze, often considered the gold standard test of animal navigation. In this task, animals are required to locate and recall the location of an escape platform hidden in a pool filled with water. Because animals can not see the platform directly, they must use various landmarks in the environment to escape. With recent advances in technology and virtual reality (VR), many tasks originally used in the animal literature can now be translated for human studies. The virtual water maze task is no exception. However, a number of issues are associated with these mazes, including cost, lack of flexibility, and lack of standardization in terms of experimental designs and procedures. Here we present a virtual water maze system (NavWell) that is readily downloadable and free to use. The system allows for the easy design of experiments and the testing of participants on a desktop computer or fully immersive VR environment. The data from four independent experiments are presented in order to validate the software. From these experiments, a set of procedures for use with a number of well-known memory tests is suggested. This potentially can help with the standardization of navigational research and with navigational testing in the clinic or in an educational environment. Finally, we discuss the limitations of the software and plans for its development and future use.},
  file         = {:D\:/Thomas/Downloads/Commins2020_Article_NavWellASimplifiedVirtual-real.pdf:pdf},
  keywords     = {Memory,Navigation,Spatial,Virtual reality,Water maze},
  pmid         = {31637666},
  publisher    = {Behavior Research Methods},
}

@Book{Nazir2019,
  author   = {Nazir, Salman and Teperi, Anna-Maria and Polak-Sopi{\'{n}}ska, Aleksandra},
  year     = {2019},
  title    = {{Erratum to: Advances in Human Factors in Training, Education, and Learning Sciences}},
  doi      = {10.1007/978-3-319-93882-0_46},
  isbn     = {9783319600178},
  pages    = {E1--E1},
  abstract = {An evolutionary framework was used to develop a model of relational functioning among friends. The proposed model focuses on the collaborative nature of close friendships and attempts to highlight two adaptive mechanisms important in creating non-zero-sum outcomes among highly interdependent parties. The model emphasizes the importance of reciprocity in creating mutually beneficial outcomes through social exchange, and it articulates how the preference for similarity is useful when creating non-zero-sum rewards through synergistic coordination. In particular, the focus is on the unique role that shared interests and mutual knowledge play when individuals attempt to pursue common goals through joint activity. The implications of the model are discussed with respect to a host of issues ranging from deceptive communication to relational satisfaction and commitment.},
  file     = {:D\:/Thomas/Downloads/2018_Book_AdvancesInHumanFactorsInTraini.pdf:pdf},
}

@Article{Katsioloudis2014,
  author       = {Katsioloudis, Petros and Jovanovic, Vukica and Jones, Mildred},
  year         = {2014},
  journal       = {Journal of Technology Education},
  title        = {{A Comparative analysis of spatial visualization ability and drafting models for industrial and technology education students}},
  doi          = {10.21061/jte.v26i1.a.6},
  issn         = {10451064},
  number       = {1},
  pages        = {88--101},
  volume       = {26},
  file         = {:D\:/Thomas/Downloads/katsioloudis.pdf:pdf},
}

@Article{Whitlock2020,
  author   = {Whitlock, Matt and Smart, Stephen and Szafir, Danielle Albers},
  year     = {2020},
  title    = {{Graphical Perception for Immersive Analytics}},
  doi      = {10.1109/vr46266.2020.00084},
  pages    = {616--625},
  abstract = {Immersive Analytics (IA) uses immersive virtual and augmented reality displays for data visualization and visual analytics. Designers rely on studies of how accurately people interpret data in different visualizations to make effective visualization choices. However, these studies focus on data analysis in traditional desktop environments. We lack empirical grounding for how to best visualize data in immersive environments. This study explores how people interpret data visualizations across different display types by measuring how quickly and accurately people conduct three analysis tasks over five visual channels: color, size, height, orientation, and depth. We identify key quantitative differences in performance and user behavior, indicating that stereo viewing resolves some of the challenges of visualizations in 3D space. We also find that while AR displays encourage increased navigation, they decrease performance with color-based visualizations. Our results provide guidelines on how to tailor visualizations to different displays in order to better leverage the affordances of IA modalities. Index},
  file     = {:D\:/Thomas/Downloads/Graphical_Perception_for_Immersive_Analytics.pdf:pdf},
}

@Article{Maeda2013,
  author       = {Maeda, Yukiko and Yoon, So Yoon and Kim-Kang, Gyenam and Imbrie, P. K.},
  year         = {2013},
  journal       = {International Journal of Engineering Education},
  title        = {{Psychometric properties of the revised PSVT:R for measuring First Year Engineering students' spatial ability}},
  issn         = {0949149X},
  number       = {3},
  pages        = {763--776},
  volume       = {29},
  abstract     = {While various spatial tests are available, the Purdue Spatial Visualization Tests: Visualization of Rotations (PSVT:R) has been commonly used to predict students' success in the engineering field. While many studies that used the PSVT:R exist, little attention had been given to its psychometric properties in measuring spatial ability and relationships to other academic indices. The purposes of this study were (a) to characterize the item- and test-level functions of the Revised PSVT:R for the use of incoming First Year Engineering (FYE) students, and (b) to investigate its relationship to academicrelated variables to provide validity evidence. Approximately 2400 FYE students enrolled in the fall of 2010 and 2011 in a large Midwestern public university completed the Revised PSVT:R. Students' academic-related variables were also retrieved from the university archive. A variety of statistical analyses, including exploratory and confirmatory factor analyses as well as item analyses, were conducted on the Revised PSVT:R scores. Pearson's product-moment correlation coefficients between the Revised PSVT:R and other academic variables were also obtained. The Revised PSVT:R measures a unidimensional subcomponent of spatial ability. Cronbach's a was 0.84. Items were relatively easy and the test provides the most precise estimate for students whose ability level is at or below average. Weak to moderate correlations were found between the Revised PSVT:R scores and the aptitude test scores. The Revised PSVT:R is a psychometrically sound instrument. However, items are relatively easy, but it is still appropriate to measure spatial visualization ability of the FYE students. {\textcopyright} 2013 TEMPUS Publications.},
  file         = {:D\:/Thomas/Downloads/009.MaedaYoon13.IJEE.PsychometricpropertiesoftheRevisedPSVT-RforFYEstudentsSA.Corrected.pdf:pdf},
  keywords     = {First year engineering students,Mental rotation,Psychometric properties,Revised PSVT:R},
}

@Article{MuhammadNizam2018,
  author       = {{Muhammad Nizam}, Siti Soleha and Lam, Meng Chun and Arshad, Haslina and Suwadi, Nur Asylah},
  year         = {2018},
  journal       = {Advances in Multimedia},
  title        = {{A Scoping Review on Tangible and Spatial Awareness Interaction Technique in Mobile Augmented Reality-Authoring Tool in Kitchen}},
  doi          = {10.1155/2018/5320984},
  issn         = {16875699},
  volume       = {2018},
  abstract     = {The interaction paradigm has changed with the emerging technology, mobile augmented reality, and spatial awareness mobile device. The traditional way for designing a kitchen can cause mistake in measurement and user's imagination is different from kitchen advisor's sketch design due to limitation of human imagination. Using mobile augmented reality technology, the user can overlay the virtual kitchen outcome that could fit with the actual kitchen environment. Interaction technique is required in order to allow the user to change the characteristic of the virtual kitchen to suite their need. Thus, this paper is intended to propose the tangible and spatial awareness interaction techniques in the kitchen design authoring tool. A scoping review related to the previous research on tangible and spatial awareness interaction is presented in this paper. The proposed techniques were based on the review on the existing interaction technique and the interview section with the Malaysian kitchen designer by understanding the kitchen design elements and flow. The proposed techniques will be further improved by going through the heuristic evaluation with the augmented reality expert.},
  file         = {:D\:/Thomas/Downloads/5320984.pdf:pdf},
}

@Article{Erkelens2015,
  author       = {Erkelens, Casper J.},
  year         = {2015},
  journal       = {i-Perception},
  title        = {{The perspective structure of visual space}},
  doi          = {10.1177/2041669515613672},
  issn         = {20416695},
  number       = {5},
  pages        = {1--13},
  volume       = {6},
  abstract     = {Luneburg's model has been the reference for experimental studies of visual space for almost seventy years. His claim for a curved visual space has been a source of inspiration for visual scientists as well as philosophers. The conclusion of many experimental studies has been that Luneburg's model does not describe visual space in various tasks and conditions. Remarkably, no alternative model has been suggested. The current study explores perspective transformations of Euclidean space as a model for visual space. Computations show that the geometry of perspective spaces is considerably different from that of Euclidean space. Collinearity but not parallelism is preserved in perspective space and angles are not invariant under translation and rotation. Similar relationships have shown to be properties of visual space. Alley experiments performed early in the nineteenth century have been instrumental in hypothesizing curved visual spaces. Alleys were computed in perspective space and compared with reconstructed alleys of Blumenfeld. Parallel alleys were accurately described by perspective geometry. Accurate distance alleys were derived from parallel alleys by adjusting the interstimulus distances according to the size-distance invariance hypothesis. Agreement between computed and experimental alleys and accommodation of experimental results that rejected Luneburg's model show that perspective space is an appropriate model for how we perceive orientations and angles. The model is also appropriate for perceived distance ratios between stimuli but fails to predict perceived distances.},
  file         = {:D\:/Thomas/Downloads/Erkelens2015d.pdf:pdf},
  keywords     = {Alley experiments,Perspective space,Visual space},
}

@Article{Quasha1937,
  author       = {Quasha, W. H. and Likert, R.},
  year         = {1937},
  journal       = {Journal of Educational Psychology},
  title        = {{The revised Minnesota paper form board test}},
  doi          = {10.1037/h0059880},
  issn         = {00220663},
  number       = {3},
  pages        = {197--204},
  volume       = {28},
  abstract     = {A multiple-choice form of the Minnesota paper form board test has been constructed in two forms that are equal in difficulty and internally consistent. The interform r was found to be .85, and the lowest correlation between the revised test and the original test was .75 (corrected r = .94). The revised test has a great advantage over the original in ease and objectivity of scoring. Norms for a number of groups, including engineering-school students and young and old adults, are given. (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1937 American Psychological Association.},
  file         = {:D\:/Thomas/Downloads/00004760-193703000-00004.pdf:pdf},
  keywords     = {INTELLIGENCE, TEST, MINNESOTA FORM BOARD,MENTAL TESTS,TEST ABSURDITIES, MINNESOTA PAPER FORM BOARD},
}

@Article{Erkelens2015a,
  author       = {Erkelens, Casper J.},
  year         = {2015},
  journal       = {i-Perception},
  title        = {{Perception of perspective angles}},
  doi          = {10.1177/2041669515593022},
  issn         = {20416695},
  number       = {3},
  volume       = {6},
  abstract     = {We perceive perspective angles, that is, angles that have an orientation in depth, differently from what they are in physical space. Extreme examples are angles between rails of a railway line or between lane dividers of a long and straight road. In this study, subjects judged perspective angles between bars lying on the floor of the laboratory. Perspective angles were also estimated from pictures taken from the same point of view. Converging and diverging angles were judged to test three models of visual space. Four subjects evaluated the perspective angles by matching them to nonperspective angles, that is, angles between the legs of a compass oriented in the frontal plane. All subjects judged both converging and diverging angles larger than the physical angle and smaller than the angles in the proximal stimuli. A model of shallow visual space describes the results. According to the model, lines parallel to visual lines, vanishing at infinity in physical space, converge to visual lines in visual space. The perceived shape of perspective angles is incompatible with the perceived length and width of the bars. The results have significance for models of visual perception and practical implications for driving and flying in poor visibility conditions.},
  file         = {:D\:/Thomas/Downloads/Erkelens2015c.pdf:pdf},
  keywords     = {Models,Perspective angles,Visual space},
}

@Article{Guen2017,
  author       = {G{\"{u}}n, Ezgi Tosik and Atasoy, Bilal},
  year         = {2017},
  journal       = {Egitim ve Bilim},
  title        = {{The effects of augmented reality on elementary school students' spatial ability and academic achievement}},
  doi          = {10.15390/EB.2017.7140},
  issn         = {13001337},
  number       = {191},
  pages        = {31--51},
  volume       = {42},
  abstract     = {The purposes of this study are to investigate the effects of an augmented reality application on students' spatial ability and academic achievement, and to analyze the opinions of students and their teacher concerning augmented reality environments. To collect quantitative data, a quasi-experimental pretest-posttest with a control group design was employed, and to collect qualitative data, a case study design was used. The study was designed around the lesson topic “geometric objects and measuring volume,” and 88 sixth grade students participated. While the students in the experimental group studied the lesson topic using augmented reality and real objects, the students in the control group used only real objects. The results indicate that though a significant increase was observed in the spatial ability of both groups, no significant difference was found between the post-test spatial ability mean scores of the experimental and control groups. In addition to the spatial ability results, the students' academic achievement scores in the experimental group significantly increased, but the small increase in the control group students' scores was not significant. No significant difference was found between the post-test academic achievement scores of the experimental and control groups. In addition to the quantitative data, the qualitative data gathered from the students and the teacher yielded valuable information that may assist researchers who attempt to integrate augmented reality in education.},
  file         = {:D\:/Thomas/Downloads/7140-41443-3-PB.pdf:pdf},
  keywords     = {Academic achievement,Augmented reality,Elementary school students,Mathematics education,Spatial ability},
}

@InCollection{Tuker2018,
  author    = {Tuker, Cetin},
  booktitle = {Encyclopedia of Computer Graphics and Games},
  year      = {2018},
  title     = {{Training Spatial Skills with Virtual Reality and Augmented Reality}},
  doi       = {https://doi.org/10.1007/978-3-319-08234-9},
  isbn      = {9783319082349},
  number    = {January},
  pages     = {1--9},
  publisher = {Springer, Cham},
  file      = {:D\:/Thomas/Downloads/Tuker2018_ReferenceWorkEntry_TrainingSpatialSkillsWithVirtu.pdf:pdf},
}

@Article{RocaGonzalez2017,
  author       = {Roca-Gonz{\'{a}}lez, Cristina and Martin-Gutierrez, Jorge and Garc{\'{i}}a-Dominguez, Melchor and Carrodeguas, Ma del Carmen Mato},
  year         = {2017},
  journal       = {Eurasia Journal of Mathematics, Science and Technology Education},
  title        = {{Virtual technologies to develop visual-spatial ability in engineering students}},
  doi          = {10.12973/eurasia.2017.00625a},
  issn         = {13058223},
  number       = {2},
  pages        = {441--468},
  volume       = {13},
  abstract     = {The present study assessed a short training experiment to improve spatial abilities using two tools based on virtual technologies: one focused on manipulation of specific geometric virtual pieces, and the other consisting of virtual orienteering game. The two tools can help improve spatial abilities required for many engineering problem-solving procedures. The results indicated that training activities improved the components of space ability (mental rotation, spatial visualization, and spatial orientation). In addition, it was concluded that there were no differences between men and women with respect to spatial ability levels before or after the training experiment. This fact resulted from masked training relating to daily living and leisure activities that are usually indistinctly performed by men and women in developed and industrialized countries.},
  file         = {:D\:/Thomas/Downloads/virtual-technologies-to-develop-visual-spatial-ability-in-engineering-students-4673.pdf:pdf},
  keywords     = {Augmented reality,Gender,Spatial ability,Virtual orienteering,Virtual reality},
}

@Article{Pelanis2020,
  author       = {Pelanis, Egidijus and Kumar, Rahul P. and Aghayan, Davit L. and Palomar, Rafael and Fretland, {\AA}smund A. and Brun, Henrik and Elle, Ole Jakob and Edwin, Bj{\o}rn},
  year         = {2020},
  journal       = {Minimally Invasive Therapy and Allied Technologies},
  title        = {{Use of mixed reality for improved spatial understanding of liver anatomy}},
  doi          = {10.1080/13645706.2019.1616558},
  issn         = {13652931},
  number       = {3},
  pages        = {154--160},
  url          = {https://doi.org/10.1080/13645706.2019.1616558},
  volume       = {29},
  abstract     = {Introduction: In liver surgery, medical images from pre-operative computed tomography and magnetic resonance imaging are the basis for the decision-making process. These images are used in surgery planning and guidance, especially for parenchyma-sparing hepatectomies. Though medical images are commonly visualized in two dimensions (2D), surgeons need to mentally reconstruct this information in three dimensions (3D) for a spatial understanding of the anatomy. The aim of this work is to investigate whether the use of a 3D model visualized in mixed reality with Microsoft HoloLens increases the spatial understanding of the liver, compared to the conventional way of using 2D images. Material and methods: In this study, clinicians had to identify liver segments associated to lesions. Results: Twenty-eight clinicians with varying medical experience were recruited for the study. From a total of 150 lesions, 89 were correctly assigned without significant difference between the modalities. The median time for correct identification was 23.5 [4–138] s using the magnetic resonance imaging images and 6.00 [1–35] s using HoloLens (p < 0.001). Conclusions: The use of 3D liver models in mixed reality significantly decreases the time for tasks requiring a spatial understanding of the organ. This may significantly decrease operating time and improve use of resources.},
  file         = {:D\:/Thomas/Downloads/Use of mixed reality for improved spatial understanding of liver anatomy (1).pdf:pdf},
  keywords     = {3D model,Liver surgery,mixed reality,parenchyma sparing,segmentation},
  pmid         = {31116053},
  publisher    = {Taylor {\&} Francis},
}

@Article{Zhiyong2020,
  author        = {Zhiyong, Guo and Zhangfan, Ding and Cheng, Miao and Chunjie, Li and Xiufa, Tang and Oncology, Neck},
  year          = {2020},
  title         = {{Application of mixed reality in oromaxillofacial head and neck oncology surgery: a preliminary study}},
  number        = {4},
  pages         = {470--474},
  volume        = {38},
  abstract      = {Mixed reality (MR), characterized by the ability to integrate digital data into human real feeling, is a new technique in medical imaging and surgical navigation. MR has tremendous value in surgery, but its application in oromaxillofacial head and neck oncology surgery is not yet reported. This paper reports the application of MR in oromaxillofacial head and neck oncology surgery. The merits, demerits, and present research situations and prospects of MR are further discussed.},
  file          = {:D\:/Thomas/Downloads/wcjs-38-04-470.pdf:pdf},
  keywords      = {augmented reality,mixed reality,neoplasms,surgical navigation},
  mendeley-tags = {augmented reality,mixed reality,neoplasms,surgical navigation},
}

@Article{Bell2019,
  author       = {Bell, Andrew and Fairbrother, Malcolm and Jones, Kelvyn},
  year         = {2019},
  journal       = {Quality and Quantity},
  title        = {{Fixed and random effects models: making an informed choice}},
  doi          = {10.1007/s11135-018-0802-x},
  issn         = {15737845},
  number       = {2},
  pages        = {1051--1074},
  url          = {https://doi.org/10.1007/s11135-018-0802-x},
  volume       = {53},
  abstract     = {This paper assesses the options available to researchers analysing multilevel (including longitudinal) data, with the aim of supporting good methodological decision-making. Given the confusion in the literature about the key properties of fixed and random effects (FE and RE) models, we present these models' capabilities and limitations. We also discuss the within-between RE model, sometimes misleadingly labelled a ‘hybrid' model, showing that it is the most general of the three, with all the strengths of the other two. As such, and because it allows for important extensions—notably random slopes—we argue it should be used (as a starting point at least) in all multilevel analyses. We develop the argument through simulations, evaluating how these models cope with some likely mis-specifications. These simulations reveal that (1) failing to include random slopes can generate anti-conservative standard errors, and (2) assuming random intercepts are Normally distributed, when they are not, introduces only modest biases. These results strengthen the case for the use of, and need for, these models.},
  file         = {:D\:/Thomas/Downloads/Fixed_and_Random_effects_models_making_an_informed.pdf:pdf},
  isbn         = {0123456789},
  keywords     = {Fixed effects,Hybrid models,Multilevel models,Mundlak,Random effects,Within and between effects},
  publisher    = {Springer Netherlands},
}

@Article{Hutmacher2019,
  author       = {Hutmacher, Fabian},
  year         = {2019},
  journal       = {Frontiers in Psychology},
  title        = {{Why Is There So Much More Research on Vision Than on Any Other Sensory Modality?}},
  doi          = {10.3389/fpsyg.2019.02246},
  issn         = {16641078},
  number       = {October},
  volume       = {10},
  abstract     = {Why is there so much more research on vision than on any other sensory modality? There is a seemingly easy answer to this question: It is because vision is our most important and most complex sense. Although there are arguments in favor of this explanation, it can be challenged in two ways: by showing that the arguments regarding the importance and complexity of vision are debatable and by demonstrating that there are other aspects that need to be taken into account. Here, I argue that the explanation is debatable, as there are various ways of defining “importance” and “complexity” and, as there is no clear consensus that vision is indeed the most important and most complex of our senses. Hence, I propose two additional explanations: According to the methodological-structural explanation, there is more research on vision because the available, present-day technology is better suited for studying vision than for studying other modalities – an advantage which most likely is the result of an initial bias toward vision, which reinforces itself. Possible reasons for such an initial bias are discussed. The cultural explanation emphasizes that the dominance of the visual is not an unchangeable constant, but rather the result of the way our societies are designed and thus heavily influenced by human decision-making. As it turns out, there is no universal hierarchy of the senses, but great historical and cross-cultural variation. Realizing that the dominance of the visual is socially and culturally reinforced and not simply a law of nature, gives us the opportunity to take a step back and to think about the kind of sensory environments we want to create and about the kinds of theories that need to be developed in research.},
  file         = {:D\:/Thomas/Downloads/fpsyg-10-02246.pdf:pdf},
  keywords     = {history of the senses,multimodal integration,perception,social constructionism,visual dominance,visual turn,visuo-centrism},
}

@Article{Cho2014,
  author       = {Cho, Isaac and Wartell, Zachary and Dou, Wenwen and Wang, Xiaoyu and Ribarsky, William},
  year         = {2014},
  journal       = {Stereoscopic Displays and Applications XXV},
  title        = {{Stereo and motion cues effect on depth perception of volumetric data}},
  doi          = {10.1117/12.2037942},
  issn         = {1996756X},
  number       = {March 2014},
  pages        = {901118},
  volume       = {9011},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2014 - Stereo and motion cues effect on depth perception of volumetric data.pdf:pdf},
  isbn         = {9780819499288},
  keywords     = {depth cues,depth perception,head-tracking,stereoscopic display,virtual reality,volumetric data},
}

@Article{Cho2012,
  author       = {Cho, Isaac and Dou, Wenwen and Wartell, Zachary and Ribarsky, William and Wang, Xiaoyu},
  year         = {2012},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Evaluating depth perception of volumetric data in semi-immersive VR}},
  doi          = {10.1109/VR.2012.6180899},
  number       = {May 2012},
  pages        = {95--96},
  abstract     = {Displays supporting stereopsis and head location based motion parallax can enhance human perception of complex three dimensional datasets. This has been demonstrated for datasets containing 3D surfaces and 3D networks. Yet many domains, such as medical imaging, weather and environment simulations and fluid flow, generate complex volumetric data. This poster present results of an initial formal experiment that examines the effectiveness of various display conditions on depth perception of volumetric data. There is an overall benefit for stereoscopy with head-tracking in enhancing depth perception. Further, familiarity with 3D games and VR-like hardware improves the users'ability to perceive such data. {\textcopyright} 2012 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho et al. - 2012 - Evaluating depth perception of volumetric data in semi-immersive VR.pdf:pdf},
  isbn         = {9781467312462},
  keywords     = {H.5.1 [Multimedia Information Systems]: Artificial},
}

@Article{Cidota2017,
  author       = {Cidota, Marina A. and Clifford, Rory M.S. and Lukosch, Stephan G. and Billinghurst, Mark},
  year         = {2017},
  journal       = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2016},
  title        = {{Using Visual Effects to Facilitate Depth Perception for Spatial Tasks in Virtual and Augmented Reality}},
  doi          = {10.1109/ISMAR-Adjunct.2016.0070},
  pages        = {172--177},
  abstract     = {When developing serious games relying on spatial placement tasks in Augmented/Virtual Reality (AR/VR), having an accurate depth perception of virtual objects is highly important. It even becomes essential when such games are used for assessment of motion disorders. Patients should be able to naturally interact with the virtual and augmented environment and to perceive the correct 3D position of objects around them. We report on the results of a user study with 16 participants exploring whether visual effects applied to 3D content in AR and VR provide a better depth perception, level of presence or engagement to users. The findings show that visual effects both in AR and VR have a negative impact on perceived depth. The measured performance in VR though is best using visual effects while in AR without using visual effects. Both in AR and VR, visual effects influenced the user's level of presence negatively.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cidota et al. - 2017 - Using Visual Effects to Facilitate Depth Perception for Spatial Tasks in Virtual and Augmented Reality.pdf:pdf},
  isbn         = {9781509037407},
  keywords     = {AR,Depth Perception,HMD,Immersion,Natural Hand Interaction,Presence,Stereo Vision,VR,Visual Effects},
  publisher    = {IEEE},
}

@Article{Keil2020,
  author       = {Keil, Julian and Korte, Annika and Ratmer, Anna and Edler, Dennis and Dickmann, Frank},
  year         = {2020},
  journal       = {PFG - Journal of Photogrammetry, Remote Sensing and Geoinformation Science},
  title        = {{Augmented Reality (AR) and Spatial Cognition: Effects of Holographic Grids on Distance Estimation and Location Memory in a 3D Indoor Scenario}},
  doi          = {10.1007/s41064-020-00104-1},
  issn         = {25122819},
  number       = {2},
  pages        = {165--172},
  url          = {https://doi.org/10.1007/s41064-020-00104-1},
  volume       = {88},
  abstract     = {Recent advances in augmented reality (AR) technology enable the projection of holograms to a fixed location in 3D space. This renders new possibilities for influencing peoples' spatial perception and to address cognitive limitations as structural distortions in cognitive representations of space. The study presented in this paper investigated whether these structural distortions can be reduced by projecting a holographic grid into 3D space. Accuracy of the cognitive representation of space was assessed based on distance estimations and an object location memory task. The findings revealed that distance estimations were indeed more accurate when a holographic grid was available. Location memory performance, on the other hand, was worse when a holographic grid was available. Based on feedback from the participants, it can be assumed that design characteristics of the used AR headset are at least partly responsible for this result. These characteristics include a reduced field of view and visual distortions in the peripheral areas of the field of view. Overall, the findings show that AR can be used to influence and, when applied correctly, improve peoples' spatial perception. However, more research is needed to specify requirements, strengths, and weaknesses of geographic AR applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keil et al. - 2020 - Augmented Reality (AR) and Spatial Cognition Effects of Holographic Grids on Distance Estimation and Location Memor.pdf:pdf},
  isbn         = {0123456789},
  keywords     = {Augmented reality,Distance estimation,Grids,Holograms,Spatial cognition},
  publisher    = {Springer International Publishing},
}

@Article{Papakostas2021,
  author       = {Papakostas, Christos and Troussas, Christos and Krouska, Akrivi and Sgouropoulou, Cleo},
  year         = {2021},
  journal       = {Informatics in Education},
  title        = {{Exploration of Augmented Reality in Spatial Abilities Training: A Systematic Literature Review for the Last Decade}},
  doi          = {10.15388/infedu.2021.06},
  issn         = {16485831},
  number       = {1},
  pages        = {107--130},
  volume       = {20},
  abstract     = {This review paper presents a systematic literature review on the use of Augmented Reality (AR) in engineering education, and specifically in student's spatial ability training, for the last decade. Researchers have explored the benefits of AR, and its application has been of increasing interest in all levels of education. Engineering students tend to have difficulties in acquiring visualization skills, and hence, AR is gaining momentum in enhancing students' learning achievements. This paper aims to present valuable information to researchers, tutors and software developers of learning technology systems concerning the advantages and limitations of AR in spatial ability training, the incorporation of adaptivity and personalization in AR applications as well as the aspects of spatial ability having been evaluated using AR and the prevalent evaluation methods for AR applications. To this direction, a total of thirty-two (32) studies were reviewed, having been published since 2010. The findings reveal an increase in the number of studies during the last three years. One major conclusion is the improvement of learners' spatial ability using AR in educational settings, and the noted challenge is the need for more learning content. One research gap that has been identified is the lack of personalization in the developed applications, offering space for future research. Concluding, this area is under-researched, and thus, there is scope for a lot of improvement.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Papakostas et al. - 2021 - Exploration of Augmented Reality in Spatial Abilities Training A Systematic Literature Review for the Last De.pdf:pdf},
  keywords     = {Augmented Reality,educational technology,literature review,spatial ability,technical drawing,visualization skills},
}

@Article{GomezTone2020,
  author       = {G{\'{o}}mez-Tone, Hugo C{\'{e}}sar and Martin-Gutierrez, Jorge and Anci, Lili Valencia and Luis, Carlos E.Mora},
  year         = {2020},
  journal       = {Symmetry},
  title        = {{International comparative pilot study of spatial skill development in engineering students through autonomous augmented reality-based training}},
  doi          = {10.3390/SYM12091401},
  issn         = {20738994},
  number       = {9},
  volume       = {12},
  abstract     = {Spatial ability is made up of several sub-components, such as the ability to perform mental rotation and object-based transformations. Together with each individual's attitudes and general skill sets, this specific ability plays an important role in technical professions such as engineering. The components of spatial ability can be enhanced using targeted training or educational programs. This study analyses the levels of spatial skills in first-year engineering students at two universities, one in Spain and one in Peru. The purpose of the study is to establish the extent of symmetry between these study groups in terms of their spatial skills. Initial comparisons indicate that the Peruvian students have a lower level of spatial skill prior to training than their Spanish cohorts. AR-based training delivering representational system content was used with engineering students at both universities to boost spatial abilities. The results obtained indicate the training was effective, as both experimental groups made significant gains in their level of spatial ability. No difference was detected in either experimental group for the variable gender. The comparison of spatial ability gains between both countries is similar, although there is significant difference in the spatial ability component spatial visualization. In this instance, gains in this component were higher amongst the student population in Peru.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\'{o}}mez-Tone et al. - 2020 - International comparative pilot study of spatial skill development in engineering students through autonomous.pdf:pdf},
  keywords     = {Augmented reality,Engineering education,Graphic engineering,Spatial development,Spatial skills},
}

@Book{Bimber2004,
  author        = {Bimber, Oliver and Raskar, Ramesh},
  year          = {2004},
  title         = {{Spatial Augmented Reality Merging Real and Virtual Worlds}},
  edition       = {1},
  isbn          = {1-56881-230-2},
  location      = {Wellesley},
  publisher     = {A K Peters Ltd.},
  annotation    = {Text book on SAR work, Used for jeremeys implemention of SAR.},
  keywords      = {Computer Graphics,Virtual Reality},
  mendeley-tags = {Computer Graphics,Virtual Reality},
}

@Article{Li2019,
  author       = {Li, Sen and Feng, Chunyong and Niu, Yunchen and Shi, Long and Wu, Zeqi and Song, Huaitao},
  year         = {2019},
  journal       = {Sensors (Switzerland)},
  title        = {{A fire reconnaissance robot based on SLAM position, thermal imaging technologies, and AR display}},
  doi          = {10.3390/s19225036},
  issn         = {14248220},
  number       = {22},
  volume       = {19},
  abstract     = {Due to hot toxic smoke and unknown risks under fire conditions, detection and relevant reconnaissance are significant in avoiding casualties. A fire reconnaissance robot was therefore developed to assist in the problem by offering important fire information to fire fighters. The robot consists of three main systems, a display operating system, video surveillance, and mapping and positioning navigation. Augmented reality (AR) goggle technology with a display operating system was also developed to free fire fighters' hands, which enables them to focus on rescuing processes and not system operation. Considering smoke disturbance, a thermal imaging video surveillance system was included to extract information from the complicated fire conditions. Meanwhile, a simultaneous localization and mapping (SLAM) technology was adopted to build the map, together with the help of a mapping and positioning navigation system. This can provide a real-time map under the rapidly changing fire conditions to guide the fire fighters to the fire sources or the trapped occupants. Based on our experiments, it was found that all the tested system components work quite well under the fire conditions, while the video surveillance system produces clear images under dense smoke and a high-temperature environment; SLAM shows a high accuracy with an average error of less than 3.43\%; the positioning accuracy error is 0.31 m; and the maximum error for the navigation system is 3.48\%. The developed fire reconnaissance robot can provide a practically important platform to improve fire rescue efficiency to reduce the fire casualties of fire fighters.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - A fire reconnaissance robot based on SLAM position, thermal imaging technologies, and AR display.pdf:pdf},
  keywords     = {AR,Fire reconnaissance robot,SLAM,Thermal imaging},
  pmid         = {31752251},
}

@Article{Debarba2018,
  author       = {Debarba, Henrique Galvan and {De Oliveira}, Marcelo Elias and Ladermann, Alexandre and Chague, Sylvain and Charbonnier, Caecilia},
  year         = {2018},
  journal       = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  title        = {{Augmented Reality Visualization of Joint Movements for Physical Examination and Rehabilitation}},
  doi          = {10.1109/VR.2018.8446368},
  number       = {March},
  pages        = {537--538},
  abstract     = {We present a visualization tool for human motion analysis in augmented reality. Our tool builds upon our previous work on joint biomechanical modelling for kinematic analysis, based on optical motion capture and personalized anatomical reconstruction of joint structures from medical imaging. It provides healthcare professionals with the in situ visualization of joint movements, where bones are accurately rendered as a holographic overlay on the subject-like if the user has an 'X-ray vision'-and in real-Time as the subject performs the movement. Currently, hip and knee joints are supported.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Debarba et al. - 2018 - Augmented Reality Visualization of Joint Movements for Physical Examination and Rehabilitation.pdf:pdf},
  isbn         = {9781538633656},
  keywords     = {I.3.7 [Computer Graphics]: Three-Dimensional Graph},
}

@Article{Fotouhi2021,
  author       = {Fotouhi, Javad and Mehrfard, Arian and Song, Tianyu and Johnson, Alex and Osgood, Greg and Unberath, Mathias and Armand, Mehran and Navab, Nassir},
  year         = {2021},
  journal       = {IEEE Transactions on Medical Imaging},
  title        = {{Development and Pre-Clinical Analysis of Spatiotemporal-Aware Augmented Reality in Orthopedic Interventions}},
  doi          = {10.1109/TMI.2020.3037013},
  issn         = {1558254X},
  number       = {2},
  pages        = {765--778},
  volume       = {40},
  abstract     = {Suboptimal interaction with patient data and challenges in mastering 3D anatomy based on ill-posed 2D interventional images are essential concerns in image-guided therapies. Augmented reality (AR) has been introduced in the operating rooms in the last decade; however, in image-guided interventions, it has often only been considered as a visualization device improving traditional workflows. As a consequence, the technology is gaining minimum maturity that it requires to redefine new procedures, user interfaces, and interactions. The main contribution of this paper is to reveal how exemplary workflows are redefined by taking full advantage of head-mounted displays when entirely co-registered with the imaging system at all times. The awareness of the system from the geometric and physical characteristics of X-ray imaging allows the exploration of different human-machine interfaces. Our system achieved an error of 4.76 ± 2.91mm for placing K-wire in a fracture management procedure, and yielded errors of 1.57 ± 1.16° and 1.46 ± 1.00° in the abduction and anteversion angles, respectively, for total hip arthroplasty (THA). We compared the results with the outcomes from baseline standard operative and non-immersive AR procedures, which had yielded errors of [4.61mm, 4.76°, 4.77°] and [5.13mm, 1.78°, 1.43°], respectively, for wire placement, and abduction and anteversion during THA. We hope that our holistic approach towards improving the interface of surgery not only augments the surgeon's capabilities but also augments the surgical team's experience in carrying out an effective intervention with reduced complications and provide novel approaches of documenting procedures for training purposes.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fotouhi et al. - 2021 - Development and Pre-Clinical Analysis of Spatiotemporal-Aware Augmented Reality in Orthopedic Interventions.pdf:pdf},
  keywords     = {Augmented reality,X-ray,frustum,interaction,surgery,visualization},
  pmid         = {33166252},
}

@Article{Kaeppler2013,
  author       = {Kaeppler, Sebastian and Wu, Wen and Chen, Terrence and Koch, Martin and Kiraly, Atilla P. and Strobel, Norbert and Hornegger, Joachim},
  year         = {2013},
  journal       = {Proceedings - International Symposium on Biomedical Imaging},
  title        = {{Semi-automatic catheter model generation using biplane x-ray images}},
  doi          = {10.1109/ISBI.2013.6556799},
  issn         = {19457928},
  pages        = {1416--1419},
  abstract     = {Recently, techniques for the automatic detection or tracking of surgical instruments in X-ray guided computer-assisted interventions have emerged. The purposes of these methods are to facilitate inter-modality registration, motion compensation, enhanced visualization or automatic landmark generation in augmented-reality applications. Most techniques incorporate a model of the device as prior information to evaluate results obtained from a low-level detector. In this paper, we present novel approaches which are able to generate both 2-D and 3-D models of circular and linear catheters from biplane X-ray images with only minimal user input. We apply these methods in the context of Electrophysiology to generate models of ablation and mapping catheters. An evaluation on clinical data sets yielded promising results. {\textcopyright} 2013 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaeppler et al. - 2013 - Semi-automatic catheter model generation using biplane x-ray images.pdf:pdf},
  isbn         = {9781467364546},
  keywords     = {Ablation,Biplane,Catheter,Detection,Electrophysiology,Mapping,Model,Reconstruction,Semi-automatic,X-ray},
  publisher    = {IEEE},
}

@Article{Sauer2005,
  author       = {Sauer, Frank},
  year         = {2005},
  journal       = {Annual International Conference of the IEEE Engineering in Medicine and Biology - Proceedings},
  title        = {{Image registration: Enabling technology for image guided surgery and therapy}},
  doi          = {10.1109/iembs.2005.1616182},
  issn         = {05891019},
  pages        = {7242--7245},
  volume       = {7 VOLS},
  abstract     = {Imaging looks inside the patient's body, exposing the patient's anatomy beyond what is visible on the surface. Medical Imaging has a very successful history for medical diagnosis. It also plays an increasingly important role as enabling technology for minimally invasive procedures. Interventional procedures (e.g. catheter based cardiac interventions) are traditionally supported by intra-procedure imaging (X-ray fluoro, ultrasound). There is realtime feedback, but the images provide limited information. Surgical procedures are traditionally supported with pre-operative images (CT, MR). The image quality can be very good; however, the link between images and patient has been lost. For both cases, image registration can play an essential role augmenting intra-op images with pre-op images, and mapping pre-op images to the patient's body. We will present examples of both approaches from an application oriented perspective, covering electrophysiology, radiation therapy, and neuro-surgery. Ultimately, as the boundaries between interventional radiology and surgery are becoming blurry, also the different methods for image guidance will merge. Image guidance will draw upon a combination of pre-op and intra-op imaging together with magnetic or optical tracking systems, and enable precise minimally invasive procedures. The information is registered into a common coordinate system, and allows advanced methods for visualization such as augmented reality or advanced methods for therapy delivery such as robotics. {\textcopyright} 2005 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sauer - 2005 - Image registration Enabling technology for image guided surgery and therapy.pdf:pdf},
  isbn         = {0780387406},
  publisher    = {IEEE},
}

@Article{Preetha2020,
  author     = {Preetha, Chandrakanth Jayachandran and Kloss, Jonathan and Wehrtmann, Fabian Siegfried and Sharan, Lalith and Fan, Carolyn and M{\"{u}}ller-Stich, Beat Peter and Felix, Nickel and Engelhardt, Sandy},
  year       = {2020},
  title      = {{Towards augmented reality-based suturing in monocular laparoscopic training}},
  doi        = {10.1117/12.2550830},
  eprint     = {2001.06894},
  eprinttype = {arXiv},
  issn       = {1996756X},
  number     = {March 2020},
  pages      = {32},
  abstract   = {Minimally Invasive Surgery (MIS) techniques have gained rapid popularity among surgeons since they offer significant clinical benefits including reduced recovery time and diminished post-operative adverse effects. However, conventional endoscopic systems output monocular video which compromises depth perception, spatial orientation and field of view. Suturing is one of the most complex tasks performed under these circumstances. Key components of this tasks are the interplay between needle holder and the surgical needle. Reliable 3D localization of needle and instruments in real time could be used to augment the scene with additional parameters that describe their quantitative geometric relation, e.g. the relation between the estimated needle plane and its rotation center and the instrument. This could contribute towards standardization and training of basic skills and operative techniques, enhance overall surgical performance, and reduce the risk of complications. The paper proposes an Augmented Reality environment with quantitative and qualitative visual representations to enhance laparoscopic training outcomes performed on a silicone pad. This is enabled by a multi-task supervised deep neural network which performs multi-class segmentation and depth map prediction. Scarcity of labels has been conquered by creating a virtual environment which resembles the surgical training scenario to generate dense depth maps and segmentation maps. The proposed convolutional neural network was tested on real surgical training scenarios and showed to be robust to occlusion of the needle. The network achieves a dice score of 0.67 for surgical needle segmentation, 0.81 for needle holder instrument segmentation and a mean absolute error of 6.5 mm for depth estimation.},
  arxivid    = {2001.06894},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preetha et al. - 2020 - Towards augmented reality-based suturing in monocular laparoscopic training.pdf:pdf},
  isbn       = {9781510633971},
}

@Article{Fiorentino2011,
  author       = {Fiorentino, Michele and Uva, Antonio E. and Monno, Giuseppe},
  year         = {2011},
  journal       = {ASME 2011 World Conference on Innovative Virtual Reality, WINVR 2011},
  title        = {{Product manufacturing information management in interactive augmented technical drawings}},
  doi          = {10.1115/WINVR2011-5516},
  number       = {March 2019},
  pages        = {113--122},
  abstract     = {This work presents a novel Augmented Realty (AR) application to superimpose interactive Product Manufacturing Information (PMI) onto paper technical drawings. We augment drawings with contextual data and use a novel tangible interface to access the data in a natural way. We present an optimized PMI data visualization algorithm for CAD models in order to avoid model and annotation cluttering. Our algorithm ranks the model faces with technical annotations according to angle, distance, occlusion and area. The number of annotations visualized on 3D model is chosen following the cognitive perception theory to avoid information overload. We also extended the navigation metaphor adding the concept of tangible model navigation and flipping using the duplex drawing. As case studies we used annotated models from ASME standards. By using PC hardware and common paper drawings, this approach can be integrated at low-cost in existing industrial processes. Copyright {\textcopyright} 2011 by ASME.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fiorentino, Uva, Monno - 2011 - Product manufacturing information management in interactive augmented technical drawings.pdf:pdf},
  isbn         = {9780791844328},
  keywords     = {Augmented reality,PMI annotations,Technical drawing},
}

@Article{Johnson2004,
  author       = {Johnson, Laura and Edwards, Philip and Griffin, Lewis and Hawkes, David},
  year         = {2004},
  journal       = {Medical Imaging 2004: Image Perception, Observer Performance, and Technology Assessment},
  title        = {{Depth perception of stereo overlays in image-guided surgery}},
  doi          = {10.1117/12.535138},
  issn         = {16057422},
  number       = {May 2004},
  pages        = {263},
  volume       = {5372},
  abstract     = {See-through augmented reality (AR) systems for image-guided surgery merge volume rendered MRI/CT data directly with the surgeon's view of the patient during surgery. Research has so far focused on optimizing the technique of aligning and registering the computer-generated anatomical images with the patient's anatomy during surgery. We have previously developed a registration and calibration method that allows alignment of the virtual and real anatomy to -1mm accuracy 1 . Recently we have been investigating the accuracy with which observers can interpret the combined visual information presented with an optical see-through AR system. We found that depth perception of a virtual image presented in stereo below a physical surface was misperceived compared to viewing the target in the absence of a surface. Observers overestimated depth for a target 0-2cm below the surface and underestimated the depth for all other presentation depths. The perceptual error could be reduced, but not eliminated, when a virtual rendering of the physical surface was displayed simultaneously with the virtual image. The findings suggest that misperception is due either to accommodation conflict between the physical surface and the projected AR image, or the lack of correct occlusion between the virtual and real surfaces.},
  annotation   = {This is a old paper and I kinda only skimmed it I don't condcider this to be X-ray vision but it is looking at ways to draw on people which is cool but probably not too relivent. Never the less I kept it because it could be handy},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Johnson et al. - 2004 - Depth perception of stereo overlays in image-guided surgery.pdf:pdf},
  keywords     = {augmented reality,depth perception error,image guided surgery},
}

@Book{Qian2019,
  author    = {Qian, Ming and Nicholson, John and Wang, Erin},
  year      = {2019},
  title     = {{Quality of Experience Comparison Between Binocular and Monocular Augmented Reality Display Under Various Occlusion Conditions for Manipulation Tasks with Virtual Instructions}},
  doi       = {10.1007/978-3-030-21607-8_38},
  isbn      = {9783030216061},
  pages     = {490--499},
  publisher = {Springer International Publishing},
  url       = {http://dx.doi.org/10.1007/978-3-030-21607-8_38},
  volume    = {11574 LNCS},
  abstract  = {Using optical head-mounted display (HMD) devices, users can see both real world and Augmented Reality (AR) content simultaneously. AR content can be displayed to both eyes (binocular) or in one eye (monocular). For a binocular display, users benefit from (a) using both eyes to focus on the same content, and (b) having depth perception. However, the vergence-accommodation conflict can negatively impact the time and accuracy of fusing the views. For a monocular display, users benefit from (a) easy and quick focal depth switches between the virtual content and the physical world, and (b) having a larger virtual information overlay in one eye while also seeing the real-world in the other eye. In this study, users performed manual tasks by following real-time, step-by-step instructions for 2D tasks (drawing cartoon characters) and 3D tasks (assembling plastic bricks). The instructions were presented on an AR HMD through various occlusion conditions, after which we compared the users' Quality of Experience (QoE) feedback. Our investigation found that users commonly chose to separate the AR content display from the physical working area, placing them adjacent in the field of view and shifting their attention between them. The overwhelming majority of users preferred the binocular display. For a monocular display, users need to balance the benefits of depth perception (for 3D tasks) and the annoyance of binocular rivalry. While most users can tolerate binocular rivalry, a significant subset have a low tolerance for binocular rivalry and prefer to mask or close the eye without the virtual display.},
  booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Nicholson, Wang - 2019 - Quality of Experience Comparison Between Binocular and Monocular Augmented Reality Display Under Various.pdf:pdf},
  issn      = {16113349},
  keywords  = {Attention switch,Augmented Reality (AR),Binocular,Binocular fusion,Binocular interaction,Binocular rivalry,Focal depth,Head-mounted device (HMD),Monocular,Quality of Experience (QoE),Stereoscopic display,Virtual content,Virtual instruction overlay},
}

@Article{Behringer1999,
  author       = {Behringer, Reinhold and Chen, Steven and Sundareswaran, Venkataraman and Wang, Kenneth and Vassiliou, Marius},
  year         = {1999},
  journal       = {Computers and Graphics (Pergamon)},
  title        = {{Distributed device diagnostics system utilizing augmented reality and 3D audio}},
  doi          = {10.1016/S0097-8493(99)00108-9},
  issn         = {00978493},
  number       = {6},
  pages        = {821--825},
  volume       = {23},
  abstract     = {Augmented reality (AR), combining virtual environments with the perception of the real world, can be used to provide instructions for routine maintenance and error diagnostics of technical devices. The Rockwell Science Center (RSC) is developing a system that utilizes AR techniques to provide `X-ray vision' into real objects. The system can overlay 3D rendered objects, animations, and text annotations onto the video image of a known object. An automated speech recognition system allows the user to query the status of device components. The response is given as an animated rendition of a CAD model and/or as auditory cues using 3D audio. This diagnostics system also allows the user to leave spoken annotations attached to device modules as ASCII text. The position of the user/camera relative to the device is tracked by a computer-vision-based tracking system using fiducial markers. The system is implemented on a distributed network of PCs, utilizing standard commercial off-the-shelf components (COTS).},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Behringer et al. - 1999 - Distributed device diagnostics system utilizing augmented reality and 3D audio.pdf:pdf},
  keywords     = {1,3d audio,areas has enabled the,augmented reality,context and related work,development of information sys-,devise diagnostics,during the past few,key,rapid progress in several,years},
}

@Article{Li2018,
  author       = {Li, Wei and Han, Yong and Liu, Yu and Zhu, Chenrong and Ren, Yibin and Wang, Yanjie and Chen, Ge},
  year         = {2018},
  journal       = {ISPRS International Journal of Geo-Information},
  title        = {{Real-time location-based rendering of urban underground pipelines}},
  doi          = {10.3390/ijgi7010032},
  issn         = {22209964},
  number       = {1},
  volume       = {7},
  abstract     = {The concealment and complex spatial relationships of urban underground pipelines present challenges in managing them. Recently, augmented reality (AR) has been a hot topic around the world, because it can enhance our perception of reality by overlaying information about the environment and its objects onto the real world. Using AR, underground pipelines can be displayed accurately, intuitively, and in real time. We analyzed the characteristics of AR and their application in underground pipeline management. We mainly focused on the AR pipeline rendering procedure based on the BeiDou Navigation Satellite System (BDS) and simultaneous localization and mapping (SLAM) technology. First, in aiming to improve the spatial accuracy of pipeline rendering, we used differential corrections received fromthe Ground-Based Augmentation System to compute the precise coordinates of users in real time, which helped us accurately retrieve and draw pipelines near the users, and by scene recognition the accuracy can be further improved. Second, in terms of pipeline rendering, we used Visual-Inertial Odometry (VIO) to track the rendered objects and made some improvements to visual effects, which can provide steady dynamic tracking of pipelines even in relatively markerless environments and outdoors. Finally, we used the occlusion method based on real-time 3D reconstruction to realistically express the immersion effect of underground pipelines. We compared our methods to the existing methods and concluded that the method proposed in this research improves the spatial accuracy of pipeline rendering and the portability of the equipment. Moreover, the updating of our rendering procedure corresponded with the moving of the user's location, thus we achieved a dynamic rendering of pipelines in the real environment.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2018 - Real-time location-based rendering of urban underground pipelines.pdf:pdf},
  isbn         = {2011301610},
  keywords     = {Augmented reality,Differential correction,Dynamic tracking,Occlusion method,Underground pipelines,Visual Inertial Odometry},
}

@Article{Teatini2021,
  author       = {Teatini, Andrea and Kumar, Rahul P. and Elle, Ole Jakob and Wiig, Ola},
  year         = {2021},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title        = {{Mixed reality as a novel tool for diagnostic and surgical navigation in orthopaedics}},
  doi          = {10.1007/s11548-020-02302-z},
  issn         = {18616429},
  number       = {3},
  pages        = {407--414},
  url          = {https://doi.org/10.1007/s11548-020-02302-z},
  volume       = {16},
  abstract     = {Purpose: This study presents a novel surgical navigation tool developed in mixed reality environment for orthopaedic surgery. Joint and skeletal deformities affect all age groups and greatly reduce the range of motion of the joints. These deformities are notoriously difficult to diagnose and to correct through surgery. Method: We have developed a surgical tool which integrates surgical instrument tracking and augmented reality through a head mounted display. This allows the surgeon to visualise bones with the illusion of possessing “X-ray” vision. The studies presented below aim to assess the accuracy of the surgical navigation tool in tracking a location at the tip of the surgical instrument in holographic space. Results: Results show that the average accuracy provided by the navigation tool is around 8 mm, and qualitative assessment by the orthopaedic surgeons provided positive feedback in terms of the capabilities for diagnostic use. Conclusions: More improvements are necessary for the navigation tool to be accurate enough for surgical applications, however, this new tool has the potential to improve diagnostic accuracy and allow for safer and more precise surgeries, as well as provide for better learning conditions for orthopaedic surgeons in training.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teatini et al. - 2021 - Mixed reality as a novel tool for diagnostic and surgical navigation in orthopaedics.pdf:pdf},
  keywords     = {Augmented reality,Holographic visualisation,Image-guided diagnosis,Image-guided treatment,Mixed reality,Orthopaedic surgery,Orthopaedics,Surgical navigation},
  pmid         = {33555563},
  publisher    = {Springer International Publishing},
}

@Article{Bailey2020,
  author       = {Bailey, Stephen A and Rufino, Miguel A},
  year         = {2020},
  journal       = {JOHNS HOPKINS APL TECHNICAL DIGEST},
  title        = {{Exploring Immersive Technology at APL Simulated X-Ray Vision Using Mixed Reality}},
  number       = {3},
  pages        = {200--204},
  volume       = {35},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bailey, Rufino - 2020 - Exploring Immersive Technology at APL Simulated X-Ray Vision Using Mixed Reality.pdf:pdf},
  keywords     = {Exploring Immersive Technology at APL},
}

@Article{DePaolis2017,
  author       = {{De Paolis}, Lucio Tommaso},
  year         = {2017},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{Augmented visualization as surgical support in the treatment of tumors}},
  doi          = {10.1007/978-3-319-56148-6_38},
  issn         = {16113349},
  pages        = {432--443},
  volume       = {10208 LNCS},
  abstract     = {Minimally Invasive Surgery is a surgery technique that provides evident advantages for the patients, but also some difficulties for the surgeons. In medicine, the Augmented Reality technology allows surgeons to have a sort of “X-ray” vision of the patient's body and can help them during the surgical procedures. In this paper is presented an application that could be used as support for a more accurate preoperative surgical planning and also for an image-guided surgery. The Augmented Reality can support the surgeon during the treatment of the liver tumors with the radiofrequency ablation in order to guide the needle and to have an accurate placement of the surgical instrument within the lesion. The augmented visualization can avoid as much as possible to destroy healthy cells of the liver.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Paolis - 2017 - Augmented visualization as surgical support in the treatment of tumors.pdf:pdf},
  isbn         = {9783319561479},
  keywords     = {Augmented reality,Computer aided surgery,Image guided surgery},
}

@Article{Fischer2016,
  author       = {Fischer, Marius and Fuerst, Bernhard and Lee, Sing Chun and Fotouhi, Javad and Habert, Severine and Weidert, Simon and Euler, Ekkehard and Osgood, Greg and Navab, Nassir},
  year         = {2016},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title        = {{Preclinical usability study of multiple augmented reality concepts for K-wire placement}},
  doi          = {10.1007/s11548-016-1363-x},
  issn         = {18616429},
  number       = {6},
  pages        = {1007--1014},
  volume       = {11},
  abstract     = {Purpose: In many orthopedic surgeries, there is a demand for correctly placing medical instruments (e.g., K-wire or drill) to perform bone fracture repairs. The main challenge is the mental alignment of X-ray images acquired using a C-arm, the medical instruments, and the patient, which dramatically increases in complexity during pelvic surgeries. Current solutions include the continuous acquisition of many intra-operative X-ray images from various views, which will result in high radiation exposure, long surgical durations, and significant effort and frustration for the surgical staff. This work conducts a preclinical usability study to test and evaluate mixed reality visualization techniques using intra-operative X-ray, optical, and RGBD imaging to augment the surgeon's view to assist accurate placement of tools. Method: We design and perform a usability study to compare the performance of surgeons and their task load using three different mixed reality systems during K-wire placements. The three systems are interventional X-ray imaging, X-ray augmentation on 2D video, and 3D surface reconstruction augmented by digitally reconstructed radiographs and live tool visualization. Results: The evaluation criteria include duration, number of X-ray images acquired, placement accuracy, and the surgical task load, which are observed during 21 clinically relevant interventions performed by surgeons on phantoms. Finally, we test for statistically significant improvements and show that the mixed reality visualization leads to a significantly improved efficiency. Conclusion: The 3D visualization of patient, tool, and DRR shows clear advantages over the conventional X-ray imaging and provides intuitive feedback to place the medical tools correctly and efficiently.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer et al. - 2016 - Preclinical usability study of multiple augmented reality concepts for K-wire placement.pdf:pdf},
  keywords     = {Interventional imaging,Orthopedic and Trauma surgery,Usability study},
  pmid         = {26995603},
}

@Article{Hernell2007,
  author       = {Hernell, Frida and Ynnerman, Anders and Smedby, {\"{O}}rjan},
  year         = {2007},
  journal       = {Studies in Health Technology and Informatics},
  title        = {{A blending technique for enhanced depth perception in medical x-ray vision applications}},
  issn         = {18798365},
  pages        = {176--178},
  volume       = {125},
  abstract     = {Depth perception is a common problem for x-ray vision in augmented reality applications since the goal is to visualize occluded and embedded objects. In this paper we present an x-ray vision blending method for neurosurgical applications that intensifies the interposition depth cue in order to achieve enhanced depth perception. The proposed technique emphasizes important structures, which provides the user with an improved depth context.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hernell, Ynnerman, Smedby - 2007 - A blending technique for enhanced depth perception in medical x-ray vision applications.pdf:pdf},
  keywords     = {MRI,X-ray vision,depth perception,neurosurgical planning},
  pmid         = {17377261},
}

@Article{Ohta2010,
  author       = {Ohta, Yuichi and Kameda, Yoshinari and Kitahara, Itaru and Hayashi, Masayuki and Yamazaki, Shinya},
  year         = {2010},
  journal       = {Communications in Computer and Information Science},
  title        = {{See-through vision: A visual augmentation method for sensing-web}},
  doi          = {10.1007/978-3-642-14058-7_71},
  issn         = {18650929},
  pages        = {690--699},
  volume       = {81 PART 2},
  abstract     = {Many surveillance cameras are being installed throughout the environments of our daily lives because they effectively maintain safety and offer security to ordinary people. On the other hand, they can also cause serious discomfort and anxiety to the same people. This paper proposes a novel framework called See-Through Vision that utilizes surveillance cameras as a public sensing device by exploiting a state-of-the-art mixed-reality technique. Some advanced systems developed by us for See-Through Vision are introduced, and we discuss their advantages and how they actually maximize the benefits of surveillance cameras. Since See-Through Vision is a powerful tool that may violate the privacy of other people, we propose a sophisticated solution that can strike a good balance between the benefits and the drawbacks of such an approach. We propose privacy-safe See-Through Vision and demonstrate the system at a shopping mall in Kyoto. {\textcopyright} Springer-Verlag Berlin Heidelberg 2010.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ohta et al. - 2010 - See-through vision A visual augmentation method for sensing-web.pdf:pdf},
  isbn         = {9783642140570},
  keywords     = {Outdoor mixed-reality,Privacy control,Sensing-web,Visual augmentation},
}

@Article{Alexander2018,
  author       = {Alexander, Clayton and Taylor, Giacomo and Lee, Sing Chun and Fuerst, Bernhard and Johnson, Alex and Osgood, Greg and Taylor, Russell and Khanuja, Herpal and Navab, Nassir and Armand, Mehran and Fotouhi, Javad and Unberath, Mathias},
  year         = {2018},
  journal       = {SPIE Medical Imaging},
  title        = {{Technical note: an augmented reality system for total hip arthroplasty}},
  doi          = {10.1117/12.2322399},
  issn         = {1605-7422},
  number       = {March 2018},
  pages        = {104},
  abstract     = {Proper implant alignment is a critical step in total hip arthroplasty (THA) procedures. In current practice, correct alignment of the acetabular cup is verified in C-arm X-ray images that are acquired in an anteriorposterior (AP) view. Favorable surgical outcome is, therefore, heavily dependent on the surgeon's experience in understanding the 3D orientation of a hemispheric implant from 2D AP projection images. This work proposes an easy to use intra-operative component planning system based on two C-arm X-ray images that is combined with 3D augmented reality (AR) visualization that simplifies impactor and cup placement according to the planning by providing a real-time RGBD data overlay. We evaluate the feasibility of our system in a user study comprising four orthopedic surgeons at the Johns Hopkins Hospital, and also report errors in translation, anteversion, and abduction as low as 1.98 mm, 1.10 degrees, and 0.53 degrees, respectively. The promising performance of this AR solution shows that deploying this system could eliminate the need for excessive radiation, simplify the intervention, and enable reproducibly accurate placement of acetabular implants.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alexander et al. - 2018 - Technical note an augmented reality system for total hip arthroplasty.pdf:pdf},
  isbn         = {9781510616417},
}

@Article{DePaolis2018,
  author       = {{De Paolis}, Lucio Tommaso and Ricciardi, Francesco},
  year         = {2018},
  journal       = {Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization},
  title        = {{Augmented visualisation in the treatment of the liver tumours with radiofrequency ablation}},
  doi          = {10.1080/21681163.2017.1287598},
  issn         = {21681171},
  number       = {4},
  pages        = {396--404},
  url          = {http://dx.doi.org/10.1080/21681163.2017.1287598},
  volume       = {6},
  abstract     = {In medicine, the augmented reality technology allows surgeons to have a sort of ‘X-ray' vision of the patient's body and can help them during the surgical procedures. This paper describes an application of augmented reality that could be used as a support for a more accurate preoperative surgical planning and also for an image-guided procedure. Augmented reality can support the surgeon during the needle insertion in the treatment of liver tumours with the radiofrequency ablation in order to guide the needle and to have an accurate placement of the surgical instrument within the lesion. The augmented visualisation can prevent as much as possible damages to the healthy cells of the liver.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Paolis, Ricciardi - 2018 - Augmented visualisation in the treatment of the liver tumours with radiofrequency ablation.pdf:pdf},
  keywords     = {Augmented reality,image-guided surgery,radiofrequency ablation},
  publisher    = {Taylor {\&} Francis},
}

@Article{ElSeoud2019,
  author       = {El-Seoud, Samir A. and Mady, Amr S. and Rashed, Essam A.},
  year         = {2019},
  journal       = {International journal of online and biomedical engineering},
  title        = {{An interactive mixed reality ray tracing rendering mobile application of medical data in minimally invasive surgeries}},
  doi          = {10.3991/ijoe.v15i06.9933},
  issn         = {26268493},
  number       = {6},
  pages        = {4--14},
  volume       = {15},
  abstract     = {Visualization of patient's anatomy is the most important pre-operation process in surgeries; minimally invasive surgeries are among these types of medical operations that counts totally on medical visualization before operating on a patient. However, medicine has a problem in visualizing patients' through looking through multiple slices of scans, trying to understand the threedimensional (3D) anatomical structure of patients. With Mixed Reality (MR) the developments in medicine visualization will become much easier and creates a better environment for surgeries. This will help reduce the excessive effort and time spent by surgeons to locate where the problem lies with patients without looking through multiple of two-dimensional (2D) slices, but to see patients' bodies in 3D in front of them augmented in their reality, and to interact with it whatever pleases them. Moreover, this will reduce the number of scans that doctors will ask their patient's for, which will result in less harmful x-ray dosages for both the patient and the radiologist. Biomedical development in medical visualization is an active research topic as it provides the physicians with required devices for clinically feasible way for diagnosis, follow-up and take decisions in different disease life line. Current clinical imaging facility can provide a 3D imaging that can be used to guide different interventional procedures. The main challenge is how to map the information presented in the digital image with the real object. This is commonly implemented by mental processing that requires skills from the medical doctor. This paper contributes to this problem by providing a mixed reality system to merge the digital image of the patient anatomy with the patient visual image. Anatomical image obtained from Computed Tomography (CT) or Magnetic Resonance Imaging (MRI) is mapped over the patient body using virtual reality (VR) head-mounted device (HMD).},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/El-Seoud, Mady, Rashed - 2019 - An interactive mixed reality ray tracing rendering mobile application of medical data in minimally invas.pdf:pdf},
  keywords     = {Medical imaging,Mixed reality,Ray-casting,Ray-tracing,Volume rendering},
}

@Article{Paas2003,
  author       = {Paas, Fred and Tuovinen, Juhani E. and Tabbers, Huib and {Van Gerven}, Pascal W.M.},
  year         = {2003},
  journal       = {Educational Psychologist},
  title        = {{Cognitive load measurement as a means to advance cognitive load theory}},
  doi          = {10.1207/S15326985EP3801_8},
  issn         = {00461520},
  number       = {1},
  pages        = {63--71},
  volume       = {38},
  abstract     = {In this article, we discuss cognitive load measurement techniques with regard to their contribution to cognitive load theory (CLT). CLT is concerned with the design of instructional methods that efficiently use people's limited cognitive processing capacity to apply acquired knowledge and skills to new situations (i.e., transfer). CLT is based on a cognitive architecture that consists of a limited working memory with partly independent processing units for visual and auditory information, which interacts with an unlimited long-term memory. These structures and functions of human cognitive architecture have been used to design a variety of novel efficient instructional methods. The associated research has shown that measures of cognitive load can reveal important information for CLT that is not necessarily reflected by traditional performance-based measures. Particularly, the combination of performance and cognitive load measures has been identified to constitute a reliable estimate of the mental efficiency of instructional methods. The discussion of previously used cognitive load measurement techniques and their role in the advancement of CLT is followed by a discussion of aspects of CLT that may benefit by measurement of cognitive load. Within the cognitive load framework, we also discuss some promising new techniques.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paas et al. - 2003 - Cognitive load measurement as a means to advance cognitive load theory.pdf:pdf},
}

@Article{Elmqvist2007,
  author       = {Elmqvist, Niklas and Assarsson, Ulf and Tsigas, Philippas},
  year         = {2007},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{Employing dynamic transparency for 3D occlusion management: Design issues and evaluation}},
  doi          = {10.1007/978-3-540-74796-3_54},
  issn         = {16113349},
  number       = {PART 1},
  pages        = {532--545},
  volume       = {4662 LNCS},
  abstract     = {Recent developments in occlusion management for 3D environments often involve the use of dynamic transparency, or virtual "X-ray vision", to promote target discovery and access in complex 3D worlds. However, there are many different approaches to achieving this effect and their actual utility for the user has yet to be evaluated. Furthermore, the introduction of semi-transparent surfaces adds additional visual complexity that may actually have a negative impact on task performance. In this paper, we report on an empirical user study comparing dynamic transparency to standard viewpoint controls. Our implementation of the technique is an image-space algorithm built using modern programmable shaders to achieve real-time performance and visually pleasing results. Results from the user study indicate that dynamic transparency is superior for perceptual tasks in terms of both efficiency and correctness. {\textcopyright} IFIP International Federation for Information Processing 2007.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elmqvist, Assarsson, Tsigas - 2007 - Employing dynamic transparency for 3D occlusion management Design issues and evaluation.pdf:pdf},
  isbn         = {9783540747949},
}

@Article{VanWaveren2016,
  author       = {{Van Waveren}, J. M.P.},
  year         = {2016},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{The asynchronous time warp for virtual reality on consumer hardware}},
  doi          = {10.1145/2993369.2993375},
  pages        = {37--46},
  volume       = {02-04-Nove},
  abstract     = {To help create a true sense of presence in a virtual reality experience, a so called "time warp" may be used. This time warp does not only correct for the optical aberration of the lenses used in a virtual reality headset, it also transforms the stereoscopic images based on the very latest head tracking information to significantly reduce the motion-to-photon delay (or end-to-end latency). The time warp operates as close as possible to the display refresh, retrieves updated head tracking information and transforms a stereoscopic pair of images from representing a view at the time it was rendered, to representing the correct view at the time it is displayed. When run asynchronously to the stereoscopic rendering, the time warp can be used to increase the perceived frame rate and to smooth out inconsistent frame rates. Asynchronous operation can also improve the overall graphics hardware utilization by not requiring the stereoscopic rendering to be synchronized with the display refresh cycle. However, on today's consumer hardware it is challenging to implement a high quality time warp that is fast, has predictable latency and throughput, and runs asynchronously. This paper discusses the various challenges and the different trade-offs that need to be considered when implementing an asynchronous time warp on consumer hardware.},
  file         = {:D\:/Thomas/Downloads/2993369.2993375.pdf:pdf},
  isbn         = {9781450344913},
  keywords     = {Image warping,Latency,Virtual reality},
}

@Article{Landvoigt2017,
  author     = {Landvoigt, M{\'{a}}rcio and Cardoso, Jonatha},
  year       = {2017},
  title      = {{Analysis about publications on Facebook pages: finding of important characteristics}},
  doi        = {10.1145/123},
  eprint     = {arXiv:1605.08325v1},
  eprinttype = {arXiv},
  number     = {February},
  pages      = {8},
  url        = {http://www.kdd.org/kdd2016/papers/files/Paper_799.pdf},
  abstract   = {Com o desenvolvimento das redes sociais e o crescimento do n{\'{u}}mero de pessoas que participam destas, h{\'{a}} a necessidade de que postagens e publica{\c{c}}{\~{o}}es sejam atrativas. Por este motivo, o presente artigo tem por objetivo apresentar uma an{\'{a}}lise das principais caracter{\'{i}}sticas encontradas nas postagens de p{\'{a}}ginas publicadas na rede social Facebook. Para tanto, foram selecionadas p{\'{a}}ginas de diferentes p{\'{u}}blicos-alvo, tamanhos e temas, obtendo, ent{\~{a}}o, dados de suas publica{\c{c}}{\~{o}}es e desempenho alcan{\c{c}}ado. Pode-se, ent{\~{a}}o, compreender quais as caracter{\'{i}}sticas que as publica{\c{c}}{\~{o}}es devem apresentar para que possam alcan{\c{c}}ar um p{\'{u}}blico maior.},
  arxivid    = {arXiv:1605.08325v1},
  file       = {:D\:/Thomas/Downloads/ARTimewarping-Manuscript.pdf:pdf},
  isbn       = {9781450342322},
  keywords   = {Facebook,Features of publication,Social Networks},
  journal   = {Sage Research Methods Cases},
  publisher = {Sage Publications}
}

@Article{Guinet2019,
  author       = {Guinet, A. L. and Bouyer, G. and Otmane, S. and Desailly, E.},
  year         = {2019},
  journal       = {Computer Methods in Biomechanics and Biomedical Engineering},
  title        = {{Reliability of the head tracking measured by Microsoft Hololens during different walking conditions}},
  doi          = {10.1080/10255842.2020.1714228},
  issn         = {1025-5842},
  number       = {sup1},
  pages        = {S169--S171},
  url          = {https://doi.org/10.1080/10255842.2020.1714228},
  volume       = {22},
  abstract     = {Augmented reality (AR) is a technology that expand the real environment by adding digital holograms into it. AR appears to be a promising field of development for serious games, especially for walk...},
  file         = {:D\:/Thomas/Downloads/Reliability of the head tracking measured by Microsoft Hololens during different walking conditions.pdf:pdf},
  publisher    = {Taylor {\&} Francis},
}

@Article{Huebner2020,
  author       = {H{\"{u}}bner, Patrick and Clintworth, Kate and Liu, Qingyi and Weinmann, Martin and Wursthorn, Sven},
  year         = {2020},
  journal       = {Sensors (Switzerland)},
  title        = {{Evaluation of hololens tracking and depth sensing for indoor mapping applications}},
  doi          = {10.3390/s20041021},
  issn         = {14248220},
  number       = {4},
  volume       = {20},
  abstract     = {The Microsoft HoloLens is a head-worn mobile augmented reality device that is capable of mapping its direct environment in real-time as triangle meshes and localize itself within these three-dimensional meshes simultaneously. The device is equipped with a variety of sensors including four tracking cameras and a time-of-flight (ToF) range camera. Sensor images and their poses estimated by the built-in tracking system can be accessed by the user. This makes the HoloLens potentially interesting as an indoor mapping device. In this paper, we introduce the different sensors of the device and evaluate the complete system in respect of the task of mapping indoor environments. The overall quality of such a system depends mainly on the quality of the depth sensor together with its associated pose derived from the tracking system. For this purpose, we first evaluate the performance of the HoloLens depth sensor and its tracking system separately. Finally, we evaluate the overall system regarding its capability for mapping multi-room environments.},
  file         = {:D\:/Thomas/Downloads/sensors-20-01021.pdf:pdf},
  keywords     = {Augmented reality,Depth camera,HoloLens,Indoor mapping,Time-of-flight camera,Tracking},
  pmid         = {32074980},
}

@Article{Drebin1988,
  author       = {Drebin, Robert A. and Carpenter, Loren and Hanrahan, Pat},
  year         = {1988},
  journal       = {Proceedings of the 15th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1988},
  title        = {{Volume rendering}},
  doi          = {10.1145/54852.378484},
  number       = {August 1988},
  pages        = {65--74},
  abstract     = {A technique for rendering images Of volumes containing mixtures of materials is presented. The shading model allows both the interior of a material and the boundary between materials to be colored. Image projection is performed by simulating the absorption of light along the ray path to the eye. The algorithms used are designed to avoid artifacts caused by aliasing and quantization and can be efficiently implemented on an image computer. Images from a variety of applications are shown.},
  file         = {:D\:/Thomas/DataThatShouldBeDeleted/Volume_Rendering.pdf:pdf},
  isbn         = {0897912756},
  keywords     = {Computed tomography (CT),Image processing,Magnetic resonance imaging (MRI),Medical imaging,Non-destructive evaluation (NDE),Scientific visualization},
}

@Article{Zhu2019,
  author       = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  year         = {2019},
  journal       = {36th International Conference on Machine Learning, ICML 2019},
  title        = {{The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects}},
  eprint       = {1803.00195},
  eprinttype   = {arXiv},
  pages        = {13199--13214},
  volume       = {2019-June},
  abstract     = {Understanding the behavior of stochastic gradient descent (SGD) in the context of deep neural networks has raised lots of concerns recently. Along this line, we study a general form of gradient based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. Through investigating this general optimization dynamics, we analyze the behavior of SGD on escaping from minima and its regularization effects. A novel indicator is derived to characterize the efficiency of escaping from minima through measuring the alignment of noise covariance and the curvature of loss function. Based on this indicator, two conditions arc established to show which type of noise structure is superior to isotropic noise in term of escaping efficiency. We further show that the anisotropic noise in SGD satisfies the two conditions, and thus helps to escape from sharp and poor minima effectively, towards more stable and flat minima that typically generalize well. We systematically design various experiments to verify the benefits of the anisotropic noise, compared with full gradient descent plus isotropic diffusion (i.e. Langevin dynamics).},
  arxivid      = {1803.00195},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2019 - The anisotropic noise in stochastic gradient descent Its behavior of escaping from sharp minima and regularization e.pdf:pdf},
  isbn         = {9781510886988},
}

@Article{Using2017,
  author = {Using, Evaluation and Method, T H E and Computer, O F and Design, Aided},
  year   = {2017},
  title  = {{3D COMPUTER PROCESSING AND IMPROVEMENT OF THE GEOMETRY OF RECEIVED PHOTOGRAMMETRIC MODELS THROUGH PHOTO SHOOTING WITH Faculty of Engineering Department of Mechanical Engineering PROCEEDINGS OF THE 5 INTERNATIONAL SCIENTIFIC CONFERENCE ON ADVANCES IN MECHA}},
  number = {December},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Using et al. - 2017 - 3D COMPUTER PROCESSING AND IMPROVEMENT OF THE GEOMETRY OF RECEIVED PHOTOGRAMMETRIC MODELS THROUGH PHOTO SHOOTING W.pdf:pdf},
}

@Article{Lagae2010,
  author       = {Lagae, A and Lefebvre, S and Drettakis, R.C.T.D.R.G. and Perlin, D.S.E.J.P.L.K. and Zwicker, M.},
  year         = {2010},
  journal       = {Eurographics},
  title        = {{State of the art in procedural noise functions}},
  number       = {May},
  url          = {http://www.cs.kuleuven.ac.be/$\sim$ares/publications/LLCDDELPZ10STARPNF/LLCDDELPZ10STARPNF.pdf},
  abstract     = {Procedural noise functions are widely used in Computer Graphics, from off-line rendering in movie production to interactive video games. The ability to add complex and intricate details at low memory and authoring cost is one of its main attractions. This state-of-the-art report is motivated by the inherent importance of noise in graphics, the widespread use of noise in industry, and the fact thatmany recent research developments justify the need for an up-to-date survey. Our goal is to provide both a valuable entry point into the field of procedural noise functions, as well as a comprehensive view of the field to the informed reader. In this report, we cover procedural noise functions in all their aspects. We outline recent advances in research on this topic, discussing and comparing recent and well established methods. We first formally define procedural noise functions based on stochastic processes and then classify and review existing procedural noise functions. We discuss how procedural noise functions are used for modeling and how they are applied on surfaces. We then introduce analysis tools and apply them to evaluate and compare the major approaches to noise generation. We finally identify several directions for future work.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lagae et al. - 2010 - State of the art in procedural noise functions.pdf:pdf},
  keywords     = {analysis,anisotropic noise,anti-aliasing,filtering,gabor noise,noise,perlin noise,power spectrum estimation,procedural,procedural modeling,procedural noise function,procedural texture,solid noise,solid texture,sparse convolution noise,spectral,spot noise,stochastic modeling,stochastic process,surface noise,texture synthesis,wavelet noise},
}

@Article{Miltiadou2021,
  author = {Miltiadou, Milto and Campbell, Neill D F and Cosker, Darren and Grant, Michael G},
  year   = {2021},
  title  = {{Lidar Voxel}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Miltiadou et al. - 2021 - Lidar Voxel.pdf:pdf},
}

@article{Dade,
abstract = {Toonify seeks to emulate the types of cel-shading effects offered by graphics engines in a lighthearted and user-friendly way. I. INTRODUCTION With the recent success of Instagram, the popularity of simple and fun photo effects apps has been on the rise. The mobile platform presents a unique arena for these applications by connecting users with both the means to capture images, and the computational power to perform sophisticated processing on these images. Toonify seeks to leverage the existing OpenCV libary for Android in order to emulate a particular effect known as cel-shading in the computer graphics world.},
author = {Dade, Kevin},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dade - Unknown - Toonify Cartoon Photo Effect Application.pdf:pdf},
pages = {1--3},
title = {{Toonify : Cartoon Photo Effect Application}},
url = {https://stacks.stanford.edu/file/druid:yt916dh6570/Dade_Toonify.pdf}
}

@Article{Weisst2006,
  author       = {Weisst, Ben},
  year         = {2006},
  journal       = {ACM Transactions on Graphics},
  title        = {{Fast median and bilateral filtering}},
  doi          = {10.1145/1141911.1141918},
  issn         = {07300301},
  number       = {3},
  pages        = {519--526},
  volume       = {25},
  abstract     = {Median filtering is a cornerstone of modern image processing and is used extensively in smoothing and de-noising applications. The fastest commercial implementations (e.g. in Adobe{\textregistered} Photo-shop{\textregistered} CS2) exhibit O(r) runtime in the radius of the filter, which limits their usefulness in realtime or resolution-independent contexts. We introduce a CPU-based, vectorizable O(log r) algorithm for median filtering, to our knowledge the most efficient yet developed. Our algorithm extends to images of any bit-depth, and can also be adapted to perform bilateral filtering. On 8-bit data our median filter outperforms Photoshop's implementation by up to a factor of fifty. Copyright {\textcopyright} 2006 by the Association for Computing Machinery, Inc.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weisst - 2006 - Fast median and bilateral filtering.pdf:pdf},
  keywords     = {Algorithms,Bilateral filtering,Complexity,Data structures,Histograms,Image processing,Median filtering,Rank-order filtering,SIMD,Sorting,Vector processing},
}

@Article{Blundell2010,
  author       = {Blundell, Charles and Teh, Yee Whye and Heller, Katherine A.},
  year         = {2010},
  journal       = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
  title        = {{Bayesian rose trees}},
  eprint       = {1203.3468},
  eprinttype   = {arXiv},
  pages        = {65--72},
  abstract     = {Hierarchical structure is ubiquitous in data across many domains. There are many hierarchical clustering methods, frequently used by domain experts, which strive to discover this structure. However, most of these methods limit discoverable hierarchies to those with binary branching structure. This limitation, while computationally convenient, is often undesirable. In this paper we explore a Bayesian hierarchical clustering algorithm that can produce trees with arbitrary branching structure at each node, known as rose trees. We interpret these trees as mixtures over partitions of a data set, and use a computationally efficient, greedy agglomerative algorithm to find the rose trees which have high marginal likelihood given the data. Lastly, we perform experiments which demonstrate that rose trees are better models of data than the typical binary trees returned by other hierarchical clustering algorithms.},
  arxivid      = {1203.3468},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blundell, Teh, Heller - 2010 - Bayesian rose trees.pdf:pdf},
  isbn         = {9780974903965},
}

@Book{Rejeb2020,
  author    = {Rejeb, Abderahman and Keogh, John G. and Wamba, Samuel Fosso and Treiblmaier, Horst},
  year      = {2020},
  title     = {{The potentials of augmented reality in supply chain management: a state-of-the-art review}},
  doi       = {10.1007/s11301-020-00201-w},
  isbn      = {0123456789},
  number    = {0123456789},
  publisher = {Springer International Publishing},
  url       = {https://doi.org/10.1007/s11301-020-00201-w},
  abstract  = {This paper examines the potentials of augmented reality (AR) technology in supply chain management (SCM) and logistics. Specifically, we provide an overview of the technology's various value propositions and its ability to support companies' business processes. Although the emergence of Industry 4.0 has renewed the interest in AR, and how it can address several issues challenging existing business models, rigorous studies investigating the potentials of AR for SCM and logistics activities are scant. To bridge this knowledge gap, we conducted a systematic literature review to compile existing literature, identify current research gaps, and systematize AR research in SCM and logistics activities. In total, forty-three (43) papers were thoroughly analyzed. The findings of this study reveal that AR can add value in five main areas, namely warehousing, manufacturing, sales and outdoor logistics, planning and design and human resource management. Moreover, we discuss organizations' challenges when deploying AR in SCM and logistics and propose exploratory research opportunities for further investigation. In this paper, we highlight numerous practical implications of AR in SCM and recommend that organizations consider AR as a potential solution for enhancing business processes, improving operational efficiencies, and increasing overall competitiveness. This study represents one of the first attempts to synthesize AR's literature from an SCM and logistics perspective.},
  booktitle = {Management Review Quarterly},
  file      = {:D\:/Thomas/Downloads/Rejeb2020_Article_ThePotentialsOfAugmentedRealit.pdf:pdf},
  issn      = {21981639},
  keywords  = {Augmented reality,Industry 4.0,Logistics,Supply chain management,Systematic literature review},
}

@Article{Hasselgren2020,
  author       = {Hasselgren, Anton and Kralevska, Katina and Gligoroski, Danilo and Pedersen, Sindre A. and Faxvaag, Arild},
  year         = {2020},
  journal       = {International Journal of Medical Informatics},
  title        = {{Blockchain in healthcare and health sciences—A scoping review}},
  doi          = {10.1016/j.ijmedinf.2019.104040},
  issn         = {18728243},
  number       = {December 2019},
  pages        = {104040},
  url          = {https://doi.org/10.1016/j.ijmedinf.2019.104040},
  volume       = {134},
  abstract     = {Background: Blockchain can be described as an immutable ledger, logging data entries in a decentralized manner. This new technology has been suggested to disrupt a wide range of data-driven domains, including the health domain. Objective: The purpose of this study was to systematically review, assess and synthesize peer-reviewed publications utilizing/proposing to utilize blockchain to improve processes and services in healthcare, health sciences and health education. Method: A structured literature search on the topic was conducted in October 2018 relevant bibliographic databases. Result: 39 publications fulfilled the inclusion criteria. The result indicates that Electronic Health Records and Personal Health Records are the most targeted areas using blockchain technology. Access control, interoperability, provenance and data integrity are all issues that are meant to be improved by blockchain technology in this field. Ethereum and Hyperledger fabric seem to be the most used platforms/frameworks in this domain. Conclusion: This study shows that the endeavors of using blockchain technology in the health domain are increasing exponentially. There are areas within the health domain that potentially could be highly impacted by blockchain technology.},
  file         = {:D\:/Thomas/Downloads/1-s2.0-S138650561930526X-main.pdf:pdf},
  keywords     = {Blockchain,Distributed ledger,Health systems,Scoping review},
  pmid         = {31865055},
  publisher    = {Elsevier},
}

@Article{Kharrazi2012,
  author       = {Kharrazi, Hadi and Lu, Amy Shirong and Gharghabi, Fardad and Coleman, Whitney},
  year         = {2012},
  journal       = {Games for Health Journal},
  title        = {{A Scoping Review of Health Game Research: Past, Present, and Future}},
  doi          = {10.1089/g4h.2012.0011},
  issn         = {2161-783X},
  number       = {2},
  pages        = {153--164},
  volume       = {1},
  abstract     = {Health game research has flourished over the last decade. The number of peer-reviewed scientific publications has surged as the clinical application of health games has diversified. In response to this growth, several past literature reviews have assessed the effectiveness of health games in specific clinical subdomains. The past literature reviews, however, have not provided a general scope of health games independent of clinical context. The present systematic review identified 149 publications. All sources were published before 2011 in a peer-reviewed venue. To be included in this review, publications were required (1) to be an original research, (2) to focus on health, (3) to utilize a sound research design, (4) to report quantitative health outcomes, and (5) to target healthcare receivers. Initial findings showed certain trends in health game publications: Focus on younger male demographics, relatively low number of study participants, increased number of controlled trials, short duration of intervention periods, short duration and frequency of user-game interaction, dominance of exercise and rehab games, lack of underlying theoretical frameworks, and concentration on clinical contexts such as physical activity and nutrition. The review concludes that future research should (1) widen the demographics to include females and elderly, (2) increase the number of participants in controlled trials, (3) lengthen both the intervention period and user-game interaction duration, and (4) expand the application of health games in new clinical contexts.},
  file         = {:D\:/Thomas/Downloads/g4h.2012.0011.pdf:pdf},
  pmid         = {24416638},
}

@Article{Gsaxner2021,
  author       = {Gsaxner, Christina and Pepe, Antonio and Li, Jianning and Ibrahimpasic, Una and Wallner, J{\"{u}}rgen and Schmalstieg, Dieter and Egger, Jan},
  year         = {2021},
  journal       = {Computer Methods and Programs in Biomedicine},
  title        = {{Augmented Reality for Head and Neck Carcinoma Imaging: Description and Feasibility of an Instant Calibration, Markerless Approach}},
  doi          = {10.1016/j.cmpb.2020.105854},
  issn         = {18727565},
  pages        = {105854},
  url          = {https://doi.org/10.1016/j.cmpb.2020.105854},
  volume       = {200},
  abstract     = {Background and Objective: Augmented reality (AR) can help to overcome current limitations in computer assisted head and neck surgery by granting “X-ray vision” to physicians. Still, the acceptance of AR in clinical applications is limited by technical and clinical challenges. We aim to demonstrate the benefit of a marker-free, instant calibration AR system for head and neck cancer imaging, which we hypothesize to be acceptable and practical for clinical use. Methods: We implemented a novel AR system for visualization of medical image data registered with the head or face of the patient prior to intervention. Our system allows the localization of head and neck carcinoma in relation to the outer anatomy. Our system does not require markers or stationary infrastructure, provides instant calibration and allows 2D and 3D multi-modal visualization for head and neck surgery planning via an AR head-mounted display. We evaluated our system in a pre-clinical user study with eleven medical experts. Results: Medical experts rated our application with a system usability scale score of 74.8 ± 15.9, which signifies above average, good usability and clinical acceptance. An average of 12.7 ± 6.6 minutes of training time was needed by physicians, before they were able to navigate the application without assistance. Conclusions: Our AR system is characterized by a slim and easy setup, short training time and high usability and acceptance. Therefore, it presents a promising, novel tool for visualizing head and neck cancer imaging and pre-surgical localization of target structures.},
  file         = {:D\:/Thomas/Downloads/XRayVision/Superimposition/1-s2.0-S0169260720316874-main.pdf:pdf},
  keywords     = {Augmented Reality,Computer-Assisted Surgery,Head and Neck Surgery,Image-guided Surgery,Medical Visualization},
  pmid         = {33261944},
  publisher    = {Elsevier B.V.},
}

@Article{Johnson2014,
  author       = {Johnson, Adrian S and Sanchez, Jaime and French, Alexander and Sun, Yu},
  year         = {2014},
  journal       = {Studies in health technology and informatics},
  title        = {{Unobtrusive augmentation of critical hidden structures in laparoscopy.}},
  issn         = {1879-8365 (Electronic)},
  language     = {eng},
  pages        = {185--191},
  volume       = {196},
  abstract     = {A fundamental problem in implementing augmented reality (AR) surgery is characterizing how visualizations effect surgeon perception. This problem is important because procedure outcomes depend on surgeon ability to perceive hidden and visible structure interrelation which may be quite dynamic. AR techniques such as x-ray vision are designed to compensate for or reintroduce depth cues lost overlaying hidden structures on a view stream. Such enhancements are necessarily deviations, which may obtrude. This paper provides discussion of hidden structure rendering, analysis, a proposed framework, protocol and experiment (n=2500) for safe evaluation within in vitro laparoscopic video from, and minimal transfer to, in vivo surgery. Results evidence our protocol enables comparison of hidden structure visualizations on task efficacy in vitro and suggest promising new direction towards validating AR in live surgery.},
  annotation   = {Start at pdf page 205},
  file         = {:D\:/Thomas/Downloads/XRayVisionPubMed-21_05_2021/Accepted/Medicine_Meets_Virtual_Reality_21_NextMed_MMVR21(205).pdf:pdf},
  keywords     = {Anatomy,Computer-Assisted,Depth Perception,Humans,Image Processing,Laparoscopy,Surgery,User-Computer Interface,methods},
  pmid         = {24732504},
}

@Article{VanEE1996,
  author       = {{Van EE}, Raymond and Erkelens, Casper J.},
  year         = {1996},
  journal       = {Vision Research},
  title        = {{Stability of binocular depth perception with moving head and eyes}},
  doi          = {10.1016/0042-6989(96)00103-4},
  issn         = {00426989},
  number       = {23},
  pages        = {3827--3842},
  volume       = {36},
  abstract     = {We systematically analyse the binocular disparity field under various eye, head and stimulus positions and orientations. From the literature we know that certain classes of disparity which involve the entire disparity held (such as those caused by horizontal lateral shift, differential rotation, horizontal scale and horizontal shear between the entire half-images of a stereogram) lead to relatively poor depth perception in the case of limited observation periods. These classes of disparity are found to be similar to the classes of disparities which are brought about by eye and head movements. Our analysis supports the suggestion that binocular depth perception is based primarily (for the first few hundred milliseconds) on classes of disparity that do not change as a result of ego-movement.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van EE, Erkelens - 1996 - Stability of binocular depth perception with moving head and eyes.pdf:pdf},
  keywords     = {Binocular disparity,Binocular vision,Depth perception,Robot vision,Stereopsis},
  pmid         = {8994584},
}

@Article{Nensa2016,
  author       = {Nensa, F. and Forsting, M. and Wetter, A.},
  year         = {2016},
  journal       = {Urologe},
  title        = {{Zukunft der Radiologie: Welche Entwicklungen k{\"{o}}nnen wir in den n{\"{a}}chsten zehn Jahren erwarten?}},
  doi          = {10.1007/s00120-016-0045-1},
  issn         = {14330563},
  number       = {3},
  pages        = {350--355},
  volume       = {55},
  abstract     = {More than other medical discipline, radiology is marked by technical innovation and continuous development, as well as the optimization of the underlying physical principles. In this respect, several trends that will crucially change and develop radiology over the next decade can be observed. Through the use of ever faster computer tomography, which also shows an ever-decreasing radiation exposure, the „workhorse“ of radiology will have an even greater place and displace conventional X‑ray techniques further. In addition, hybrid imaging, which is based on a combination of nuclear medicine and radiological techniques (keywords: PET/CT, PET/MRI) will become much more established and, in particular, will improve oncological imaging further, allowing increasingly individualized imaging for specific tracers and techniques of functional magnetic resonance imaging for a particular tumour. Future radiology will be strongly characterized by innovations in the software and Internet industry, which will enable new image viewing and processing methods and open up new possibilities in the context of the organization of radiological work.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nensa, Forsting, Wetter - 2016 - Zukunft der Radiologie Welche Entwicklungen k{\"{o}}nnen wir in den n{\"{a}}chsten zehn Jahren erwarten.pdf:pdf},
  isbn         = {0012001600},
  keywords     = {3D printing,3D visualization,Dose management,Hybrid process,Low-dose CT,Machine Learning,Standardized Reporting,Teleradiology,Virtual/Augmented Reality},
  pmid         = {26893136},
}

@Article{Broecker2014,
  author       = {Broecker, Markus and Smith, Ross T. and Thomas, Bruce H.},
  year         = {2014},
  journal       = {Conferences in Research and Practice in Information Technology Series},
  title        = {{Depth perception in view-dependent near-field spatial AR}},
  issn         = {14451336},
  pages        = {87--88},
  volume       = {150},
  abstract     = {View-dependent rendering techniques are an important tool in Spatial Augmented Reality. These allow the addition of more detail and the depiction of purely virtual geometry inside the shape of physical props. This paper investigates the impact of different depth cues onto the depth perception of users.},
  file         = {:D\:/Thomas/Downloads/2667657.2667667.pdf:pdf},
  isbn         = {9781921770333},
  keywords     = {Augmented reality,Depth perception,Projector-based rendering,Spatial AR},
}

@Article{Hua2014,
  author       = {Hua, Chunya and Swan, J. Edward},
  year         = {2014},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{The effect of an occluder on near field depth matching in optical see-through augmented reality}},
  doi          = {10.1109/VR.2014.6802061},
  pages        = {81--82},
  abstract     = {We have conducted an experiment to study the effect of an occluding surface on the accuracy of near field depth matching in augmented reality (AR). Our experiment was based on replicating a similar experiment conducted by Edwards et al. [2]. We used an AR haploscope, which allows us to independently manipulate accommodative demand and vergence angle. Sixteen observers matched the perceived depth of an AR-presented virtual object with a physical pointer. Overall, observers overestimated depth by 6 mm or less with or without the presence of the occluder. The data from Edwards et al. [2] is normalized, and when we performed the same normalization procedure on our own data, our results do not agree with Edwards et al. [2]. We suspect that eye vergence explains these results. {\textcopyright} 2014 IEEE.},
  file         = {:D\:/Thomas/Downloads/06802061.pdf:pdf},
  isbn         = {9781479928712},
  keywords     = {Depth perception,augmented reality},
  publisher    = {IEEE},
}

@Article{Oeney2020,
  author       = {{\"{O}}ney, Seyda and Rodrigues, Nils and Becher, Michael and Ertl, Thomas and Reina, Guido},
  year         = {2020},
  journal       = {Eye Tracking Research and Applications Symposium (ETRA)},
  title        = {{Evaluation of gaze depth estimation from eye tracking in augmented reality}},
  doi          = {10.1145/3379156.3391835},
  pages        = {2--6},
  abstract     = {Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data.},
  file         = {:D\:/Thomas/Downloads/3379156.3391835.pdf:pdf},
  isbn         = {9781450371346},
  keywords     = {Augmented reality,Depth perception,Eye tracking,Immersive analytics,User study,Visualization},
}

@Article{Christensen2017,
  author       = {Christensen, Nicklas H. and Hjermitslev, Oliver G. and Falk, Frederik and Madsen, Marco B. and {\O}stergaard, Frederik H. and Kibsgaard, Martin and Kraus, Martin and Poulsen, Johan and Petersson, Jane},
  year         = {2017},
  journal       = {Proceedings of the 21st International Academic Mindtrek Conference, AcademicMindtrek 2017},
  title        = {{Depth cues in augmented reality for training of robot-Assisted minimally invasive surgery}},
  doi          = {10.1145/3131085.3131123},
  volume       = {2017-Janua},
  abstract     = {Training of robot-Assisted minimally invasive surgery o.en includes supervised practice with a robotic surgical system. In this case, augmented reality can improve the communication between instructor and trainee, for example, by allowing the instructor to demonstrate skills with virtual surgical instruments that are shown to the trainee by means of augmented reality. However, virtual instruments are more diffcult to handle than the real instruments partly due to the lack of depth cues. In order to improve the usability of virtual surgical instruments, we compared €ve depth cues. Results showed both a preference for the arti€cial highlight cue and an aversion to transparency and depth of €eld. .e highlight cue was therefore reviewed by experienced surgery instructors. .ese experts agreed that the highlight cue was bene€cial and that the prototype could be used to some extent already and fully upon further development.},
  file         = {:D\:/Thomas/Downloads/3131085.3131123.pdf:pdf},
  isbn         = {9781450354264},
  keywords     = {Augmented reality,Depth perception,Minimally invasive surgery,Mixed reality,Robot-Assisted surgery,Telementoring,Teleoperation,Telesurgery,Training},
}

@Article{Jones2011,
  author       = {Jones, J. Adam and Swan, J. Edward and Singh, Gurjot and Ellis, Stephen R.},
  year         = {2011},
  journal       = {Proceedings - APGV 2011: ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization},
  title        = {{Peripheral visual information and its effect on distance judgments in virtual and augmented environments}},
  doi          = {10.1145/2077451.2077457},
  pages        = {29--36},
  abstract     = {A frequently observed problem in medium-field virtual environments is the underestimation of egocentric depth. This problem has been described numerous times and with widely varying degrees of severity, and although there has been considerable progress made in modifying observer behavior to compensate for these misperceptions, the question of why these errors exist is still an open issue. This paper presents the findings of a series of experiments, comprising 103 participants, that attempts to identify and quantify the source of a pattern of adaptation and improved depth judgment accuracy over time scales of less than one hour. Taken together, these experiments suggest that peripheral visual information is an important source of information for the calibration of movement within mediumfield virtual environments. {\textcopyright} 2011 ACM.},
  file         = {:D\:/Thomas/Downloads/2077451.2077457.pdf:pdf},
  isbn         = {9781450308892},
  keywords     = {Augmented reality,Depth perception,Optical see-through display,Peripheral vision,Virtual reality},
}

@Article{Rosa2016,
  author       = {Rosa, Nina and H{\"{u}}rst, Wolfgang and Werkhoven, Peter and Veltkamp, Remco},
  year         = {2016},
  journal       = {ICMI 2016 - Proceedings of the 18th ACM International Conference on Multimodal Interaction},
  title        = {{Visuotactile integration for depth perception in augmented reality}},
  doi          = {10.1145/2993148.2993156},
  pages        = {45--52},
  abstract     = {Augmented reality applications using stereo head-mounted displays are not capable of perfectly blending real and virtual objects. For example, depth in the real world is perceived through cues such as accommodation and vergence. However, in stereo head-mounted displays these cues are disconnected since the virtual is generally projected at a static distance, while vergence changes with depth. This conict can result in biased depth estimation of virtual objects in a real environment. In this research, we examined whether redundant tactile feedback can reduce the bias in perceived depth in a reaching task. In particular, our experiments proved that a tactile mapping of distance to vibration intensity or vibration position on the skin can be used to determine a virtual object's depth. Depth estimation when using only tactile feedback was more accurate than when using only visual feedback, and when using visuotactile feedback it was more precise and occurred faster than when using unimodal feedback. Our work demonstrates the value of multimodal feedback in augmented reality applications that require correct depth perception, and provides insights on various possible visuotactile implementations.},
  file         = {:D\:/Thomas/Downloads/2993148.2993156.pdf:pdf},
  isbn         = {9781450345569},
  keywords     = {Augmented reality,Depth perception,Haptics,Multisensory perception,Sensory redundancy,Sensory substitution},
}

@Article{Copyright2008,
  author       = {Copyright, C and Software, Foxit and Jones, J Adam and Ii, J Edward Swan and Kolstad, Eric and Ellis, Stephen R},
  year         = {2008},
  journal       = {Evaluation},
  title        = {{Edited by Foxit Reader The Effects of Virtual Reality , Augmented Reality , and Motion Parallax on Egocentric Depth Perception Edited by Foxit Reader Copyright ( C ) by Foxit Software Company , 2005-2008}},
  number       = {C},
  pages        = {2005--2008},
  volume       = {1},
  file         = {:D\:/Thomas/Downloads/1394281.1394283.pdf:pdf},
  isbn         = {9781595939814},
  keywords     = {augmented reality,depth perception,virtual reality},
}

@Article{PuigCentelles2014,
  author       = {Puig-Centelles, Anna and Ramos, Francisco and Ripolles, Oscar and Chover, Miguel and Sbert, Mateu},
  year         = {2014},
  journal       = {The Scientific World Journal},
  title        = {{View-dependent tessellation and simulation of ocean surfaces}},
  doi          = {10.1155/2014/979418},
  issn         = {1537744X},
  volume       = {2014},
  abstract     = {Modeling and rendering realistic ocean scenes have been thoroughly investigated for many years. Its appearance has been studied and it is possible to find very detailed simulations where a high degree of realism is achieved. Nevertheless, among the solutions to ocean rendering, real-time management of the huge heightmaps that are necessary for rendering an ocean scene is still not solved. We propose a new technique for simulating the ocean surface on GPU. This technique is capable of offering view-dependent approximations of the mesh while maintaining coherence among the extracted approximations. This feature is very important as most solutions previously presented must retessellate from the initial mesh. Our solution is able to use the latest extracted approximation when refining or coarsening the mesh. {\textcopyright} 2014 Anna Puig-Centelles et al.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Puig-Centelles et al. - 2014 - View-dependent tessellation and simulation of ocean surfaces.pdf:pdf},
  pmid         = {24672405},
}

@Article{Ghasemi2018,
  author = {Ghasemi, Sanaz},
  year   = {2018},
  title  = {{An Investigation of Using Random Dot Patterns to Achieve X-Ray Vision for Near-Field Applications of Stereoscopic Video Based Augmented Reality Displays}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ghasemi - 2018 - An Investigation of Using Random Dot Patterns to Achieve X-Ray Vision for Near-Field Applications of Stereoscopic Video.pdf:pdf},
  journal = {MIT Press},
}

@Article{Ventura2009,
  author       = {Ventura, Jonathan and Jang, Marcus and Crain, Tyler and H{\"{o}}llerer, Tobias and Bowman, Doug},
  year         = {2009},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{Evaluating the effects of tracker reliability and field of view on a target following task in augmented reality}},
  doi          = {10.1145/1643928.1643963},
  pages        = {151--154},
  abstract     = {We examine the effect of varying levels of immersion on the performance of a target following task in augmented reality (AR) X-ray vision. We do this using virtual reality (VR) based simulation. We analyze participant performance while varying the field of view of the AR display, as well as the reliability of the head tracking sensor as our components of immersion. In low reliability conditions, we simulate sensor dropouts by disabling the augmented view of the scene for brief time periods. Our study gives insight into the effect of tracking sensor reliability, as well as the relationship between sensor reliability and field of view on user performance in a target following task in a simulated AR system. Copyright {\textcopyright} 2009 by the Association for Computing Machinery, Inc.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ventura et al. - 2009 - Evaluating the effects of tracker reliability and field of view on a target following task in augmented reality.pdf:pdf},
  isbn         = {9781605588698},
  keywords     = {Augmented reality,Immersion,Simulation,User study},
}

@Article{Rauhala2006,
  author       = {Rauhala, Malinda and Gunnarsson, Ann Sofie and Henrysson, Anders and Ynnerman, Anders},
  year         = {2006},
  journal       = {ACM International Conference Proceeding Series},
  title        = {{A novel interface to sensor networks using handheld augmented reality}},
  doi          = {10.1145/1152215.1152245},
  pages        = {145--148},
  volume       = {159},
  abstract     = {Augmented Reality technology enables a mobile phone to be used as an x-ray tool, visualizing structures and states not visible to the naked eye. In this paper we evaluate a set of techniques used augmenting the world with a visualization of data from a sensor network. Combining virtual and real information introduces challenges as information from the two domains might interfere. We have applied our system to humidity data and present a user study together with feedback from domain experts. The prototype system can be seen as the first step towards a novel tool for inspection of building elements. Copyright 2006 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rauhala et al. - 2006 - A novel interface to sensor networks using handheld augmented reality.pdf:pdf},
  isbn         = {1595933905},
  keywords     = {Intelligent environments,Mobile phone augmented reality,Sensor networks,Visualization},
}

@Article{Blum2012a,
  author       = {Blum, Tobias and Kleeberger, Valerie and Bichlmeier, Christoph and Navab, Nassir},
  year         = {2012},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Mirracle: Augmented reality in-situ visualization of human anatomy using a magic mirror}},
  doi          = {10.1109/VR.2012.6180934},
  pages        = {169--170},
  abstract     = {The mirracle system extends the concept of an Augmented Reality (AR) magic mirror to the visualization of human anatomy on the body of the user. Using a medical volume renderer a CT dataset is augmented onto the user. By a slice based user interface, slice from the CT and an additional photographic dataset can be selected. {\textcopyright} 2012 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum et al. - 2012 - Mirracle Augmented reality in-situ visualization of human anatomy using a magic mirror.pdf:pdf},
  isbn         = {9781467312462},
  keywords     = {H.5.1 [Information Interfaces and Presentation]: A},
  publisher    = {IEEE},
}

@Article{Avveduto2017,
  author       = {Avveduto, Giovanni and Tecchia, Franco and Fuchs, Henry},
  year         = {2017},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology},
  title        = {{Real-world occlusion in optical see-through AR displays}},
  doi          = {10.1145/3139131.3139150},
  volume       = {Part F1319},
  abstract     = {In this work we describe a system composed by an optical see-through AR headset-a Microsoft HoloLens-, stereo projectors and shutter glasses. Projectors are used to add to the device the capability of occluding real-world surfaces to make the virtual objects to appear more solid and less transparent. A framework was developed in order to allow us to evaluate the importance of occlusion capabilities in optical see-through AR headset. We designed and conducted two experiment to test whether making virtual elements solid would improve the performance of certain tasks with an AR system. Results suggest that making virtual objects to appear more solid by projecting an occlusion mask onto the real-world is useful in some situations. Using an occlusion mask it is also possible to eliminate ambiguities that could arise when enhancing user's perception in some ways that are not possible in real-life, like when a “x-ray vision” is enabled. In this case we wanted to investigate if using an occlusion mask to eliminate perceptual conflicts will hit user's performance in some AR applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Avveduto, Tecchia, Fuchs - 2017 - Real-world occlusion in optical see-through AR displays.pdf:pdf},
  isbn         = {9781450355483},
  keywords     = {Occlusion mask,Optical see-through displays,X-ray vision},
}

@Article{Rousi2016,
  author       = {Rousi, Rebekah},
  year         = {2016},
  journal       = {AcademicMindtrek 2016 - Proceedings of the 20th International Academic Mindtrek Conference},
  title        = {{Using human-values as a guide for understanding worthy design directions in augmented reality}},
  doi          = {10.1145/2994310.2994322},
  pages        = {243--252},
  abstract     = {Augmented reality is a fast developing field, which will no doubt gain strong footing in the area of social media in the near future. Recently, Google Glass placed AR towards the top of the technological hype curve in regards to interaction possibilities, information overlay, information search and recording. Questions still remain however, regarding the added-value that AR offers to already existing interaction modes and technologies. In this study four concepts were designed and tested via video scenarios. The concepts related to three main product categories: educational tools; information presentation; and x-ray vision. The results positively reflected on the application of AR for educational purposes and AR in navigation is also a perceived benefit. Participants projected distrust towards several points including: privacy, human-to-human interaction and safety. The findings indicate key areas of interest for development and raise questions to be dealt with relating to interaction outcomes and consequences.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rousi - 2016 - Using human-values as a guide for understanding worthy design directions in augmented reality.pdf:pdf},
  isbn         = {9781450343671},
  keywords     = {Augmented reality,Design,Human factors,Life-based design,Scenarios,Values},
}

@Article{Oishi2017,
  author       = {Oishi, Kei and Mori, Shohei and Saito, Hideo},
  year         = {2017},
  journal       = {Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017},
  title        = {{An Instant See-Through Vision System Using a Wide Field-of-View Camera and a 3D-Lidar}},
  doi          = {10.1109/ISMAR-Adjunct.2017.99},
  pages        = {344--347},
  abstract     = {Diminished reality (DR) enables us to see through real objects occluding some areas in our field of view. This interactive display has various applications, such as see-through vision to visualize invisible areas, work area visualization in surgery and landscape simulation. In this paper, we propose two underlying problems in see-through vision, in which hidden areas are observed in real time. First, see-through vision methods require a common area to calibrate every camera in the environment. However, the field of view is limited and many approaches rely on a time-consuming calibration, sensors, or fiducial markers. Second, see-through vision applications assume that the background is planar to ease image alignment. We therefore present a place-and-play see-through vision system using a wide field-of-view RGB-D camera. We validated the accuracy and the robustness of our system and showed results in various environments to show the applicability.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oishi, Mori, Saito - 2017 - An Instant See-Through Vision System Using a Wide Field-of-View Camera and a 3D-Lidar.pdf:pdf},
  isbn         = {9780769563275},
  keywords     = {3D-Lidar,Diminished reality,Fish-eye camera},
}

@Article{Pauly2012,
  author       = {Pauly, Olivier and Katouzian, Amin and Eslami, Abouzar and Fallavollita, Pascal and Navab, Nassir},
  year         = {2012},
  journal       = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
  title        = {{Supervised classification for customized intraoperative augmented reality visualization}},
  doi          = {10.1109/ISMAR.2012.6402589},
  pages        = {311--312},
  abstract     = {In this paper, we present a fusion algorithm supplemented with appropriate visualization by selecting relevant information from different modalities in mixed and augmented reality (AR). This encompasses a learning based method upon relevance of information, defined by an expert, which ultimately enables confident interventional decisions based on mixed reality (MR) images. The performance of our developed fusion and tailored visualization techniques was evaluated by employing X-ray/optical images during surgery and validated qualitatively using a 5-point Likert scale. Our observations indicated that the proposed technique provided semantic contextual information about underlying pixels and in general was preferred over the traditional pixel-wise linear alpha-blending method. {\textcopyright} 2012 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pauly et al. - 2012 - Supervised classification for customized intraoperative augmented reality visualization.pdf:pdf},
  isbn         = {9781467346603},
  keywords     = {CamC,Fusion,Medical Augmented Reality,Relevant Information,Visualization,X-ray},
}

@Article{Dey2012,
  author       = {Dey, Arindam and Jarvis, Graeme and Sandor, Christian and Reitmayr, Gerhard},
  year         = {2012},
  journal       = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
  title        = {{Tablet versus phone: Depth perception in handheld augmented reality}},
  doi          = {10.1109/ISMAR.2012.6402556},
  number       = {November},
  pages        = {187--196},
  abstract     = {Augmented Reality (AR) applications on mobile devices like smartphones and tablet computers have become increasingly popular. In this paper, for the first time in the AR domain, we present: (1) the influence of different handheld displays and (2) the exocentric depth perception. Unlike egocentric depth perception, exocentric depth perception has not been investigated in AR. {\textcopyright} 2012 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey et al. - 2012 - Tablet versus phone Depth perception in handheld augmented reality.pdf:pdf},
  isbn         = {9781467346603},
  keywords     = {Augmented Reality,Depth Perception,Handheld Displays,Outdoor Environment,User Evaluation,X-ray Visualization},
  publisher    = {IEEE},
}

@Article{Erat2018,
  author       = {Erat, Okan and Isop, Werner Alexander and Kalkofen, Denis and Schmalstieg, Dieter},
  year         = {2018},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Drone-Augmented human vision: Exocentric control for drones exploring hidden areas}},
  doi          = {10.1109/TVCG.2018.2794058},
  issn         = {10772626},
  number       = {4},
  pages        = {1437--1446},
  volume       = {24},
  abstract     = {Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-Augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-Through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-Augmented human vision supports spatial understanding and improves natural interaction with the drone.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erat et al. - 2018 - Drone-Augmented human vision Exocentric control for drones exploring hidden areas.pdf:pdf},
  keywords     = {Drone,Hololens,Mixed reality,Pick-And-place,X-ray},
  pmid         = {29543162},
  publisher    = {IEEE},
}

@Article{Eren2018,
  author       = {Eren, Mustafa Tolga and Balcisoy, Selim},
  year         = {2018},
  journal       = {Visual Computer},
  title        = {{Evaluation of X-ray visualization techniques for vertical depth judgments in underground exploration}},
  doi          = {10.1007/s00371-016-1346-5},
  issn         = {01782789},
  number       = {3},
  pages        = {405--416},
  volume       = {34},
  abstract     = {This paper investigates depth judgment-related performances of X-ray visualization techniques for rendering fully occluded geometries in augmented reality. The techniques we selected for this evaluation are careless overlay (CO), edge overlay (EO), excavation box (EB) and a cross-sectional visualization technique (CS). We have designed and conducted a comprehensive user study with 16 participants to examine and analyze the effects related to visualization techniques, having additional virtual objects and the scale of the vertical depths. To the best of our knowledge, this is the first user study on judged vertical depth distances that these techniques were compared against each other. We report our findings using four dependent variables: accuracy, signed error, absolute error and response time to shed some light into real-world performances and also to reveal estimation tendencies of each technique. Our findings suggest similar and better performance for EB, CS compared to CO and EO. We also observed significantly better results for EB and CS techniques when judging Top and Bottom distances compared to Middle distances. Derived from our findings, we proposed a new visualization technique for underground investigation with multiple views. The multi-view technique is our own implementation inspired by magic lens and cross-sectional visualizations with correlating displays.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eren, Balcisoy - 2018 - Evaluation of X-ray visualization techniques for vertical depth judgments in underground exploration.pdf:pdf},
  keywords     = {Augmented reality,Depth perception,User study,X-ray visualization},
  publisher    = {Springer Berlin Heidelberg},
}

@Article{Robertson2008,
  author       = {Robertson, Cindy M. and Maclntyre, Blair and Walker, Bruce N.},
  year         = {2008},
  journal       = {Proceedings - 7th IEEE International Symposium on Mixed and Augmented Reality 2008, ISMAR 2008},
  title        = {{An evaluation of graphical context when the graphics are outside of the task area}},
  doi          = {10.1109/ISMAR.2008.4637328},
  pages        = {73--76},
  abstract     = {An ongoing research problem in Augmented Reality (AR) is to improve tracking and display technology in order to minimize registration errors. However, perfect registration is not always necessary for users to understand the intent of an augmentation. This paper describes the results of an experiment to evaluate the effects of graphical context in a Lego block placement task when the graphics are located outside of the task area. Four conditions were compared: fully registered AR; non-registered AR; a heads-up display (HUD) with the graphics always visible in the field of view; and a HUD with the graphics not always visible in the field of view. The results of this experiment indicated that registered AR outperforms both non-registered AR and graphics displayed on a HUD. The results also indicated that non-registered AR does not offer any significant performance advantages over a HUD, but is rated as less intrusive and can keep non-registered graphics from cluttering the task space. {\textcopyright} 2008 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Maclntyre, Walker - 2008 - An evaluation of graphical context when the graphics are outside of the task area.pdf:pdf},
  isbn         = {9781424428403},
  keywords     = {Augmented environments,Augmented reality,Communicative intent,Human-computer interaction},
}

@Article{Reiners2020,
  author       = {Reiners, Dirk and Stricker, Didier and Klinker, Gudrun and Stefan, M},
  year         = {2020},
  journal       = {Augmented Reality},
  title        = {{Augmented Reality for Construction Tasks: Doorlock Assembly}},
  doi          = {10.1201/9781439863992-10},
  number       = {November},
  pages        = {51--66},
  abstract     = {Augmented Reality is a technology that integrates pictures of virtual objects into images of the real world. Besides the technical problems still to be solved, for industry to be interested in this technology the additional amount of work needed to use the technology in relation to the benefit has to be visible. Furthermore the question how this integrates into the information technology infrastructure of the company is important. This paper describes an Augmented Reality demonstrator for the task of doorlock assembly into a car door that was developed trying to create a practical, realistic application that can transport the concepts behind Augmented Reality to a casual observer. To reach that goal a new fast and robust optical tracking algorithm was developed and integrated into a three-dimensional animation and rendering system, creating a real-time fully three-dimensional HMD-based training application showing how to assemble the doorlock into the door. The system was demonstrated to the general public at the Hannover Industrial Fair 1998 and this demonstration of Augmented Reality for one of the first times to a large non-expert audience created a lot of interest into this new area.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reiners et al. - 2020 - Augmented Reality for Construction Tasks Doorlock Assembly.pdf:pdf},
}

@Article{Chen2019a,
  author       = {Chen, Y. and Cui, Z. and Hao, L.},
  year         = {2019},
  journal       = {Lighting Research and Technology},
  title        = {{Virtual reality in lighting research: Comparing physical and virtual lighting environments}},
  doi          = {10.1177/1477153518825387},
  issn         = {14771535},
  number       = {6},
  pages        = {820--837},
  volume       = {51},
  abstract     = {In the study of lighting, as the construction of a physical test room is costly and time-consuming, researchers have been actively looking for alternative media to present physical environments. Virtual reality, photo and video are the most commonly used approaches in the lighting community, and they have all been used by researchers around the world. Most such studies have been conducted without discussing what gives the subjects a better sense of realism, presence, etc., and which type of media is closer to the ideal, the physical lighting environment. In this paper, we aim to select the optimal alternative media that can present physical lighting environments. We compare a human's subjective feeling towards a physical lighting environment and three alternative reproduction technologies, namely, virtual reality reproduction, video reproduction and photographic reproduction. We also discuss the feasibility of using virtual reality in representing lighting environments. The selection of the most optimal media is based on the perceptual attributes of lighted space, and the findings are only related to these criteria. The main results of this study are the following: (a) The order of the overall presentation-ability of the media is physical space > virtual reality reproductions > video reproductions > photo reproductions. (b) In terms of subjective rating, virtual reality lighting environments are rated closest to the physical lighting environments, and the order of the approximate coefficient of the media is physical space (1) > VR reproductions (0.886) > video reproductions (0.752) > photo reproductions (0.679). (c) Virtual reality can present lighting attributes of open/close, diffuse/glaring, bright/dim and noisy/quiet consistent with the physical environment. (d) Human subjects are most satisfied with VR reproductions.},
  file         = {:D\:/Thomas/Downloads/1477153518825387.pdf:pdf},
}

@Article{Iehisa2020,
  author       = {Iehisa, Ikko and Ayaki, Masahiko and Tsubota, Kazuo and Negishi, Kazuno},
  year         = {2020},
  journal       = {Heliyon},
  title        = {{Factors affecting depth perception and comparison of depth perception measured by the three-rods test in monocular and binocular vision}},
  doi          = {10.1016/j.heliyon.2020.e04904},
  issn         = {24058440},
  number       = {9},
  pages        = {e04904},
  url          = {https://doi.org/10.1016/j.heliyon.2020.e04904},
  volume       = {6},
  abstract     = {Purpose: The purpose of this study was to explore the effects of factors affecting depth perception of moving objects using a modified three-rods test, which can be used at longer distances than the conventional one, and to compare differences in the results between binocular and monocular vision. Methods: This study included 24 volunteers (10 women, 14 men; mean age, 35.2 years; standard deviation, 6.8 years; range, 22–56 years). We measured depth perception using a modified three-rods test under eight different conditions and investigated the factors affecting depth perception using a linear-effect model. Results: The results identified test distance, binocularity, masking, and direction of movement as significant factors affecting depth perception of a moving object. Conclusions: The current study successfully determined factors affecting depth perception using the three-rods test with a moving object and the results should contribute to further clinical and social applications of the three-rods test.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iehisa et al. - 2020 - Factors affecting depth perception and comparison of depth perception measured by the three-rods test in monocula.pdf:pdf},
  keywords     = {Biocularity,Clinical psychology,Clinical research,Depth perception,Driving,Eye-ear-nose-throat,Occupational health,Ophthalmology,Quality of life,Visual function},
  publisher    = {Elsevier Ltd},
}

@Article{Naderifar2017,
  author       = {Naderifar, Mahin and Goli, Hamideh and Ghaljaie, Fereshteh},
  year         = {2017},
  journal       = {Strides in Development of Medical Education},
  title        = {{Snowball Sampling: A Purposeful Method of Sampling in Qualitative Research}},
  doi          = {10.5812/sdme.67670},
  issn         = {2645-3525},
  number       = {3},
  volume       = {14},
  abstract     = {Background and Objectives: Snowball sampling is applied when samples with the target characteristics are not easily accessible. This research describes snowball sampling as a purposeful method of data collection in qualitative research. Methods: This paper is a descriptive review of previous research papers. Data were gathered using English keywords, including "re-view," "declaration," "snowball," and "chain referral," as well as Persian keywords that are equivalents of the following: "purposeful sampling," "snowball," "qualitative research," and "descriptive review." The databases included Google Scholar, Scopus, Irandoc, Pro-Quest, Science Direct, SID, MagIran, Medline, and Cochrane. The search was limited to Persian and English articles written between 2005 and 2013. Results: The preliminary search yielded 433 articles from PubMed, 88 articles from Scopus, 1 article from SID, and 18 articles from MagIran. Among 125 articles, methodological and non-research articles were omitted. Finally, 11 relevant articles, which met the criteria, were selected for review. Conclusions: Different methods of snowball sampling can be applied to facilitate scientific research, provide community-based data, and hold health educational programs. Snowball sampling can be effectively used to analyze vulnerable groups or individuals under special care. In fact, it allows researchers to access susceptible populations. Thus, it is suggested to consider snowball sampling strategies while working with the attendees of educational programs or samples of research studies.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Naderifar, Goli, Ghaljaie - 2017 - Snowball Sampling A Purposeful Method of Sampling in Qualitative Research.pdf:pdf},
}

@Article{Shah2012,
  author       = {Shah, Manisah Mohd and Arshad, Haslina and Sulaiman, Riza},
  year         = {2012},
  journal       = {Proceedings - ICIDT 2012, 8th International Conference on Information Science and Digital Content Technology},
  title        = {{Occlusion in augmented reality}},
  pages        = {372--378},
  volume       = {2},
  abstract     = {Augmented reality in video-based display simply overlay virtual objects on real environment. In many cases this does not represent the actual situation in AR scene. When a real object supposes to occlude a virtual object, the augmented image may cause confusion in users' perception. This incorrect display contributes to misconceptions and wrong operations of task amongst users. The scenario of occlusion in AR application is discussed in this paper. Next, the main method used by other researchers to resolve occlusion problem was discussed. This is followed by a discussion about dependency factors and characteristics that influence the selection of occlusion handling approach and a comparison between a model-based and depth-based approach. Finally, a conclusion and proposed work are presented. {\textcopyright} 2012 AICIT.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shah, Arshad, Sulaiman - 2012 - Occlusion in augmented reality.pdf:pdf},
  isbn         = {9788988678695},
  keywords     = {augmented reality,occlusion},
  publisher    = {IEEE},
}

@Article{Henkes2006,
  author       = {Henkes, Hans and D, Ph and Fischer, Sebastian and Liebig, Thomas and Weber, Werner},
  year         = {2006},
  journal       = {Neuroradiology},
  title        = {{R Epeated E Ndovascular C Oil O Cclusion in 350 of}},
  number       = {2},
  pages        = {224--232},
  volume       = {58},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henkes et al. - 2006 - R Epeated E Ndovascular C Oil O Cclusion in 350 of.pdf:pdf},
  isbn         = {9781467346627},
  keywords     = {0000194831,01,10,1227,2006,224-232,3f,54183,d,detachable coils,doi,elina miloslavski,endovascular treatment,intracranial aneurysm,m,neu,neurosurgery 58,outcome,recurrence},
  publisher    = {IEEE},
}

@Article{Santos2013a,
  author       = {Santos, Marc Ericson C. and Chen, Angie and Terawaki, Mitsuaki and Yamamoto, Goshiro and Taketomi, Takafumi and Miyazaki, Jun and Kato, Hirokazu},
  year         = {2013},
  journal       = {Proceedings - 2013 IEEE 13th International Conference on Advanced Learning Technologies, ICALT 2013},
  title        = {{Augmented reality x-ray interaction in k-12 education: Theory, student perception and teacher evaluation}},
  doi          = {10.1109/ICALT.2013.45},
  pages        = {141--145},
  abstract     = {Augmented reality (AR) x-ray interaction is an enabling technology for providing students with virtual abstractions of the interior of an object. It provides students contextual visualization-the presentation of virtual information in the rich context of a real environment-thereby offering compelling experiences. According to experiential learning theory, such personal experiences are necessary for reaching different types of learners. AR x-ray is a novel interaction technique for education, thus, it is necessary to investigate how it affects the students' perception. We implemented AR x-ray using a state-of-the-art occlusion technique, and compared it to viewing 3D objects without occlusion. Results of two user studies (n=23 and n=47) show that there are no significant differences in realism, perception of depth, and visibility with occlusion and without occlusion, and that the current technique is usable for educational purposes. We also conducted interviews with both students (n=23) and teachers (n=12). Results indicate that AR x-ray is perceived to be useful for motivating and explaining to students. The teachers expressed willingness to adopt AR x-ray and to undergo training for using AR-based teaching materials. {\textcopyright} 2013 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos et al. - 2013 - Augmented reality x-ray interaction in k-12 education Theory, student perception and teacher evaluation.pdf:pdf},
  isbn         = {9780769550091},
  keywords     = {AR x-ray,augmented reality,augmented reality learning experience,experiential learning},
  publisher    = {IEEE},
}

@Article{Padilha2015,
  author       = {Padilha, Arthur and Teichrieb, Veronica},
  year         = {2015},
  journal       = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality Workshops, ISMARW 2015},
  title        = {{Motion-aware ghosted views for single layer occlusions in augmented reality}},
  doi          = {10.1109/ISMARW.2015.20},
  pages        = {60--67},
  abstract     = {Functional realism focuses on helping users with tasks execution through an enhanced perception of the augmented scene. This work applies a common visualization technique, Ghosting, to improve depth perception in Augmented Reality scenes. Computer Vision and Image Processing techniques are used to extract natural features from a real scene, which will guide the assignment of transparency to each pixel of the virtual object, and provide the ghosting effect while blending the virtual object into the real scene. A moving object in a real scene catches users' attention. So, it is expected that natural and important visual information of the scene does not get occluded when the moving object passes over it. Because of that, the main contribution of this work is the inclusion of a motion detection technique to the scene feature analysis step of the Ghosting technique pipeline. A qualitative evaluation of the results achieved shows that the case studies of this work, in indoor and outdoor environments, using the proposed technique led to a better depth perception of the augmented scene, preserving the most relevant information for visual attention.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Padilha, Teichrieb - 2015 - Motion-aware ghosted views for single layer occlusions in augmented reality.pdf:pdf},
  isbn         = {9781467384711},
  keywords     = {Augmented reality,Ghosting,Motion detection,Occlusion,Saliency map,Visualization},
  publisher    = {IEEE},
}

@Article{Padilha2014,
  author       = {Padilha, Arthur and Teichrieb, Veronica},
  year         = {2014},
  journal       = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
  title        = {{Motion detection based ghosted views for occlusion handling in augmented reality}},
  doi          = {10.1109/ISMAR.2014.6948455},
  pages        = {291--292},
  abstract     = {This work presents an improvement to the scene analysis pipeline of a visualization technique called Ghosting. Computer vision and image processing techniques are used to extract natural features, from each video frame. These features will guide the assignment of transparency to pixels, in order to give the ghosting effect, while blending the virtual object into the real scene. Video sequences were obtained from traditional RGB cameras. The main contribution of this work is the inclusion of a motion detection technique to the scene feature analysis step. This procedure leads to a better perception of the augmented scene because the proper ghosting effect is achieved when a moving natural salient object, that catches users attention, passes in front of an augmented one.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Padilha, Teichrieb - 2014 - Motion detection based ghosted views for occlusion handling in augmented reality.pdf:pdf},
  isbn         = {9781479961849},
  keywords     = {H.5.1 [Information Interfaces and Presentation Sys,I.4.8 [Image Processing and Computer Vision]: Scen,augmented,virtual realities},
  publisher    = {IEEE},
}

@Article{ElSeoud2018,
  author       = {El-Seoud, Samir A. and Mady, Amr S. and Rashed, Essam A.},
  year         = {2018},
  journal       = {ACM International Conference Proceeding Series},
  title        = {{An interactive mixed reality imaging system for minimally invasive surgeries}},
  doi          = {10.1145/3220267.3220290},
  pages        = {76--80},
  abstract     = {In orthopedic surgery, it is important for physicians to completely understand the three-dimensional (3D) anatomical structures for several procedures. With the current revolution in technology in every aspect of our life, mixed reality in the medical field is going to be very useful. However, medicine has a visualization problem hindering how surgeons operate. The surgeons are required to imagine the actual 3D structure of the patient by looking at multiple 2D slices of the patients' body. This process is time consuming, exhausting and requires special skill and experience. Moreover, patients and surgeons are exposed to extra x-ray doses. Therefore, it is important to provide the surgeon with a better way to diagnose the patient; a way that is more accurate and locates where the problem is in a faster and more efficient manner. Medical imaging systems usually provide 3D images that can guide interventional clinical procedures. However, it is difficult to map the 3D anatomical structure with real objects. This project investigates and solves this problem by providing a mixed reality technology solution that merges the 3D image with real objects to facilitate the work progress of the surgeon. The proposed solution is an interactive mixed reality (MR) system for minimally invasive surgeries. The system is based on mapping the patient volume scan using computed tomography (CT) or Magnetic Resonance Imaging (MRI) to a 3D model of the patient's body. The rendered model can be used in MR system to view 3D human structures through a set of wearable glasses.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/El-Seoud, Mady, Rashed - 2018 - An interactive mixed reality imaging system for minimally invasive surgeries.pdf:pdf},
  isbn         = {9781450364690},
  keywords     = {Component: mixed reality,Medical imaging,Volume rendering},
}

@Article{Santos2013,
  author       = {Santos, Marc Ericson C. and Yamamoto, Goshiro and Terawaki, Mitsuaki and Miyazaki, Jun and Taketomi, Takafumi and Kato, Hirokazu},
  year         = {2013},
  journal       = {ACM International Conference Proceeding Series},
  title        = {{Towards participatory design for contextual visualization in education using augmented reality X-ray}},
  doi          = {10.1145/2459236.2459281},
  number       = {October 2010},
  pages        = {241},
  abstract     = {We propose Augmented Reality (AR) x-ray as an educational tool for contextual visualization-presenting virtual information in the rich context of a real environment. Teachers and students evaluated a state-of-the-art implementation of AR x-ray. Results show that realism, visibility, and perception of depth in AR x-ray are not significantly different from viewing 3D models with no occlusion cues. Moreover, teachers perceive AR x-ray useful. Copyright 2013 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Santos et al. - 2013 - Towards participatory design for contextual visualization in education using augmented reality X-ray.pdf:pdf},
  isbn         = {9781450319041},
  keywords     = {Augmented reality,Contextual learning,Participatory design},
}

@Article{Thomas2009,
  author       = {Thomas, Bruce H and Piekarski, Wayne},
  year         = {2009},
  journal       = {IEEE CS},
  title        = {{Through-Walls Collaboration}},
  pages        = {42--49},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas, Piekarski - 2009 - Through-Walls Collaboration.pdf:pdf},
}

@Article{Zollmann2010,
  author       = {Zollmann, Stefanie and Kalkofen, Denis and Mendez, Erick and Reitmayr, Gerhard},
  year         = {2010},
  journal       = {9th IEEE International Symposium on Mixed and Augmented Reality 2010: Science and Technology, ISMAR 2010 - Proceedings},
  title        = {{Image-based ghostings for single layer occlusions in augmented reality}},
  doi          = {10.1109/ISMAR.2010.5643546},
  pages        = {19--26},
  abstract     = {In augmented reality displays, X-Ray visualization techniques make hidden objects visible through combining the physical view with an artificial rendering of the hidden information. An important step in X-Ray visualization is to decide which parts of the physical scene should be kept and which should be replaced by overlays. The combination should provide users with essential perceptual cues to understand the relationship of depth between hidden information and the physical scene. In this paper we present an approach that addresses this decision in unknown environments by analyzing camera images of the physical scene and using the extracted information for occlusion management. Pixels are grouped into perceptually coherent image regions and a set of parameters is determined for each region. The parameters change the X-Ray visualization for either preserving existing structures or generating synthetic structures. Finally, users can customize the overall opacity of foreground regions to adapt the visualization. {\textcopyright}2010 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zollmann et al. - 2010 - Image-based ghostings for single layer occlusions in augmented reality.pdf:pdf},
  isbn         = {9781424493449},
  keywords     = {H.5.1 [information interfaces and presentation]: m,I.4.8 [image processing and computer vision]: scen},
  publisher    = {IEEE},
}

@Article{Livingston2013a,
  author       = {Livingston, Mark A. and Moser, Kenneth R.},
  year         = {2013},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Effectiveness of occluded object representations at displaying ordinal depth information in augmented reality}},
  doi          = {10.1109/VR.2013.6549385},
  pages        = {107--108},
  abstract     = {An experiment was conducted to investigate the utility of a number of iconographic styles in relaying ordinal depth information at vista space distances of more than 1900m. The experiment consisted of two tasks: distance judgments with respect to discrete zones, and ordinal depth determination in the presence of icon overlap. The virtual object representations were chosen based on their effectiveness, as demonstrated in previous studies. The first task is an adaptation of a previous study investigating distance judgments of occluded objects at medium field distances. We found that only one of the icon styles fared better than guessing. The second is a novel task important to situation awareness and tested two specific cases: ordinal depth of icons with 50\% and 100\% overlap. We found that the case of full overlap made the task effectively impossible with all icon styles, whereas in the case of partial overlap, the Ground Plane had a clear advantage. {\textcopyright} 2013 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Livingston, Moser - 2013 - Effectiveness of occluded object representations at displaying ordinal depth information in augmented reality.pdf:pdf},
  isbn         = {9781467347952},
  keywords     = {Augmented reality,X-ray vision,human factors evaluation,ordinal depth,situation awareness},
  publisher    = {IEEE},
}

@Article{Livingston2003,
  author       = {Livingston, Mark A. and Swan, J. Edward and Gabbard, Joseph L. and Hollerer, Tobias H. and Hix, Deborah and Julier, Simon J. and Baillot, Yohan and Brown, Dennis},
  year         = {2003},
  journal       = {Proceedings - 2nd IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR 2003},
  title        = {{Resolving multiple occluded layers in augmented reality}},
  doi          = {10.1109/ISMAR.2003.1240688},
  pages        = {56--65},
  abstract     = {A useful function of augmented reality (AR) systems is their ability to visualize occluded infrastructure directly in a user's view of the environment. This is especially important for our application context, which utilizes mobile AR for navigation and other operations in an urban environment. A key problem in the AR field is how to best depict occluded objects in such a way that the viewer can correctly infer the depth relationships between different physical and virtual objects. Showing a single occluded object with no depth context presents an ambiguous picture to the user. But showing all occluded objects in the environments leads to the «Superman's X-ray vision» problem, in which the user sees too much information to make sense of the depth relationships of objects. Our efforts differ qualitatively from previous work in AR occlusion, because our application domain involves far-field occluded objects, which are tens of meters distant from the user. Previous work has focused on near-field occluded objects, which are within or just beyond arm's reach, and which use different perceptual cues. We designed and evaluated a number of sets of display attributes. We then conducted a user study to determine which representations best express occlusion relationships among far-field objects. We identify a drawing style and opacity settings that enable the user to accurately interpret three layers of occluded objects, even in the absence of perspective constraints.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Livingston et al. - 2003 - Resolving multiple occluded layers in augmented reality.pdf:pdf},
  isbn         = {0769520065},
}

@Article{Debarba2018a,
  author       = {Debarba, Henrique Galvan and {De Oliveira}, Marcelo Elias and Ladermann, Alexandre and Chague, Sylvain and Charbonnier, Caecilia},
  year         = {2018},
  journal       = {Proceedings - 2018 20th Symposium on Virtual and Augmented Reality, SVR 2018},
  title        = {{Augmented reality visualization of joint movements for rehabilitation and sports medicine}},
  doi          = {10.1109/SVR.2018.00027},
  pages        = {114--121},
  abstract     = {We present a visualization tool for human motion analysis in augmented reality. Our tool builds upon our previous work on joint biomechanical modelling for kinematic analysis, based on optical motion capture and personalized anatomical reconstruction of joint structures from medical imaging. It provides healthcare professionals with the in situ visualization of joint movements, where bones are accurately rendered as a holographic overlay on the subject-like if the user has an 'X-ray vision'-and in real-time as the subject performs the movement. The tool also provides a recording mechanism for the examination and acquisition of movements and range of motion information. Recorded information can be for instance retrieved at a later moment to assess patient's progress in terms of kinematics during the rehabilitation phase. We also propose an intuitive non-sequential mean of navigating through recordings. It consists of pointing at movement trajectories for easy and intuitive retrieval of the meaningful portions of a movement. This tool allows for the post hoc replay and analysis of fast movements, such as from athletes movements in sports injury evaluation. Currently, hip and knee joints are supported.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Debarba et al. - 2018 - Augmented reality visualization of joint movements for rehabilitation and sports medicine.pdf:pdf},
  isbn         = {9781728106045},
  keywords     = {Augmented reality,Calibration,Joint kinematics,Rehabilitation,Sports medicine,Visualization},
}

@Article{Tsuda2005,
  author       = {Tsuda, Takahiro and Yamamoto, Haruyoshi and Kameda, Yoshinari and Ohta, Yuichi},
  year         = {2005},
  journal       = {ACM International Conference Proceeding Series},
  title        = {{Visualization methods for outdoor see-through vision}},
  doi          = {10.1145/1152399.1152412},
  pages        = {62--69},
  volume       = {157},
  abstract     = {Visualizing occluded objects is a useful applications of Mixed Reality (MR), which we call "see-through vision." For this application, it is important to display occluded objects in such a manner that they can be recognized intuitively by the user.Here, we evaluated five visualization methods for see-through vision that help the user to intuitively recognize occluded objects in outdoor scenes: "elimination of occluding object," "ground grid," "overlaying model of occluding object," "top-down view," and "on-off switching of MR display." As we applied a new handheld MR device for outdoor see-through vision, we conducted subjective experiments to determine the best combination of methods. The experimental results indicated the combination of showing the ground grid, overlaying the wire-frame models of occluding objects, and top-down view to be optimal, while it is not necessary to display occluding objects for outdoor see-through vision with a handheld device, because users can see them with the naked eye.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsuda et al. - 2005 - Visualization methods for outdoor see-through vision.pdf:pdf},
  isbn         = {0473106574},
  keywords     = {augmented reality,mixed reality,outdoor,see-through,subjective evaluation},
}

@Article{Hoang2017,
  author   = {Hoang, Thuong and Reinoso, Martin and Joukhadar, Zaher and Vetere, Frank and Kelly, David},
  year     = {2017},
  title    = {{Augmented Studio: Projection Mapping on Moving Body for Physiotherapy Education}},
  doi      = {10.1145/3025453.3025860},
  pages    = {1419--1430},
  journal  = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  abstract = {Figure 1. Augmented Studio setup: Left: the 3-sided stage, including scaffolding and 2 projectors with 2 Kinect sensors, set up in a physiotherapy practical classroom; Middle: the stage is captured virtually through projection mapping with a virtual anatomy model; Right: projected virtual anatomy model on a moving body for a physiotherapy practical class ABSTRACT Physiotherapy students often struggle to translate anatomical knowledge from textbooks into a dynamic understanding of the mechanics of body movements in real life patients. We present the Augmented Studio, an augmented reality system that uses body tracking to project anatomical structures and annotations over moving bodies for physiotherapy education. Through a user and learner centered design approach, we established an understanding that through augmentation and annotation, augmented reality technology can enhance physiotherapy education. Augmented Studio enables augmentation through projection mapping to display anatomical information such as muscles and skeleton in real time on the body as it moves. We created a technique for annotation to create projected hand-drawing on the moving body, to enable explicit communication of the teacher's clinical reasoning strategies to the students. Findings from our pilot usability study demonstrate a more engaging learning and teaching experience and increased communication between teacher and students when using Augmented Studio.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoang et al. - 2017 - Augmented Studio Projection Mapping on Moving Body for Physiotherapy Education.pdf:pdf},
}

@Article{Schall2013,
  author       = {Schall, Gerhard and Zollmann, Stefanie and Reitmayr, Gerhard},
  year         = {2013},
  journal       = {Personal and Ubiquitous Computing},
  title        = {{Smart Vidente: Advances in mobile augmented reality for interactive visualization of underground infrastructure}},
  doi          = {10.1007/s00779-012-0599-x},
  issn         = {16174909},
  number       = {7},
  pages        = {1533--1549},
  volume       = {17},
  abstract     = {Many civil engineering tasks require to access geospatial data in the field and reference the stored information to the real-world situation. Augmented reality (AR), which interactively overlays 3D graphical content directly over a view of the world, can be a useful tool to visualize but also create, edit and update geospatial data representing real-world artifacts. We present research results on the next-generation field information system for companies relying on geospatial data, providing mobile workforces with capabilities for on-site inspection and planning, data capture and as-built surveying. To achieve this aim, we used mobile AR technology for on-site surveying of geometric and semantic attributes of geospatial 3D models on the user's handheld device. The interactive 3D visualizations automatically generated from production databases provide immediate visual feedback for many tasks and lead to a round-trip workflow where planned data are used as a basis for as-built surveying through manipulation of the planned data. Classically, surveying of geospatial objects is a typical scenario performed from utility companies on a daily basis. We demonstrate a mobile AR system that is capable of these operations and present first field trials with expert end users from utility companies. Our initial results show that the workflows of planning and surveying of geospatial objects benefit from our AR approach. {\textcopyright} Springer-Verlag London Limited 2012.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schall, Zollmann, Reitmayr - 2013 - Smart Vidente Advances in mobile augmented reality for interactive visualization of underground infr.pdf:pdf},
  keywords     = {3D GIS,Geospatial interaction,Location- and context-aware computing,Mobile augmented reality,Surveying},
}

@Article{Chastine2007,
  author       = {Chastine, Jeffrey W. and Nagel, Kristine and Zhu, Ying and Yearsovich, Luca},
  year         = {2007},
  journal       = {Proceedings - Graphics Interface},
  title        = {{Understanding the design space of referencing in collaborative augmented reality environments}},
  doi          = {10.1145/1268517.1268552},
  issn         = {07135424},
  pages        = {207--214},
  abstract     = {For collaborative environments to be successful, it is critical that participants have the ability to generate effective references. Given the heterogeneity of the objects and the myriad of possible scenarios for collaborative augmented reality environments, generating meaningful references within them can be difficult. Participants in co-located physical spaces benefit from non-verbal communication, such as eye gaze, pointing and body movement; however, when geographically separated, this form of communication must be synthesized using computer-mediated techniques. We have conducted an exploratory study using a collaborative building task of constructing both physical and virtual models to better understand inter-referential awareness - or the ability for one participant to refer to a set of objects, and for that reference to be understood. Our contributions are not necessarily in presenting novel techniques, but in narrowing the design space for referencing in collaborative augmented reality. This study suggests collaborative reference preferences are heavily dependent on the context of the workspace.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chastine et al. - 2007 - Understanding the design space of referencing in collaborative augmented reality environments.pdf:pdf},
  isbn         = {1568813376},
  keywords     = {Awareness,Referencing},
}

@Article{Kalkofen2009,
  author       = {Kalkofen, Denis and Mendez, Erick and Schmalstieg, Dieter},
  year         = {2009},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Comprehensible visualization for augmented reality}},
  doi          = {10.1109/TVCG.2008.96},
  issn         = {10772626},
  number       = {2},
  pages        = {193--204},
  volume       = {15},
  abstract     = {This article presents interactive visualizations to support the comprehension of spatial relationships between virtual and real world objects for Augmented Reality (AR) applications. To enhance the clarity of such relationships we discuss visualization techniques and their suitability for AR. We apply them on different AR applications with different goals, e.g. in X-Ray vision or in applications which draw a user's attention to an object of interest. We demonstrate how Focus and Context (F+C) visualizations are used to affect the user's perception of hidden or nearby objects by presenting contextual information in the area of augmentation. We discuss the organization and the possible sources of data for visualizations in Augmented Reality and present cascaded and multi level F+C visualizations to address complex, cluttered scenes that are inevitable in real environments. This article also shows filters and tools to interactively control the amount of augmentation. It compares the impact of real world context preserving to a pure virtual and uniform enhancement of these structures for augmentations of real world imagery. Finally this paper discusses the stylization of sparse object representations for AR to improve X-Ray vision. {\textcopyright} 2006 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalkofen, Mendez, Schmalstieg - 2009 - Comprehensible visualization for augmented reality.pdf:pdf},
  keywords     = {Artificial,Augmented,Data structures,Graphics data structures and data types,Graphs and networks,Interaction techniques,Methodology and techniques,Multimedia information systems,Style guides,User interfaces,Virtual realities},
  pmid         = {19147885},
  publisher    = {IEEE},
}

@Article{Ozgur2017,
  author       = {Ozgur, Erol and Lafont, Alexis and Bartoli, Adrien},
  year         = {2017},
  journal       = {Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017},
  title        = {{Visualizing In-Organ Tumors in Augmented Monocular Laparoscopy}},
  doi          = {10.1109/ISMAR-Adjunct.2017.30},
  pages        = {46--51},
  abstract     = {One of the important goals of medical augmented reality is to reveal the hidden anatomy, such as a tumor in an organ. However, conveying a hidden tumor's depth to the user effortlessly and precisely is still an unsolved problem. This is especially difficult in monocular laparoscopy. First, the number of available depth cues is in practice limited to only two: occlusion and relative size. Second, exploiting these cues is not an easy task either. We propose a specific visualization consisting of auxiliary orthographic tumor silhouettes on the front and back surfaces of the organ and a semi-transparent tumor in between. This creates two depth planes forming a perceivable ratio-scaled metric space for the tumor. We conducted a user study to evaluate the proposed visualization. The results show that subsurface tumor depth perception is improved dramatically compared to the conventional transparent overlay.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ozgur, Lafont, Bartoli - 2017 - Visualizing In-Organ Tumors in Augmented Monocular Laparoscopy.pdf:pdf},
  isbn         = {9780769563275},
}

@Article{Habert2017,
  author       = {Habert, {\'{S}}everine and Meng, Ma and Fallavollitaz, Pascal and Navab, Nassir},
  year         = {2017},
  journal       = {arXiv},
  title        = {{Multi-layer Visualization for Medical Mixed Reality}},
  eprint       = {1709.08962},
  eprinttype   = {arXiv},
  issn         = {23318422},
  abstract     = {Medical Mixed Reality helps surgeons to contextualize intraoperative data with video of the surgical scene. Nonetheless, the surgical scene and anatomical target are often occluded by surgical instruments and surgeon hands. In this paper and to our knowledge, we propose a multi-layer visualization in Medical Mixed Reality solution which subtly improves a surgeons visualization by making transparent the occluding objects. As an example scenario, we use an augmented reality C-Arm fluoroscope device. A video image is created using a volumetric-based image synthesization technique and stereo-RGBD cameras mounted on the C-Arm. From this synthesized view, the background which is occluded by the surgical instruments and surgeon hands is recovered by modifying the volumetric-based image synthesization technique. The occluding objects can therefore become transparent over the surgical scene. Experimentation with different augmented reality scenarios yield results demonstrating that the background of the surgical scenes can be recovered with accuracy between 45\%-99\%. In conclusion, we presented a solution that a Mixed Reality solution for medicine, providing transparency to objects occluding the surgical scene. This work is also the first application of volumetric field for Diminished Reality/ Mixed Reality.},
  arxivid      = {1709.08962},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Habert et al. - 2017 - Multi-layer Visualization for Medical Mixed Reality.pdf:pdf},
  keywords     = {Diminished Reality,Medicine,Mixed Reality,Multi-Layer,Surgery,Visualization},
}

@Article{Avery2008,
  author       = {Avery, Benjamin and Thomas, Bruce H. and Piekarski, Wayne},
  year         = {2008},
  journal       = {Proceedings - 7th IEEE International Symposium on Mixed and Augmented Reality 2008, ISMAR 2008},
  title        = {{User evaluation of see-through vision for mobile outdoor augmented reality}},
  doi          = {10.1109/ISMAR.2008.4637327},
  pages        = {69--72},
  abstract     = {We have developed a system built on our mobile AR platform that provides users with see-through vision, allowing visualization of occluded objects textured with real-time video information. We present a user study that evaluates the user's ability to view this information and understand the appearance of an outdoor area occluded by a building while using a mobile AR computer. This understanding was compared against a second group of users who watched video footage of the same outdoor area on a regular computer monitor. The comparison found an increased accuracy in locating specific points from the scene for the outdoor AR participants. The outdoor participants also displayed more accurate results, and showed better speed improvement than the indoor group when viewing more than one video simultaneously. {\textcopyright} 2008 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Avery, Thomas, Piekarski - 2008 - User evaluation of see-through vision for mobile outdoor augmented reality.pdf:pdf},
  isbn         = {9781424428403},
  keywords     = {Image-based rendering,Occlusion,Outdoor augmented reality,Telepresence,Wearable computers},
}

@Article{Geethan2015,
  author       = {Geethan, P. and Jithin, P. and Naveen, T. and Padminy, K. V. and {Shruthi Krithika}, J. and Vasudevan, Shriram K.},
  year         = {2015},
  journal       = {Indian Journal of Science and Technology},
  title        = {{Augmented reality X-ray vision with gesture interaction}},
  doi          = {10.17485/ijst/2015/v8iS7/63354},
  issn         = {09745645},
  number       = {April},
  pages        = {43--47},
  volume       = {8},
  abstract     = {Augmented reality is a new technology which is capable of presenting possibilities that are difficult for other technologies to offer and meet. AR will really alter the way individuals view the world. Augmented reality X-Ray Vision is an emerging concept. While AR deals with virtual and real objects coexisting in the same space, AR X-Ray Vision is a subdivision of the broad spectrum of AR, which provides a "see through" vision among real world objects. In this paper, we have thoroughly analysed the existing methodologies dealing with AR X-Ray Vision and we have come up with a convenient method that enables easy implementation. This paper deals with creating a methodology to provide an X-Ray vision using the anaglyph technique and finally integrating it with the Leap Motion Controller to enable gesture interaction to move the window around through which the point of interest can be viewed. The limitations of the suggested methodology have also been discussed. This system enables the user to perceive depth between two regions with the help of just anaglyph glasses without the use of any head mounted display devices. Can be extended to that of medical field, where X-Ray vision is of increasing importance to view the layers of skin and bones of a patient giving the doctors and surgeons an approximate depth perception.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geethan et al. - 2015 - Augmented reality X-ray vision with gesture interaction.pdf:pdf},
  keywords     = {Anaglyph,Augmented reality,Depth perception,Gesture interaction,Leap motion controller,X-ray vision},
}

@Article{Fichtinger2005,
  author       = {Fichtinger, Gabor and Deguet, Anton and Masamune, Ken and Balogh, Emese and Fischer, Gregory S. and Mathieu, Herve and Taylor, Russell H. and Zinreich, S. James and Fayad, Laura M.},
  year         = {2005},
  journal       = {IEEE Transactions on Biomedical Engineering},
  title        = {{Image overlay guidance for needle insertion in CT scanner}},
  doi          = {10.1109/TBME.2005.851493},
  issn         = {00189294},
  number       = {8},
  pages        = {1415--1424},
  volume       = {52},
  abstract     = {We present an image overlay system to aid needle insertion procedures in computed tomography (CT) scanners. The device consists of a display and a semitransparent mirror that is mounted on the gantry. Looking at the patient through the mirror, the CT image appears to be floating inside the patient with correct size and position, thereby providing the physician with two-dimensional (2-D) "X-ray vision" to guide needle insertions. The physician inserts the needle following the optimal path identified in the CT image rendered on the display and, thus, reflected in the mirror. The system promises to reduce X-ray dose, patient discomfort, and procedure time by significantly reducing faulty insertion attempts. It may also increase needle placement accuracy. We report the design and implementation of the image overlay system followed by the results of phantom and cadaver experiments in several clinical applications. {\textcopyright} 2005 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fichtinger et al. - 2005 - Image overlay guidance for needle insertion in CT scanner.pdf:pdf},
  keywords     = {Augmented reality,Computed tomography,Image guidance,Image overlay,Needle insertion},
  pmid         = {16119237},
}



@Article{Maia2016,
  author       = {Maia, Lu{\'{i}}s Fernando and Viana, Windson and Trinta, Fernando},
  year         = {2016},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{A real-time X-ray mobile application using augmented reality and google street view}},
  doi          = {10.1145/2993369.2993370},
  pages        = {111--119},
  volume       = {02-04-Nove},
  abstract     = {X-ray view can be defined as the ability one has to see through real surfaces. Although this skill is often associated with superheroes and medical examination, there are several researches conducted to employ X-ray view in numerous applications. However, the generation of X-ray visualization includes numerous challenges regarding occlusion, realistic appearance, and depth perception. In this paper, we present a mobile application that uses Augmented Reality and Google Street View to allow users experience real-time X-ray vision. The proposed application was designed to enhance previous Augmented Reality X-ray systems, by introducing a silhouette computation method to provide visual context from the occluder and a perspective estimation system that improves the projection of occluded images into the real scene. Additionally, we implemented two usability studies to assess qualitative aspects of both silhouettes and perspective estimation to generate better X-ray effects. Results indicate good acceptance of the novel X-ray visualization method and a great usability score on the SUS scale for the mobile application.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maia, Viana, Trinta - 2016 - A real-time X-ray mobile application using augmented reality and google street view.pdf:pdf},
  isbn         = {9781450344913},
  keywords     = {Augmented reality,Image and Video Processing in UI,Mobile and embedded devices},
}

@Article{Khalil2016,
  author       = {Khalil, Hanan and Peters, Micah and Godfrey, Christina M. and Mcinerney, Patricia and Soares, Cassia Baldini and Parker, Deborah},
  year         = {2016},
  journal       = {Worldviews on Evidence-Based Nursing},
  title        = {{An Evidence-Based Approach to Scoping Reviews}},
  doi          = {10.1111/wvn.12144},
  issn         = {17416787},
  number       = {2},
  pages        = {118--123},
  volume       = {13},
  abstract     = {Objective: Scoping reviews are used to assess the extent of a body of literature on a particular topic, and often to ensure that further research in that area is a beneficial addition to world knowledge. The aim of this paper reports upon the development of a methodology for scoping reviews based upon the Arksey and O'Malley framework, the Levac, Colquhoun, and O'Brien, and the Joanna Briggs Institute methods of evidence synthesis. Methods: A working group consisting of members of the Joanna Briggs collaborating organizations met to discuss the proposed framework for the methodology and develop a draft for the scoping review methodology based on the Arksey and O'Malley framework and the work of Levac et al. This was followed by a workshop attended by other members of the organizations consisting of 30 international researchers to discuss the proposed methodology. Further refinement of the methodology was undertaken as a result of the feedback received from the workshop. Results: The development of the methodology focused on five stages of the protocol and review development. These were identifying the research question by clarifying and linking the purpose and research question, identifying the relevant studies using a three-step literature search in order to balance feasibility with breadth and comprehensiveness, careful selection of the studies to using a team approach, charting the data and collating the results to identify the implications of the study findings for policy, practice, or research. Linking Evidence to Action: The current methodology recommends including both quantitative and qualitative research, as well as evidence from economic and expert opinion sources to answer questions of effectiveness, appropriateness, meaningfulness and feasibility of health practices and delivery methods. The proposed framework has the potential to provide options when faced with complex concepts or broad research questions.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khalil et al. - 2016 - An Evidence-Based Approach to Scoping Reviews.pdf:pdf},
  keywords     = {Application of theory,International health/Global health,Metaanalysis/Data pooling,Outcome evaluation,Research Methods,Reviews,Scoping,Theory},
  pmid         = {26821833},
}

@Article{Matthew2019,
  author       = {Matthew, Page and Joanne, McKenzie and Patrick, Bossuyt and Isabelle, Boutron and Tammy, Hoffmann and Cynthia, Mulrow and Larissa, Shamseer and Roger, Chou and Julie, Glanville and Jeremy, Grimshaw and Asbj{\o}rn, Hr{\'{o}}bjartsson and Manoj, Lalu and Li, Tianjing and Elizabeth, Loder and Evan, Mayo-Wilson and Steve, McDonald and Lesley, Stewart and Jennifer, Tetzlaff and James, Thomas and Andrea, Tricco and Vivian, Welch and Penny, Whiting and David, Moher and 2020, Group the PRISMA},
  year         = {2019},
  journal       = {26th Cochrane Colloquium, Santiago, Chile},
  title        = {{PRISMA 2020 statement: updated guidelines for reporting systematic reviews and meta analyses}},
  doi          = {10.1016/j.jclinepi.2021.03.001},
  issn         = {0895-4356},
  number       = {xxxx},
  pages        = {1--12},
  url          = {https://doi.org/10.1016/j.jclinepi.2021.03.001},
  abstract     = {, , ,},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthew et al. - 2019 - PRISMA 2020 statement updated guidelines for reporting systematic reviews and meta analyses.pdf:pdf},
  keywords     = {Systematic review,Meta-analysis,Reporting guidelin},
  publisher    = {Elsevier Inc.},
}

@Article{Tai2009,
  author       = {Tai, Nan-ching and Inanici, Mehlika},
  year         = {2009},
  journal       = {IES 2009 Annual Conference},
  title        = {{Depth Perception as a Function of Lighting, Time and Spatiality}},
  number       = {November},
  pages        = {40--43},
  abstract     = {Perceptual qualities of luminous environment change based on spatiality, time, and observer. This paper focuses on the complex interrelationships between architectural configurations, luminance distribution patterns, and the perception of spatial depth. A computational framework that draws from the recent developments in computer graphics (physically based renderings and perceptually based tone mapping techniques) is utilized to generate pictorial spaces. Daylighting conditions are parametrically changed and psychophysical experiments are conducted to measure the perceived distances of targets. Results reveal that luminance contrast is an effective pictorial cue that can increase the perception of the spatial depth.},
  file         = {:C\:/Users/adminuser/Desktop/Depth_perception_lighting_time_spatiality1.pdf:pdf},
}

@Article{Koizumi2018,
  author       = {Koizumi, Tomomi and Ito, Hiroyuki and Sunaga, Shoji and Ogawa, Masaki and Tomimatsu, Erika},
  year         = {2018},
  journal       = {i-Perception},
  title        = {{Assumed Lighting Direction in the Interpretation of Cast Shadows}},
  doi          = {10.1177/2041669518790576},
  issn         = {20416695},
  number       = {4},
  pages        = {4--9},
  volume       = {9},
  abstract     = {Assumed lighting direction in cast-shadow interpretation was investigated. Experiment 1 used an ambiguous object–shadow-matching task to measure bias in shadow-matching direction. The shadow-matching bias was largest when the lighting direction was on average 38.3° left from above (a median of 25.1°). Experiment 2 tested the effect of body posture (head orientation) on cast-shadow interpretation using stimuli aligned in a head-centrically vertical or horizontal orientation. The below-shadow (light-from-above) bias in the head-centric frame was robust across the sitting upright, reclining-on-the-left-side, reclining-on-the-right-side, and supine conditions. A right-shadow (light-from-left) bias in the head-centric frame was found for the sitting upright and reclining-on-the-right-side conditions. In the reclining-on-the-left-side condition, shadow biases to the gravitational below direction and head-centric right direction may have cancelled each other out. These results are consistent with findings from previous shape-from-shading studies, suggesting that the same light-source assumption is applied to shading and shadow interpretations.},
  file         = {:C\:/Users/adminuser/Desktop/2041669518790576.pdf:pdf},
  keywords     = {depth,frames of reference,light,three-dimensional perception},
}

@Article{Atli2010,
  author   = {Atlı, Deniz},
  year     = {2010},
  title    = {{Effects of Color and Colored Light on Depth Perception}},
  url      = {http://www.thesis.bilkent.edu.tr/0004054.pdf},
  abstract = {The main purpose of this study is to understand the relationship between different objects and background colors, and depth perception in interior spaces. The experiment was conducted in two phases which consist of colored background light pairs (cool white-orange, cool white-blue, cool white-green, cool white-red, warm white-cool white, red-green and orange-blue) with colored objects (orange, blue and gray) in front of them. A forced choice paired comparison method was used to evaluate the differences in depth perception caused by colors. The participants were students who were having their internships in Philips Research Eindhoven, Netherlands. Firstly, participants were tested for color blindness and visual acuity, and the ones who passed these tests participated in the experiment. After the first phase of the experiment, a second part was required in order to obtain more accurate results. The participants who had internally consistent results in the first phase participated in the second phase of the experiment. In both phases, participants judged the distances of two same colored objects in front of colored lit background by choosing the one which they perceived as closer to themselves. As a result, differences between hues are smaller than the variations in perception of the participants, so hue has a really small effect on depth perception when evaluated monocularly.},
  file     = {:C\:/Users/adminuser/Desktop/0004054.pdf:pdf},
  keywords = {color,colored lighting.,depth perception},
}

@Article{Otsuki2015,
  author       = {Otsuki, Mai and Kamioka, Yuko and Kitai, Yuka and Kanzaki, Mao and Kuzuoka, Hideaki and Uchiyama, Hiroko},
  year         = {2015},
  journal       = {SIGGRAPH Asia 2015 Emerging Technologies, SA 2015},
  title        = {{Please show me inside: Improving the depth perception using virtual mask in stereoscopic AR}},
  doi          = {10.1145/2818466.2818469},
  number       = {February 2018},
  pages        = {2--5},
  abstract     = {A practical application of Augmented Reality (AR) is see-through vision, a technique that enables a user to observe a virtual object located behind a real object by superimposing the virtually visualized inner object onto the real object surface. This technique is considered to be effective in several areas, including medical [Bichlmeier et al. 2007] [Lerotic et al. 2007] [Nicolau et al. 2011] [Sielhorst, et al. 2006] and industrial visualizations [Schall et al. 2009] [Zollmann et al. 2010]. In these applications, one challenge is determining how to cause a virtual object to appear behind a real object surface.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Otsuki et al. - 2015 - Please show me inside Improving the depth perception using virtual mask in stereoscopic AR.pdf:pdf;:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Otsuki et al. - 2015 - Please show me inside Improving the depth perception using virtual mask in stereoscopic AR(2).pdf:pdf},
  isbn         = {9781450339254},
}

@Article{Chum2005,
  author       = {Chum, Ondřej and Pajdla, Tom{\'{a}}{\v{s}} and Sturm, Peter},
  year         = {2005},
  journal       = {Computer Vision and Image Understanding},
  title        = {{The geometric error for homographies}},
  doi          = {10.1016/j.cviu.2004.03.004},
  issn         = {10773142},
  number       = {1},
  pages        = {86--102},
  volume       = {97},
  abstract     = {We address the problem of finding optimal point correspondences between images related by a homography: given a homography and a pair of matching points, determine a pair of points that are exactly consistent with the homography and that minimize the geometric distance to the given points. This problem is tightly linked to the triangulation problem, i.e., the optimal 3D reconstruction of points from image pairs. Our problem is non-linear and iterative optimization methods may fall into local minima. In this paper, we show how the problem can be reduced to the solution of a polynomial of degree eight in a single variable, which can be computed numerically. Local minima are thus explicitly modeled and can be avoided. An application where this method significantly improves reconstruction accuracy is discussed. Besides the general case of homographies, we also examine the case of affine transformations, and closely study the relationships between the geometric error and the commonly used Sampson's error, its first order approximation. Experimental results comparing the geometric error with its approximation by Sampson's error are presented. {\textcopyright} 2004 Elsevier Inc. All rights reserved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chum, Pajdla, Sturm - 2005 - The geometric error for homographies.pdf:pdf},
  keywords     = {Geometric error,Homography,Sampson's error,Triangulation},
}

@Article{Li2010,
  author       = {Li, T. T. and Zhang, H. Y. and Geng, J.},
  year         = {2010},
  journal       = {International Conference Image and Vision Computing New Zealand},
  title        = {{Geometric calibration of a camera-projector 3D imaging system}},
  doi          = {10.1109/IVCNZ.2010.6148798},
  issn         = {21512191},
  number       = {November 2010},
  abstract     = {Use of projector in structured light based 3D imaging systems has grown its popularity recently, due to rapid advance of DLP and LCOS chip technologies. Yet, simple and accurate calibration method for camera-projector has not received its deserved attention in written literature due to some fundamental difficulties in the geometric calibration of projectors. Existing projector calibration methods are based on a separately calibrated camera, therefore the accuracy of the projector calibration depends heavily on the method and accuracy of the camera calibration. In this paper, we propose a novel method that is able to perform simultaneous geometric calibration of both the camera and projector. The calibration procedure is based on images of a colored chessboard and a projected pattern from the projector in different colors on the same chessboard. These images are acquired by the un-calibrated camera. The unique design of our color scheme of the chessboard and the projected pattern enables the un-calibrated camera to acquire multiple images of original chessboard pattern on the board and the projected pattern (in different colors) on the same plane. We then use a local linearization approach to establish point correspondence. This unique design greatly simplifies the calibration algorithms to establish relationship between these patterns thus enables a practical method of simultaneous geometric calibration of both the camera and the projector. {\textcopyright} 2010 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Zhang, Geng - 2010 - Geometric calibration of a camera-projector 3D imaging system.pdf:pdf},
  isbn         = {9781424496303},
  keywords     = {3D imaging,camera calibration,projector calibration,simultaneous geometric calibration},
}

@Article{Chang2015,
  author       = {Chang, Chun Fa and Chen, Kuan Wei and Chuang, Chin Chien},
  year         = {2015},
  journal       = {International Conference on Digital Signal Processing, DSP},
  title        = {{Performance comparison of rasterization-based graphics pipeline and ray tracing on GPU shaders}},
  doi          = {10.1109/ICDSP.2015.7251842},
  pages        = {120--123},
  volume       = {2015-Septe},
  abstract     = {The interactive rendering of 3D computer graphics has approached the photorealistic quality, as evident by the vivid shading effects and lush outdoor scenes in recent game engines. Clearly, the traditional 3D graphics APIs are reaching their limits, and the need to switch to more extendable ray-tracing based techniques has arisen. In this work, we explore the fundamental differences between ray tracing based and rasterization based techniques, including how they are supported by the processors and by the programming platforms. We duplicate the typical shading effects in both ray tracing and rasterization, starting from the simple Phong lighting, to slightly more complex Whitted-style shadow and reflection. Although the rasterization-based techniques clearly outperform ray tracing in current generations of graphics processors, we show by more precise quantitative analysis that the performance gaps are not as wide as thought. And the gap may narrow further when the requirement of image quality increases in the future.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Chen, Chuang - 2015 - Performance comparison of rasterization-based graphics pipeline and ray tracing on GPU shaders.pdf:pdf},
  isbn         = {9781479980581},
  keywords     = {GPU shaders,rasterization,ray tracing},
  publisher    = {IEEE},
}

@Article{Gruenefeld2020,
  author       = {Gruenefeld, Uwe and Br{\"{u}}ck, Yvonne and Boll, Susanne},
  year         = {2020},
  journal       = {ACM International Conference Proceeding Series},
  title        = {{Behind the Scenes: Comparing X-Ray Visualization Techniques in Head-mounted Optical See-through Augmented Reality}},
  doi          = {10.1145/3428361.3428402},
  pages        = {179--185},
  abstract     = {Locating objects in the environment can be a difficult task, especially when the objects are occluded. With Augmented Reality, we can alternate our perceived reality by augmenting it with visual cues or removing visual elements of reality, helping users to locate occluded objects. However, to our knowledge, it has not yet been evaluated which visualization technique works best for estimating the distance and size of occluded objects in optical see-through head-mounted Augmented Reality. To address this, we compare four different visualization techniques derived from previous work in a laboratory user study. Our results show that techniques utilizing additional aid (textual or with a grid) help users to estimate the distance to occluded objects more accurately. In contrast, a realistic rendering of the scene, such as a cutout in the wall, resulted in higher distance estimation errors.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gruenefeld, Br{\"{u}}ck, Boll - 2020 - Behind the Scenes Comparing X-Ray Visualization Techniques in Head-mounted Optical See-through Augmente.pdf:pdf},
  isbn         = {9781450388702},
  keywords     = {X-ray,augmented reality,head-mounted,occluded objects,occlusion,optical see-through,visualization techniques},
}

@Article{Gutierrez2006,
  author       = {Guti{\'{e}}rrez, {\'{A}}ngel and Boero, Paulo},
  year         = {2006},
  journal       = {Handbook of Research on the Psychology of Mathematics Education},
  title        = {{Research on Visualization in Learning and Teaching Mathematics}},
  doi          = {10.1163/9789087901127_009},
  number       = {January 2006},
  pages        = {205--235},
  abstract     = {cit{\'{e}} 288 fois, d{\'{e}}c. 2015. In 1988, at the 12th Annual Conference of the International Group for the Psychology of Mathematics Education (PME-12), in Veszprem, Hungary, Alan Bishop introduced his review of research on visualization in mathematics education as follows: This review builds on and extends from earlier reviews written either by the author or by others (Bishop, 1980; Bishop, 1983; Bishop, 1986; Clements, 1982; Presmeg, 1986b; Mitchelmore, 1976) but will be restricted to the notion of ‘visualisation'. This construct interacts in the research literature with the ideas of imagery, spatial ability, and intuition, but it is certainly not the case that visualisation has been felt to be a significant research area in mathematics education in the recent past. Whilst searching the literature in preparation for this review, it was surprising to discover that in the JRME listing of 223 research articles in 1985 only 8 were remotely connected with the topic, that in the same listing for 1986 only 7 out of the 236 articles were related and at PME XI no papers were specifically focused on visualisation in mathematics education. (1: p. 170, his emphasis).},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guti{\'{e}}rrez, Boero - 2006 - Research on Visualization in Learning and Teaching Mathematics.pdf:pdf},
}

@Article{Friedman1995,
  author = {Friedman, Lynn},
  year   = {1995},
  title  = {{The Space Factor in Mathematics : Gender Differences Author ( s ): Lynn Friedman Source : Review of Educational Research , Vol . 65 , No . 1 ( Spring , 1995 ), pp . 22-50 Published by : American Educational Research Association Stable URL : http://www.jst}},
  number = {1},
  pages  = {22--50},
  volume = {65},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedman - 1995 - The Space Factor in Mathematics Gender Differences Author ( s ) Lynn Friedman Source Review of Educational Research.pdf:pdf},
}

@Article{Endris2010,
  author       = {Endris, Solomon},
  year         = {2010},
  journal       = {Educational Studies in Mathematics},
  title        = {{Types of reasoning in 3D geometry thinking and their relation with spatial ability}},
  doi          = {10.1007/s},
  number       = {2},
  pages        = {2008--2010},
  volume       = {75},
  abstract     = {The purpose of this research is to examine preservice elementary school teachers' geometry learning as investigated by both qualitative and quantitative methods. For the qualitative investigation, narrative analysis and thematic analysis methods were used. The findings of narrative analysis indicated two main kinds of stories: as a learner and as a beginning teacher. The thematic analysis findings yield to three themes: history of learning geometry, perceptions about geometry, effective geometry instructional practices. The findings informed the quantitative investigation on geometry content knowledge for the case of quadrilaterals. During the second phase of the study, 102 participants who enrolled in the methods course completed pre and post test of teachers' geometry content knowledge. Treatment group participants (n=54) received series of activities (geometry activities and student work analysis) focusing on quadrilaterals, and control group participants (n=48) received traditional instruction. Repeated measures ANOVA results showed a significant change in treatment group participants' geometry content knowledge. The mixed ANOVA results indicated a significant main effect of knowledge but no significant interaction between geometry content knowledge and grouping. Even though treatment group participants' geometry content knowledge growth was significant, the difference between treatment group and control group participants' growth in geometry content knowledge was not significant. This study informs mathematics teacher education in three important areas; limited knowledge of preservice teachers' geometry content knowledge, integrating mathematics content and the context of teaching into methods course, and use of student work with preservice teachers.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Endris - 2010 - Types of reasoning in 3D geometry thinking and their relation with spatial ability.pdf:pdf},
  keywords     = {3d geometry,representation,spatial abilities,visualisation},
}

@Article{Cheveigne2019,
  author       = {de Cheveign{\'{e}}, Alain and Nelken, Israel},
  year         = {2019},
  journal       = {Neuron},
  title        = {{Filters: When, Why, and How (Not) to Use Them}},
  doi          = {10.1016/j.neuron.2019.02.039},
  issn         = {10974199},
  number       = {2},
  pages        = {280--293},
  volume       = {102},
  abstract     = {Filters are commonly used to reduce noise and improve data quality. Filter theory is part of a scientist's training, yet the impact of filters on interpreting data is not always fully appreciated. This paper reviews the issue and explains what a filter is, what problems are to be expected when using them, how to choose the right filter, and how to avoid filtering by using alternative tools. Time-frequency analysis shares some of the same problems that filters have, particularly in the case of wavelet transforms. We recommend reporting filter characteristics with sufficient details, including a plot of the impulse or step response as an inset.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Cheveign{\'{e}}, Nelken - 2019 - Filters When, Why, and How (Not) to Use Them.pdf:pdf},
  keywords     = {Fourier analysis,causality,distortions,filter,impulse response,oscillations,ringing,time-frequency representation},
  pmid         = {30998899},
}

@Article{Casiez2012,
  author       = {Casiez, G{\'{e}}ry and Roussel, Nicolas and Vogel, Daniel},
  year         = {2012},
  journal       = {Conference on Human Factors in Computing Systems - Proceedings},
  title        = {{1 Euro filter: A simple speed-based low-pass filter for noisy input in interactive systems}},
  doi          = {10.1145/2207676.2208639},
  pages        = {2527--2530},
  abstract     = {The 1€ filter ("one Euro filter") is a simple algorithm to filter noisy signals for high precision and responsiveness. It uses a first order low-pass filter with an adaptive cutoff frequency: at low speeds, a low cutoff stabilizes the signal by reducing jitter, but as speed increases, the cutoff is increased to reduce lag. The algorithm is easy to implement, uses very few resources, and with two easily understood parameters, it is easy to tune. In a comparison with other filters, the 1€ filter has less lag using a reference amount of jitter reduction. Copyright 2012 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Casiez, Roussel, Vogel - 2012 - 1€ filter A simple speed-based low-pass filter for noisy input in interactive systems.pdf:pdf},
  isbn         = {9781450310154},
  keywords     = {Filtering,Jitter,Lag,Noise,Precision,Responsiveness,Signal},
}

@Article{Lee2016,
  author       = {Lee, Sangyoon and Hu, Xinda and Hua, Hong},
  year         = {2016},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Effects of optical combiner and IPD change for convergence on near-field depth perception in an optical see-through HMD}},
  doi          = {10.1109/TVCG.2015.2440272},
  issn         = {10772626},
  number       = {5},
  pages        = {1540--1554},
  volume       = {22},
  abstract     = {Many error sources have been explored in regards to the depth perception problem in augmented reality environments using optical see-through head-mounted displays (OST-HMDs). Nonetheless, two error sources are commonly neglected: the ray-shift phenomenon and the change in interpupillary distance (IPD). The first source of error arises from the difference in refraction for virtual and see-through optical paths caused by an optical combiner, which is required of OST-HMDs. The second occurs from the change in the viewer's IPD due to eye convergence. In this paper, we analyze the effects of these two error sources on near-field depth perception and propose methods to compensate for these two types of errors. Furthermore, we investigate their effectiveness through an experiment comparing the conditions with and without our error compensation methods applied. In our experiment, participants estimated the egocentric depth of a virtual and a physical object located at seven different near-field distances (40∼200 cm) using a perceptual matching task. Although the experimental results showed different patterns depending on the target distance, the results demonstrated that the near-field depth perception error can be effectively reduced to a very small level (at most 1 percent error) by compensating for the two mentioned error sources.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Hu, Hua - 2016 - Effects of optical combiner and IPD change for convergence on near-field depth perception in an optical see-throug.pdf:pdf},
  keywords     = {Near-field depth perception,mixed/augmented reality,optical see-through head-mounted display},
  publisher    = {IEEE},
}

@Article{Singh2018,
  author       = {Singh, Gurjot and Ellis, Stephen R. and Swan, J. Edward},
  year         = {2018},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{The Effect of Focal Distance, Age, and Brightness on Near-Field Augmented Reality Depth Matching}},
  doi          = {10.1109/TVCG.2018.2869729},
  eprint       = {1712.00088},
  eprinttype   = {arXiv},
  issn         = {19410506},
  number       = {2},
  pages        = {1385--1398},
  volume       = {26},
  abstract     = {Many augmented reality (AR) applications operate within near-field reaching distances, and require matching the depth of a virtual object with a real object. The accuracy of this matching was measured in three experiments, which examined the effect of focal distance, age, and brightness, within distances of 33.3 to 50$\sim$cm, using a custom-built AR haploscope. Experiment$\sim$I examined the effect of focal demand, at the levels of collimated (infinite focal distance), consistent with other depth cues, and at the midpoint of reaching distance. Observers were too young to exhibit age-related reductions in accommodative ability. The depth matches of collimated targets were increasingly overestimated with increasing distance, consistent targets were slightly underestimated, and midpoint targets were accurately estimated. Experiment$\sim$II replicated Experiment$\sim$I, with older observers. Results were similar to Experiment$\sim$I. Experiment$\sim$III replicated Experiment$\sim$I with dimmer targets, using young observers. Results were again consistent with Experiment$\sim$I, except that both consistent and midpoint targets were accurately estimated. In all cases, collimated results were explained by a model, where the collimation biases the eyes&#x0027; vergence angle outwards by a constant amount. Focal demand and brightness affect near-field AR depth matching, while age-related reductions in accommodative ability have no effect.},
  arxivid      = {1712.00088},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Ellis, Swan - 2018 - The Effect of Focal Distance, Age, and Brightness on Near-Field Augmented Reality Depth Matching.pdf:pdf},
  keywords     = {Augmented reality,Brightness,Observers,Optical imaging,Perception and psychophysics,Retina,Surgery,Visualization,depth perception,human performance,virtual and augmented reality},
  pmid         = {30222576},
  publisher    = {IEEE},
}

@Article{Singh2011,
  author       = {Singh, Gurjot and Swan, J. Edward and Jones, J. Adam and Ellis, Stephen R.},
  year         = {2011},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Depth judgment tasks and environments in near-field augmented reality}},
  doi          = {10.1109/VR.2011.5759488},
  number       = {March},
  pages        = {241--242},
  abstract     = {In this poster abstract we describe an experiment that measured depth judgments in optical see-through augmented reality at near-field distances of 34 to 50 centimeters. The experiment compared two depth judgment tasks: perceptual matching, a closed-loop task, and blind reaching, a visually open-loop task. The experiment tested each of these tasks in both a real-world environment and an augmented reality environment, and used a between-subjects design that included 40 participants. The experiment found that matching judgments were very accurate in the real world, with errors on the order of millimeters and very little variance. In contrast, matching judgments in augmented reality showed a linear trend of increasing overestimation with increasing distance, with a mean overestimation of ∼ 1 cm. With reaching judgments participants underestimated ∼ 4.5 cm in both augmented reality and the real world. We also discovered and solved a calibration problem that arises at near-field distances. {\textcopyright} 2011 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh et al. - 2011 - Depth judgment tasks and environments in near-field augmented reality.pdf:pdf},
  isbn         = {9781457700361},
  keywords     = {augmented reality,depth perception,optical see-through display,x-ray vision},
}

@Article{Krajancich2020,
  author       = {Krajancich, Brooke and Kellnhofer, Petr and Wetzstein, Gordon},
  year         = {2020},
  journal       = {ACM Transactions on Graphics},
  title        = {{Optimizing depth perception in virtual and augmented reality through gaze-contingent stereo rendering}},
  doi          = {10.1145/3414685.3417820},
  issn         = {15577368},
  number       = {6},
  volume       = {39},
  abstract     = {Virtual and augmented reality (VR/AR) displays crucially rely on stereoscopic rendering to enable perceptually realistic user experiences. Yet, existing near-eye display systems ignore the gaze-dependent shift of the no-parallax point in the human eye. Here, we introduce a gaze-contingent stereo rendering technique that models this effect and conduct several user studies to validate its effectiveness. Our findings include experimental validation of the location of the no-parallax point, which we then use to demonstrate significant improvements of disparity and shape distortion in a VR setting, and consistent alignment of physical and digitally rendered objects across depths in optical see-through AR. Our work shows that gaze-contingent stereo rendering improves perceptual realism and depth perception of emerging wearable computing systems.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krajancich, Kellnhofer, Wetzstein - 2020 - Optimizing depth perception in virtual and augmented reality through gaze-contingent stereo r.pdf:pdf},
  keywords     = {applied perception,augmented reality,rendering,virtual reality},
}

@Article{Sompagnimdi2017,
  author = {Sompagnimdi, Michael Traor{\'{e}} and Hurter, Christophe and Sompagnimdi, Michael Traor{\'{e}} and Hurter, Christophe and Hurter, Christophe},
  year   = {2017},
  title  = {{Interactive exploration of 3D scanned baggage To cite this version : HAL Id : hal-01469783 Interactive Exploration of 3D Scanned Baggage}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sompagnimdi et al. - 2017 - Interactive exploration of 3D scanned baggage To cite this version HAL Id hal-01469783 Interactive Explora.pdf:pdf},
}

@Article{Merks2018,
  author       = {Merks, Sarah and Hattenschwiler, Nicole and Zeballos, Melina and Schwaninger, Adrian},
  year         = {2018},
  journal       = {Proceedings - International Carnahan Conference on Security Technology},
  title        = {{X-ray Screening of Hold Baggage: Are the Same Visual-Cognitive Abilities Needed for 2D and 3D Imaging?}},
  doi          = {10.1109/CCST.2018.8585715},
  issn         = {10716572},
  number       = {October},
  volume       = {2018-Octob},
  abstract     = {2D multi-view X-ray imaging technology is widely used for security screening of hold baggage at airports. Newer technology is based on 3D CT imaging. Such systems offer the possibility to rotate a bag around 360 degrees. With the transition from 2D multi-view to advanced CT imaging, the question arises whether airport security officers (screeners) need the same visual-cognitive abilities when visually inspecting X-ray images of hold baggage. This study investigated the relationship between visual-cognitive abilities and visual inspection performance of screeners. Screeners conducted a computer-based visual cognitive test battery (VCTB) and a simulated hold baggage screening task with 2D and 3D imaging. We found that aspects of processing speed and visual processing correlated significantly with visual inspection performance of screeners using 2D imaging technology. In comparison, performance of screeners that visually inspected 3D images showed less correlations with the VCTB. These results indicate that with the expected change from 2D to 3D imaging technology in airport security, visual-cognitive requirements of the screeners might change. Therefore, further studies need to elucidate in more detail what visual-cognitive skills future 3D screeners need as it could affect personnel selection and development.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Merks et al. - 2018 - X-ray Screening of Hold Baggage Are the Same Visual-Cognitive Abilities Needed for 2D and 3D Imaging.pdf:pdf},
  isbn         = {9781538679319},
  keywords     = {2D multiview imaging,3D imaging,X-ray imaging technology,airport security,hold baggage screening,operator performance,visual cognitive abilities,visual inspection},
}

@Article{Mery2017,
  author       = {Mery, Domingo and Svec, Erick and Arias, Marco and Riffo, Vladimir and Saavedra, Jose M. and Banerjee, Sandipan},
  year         = {2017},
  journal       = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  title        = {{Modern computer vision techniques for X-Ray testing in baggage inspection}},
  doi          = {10.1109/TSMC.2016.2628381},
  issn         = {21682232},
  number       = {4},
  pages        = {682--692},
  volume       = {47},
  abstract     = {X-ray screening systems have been used to safeguard environments in which access control is of paramount importance. Security checkpoints have been placed at the entrances to many public places to detect prohibited items, such as handguns and explosives. Generally, human operators are in charge of these tasks as automated recognition in baggage inspection is still far from perfect. Research and development on X-ray testing is, however, exploring new approaches based on computer vision that can be used to aid human operators. This paper attempts to make a contribution to the field of object recognition in X-ray testing by evaluating different computer vision strategies that have been proposed in the last years. We tested ten approaches. They are based on bag of words, sparse representations, deep learning, and classic pattern recognition schemes among others. For each method, we: 1) present a brief explanation; 2) show experimental results on the same database; and 3) provide concluding remarks discussing pros and cons of each method. In order to make fair comparisons, we define a common experimental protocol based on training, validation, and testing data (selected from the public GDXray database). The effectiveness of each method was tested in the recognition of three different threat objects: 1) handguns; 2) shuriken (ninja stars); and 3) razor blades. In our experiments, the highest recognition rate was achieved by methods based on visual vocabularies and deep features with more than 95\% of accuracy. We strongly believe that it is possible to design an automated aid for the human inspection task using these computer vision algorithms.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mery et al. - 2017 - Modern computer vision techniques for X-Ray testing in baggage inspection.pdf:pdf},
  keywords     = {Baggage screening,X-ray testing,deep learning,implicit shape model (ISM),object categorization,object detection,object recognition,sparse representations,threat objects},
}

@Article{ZhengyouZhang1998,
  author       = {{Zhengyou Zhang}},
  year         = {1998},
  journal       = {Technical Report MSR TR-98-71 Microsoft},
  title        = {{A Flexible New Technique for Camera Calibration Zhengyou}},
  number       = {last updated on Aug. 13, 2008},
  abstract     = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthog- onal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhengyou Zhang - 1998 - A Flexible New Technique for Camera Calibration Zhengyou.pdf:pdf},
}

@Article{Brown1966,
  author       = {Brown, Duane},
  year         = {1966},
  journal       = {Photometric Engineering},
  title        = {{Decentering Distortion of Lenses - The Prism Effect Encountered in Metric Cameras can be Overcome Through Analytic Calibration}},
  number       = {3},
  pages        = {444--462},
  volume       = {32},
  abstract     = {The thin prism model has been widely adopted in the photogram- metric literature to describe the effects of a sensibly decentered lens. Exact ex- pressions are derived for the radial and tangential components of the disto'rtion introduced by a thin prism placed in front of a perfectly centered lens. This model is compared with an alternative model (Conrady, 1919) based on rigorous analytical ray tracing through a decentered lens. When the principal point of autocollimation is adopted as the plate origin, the two models are found to be in precise agreement regarding the tangential component of decentering distortion, but are found to be at variance by a factor of three regarding the radial compo- nent. However, when compensatory translation of the plate and tipping of the camera are permitted to operate, the two models are found to be projectively equivalent to terms of leading order. Because this projective equivalence does not extend to higher order effects (which may assume prominence with wide angle cameras), Conrady's model is clearly to be preferred for general application. A n extended form of Conrady's model has been put to practical application in the stellar calibration of ballistic cameras. Results of ltctlutl calibrations are presented and discussed, and the implications of the present development to analytical photogrammetry are examined.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown - 1966 - Decentering Distortion of Lenses - The Prism Effect Encountered in Metric Cameras can be Overcome Through Analytic Calibr.pdf:pdf},
  keywords     = {May 1966,No. 3,Photogrammetric Engineering,Vol. 32,pp 444-462},
}

@Article{Kjelldahl1995,
  author       = {Kjelldahl, Lars and Prime, Martin},
  year         = {1995},
  journal       = {Computers and Graphics},
  title        = {{A study on how depth perception is affected by different presentation methods of 3D objects on a 2D display}},
  doi          = {10.1016/0097-8493(94)00143-M},
  issn         = {00978493},
  number       = {2},
  pages        = {199--202},
  volume       = {19},
  abstract     = {The experimental results of viewing 3D objects on a 2D display are presented. The effect of object relationship; presentation quality (wireframe and shaded); and illumination on depth perception were studied. After the depth estimation phase, subjects were then asked to make a subjective judgement: whether they thought better estimates had been made with the shaded rather than wireframe form. Subsequent analysis, showed that the experimental figures did not support this guess. Lighting influenced accuracy, which illustrates the importance of careful choices of light sources. Relative object placement was also established as an important factor. {\textcopyright} 1995.},
  annotation   = {This paper notes how the 3D objects are no more or less easier to tell the distance of than regular objects.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kjelldahl, Prime - 1995 - A study on how depth perception is affected by different presentation methods of 3D objects on a 2D display.pdf:pdf},
}

@Article{Naceri2011,
  author       = {Naceri, Abdeldjallil and Chellali, Ryad and Hoinville, Thierry},
  year         = {2011},
  journal       = {Presence: Teleoperators and Virtual Environments},
  title        = {{Depth perception within peripersonal space using head-mounted display}},
  doi          = {10.1162/PRES_a_00048},
  issn         = {10547460},
  number       = {3},
  pages        = {254--272},
  volume       = {20},
  abstract     = {In this paper, we address depth perception in the peripersonal space within three virtual environments: poor environment (dark room), reduced cues environment (wireframe room), and rich cues environment (a lit textured room). Observers binocularly viewed virtual scenes through a head-mounted display and evaluated the egocentric distance to spheres using visually open-loop pointing tasks. We conducted two different experiments within all three virtual environments. The apparent size of the sphere was held constant in the first experiment and covaried with distance in the second one. The results of the first experiment revealed that observers more accurately estimated depth in the rich virtual environment compared to the visually poor and the wireframe environments. Specifically, observers' pointing errors were small in distances up to 55 cm, and increased with distance once the sphere was further than 55 cm. Individual differences were found in the second experiment. Our results suggest that the quality of virtual environments has an impact on distance estimation within reaching space. Also, manipulating the targets' size cue led to individual differences in depth judgments. Finally, our findings confirm the use of vergence as an absolute distance cue in virtual environments within the arm's reaching space. {\textcopyright} 2011 by the Massachusetts Institute of Technology.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Naceri, Chellali, Hoinville - 2011 - Depth perception within peripersonal space using head-mounted display.pdf:pdf},
}

@Article{Yeo2015,
  author       = {Yeo, Hui Shyong and Lee, Byung Gook and Lim, Hyotaek},
  year         = {2015},
  journal       = {Multimedia Tools and Applications},
  title        = {{Hand tracking and gesture recognition system for human-computer interaction using low-cost hardware}},
  doi          = {10.1007/s11042-013-1501-1},
  issn         = {15737721},
  number       = {8},
  pages        = {2687--2715},
  volume       = {74},
  abstract     = {Human-Computer Interaction (HCI) exists ubiquitously in our daily lives. It is usually achieved by using a physical controller such as a mouse, keyboard or touch screen. It hinders Natural User Interface (NUI) as there is a strong barrier between the user and computer. There are various hand tracking systems available on the market, but they are complex and expensive. In this paper, we present the design and development of a robust marker-less hand/finger tracking and gesture recognition system using low-cost hardware. We propose a simple but efficient method that allows robust and fast hand tracking despite complex background and motion blur. Our system is able to translate the detected hands or gestures into different functional inputs and interfaces with other applications via several methods. It enables intuitive HCI and interactive motion gaming. We also developed sample applications that can utilize the inputs from the hand tracking system. Our results show that an intuitive HCI and motion gaming system can be achieved with minimum hardware requirements.},
  annotation   = {This contains a very easy to follow implemention of handtracking for low cost devices.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeo, Lee, Lim - 2015 - Hand tracking and gesture recognition system for human-computer interaction using low-cost hardware.pdf:pdf},
  keywords     = {Gesture recognition,HCI,Hand/Finger tracking,Kinect,Motion game,NUI},
}

@Article{Webster1996,
  author       = {Webster, Anthony and Feiner, Steven and MacIntyre, Blair and Massie, William and Krueger, Theodore},
  year         = {1996},
  journal       = {Computing in Civil Engineering (New York)},
  title        = {{Augmented reality in architectural construction, inspection, and renovation}},
  number       = {September 2000},
  pages        = {913--919},
  abstract     = {We present our preliminary work in developing augmented reality systems to improve methods for the construction, inspection, and renovation of architectural structures. Augmented reality systems add virtual computer-generated material to the surrounding physical world. Our augmented reality systems use see-through head-worn displays to overlay graphics and sounds on a person's naturally occurring vision and hearing. As the person moves about, the position and orientation of his or her head is tracked, allowing the overlaid material to remain tied to the physical world. We describe an experimental augmented reality system that shows the location of columns behind a finished wall, the location of re-bars inside one of the columns, and a structural analysis of the column. We also discuss our preliminary work in developing an augmented reality system for improving the construction of spaceframes. Potential uses of more advanced augmented reality systems are presented.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Webster et al. - 1996 - Augmented reality in architectural construction, inspection, and renovation(2).pdf:pdf},
}

@Article{Feiner1997,
  author       = {Feiner, Steven and Maclntyre, Blair and H{\"{o}}llerer, Tobias and Webster, Anthony},
  year         = {1997},
  journal       = {Personal Technologies},
  title        = {{A Touring Hachine: Prototgping 3D Hobite Augmented Reatitg Sgstems for Exptoring the Urban Environment}},
  pages        = {208--217},
  abstract     = {We describe a prototype system that combines the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university's campus, using a head-tracked, see-through, head-worn, 3D display, and an untracked, opaque, hand-held, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feiner et al. - 1997 - A Touring Hachine Prototgping 3D Hobite Augmented Reatitg Sgstems for Exptoring the Urban Environment.pdf:pdf},
}

@Article{Feiner1995,
  author       = {Feiner, Steven K and Webster, Anthony C and Krueger, Theodore E and MacIntyre, Blair and Keller, Edward J},
  year         = {1995-01},
  journal       = {Presence: Teleoper. Virtual Environ.},
  title        = {{Architectural Anatomy}},
  doi          = {10.1162/pres.1995.4.3.318},
  issn         = {1054-7460},
  number       = {3},
  pages        = {318--325},
  url          = {https://doi.org/10.1162/pres.1995.4.3.318},
  volume       = {4},
  abstract     = {We provide an overview of the early stages of three related research projects whose goals are to exploit augmented reality, virtual worlds, and artificial intelligence to explore relationships between perceived architectural space and the structural systems that support it. In one project, we use a see-through head-mounted display to overlay a graphic representation of a building's structural systems on the user's view of a room within the building. This overlaid virtual world shows the out-lines of the concrete joists, beams, and columns surrounding the room, as well as the reinforcing steel inside them, and includes displays from a commercially available structural analysis program. In a related project, the structural view is exposed by varying the opacity of room finishes and concrete in a 3D model of the room and surrounding structure rendered on a conventional CRT. We also describe a hypermedia database, currently under construction, depicting major, twentieth-century American buildings. The interactive, multidisciplinary elements of the database-including structural and thermal analyses, free body diagrams which show how forces are resisted by portions of a structure under various loading conditions, facsimiles of construction documents, and critical essays-are bound together and made available over the World-Wide Web. Finally, we discuss the relationships among all these projects, and their potential applications to teaching architecture students and to construction, assembly, and repair of complex structures.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feiner et al. - 1995 - Architectural Anatomy(2).pdf:pdf},
  location     = {Cambridge, MA, USA},
  publisher    = {MIT Press},
}

@Article{Achibet2014,
  author       = {Achibet, Merwan and Marchal, Maud and Argelaguet, Ferran and Lecuyer, Anatole},
  year         = {2014},
  journal       = {IEEE Symposium on 3D User Interfaces 2014, 3DUI 2014 - Proceedings},
  title        = {{The Virtual Mitten: A novel interaction paradigm for visuo-haptic manipulation of objects using grip force}},
  doi          = {10.1109/3DUI.2014.6798843},
  pages        = {59--66},
  abstract     = {In this paper, we propose a novel visuo-haptic interaction paradigm called the 'Virtual Mitten' for simulating the 3D manipulation of objects. Our approach introduces an elastic handheld device that provides a passive haptic feedback through the fingers and a mitten interaction metaphor that enables to grasp and manipulate objects. The grasping performed by the mitten is directly correlated with the grip force applied on the elastic device and a supplementary pseudo-haptic feedback modulates the visual feedback of the interaction in order to simulate different haptic perceptions. The Virtual Mitten allows natural interaction and grants users with an extended freedom of movement compared with rigid devices with limited workspaces. Our approach has been evaluated within two experiments focusing both on subjective appreciation and perception. Our results show that participants were able to well perceive different levels of effort during basic manipulation tasks thanks to our pseudo-haptic approach. They could also rapidly appreciate how to achieve different actions with the Virtual Mitten such as opening a drawer or pulling a lever. Taken together, our results suggest that our novel interaction paradigm could be used in a wide range of applications involving one or two-hand haptic manipulation such as virtual prototyping, virtual training or video games. {\textcopyright} 2014 IEEE.},
  annotation   = {details how a machine use for haptics but not pusdo},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Achibet et al. - 2014 - The Virtual Mitten A novel interaction paradigm for visuo-haptic manipulation of objects using grip force.pdf:pdf},
  isbn         = {9781479936243},
  publisher    = {IEEE},
}

@Article{Michels2020,
  author = {Michels, Dominik L},
  year   = {2020},
  title  = {{Surface-Only Ferrofluids}},
  number = {6},
  volume = {39},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michels - 2020 - Surface-Only Ferrofluids.pdf:pdf},
}

@Article{Paljic2004,
  author       = {Paljic, Alexis and Burkhardt, Jean Marie and Coquillart, Sabine},
  year         = {2004},
  journal       = {Proceedings - 12th International Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems, HAPTICS},
  title        = {{Evaluation of pseudo-haptic feedback for simulating torque: A comparison between isometric and elastic input devices}},
  doi          = {10.1109/HAPTIC.2004.1287199},
  pages        = {216--223},
  abstract     = {In this work, we investigate whether pseudo-haptic feedback is suitable for simulating torque feedback. Pseudo-haptic feedback is based on the coupling of visual feedback and the internal resistance of an input device which passively reacts to the user's applied force. An experiment was conducted to evaluate this feedback and compare isometric and elastic input devices. It involved compliance discrimination between real torsion springs and pseudo-haptic simulated torsion springs. Results show that torque haptic feedback was successfully simulated, with a difference in performance between device types. The elastic device yielded better resolution but higher subjective distortion of perception compared to the isometric device. Results are discussed on the basis of user answers, answer time, and applied torque.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paljic, Burkhardt, Coquillart - 2004 - Evaluation of pseudo-haptic feedback for simulating torque A comparison between isometric and ela.pdf:pdf},
  isbn         = {0769521126},
}

@Article{Pusch2009,
  author       = {Pusch, Andreas and Martin, Olivier and Coquillart, Sabine},
  year         = {2009},
  journal       = {International Journal of Human Computer Studies},
  title        = {{HEMP-hand-displacement-based pseudo-haptics: A study of a force field application and a behavioural analysis}},
  doi          = {10.1016/j.ijhcs.2008.09.015},
  issn         = {10715819},
  number       = {3},
  pages        = {256--268},
  volume       = {67},
  abstract     = {This paper introduces a novel pseudo-haptic approach called HEMP-Hand-displacEMent-based Pseudo-haptics. The main idea behind HEMP is to provide haptic-like sensations by dynamically displacing the visual representation of the user's hand. This paper studies the possible application of HEMP to the simulation of force fields (FFs). The proposed hardware solution for simulating the hand displacement is based on an augmented reality configuration, the video see-through head-mounted display. A response model is proposed for controlling the hand displacement. This model adapts to the user's hand movements. It also accounts for a number of perceptual and system constraints. An experiment has been carried out to investigate the potential of the proposed technique. Subjects had to perform an FF strength comparison task and to fill in an illusion evaluation questionnaire. Comparison response results show that different FF strength levels are discriminable and the questionnaire indicates that subjects perceive flow pressure-like sensations. The analysis of arm muscular activity seems to confirm these results. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pusch, Martin, Coquillart - 2009 - HEMP-hand-displacement-based pseudo-haptics A study of a force field application and a behavioural an.pdf:pdf},
  keywords     = {Force illusion,Pseudo-haptics,Visuo-proprioceptive conflict},
}

@Article{Hettinga2018,
  author       = {Hettinga, G. J. and Barendrecht, P. J. and Kosinka, J.},
  year         = {2018},
  journal       = {European Association for Computer Graphics - 39th Annual Conference, EUROGRAPHICS 2018 - Short Papers},
  title        = {{A comparison of GPU tessellation strategies for multisided patches}},
  doi          = {10.2312/egs.20181041},
  number       = {April},
  pages        = {45--48},
  abstract     = {We propose augmenting the traditional tessellation pipeline with several different strategies that efficiently render multisided patches using generalized barycentric coordinates. The strategies involve different subdivision steps and the usage of textures. In addition, we show that adaptive tessellation techniques naturally extend to some of these strategies, whereas others need a slight adjustment. The technique of Loop et al. [LSNC09], commonly known as ACC-2, is extended to multisided faces to illustrate the effectiveness of multisided techniques. A performance and quality comparison is made between the different strategies and remarks on the techniques and implementation details are provided.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hettinga, Barendrecht, Kosinka - 2018 - A comparison of GPU tessellation strategies for multisided patches(2).pdf:pdf},
  keywords     = {Computer Graphics Forum, EUROGRAPHICS},
}

@Article{Achibet2017,
  author       = {Achibet, Merwan and {Le Gouis}, Beno{\^{i}}t and Marchal, Maud and L{\'{e}}ziart, Pierre Alexandre and Argelaguet, Ferran and Girard, Adrien and L{\'{e}}cuyer, Anatole and Kajimoto, Hiroyuki},
  year         = {2017},
  journal       = {2017 IEEE Symposium on 3D User Interfaces, 3DUI 2017 - Proceedings},
  title        = {{FlexiFingers: Multi-finger interaction in VR combining passive haptics and pseudo-haptics}},
  doi          = {10.1109/3DUI.2017.7893325},
  pages        = {103--106},
  abstract     = {3D interaction in virtual reality often requires to manipulate and feel virtual objects with our fingers. Although existing haptic interfaces can be used for this purpose (e.g. force-feedback exoskeleton gloves), they are still bulky and expensive. This paper introduces a novel multi-finger device called 'FlexiFingers' that constrains each digit individually and produces elastic force feedback. FlexiFingers leverages passive haptics to offer a lightweight, modular, and affordable alternative to active devices. Moreover, we combine Flexifingers with a pseudo-haptic approach that simulates different stiffness levels when interacting with virtual objects. We illustrate how this combination of passive haptics and pseudo-haptics can benefit multi-finger interaction through several use cases related to music learning and medical training. Those examples suggest that our approach could find applications in various domains that require an accessible and portable way of providing haptic feedback to the fingers.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Achibet et al. - 2017 - FlexiFingers Multi-finger interaction in VR combining passive haptics and pseudo-haptics.pdf:pdf},
  isbn         = {9781509067169},
  keywords     = {H.5.2 [Information Interfaces and Presentation]: U},
}

@Article{Samad2019,
  author   = {Samad, Majed and Gatti, Elia and Hermes, Anne and Benko, Hrvoje and Parise, Cesare},
  year     = {2019},
  title    = {{Pseudo-Haptic Weight}},
  doi      = {10.1145/3290605.3300550},
  pages    = {1--13},
  abstract = {Figure 1: An illustration of the Control/Display Ratio manipulation utilized in the experiment. The user is shown lifting an optically tracked cube with the corresponding virtual representation, the movements of which are a fraction of the user's movements. ABSTRACT In virtual reality, the lack of kinesthetic feedback often prevents users from experiencing the weight of virtual objects. Control-to-display (C/D) ratio manipulation has been proposed as a method to induce weight perception without kinesthetic feedback. Based on the fact that lighter (heav-ier) objects are easier (harder) to move, this method induces an illusory perception of weight by manipulating the rendered position of users' hands-increasing or decreasing their displayed movements. In a series of experiments we Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). demonstrate that C/D-ratio induces a genuine perception of weight, while preserving ownership over the virtual hand. This means that such a manipulation can be easily introduced in current VR experiences without disrupting the sense of presence. We discuss these fndings in terms of estimation of physical work needed to lift an object. Our fndings provide the frst quantifcation of the range of C/D-ratio that can be used to simulate weight in virtual reality. CCS CONCEPTS • Human-centered computing → Mixed / augmented reality ; Virtual reality; Haptic devices; HCI theory, concepts and models; User interface programming.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Samad et al. - 2019 - Pseudo-Haptic Weight.pdf:pdf},
  isbn     = {9781450359702},
  keywords = {acm reference format,and cesare,anne hermes,elia gatti,hrvoje benko,majed samad,multisensory integration,pseudo-haptics,virtual weight},
}

@Article{Burns2006,
  author       = {Burns, Eric and Razzaque, Sharif and Panter, Abigail T. and Whitton, Mary C. and McCallus, Matthew R. and Brooks, Frederick P.},
  year         = {2006},
  journal       = {Presence: Teleoperators and Virtual Environments},
  title        = {{The hand is more easily fooled than the eye: Users are more sensitive to visual interpenetration than to visual-proprioceptive discrepancy}},
  doi          = {10.1162/pres.2006.15.1.1},
  issn         = {15313263},
  number       = {1},
  pages        = {1--15},
  volume       = {15},
  abstract     = {A virtual environment (VE) user's avatar may penetrate virtual objects. Some VE designers prevent visual interpenetration, assuming that prevention improves user experience. However, preventing visual avatar interpenetration causes a discrepancy between visual and proprioceptive cues. We investigated users' detection thresholds for visual interpenetration and visual- proprioceptive discrepancy and found that users are much less sensitive to visual-proprioceptive discrepancy than to visual interpenetration. We propose using this result to better deal with user penetration of virtual objects. {\textcopyright} 2006 by the Massachusetts Institute of Technology.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burns et al. - 2006 - The hand is more easily fooled than the eye Users are more sensitive to visual interpenetration than to visual-pro.pdf:pdf},
}

@Article{Bozzacchi2014,
  author       = {Bozzacchi, Chiara and Volcic, Robert and Domini, Fulvio},
  year         = {2014},
  journal       = {Journal of Neurophysiology},
  title        = {{Effect of visual and haptic feedback on grasping movements}},
  doi          = {10.1152/jn.00439.2014},
  issn         = {15221598},
  number       = {12},
  pages        = {3189--3196},
  volume       = {112},
  abstract     = {Perceptual estimates of three-dimensional (3D) properties, such as the distance and depth of an object, are often inaccurate. Given the accuracy and ease with which we pick up objects, it may be expected that perceptual distortions do not affect how the brain processes 3D information for reach-to-grasp movements. Nonetheless, empirical results show that grasping accuracy is reduced when visual feedback of the hand is removed. Here we studied whether specific types of training could correct grasping behavior to perform adequately even when any form of feedback is absent. Using a block design paradigm, we recorded the movement kinematics of subjects grasping virtual objects located at different distances in the absence of visual feedback of the hand and haptic feedback of the object, before and after different training blocks with different feedback combinations (vision of the thumb and vision of thumb and index finger, with and without tactile feedback of the object). In the Pretraining block, we found systematic biases of the terminal hand position, the final grip aperture, and the maximum grip aperture like those reported in perceptual tasks. Importantly, the distance at which the object was presented modulated all these biases. In the Posttraining blocks only the hand position was partially adjusted, but final and maximum grip apertures remained unchanged. These findings show that when visual and haptic feedback are absent systematic distortions of 3D estimates affect reach-to-grasp movements in the same way as they affect perceptual estimates. Most importantly, accuracy cannot be learned, even after extensive training with feedback.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bozzacchi, Volcic, Domini - 2014 - Effect of visual and haptic feedback on grasping movements.pdf:pdf},
  keywords     = {Calibration,Feedback,Grasping,Perceptual biases,Visuomotor learning},
  pmid         = {25231616},
}

@Article{Biocca2002,
  author       = {Biocca, Frank and Inoue, Y. and Lee, A. and Polinsky, H. and Tang, A.},
  year         = {2002},
  journal       = {Presence 2002},
  title        = {{Visual cues and virtual touch: Role of visual stimuli and intersensory integration in cross-modal haptic illusions and the sense of presence.}},
  issn         = {00963003},
  number       = {December},
  pages        = {376--394},
  abstract     = {This study examined the hypotheses that positive mood enhances conformity and that negative mood reduces it. Participants were induced to feel positive, neutral, or negative moods and then answered, in private, six mathematical questions. They observed that wrong answers were unanimously given by five bogus participants for three of the questions. Conformity was measured by whether they indicated the erroneous answers given by these bogus participants in these three questions. The results were supportive of the hypothesis. The current results are consistent with past findings about mood differences in heuristic versus elaborative processing.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Biocca et al. - 2002 - Visual cues and virtual touch Role of visual stimuli and intersensory integration in cross-modal haptic illusions.pdf:pdf},
  keywords     = {cross-modal illusion,human-computer interaction,intersensory integration},
}

@Article{Seno2013,
  author       = {Seno, Takeharu and Palmisano, Stephen and Ito, Hiroyuki and Sunaga, Shoji},
  year         = {2013},
  journal       = {Aviation Space and Environmental Medicine},
  title        = {{Perceived gravitoinertial force during vection}},
  doi          = {10.3357/ASEM.3601.2013},
  issn         = {00956562},
  number       = {9},
  pages        = {971--974},
  volume       = {84},
  abstract     = {Background: When we ride on a roller coaster, our experience of selfmotion is accompanied by salient changes in gravitoinertial force. Here we examined whether a similar relationship exists between visually induced self-motion (vection) and perceived gravitoinertial force. Methods: There were 15 stationary subjects, each wearing a weight jacket, who were exposed to visual displays simulating upward, backward, or no self-motion. At the end of each 30-s display exposure, subjects: 1) rated the strength of their vection experience; and 2) had the experimenter add/remove weights from their weight jackets to recreate their perceived weight during exposure to the stimulus display. Results: We found that upward vection increased and downward vection decreased perceived weight. Importantly, the size of these perceived weight changes depended on the strength of the vection experience. Conclusions: We conclude that the observed strong relationship between vection and perceived weight stems from the brain's attempt to reconcile the inputs from the different self-motion senses. The current findings have important implications for all simulated self-motions either in virtual reality or in vehicle simulators (particularly fixed-base flight and driving simulators). {\textcopyright} by the Aerospace Medical Association.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seno et al. - 2013 - Perceived gravitoinertial force during vection.pdf:pdf},
  keywords     = {Gravity,Optic flow,Self-motion,Vection},
  pmid         = {24024309},
}

@Article{Kimura2012,
  author   = {Kimura, Takashi and Nojima, Takuya},
  year     = {2012},
  title    = {{Pseudo-haptic Feedback on Softness Induced}},
  pages    = {202--205},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kimura, Nojima - 2012 - Pseudo-haptic Feedback on Softness Induced.pdf:pdf},
  keywords = {hand squeeze,pseudo-haptic feedback,softness},
}

@Article{Kim2020,
  author       = {Kim, Mingyu and Kim, Jinmo and Jeong, Kisung and Kim, Changhun},
  year         = {2020},
  journal       = {International Journal of Human-Computer Interaction},
  title        = {{Grasping VR: Presence of Pseudo-Haptic Interface Based Portable Hand Grip System in Immersive Virtual Reality}},
  doi          = {10.1080/10447318.2019.1680920},
  issn         = {15327590},
  number       = {7},
  pages        = {685--698},
  url          = {https://doi.org/10.1080/10447318.2019.1680920},
  volume       = {36},
  abstract     = {This study proposes a pseudo-haptic interface optimized for a novel portable hand grip haptic system, providing simple and realistic interfaces in various situations using the hand grips of users in immersive virtual reality. The proposed haptic system accurately determines the force of the user grip through an Arduino-based system design using portable low-cost electromyography (EMG) sensors. Using this system, a visual illusion-based pseudo-haptic interface is designed to maximize presence in virtual reality. This involves processes that use a grasping illusion to enable the user to quickly become familiar with the system when tactile sensation is excluded, which maximizes immersion. Based on this process, an immersive virtual reality application is produced to survey and analyze user presence for the provided hand grip. Finally, the positive experience and improved presence for hand grip in immersive virtual reality are statistically analyzed and verified through user surveys.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2020 - Grasping VR Presence of Pseudo-Haptic Interface Based Portable Hand Grip System in Immersive Virtual Reality.pdf:pdf},
  publisher    = {Taylor {\&} Francis},
}

@Article{Grimm2014,
  author     = {Grimm, Cindy},
  year       = {2014},
  title      = {{The real effect of warm-cool colors Department of Computer Science \& Engineering The Real Effect of Warm-Cool Colors Authors : Bailey , Reynold J .; Grimm , Cindy M .; Davoli , Chris Corresponding Author : rjb1@cse.wustl.edu Department of Computer Science}},
  number     = {May},
  annotation = {This paper notes that futher away colors seem closer to the user than cool colors},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grimm - 2014 - The real effect of warm-cool colors Department of Computer Science \& Engineering The Real Effect of Warm-Cool Colors Auth.pdf:pdf},
}

@Article{Rodriguez2019,
  author       = {Rodr{\'{i}}guez, Jos{\'{e}} Luis and Vel{\'{a}}zquez, Ramiro and Del-Valle-soto, Carolina and Guti{\'{e}}rrez, Sebasti{\'{a}}n and Varona, Jorge and Enr{\'{i}}quez-Zarate, Josu{\'{e}}},
  year         = {2019},
  journal       = {Electronics (Switzerland)},
  title        = {{Active and passive haptic perception of shape: Passive haptics can support navigation}},
  doi          = {10.3390/electronics8030355},
  issn         = {20799292},
  number       = {3},
  pages        = {1--12},
  volume       = {8},
  abstract     = {Real-time haptic interactions occur under two exploration modes: active and passive. In this paper, we present a series of experiments that evaluate the main perceptual characteristics of both exploration modes. In particular, we focus on haptic shape recognition as it represents a fundamental task in many applications using haptic environments. The results of four experiments conducted with a group of 10 voluntary subjects show that the differences in motor activity between active and passive haptics ease the perception of surfaces for the first case and the perception of pathways for the latter. In addition, the guidance nature of passive haptics makes the pathway direction easy to recognize. This work shows that this last observation could find application in more challenging tasks such as navigation in space.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{i}}guez et al. - 2019 - Active and passive haptic perception of shape Passive haptics can support navigation.pdf:pdf},
  keywords     = {Active haptics,Haptic exploration,Kinesthetic feedback,Navigation assistance,Passive haptics,Pathway perception,Shape perception},
}

@Article{Do2020,
  author       = {Do, Tiffany D and Jr, Joseph J Laviola and Mcmahan, Ryan P},
  year         = {2020},
  journal       = {IEEE International Symposium on Mixed and Augmented Reality},
  title        = {{The Effects of Object Shape, Fidelity, Color, and Luminance on Depth Perception in Handheld Mobile Augmented Reality}},
  abstract     = {Depth perception of objects can greatly affect a user's experience of an augmented reality (AR) application. Many AR applications require depth matching of real and virtual objects and have the possibility to be influenced by depth cues. Color and luminance are depth cues that have been traditionally studied in two-dimensional (2D) objects. However, there is little research investigating how the properties of three-dimensional (3D) virtual objects interact with color and luminance to affect depth perception, despite the substantial use of 3D objects in visual applications. In this paper, we present the results of a paired comparison experiment that investigates the effects of object shape, fidelity, color, and luminance on depth perception of 3D objects in handheld mobile AR. The results of our study indicate that bright colors are perceived as nearer than dark colors for a high-fidelity, simple 3D object, regardless of hue. Additionally, bright red is perceived as nearer than any other color. These effects were not observed for a low-fidelity version of the simple object or for a more-complex 3D object. High-fidelity objects had more perceptual differences than low-fidelity objects, indicating that fidelity interacts with color and luminance to affect depth perception. These findings reveal how the properties of 3D models influence the effects of color and luminance on depth perception in handheld mobile AR and can help developers select colors for their applications.},
  annotation   = {This paper focuses on the perception of shapes wiht in Augmented reality depth perception It is a relitivly novle platform for this shape in short what it proved is that generally cubes are simple to depth with, spheres are hard to Polyiganal shapes exist in the center of this, whith higher desitity models performing worse off.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Do, Jr, Mcmahan - 2020 - The Effects of Object Shape, Fidelity, Color, and Luminance on Depth Perception in Handheld Mobile Augmented Re.pdf:pdf},
  keywords     = {depth perception,handheld mobile augmented reality,hci,hci design and evaluation,human computer,human-centered computing,index terms,interaction,methods,user},
}

@Article{Eck2015,
  author       = {Eck, Ulrich and Pankratz, Frieder and Sandor, Christian and Klinker, Gudrun and Laga, Hamid},
  year         = {2015},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality}},
  doi          = {10.1109/TVCG.2015.2480087},
  issn         = {10772626},
  number       = {12},
  pages        = {1427--1441},
  volume       = {21},
  abstract     = {Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. PHANToM haptic devices are often employed to provide haptic feedback. Precise co-location of computer-generated graphics and the haptic stylus is necessary to provide a realistic user experience. Previous work has focused on calibration procedures that compensate the non-linear position error caused by inaccuracies in the joint angle sensors. In this article we present a more complete procedure that additionally compensates for errors in the gimbal sensors and improves position calibration. The proposed procedure further includes software-based temporal alignment of sensor data and a method for the estimation of a reference for position calibration, resulting in increased robustness against haptic device initialization and external tracker noise. We designed our procedure to require minimal user input to maximize usability. We conducted an extensive evaluation with two different PHANToMs, two different optical trackers, and a mechanical tracker. Compared to state-of-the-art calibration procedures, our approach significantly improves the co-location of the haptic stylus. This results in higher fidelity visual and haptic augmentations, which are crucial for fine-motor tasks in areas such as medical training simulators, assembly planning tools, or rapid prototyping applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eck et al. - 2015 - Precise Haptic Device Co-Location for Visuo-Haptic Augmented Reality.pdf:pdf},
  keywords     = {Calibration,Haptic interfaces,Phantoms,Sensors,Target tracking,Transforms,Virtual reality},
  pmid         = {26394430},
}

@InProceedings{Bork2017a,
  author    = {Bork, F and Barmaki, R and Eck, U and Yu, K and Sandor, C and Navab, N},
  booktitle = {2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2017-10},
  title     = {{Empirical Study of Non-Reversing Magic Mirrors for Augmented Reality Anatomy Learning}},
  doi       = {10.1109/ISMAR.2017.33},
  pages     = {169--176},
  abstract  = {Left-right confusion occurs across the entire population and refers to an impeded ability to distinguish between left and right. In medicine this phenomenon is particularly relevant as left and right are always defined with respect to the patient's point of view, i.e. the doctor's right is the patient's left. Traditional anatomy learning resources such as illustrations in textbooks naturally consider this by consistently depicting the anatomy of a patient as seen by an observer standing in front. Augmented Reality Magic Mirrors (MM) are one example of novel anatomy teaching resources and show a user's digital mirror image augmented with virtual anatomy on a large display. As left and right appear to be reversed in such MM setups, similar to real-world physical mirrors, intriguing perceptual questions arise: is a non-reversing MM (NRMM) the more natural choice for the task of anatomy learning and do users even learn anatomy the wrong way with a traditional, reversing MM (RMM)? In this paper, we explore the perceptual differences between an NRMM and RMM design and present the first empirical study comparing these two concepts for the purpose of anatomy learning. Experimental results demonstrate that medical students perform significantly better at identifying anatomically correct placement of virtual organs in an NRMM. However, interaction was significantly more difficult compared to an RMM. We explore the underlying psychological effects and discuss the implications of using an NRMM on user perception, knowledge transfer, and interaction. This study is relevant for the design of future MM systems in the medical domain and lessons-learned can be transferred to other application domains.},
  keywords  = {augmented reality;biomedical education;computer ai},
}

@Article{Bork2019,
  author       = {Bork, Felix and Stratmann, Leonard and Enssle, Stefan and Eck, Ulrich and Navab, Nassir and Waschke, Jens and Kugelmann, Daniela},
  year         = {2019},
  journal       = {Anatomical Sciences Education},
  title        = {{The Benefits of an Augmented Reality Magic Mirror System for Integrated Radiology Teaching in Gross Anatomy}},
  doi          = {10.1002/ase.1864},
  issn         = {19359780},
  number       = {6},
  pages        = {585--598},
  volume       = {12},
  abstract     = {Early exposure to radiological cross-section images during introductory anatomy and dissection courses increases students' understanding of both anatomy and radiology. Novel technologies such as augmented reality (AR) offer unique advantages for an interactive and hands-on integration with the student at the center of the learning experience. In this article, the benefits of a previously proposed AR Magic Mirror system are compared to the Anatomage, a virtual dissection table as a system for combined anatomy and radiology teaching during a two-semester gross anatomy course with 749 first-year medical students, as well as a follow-up elective course with 72 students. During the former, students worked with both systems in dedicated tutorial sessions which accompanied the anatomy lectures and provided survey-based feedback. In the elective course, participants were assigned to three groups and underwent a self-directed learning session using either Anatomage, Magic Mirror, or traditional radiology atlases. A pre- and posttest design with multiple choice questions revealed significant improvements in test scores between the two tests for both the Magic Mirror and the group using radiology atlases, while no significant differences in test scores were recorded for the Anatomage group. Furthermore, especially students with low mental rotation test (MRT) scores benefited from the Magic Mirror and Anatomage and achieved significantly higher posttest scores compared to students with a low MRT score in the theory group. Overall, the results provide supporting evidence that the Magic Mirror system achieves comparable results in terms of learning outcome to established anatomy learning tools such as Anatomage and radiology atlases.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bork et al. - 2019 - The Benefits of an Augmented Reality Magic Mirror System for Integrated Radiology Teaching in Gross Anatomy.pdf:pdf},
  keywords     = {anatomy curriculum,augmented reality,clinical anatomy,gross anatomy education,novel teaching modalities,radiology education,spatial understanding,undergraduate education},
  pmid         = {30697948},
}

@Article{Bork2017,
  author       = {Bork, Felix and Barmaki, Roghayeh and Eck, Ulrich and Fallavolita, Pascal and Fuerst, Bernhard and Navab, Nassir},
  year         = {2017},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Exploring non-reversing magic mirrors for screen-based augmented reality systems}},
  doi          = {10.1109/VR.2017.7892332},
  eprint       = {1611.03354},
  eprinttype   = {arXiv},
  issn         = {2375-5334},
  pages        = {373--374},
  abstract     = {Screen-based Augmented Reality (AR) systems can be built as a window into the real world as often done in mobile AR applications or using the Magic Mirror metaphor, where users can see themselves with augmented graphics on a large display. The term Magic Mirror implies that the display shows the users enantiomorph, i.e. the mirror image, such that the system mimics a real-world physical mirror. However, the question arises whether one should design a traditional mirror, or instead display the true mirror image by means of a non-reversing mirror? We discuss the perceptual differences between these two mirror visualization concepts and present a first comparative study in the context of Magic Mirror anatomy teaching.},
  arxivid      = {1611.03354},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bork et al. - 2017 - Exploring non-reversing magic mirrors for screen-based augmented reality systems.pdf:pdf},
  isbn         = {9781509066476},
  keywords     = {Augmented,H.5.1 [Information Interfaces and Presentation]: M,H.5.2 [Information interfaces and presentation]: U,Virtual realities},
}

@Article{Vi2017,
  author       = {Vi, Chi Thanh and Ablart, Damien and Gatti, Elia and Velasco, Carlos and Obrist, Marianna},
  year         = {2017},
  journal       = {International Journal of Human Computer Studies},
  title        = {{Not just seeing, but also feeling art: Mid-air haptic experiences integrated in a multisensory art exhibition}},
  doi          = {10.1016/j.ijhcs.2017.06.004},
  issn         = {10959300},
  number       = {May 2016},
  pages        = {1--14},
  volume       = {108},
  abstract     = {The use of the senses of vision and audition as interactive means has dominated the field of Human-Computer Interaction (HCI) for decades, even though nature has provided us with many more senses for perceiving and interacting with the world around us. That said, it has become attractive for HCI researchers and designers to harness touch, taste, and smell in interactive tasks and experience design. In this paper, we present research and design insights gained throughout an interdisciplinary collaboration on a six-week multisensory display – Tate Sensorium – exhibited at the Tate Britain art gallery in London, UK. This is a unique and first time case study on how to design art experiences whilst considering all the senses (i.e., vision, sound, touch, smell, and taste), in particular touch, which we exploited by capitalizing on a novel haptic technology, namely, mid-air haptics. We first describe the overall set up of Tate Sensorium and then move on to describing in detail the design process of the mid-air haptic feedback and its integration with sound for the Full Stop painting by John Latham (1961). This was the first time that mid-air haptic technology was used in a museum context over a prolonged period of time and integrated with sound to enhance the experience of visual art. As part of an interdisciplinary team of curators, sensory designers, sound artists, we selected a total of three variations of the mid-air haptic experience (i.e., haptic patterns), which were alternated at dedicated times throughout the six-week exhibition. We collected questionnaire-based feedback from 2500 visitors and conducted 50 interviews to gain quantitative and qualitative insights on visitors' experiences and emotional reactions. Whilst the questionnaire results are generally very positive with only a small variation of the visitors' arousal ratings across the three tactile experiences designed for the Full Stop painting, the interview data shed light on the differences in the visitors' subjective experiences. Our findings suggest multisensory designers and art curators can ensure a balance between surprising experiences versus the possibility of free exploration for visitors. In addition, participants expressed that experiencing art with the combination of mid-air haptic and sound was immersive and provided an up-lifting experience of touching without touch. We are convinced that the insights gained from this large-scale and real-world field exploration of multisensory experience design exploiting a new and emerging technology provide a solid starting point for the HCI community, creative industries, and art curators to think beyond conventional art experiences. Specifically, our work demonstrates how novel mid-air technology can make art more emotionally engaging and stimulating, especially abstract art that is often open to interpretation.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vi et al. - 2017 - Not just seeing, but also feeling art Mid-air haptic experiences integrated in a multisensory art exhibition.pdf:pdf},
  keywords     = {Art exhibition,Art gallery,Emotion,Mid-air haptic feedback,Multisensory experience,Museum,Smell,Sound,Taste,User experience},
  publisher    = {Elsevier Ltd},
}

@Article{Schneider2017,
  author       = {Schneider, Oliver and MacLean, Karon and Swindells, Colin and Booth, Kellogg},
  year         = {2017},
  journal       = {International Journal of Human Computer Studies},
  title        = {{Haptic experience design: What hapticians do and where they need help}},
  doi          = {10.1016/j.ijhcs.2017.04.004},
  issn         = {10959300},
  number       = {May},
  pages        = {5--21},
  url          = {http://dx.doi.org/10.1016/j.ijhcs.2017.04.004},
  volume       = {107},
  abstract     = {From simple vibrations to roles in complex multisensory systems, haptic technology is often a critical, expected component of user experience – one face of the rapid progression towards blended physical-digital interfaces. Haptic experience design, which is woven together with other multisensory design efforts, interfaces is now becoming part of many designers' jobs. We can expect it to present unique challenges, and yet we know almost nothing of what it looks like “in the wild” due to the field's relative youth, its technical complexity, the multisensory interactions between haptics, sight, and sound, and the difficulty of accessing practitioners in professional and proprietary environments. In this paper, we analyze interviews with six professional haptic designers to document and articulate haptic experience design by observing designers' goals and processes and finding themes at three levels of scope: the multisensory nature of haptic experiences, a map of the collaborative ecosystem, and the cultural context of haptics. Our findings are augmented by feedback obtained in a recent design workshop at an international haptics conference. We find that haptic designers follow a familiar design process, but face specific challenges when working with haptics. We capture and summarize these challenges, make concrete recommendations to conquer them, and present a vision for the future of haptic experience design.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schneider et al. - 2017 - Haptic experience design What hapticians do and where they need help.pdf:pdf},
  keywords     = {Design,Grounded theory,Haptics,Interview,User experience},
  publisher    = {Elsevier Ltd},
}

@Article{Weir2013,
  author       = {Weir, Peter and Sandor, Christian and Swoboda, Matt and Nguyen, Thanh and Eck, Ulrich and Reitmayr, Gerhard and Day, Arindam},
  year         = {2013},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Burnar: Involuntary heat sensations in augmented reality}},
  doi          = {10.1109/VR.2013.6549357},
  pages        = {43--46},
  abstract     = {Augmented Reality systems that run interactively and in real time, using high quality graphical displays and sensational cues, can create the illusion of virtual objects appearing to be real. This paper presents the design, implementation, and evaluation of BurnAR, a novel demonstration which enables users to experience the illusion of seeing their own hands burning, which we achieve by overlaying virtual flames and smoke on their hands. Surprisingly, some users reported an involuntary warming sensation of their hands. Based on these comments, we hypothesized that stimulation of multiple sensory modalities presented in this AR environment can induce an involuntary experience in an additional sensory pathway: observation of virtual flames resulting in a heat sensation. This cross-modal transfer, known as virtual synesthe-sia, is a temporary experience which affects some people who are not synesthetes and only lasts for a short time during the illusory experience. To verify our hypothesis, we conducted an exploratory study where participants experienced the BurnAR demonstration under controlled conditions. {\textcopyright} 2013 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weir et al. - 2013 - Burnar Involuntary heat sensations in augmented reality.pdf:pdf},
  isbn         = {9781467347952},
  keywords     = {H.5.1. [Information Interfaces and Presentation]:,H.l.2. [Information Systems]: Models and Principle},
}

@Article{Costes2019,
  author       = {Costes, Antoine and Argelaguet, Ferran and Danieau, Fabien and Guillotel, Philippe and L{\'{e}}cuyer, Anatole},
  year         = {2019},
  journal       = {Frontiers in ICT},
  title        = {{Touchy: A visual approach for simulating haptic effects on touchscreens}},
  doi          = {10.3389/fict.2019.00001},
  issn         = {2297198X},
  number       = {FEB},
  pages        = {1--11},
  volume       = {6},
  abstract     = {Haptic enhancement of touchscreens usually involves vibrating motors producing limited sensations or custom mechanical actuators that are difficult to disseminate. In this paper, we propose an alternative approach called "Touchy," where a symbolic cursor is introduced under the user's finger, to evoke various haptic properties through changes in its shape and motion. This novel metaphor enables to address four different perceptual dimensions, namely: hardness, friction, fine roughness, and macro roughness. Our metaphor comes with a set of seven visual effects that we compared with real texture samples within a user study conducted with 14 participants. Taken together our results show that Touchy is able to elicit clear and distinct haptic properties: stiffness, roughness, reliefs, stickiness, and slipperiness.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Costes et al. - 2019 - Touchy A visual approach for simulating haptic effects on touchscreens.pdf:pdf},
  keywords     = {Friction,Hardness,Perception,Pseudo-haptics,Roughness,Touchscreen},
}

@Article{KerstenOertel2012,
  author       = {Kersten-Oertel, Marta and Chen, Sean S.J. and Drouin, Simon and Sinclair, David S. and Collins, D. Louis},
  year         = {2012},
  journal       = {Studies in Health Technology and Informatics},
  title        = {{Augmented reality visualization for guidance in neurovascular surgery}},
  doi          = {10.3233/978-1-61499-022-2-225},
  issn         = {18798365},
  number       = {January},
  pages        = {225--229},
  volume       = {173},
  abstract     = {In neurovascular surgery, and in particular surgery for arteriovenous malformations (AVMs), the surgeon maps pre-operative images to the patient on the operating table to aid in vessel localization and resection. This type of spatial mapping is not trivial, is time consuming, and may be prone to error. Using augmented reality (AR) we can register the microscope/camera image of the patient to pre-operative data in order to help the surgeon better understand the topology and locations of vessels that lie below the visible surface of the cortex. In this work we describe a prototype system, developed using open source software and built with off-the-shelf hardware, for AR visualization for AVM neurosurgery. Furthermore, we consider two visualization techniques, colour-coding and chromadepth, to enhance the depth perception of vessels. {\textcopyright} 2012 The authors and IOS Press. All rights reserved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kersten-Oertel et al. - 2012 - Augmented reality visualization for guidance in neurovascular surgery.pdf:pdf},
  isbn         = {9781614990215},
  keywords     = {Arteriovenous malformations,Augmented reality,Image-guided surgery,Neurovascular surgery,Visualization},
  pmid         = {22356991},
}

@Article{Pusch2011,
  author       = {Pusch, Andreas and L{\'{e}}cuyer, Anatole},
  year         = {2011},
  journal       = {ICMI'11 - Proceedings of the 2011 ACM International Conference on Multimodal Interaction},
  title        = {{Pseudo-haptics: From the theoretical foundations to practical system design guidelines}},
  doi          = {10.1145/2070481.2070494},
  pages        = {57--64},
  abstract     = {Pseudo-haptics, a form of haptic illusion exploiting the brain's capabilities and limitations, has been studied for about a decade. Various interaction techniques making use of it emerged in different fields. However, important questions remain unanswered concerning the nature and the fundamentals of pseudo-haptics, the problems frequently encountered, and sophisticated means supporting the development of new systems and applications. We provide the theoretical background needed to understand the key mechanisms involved in the perception of/interaction with pseudo-haptic phenomena. We synthesise a framework resting on two theories of human perception, cognition and action: The Interacting Cognitive Subsystems model by Barnard et al. and the Bayesian multimodal cue integration framework by Ernst et al. Based on this synthesis and in order to test its utility, we discuss a recent pseudo-haptics example. Finally, we derive system design recommendations meant to facilitate the advancement in the field of pseudo-haptics for user interface researchers and practitioners. {\textcopyright} 2011 ACM.},
  annotation   = {The paper that defined the term pusdo haptics This paper also notes that you need one or more visuo haptic sensory confilt inorder to achive this goal. However the other princibles seem a bit lacking},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pusch, L{\'{e}}cuyer - 2011 - Pseudo-haptics From the theoretical foundations to practical system design guidelines.pdf:pdf},
  isbn         = {9781450306416},
  keywords     = {foundations framework,human information processing,human perception and action,pseudo-haptics,system design guidelines},
}

@Article{Furmanski2002,
  author       = {Furmanski, Chris and Azuma, Ronald and Daily, Mike},
  year         = {2002},
  journal       = {Proceedings - International Symposium on Mixed and Augmented Reality, ISMAR 2002},
  title        = {{Augmented-reality visualizations guided by cognition: Perceptual heuristics for combining visible and obscured information}},
  doi          = {10.1109/ISMAR.2002.1115091},
  pages        = {215--224},
  abstract     = {One unique feature of mixed and augmented reality (MR/AR) systems is that hidden and occluded objects an be readily visualized. We call this specialized use of MR/AR, obscured information visualization (OIV). In this paper, we describe the beginning of a research program designed to develop such visualizations through the use of principles derived from perceptual psychology and cognitive science. In this paper we surveyed the cognitive science literature as it applies to such visualization tasks, described experimental questions derived from these cognitive principles, and generated general guidelines that can be used in designing future OIV systems (as well improving AR displays more generally). We also report the results from an experiment that utilized a functioning AR-OIV system: we found that in relative depth judgment, subjects reported rendered objects as being in front of real-world objects, except when additional occlusion and motion cues were presented together.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Furmanski, Azuma, Daily - 2002 - Augmented-reality visualizations guided by cognition Perceptual heuristics for combining visible and ob.pdf:pdf},
  isbn         = {0769517811},
  keywords     = {augmented and mixed reality,cognition,human-computer interaction,motion,occlusion,perception},
  publisher    = {IEEE},
}

@Article{Coeltekin2020,
  author       = {{\c{C}}{\"{o}}ltekin, Arzu and Lochhead, Ian and Madden, Marguerite and Christophe, Sidonie and Devaux, Alexandre and Pettit, Christopher and Lock, Oliver and Shukla, Shashwat and Herman, Luk{\'{a}}{\v{s}} and Stachoň, Zden{\v{e}}k and Kub{\'{i}}{\v{c}}ek, Petr and Snopkov{\'{a}}, Dajana and Bernardes, Sergio and Hedley, Nicholas},
  year         = {2020},
  journal       = {ISPRS International Journal of Geo-Information},
  title        = {{Extended reality in spatial sciences: A review of research challenges and future directions}},
  doi          = {10.3390/ijgi9070439},
  issn         = {22209964},
  number       = {7},
  volume       = {9},
  abstract     = {This manuscript identifies and documents unsolved problems and research challenges in the extended reality (XR) domain (i.e., virtual (VR), augmented (AR), and mixed reality (MR)). The manuscript is structured to include technology, design, and human factor perspectives. The text is visualization/display-focused, that is, other modalities such as audio, haptic, smell, and touch, while important for XR, are beyond the scope of this paper. We further narrow our focus to mainly geospatial research, with necessary deviations to other domains where these technologies are widely researched. The main objective of the study is to provide an overview of broader research challenges and directions in XR, especially in spatial sciences. Aside from the research challenges identified based on a comprehensive literature review, we provide case studies with original results from our own studies in each section as examples to demonstrate the relevance of the challenges in the current research. We believe that this paper will be of relevance to anyone who has scientific interest in extended reality, and/or uses these systems in their research.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\c{C}}{\"{o}}ltekin et al. - 2020 - Extended reality in spatial sciences A review of research challenges and future directions.pdf:pdf},
  keywords     = {Augmented reality (AR),Extended reality (XR),GIScience,Mixed reality (MR),Research challenges,Virtual environments (VE),Virtual reality (VR)},
}

@article{Goodstadt,
author = {Goodstadt, Mike and Marti-renom, Marc A and Group, Structural Genomics and Regulation, Gene and Program, Cancer},
doi = {10.1111/ijlh.12426},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodstadt et al. - Unknown - Challenges for visualizing three-dimensional data in genomic browsers.pdf:pdf},
pages = {0--2},
title = {{Challenges for visualizing three-dimensional data in genomic browsers.}}
}

@Article{ElSayed2016,
  author       = {ElSayed, Neven A.M. and Thomas, Bruce H. and Marriott, Kim and Piantadosi, Julia and Smith, Ross T.},
  year         = {2016},
  journal       = {Journal of Visual Languages and Computing},
  title        = {{Situated Analytics: Demonstrating immersive analytical tools with Augmented Reality}},
  doi          = {10.1016/j.jvlc.2016.07.006},
  issn         = {1045926X},
  pages        = {13--23},
  url          = {http://dx.doi.org/10.1016/j.jvlc.2016.07.006},
  volume       = {36},
  abstract     = {This paper introduces the use of Augmented Reality as an immersive analytical tool in the physical world. We present Situated Analytics, a novel combination of real-time interaction and visualization techniques that allows exploration and analysis of information about objects in the user's physical environment. Situated Analytics presents both situated and abstract summary and contextual information to a user. We conducted a user study to evaluate its use in three shopping analytics tasks, comparing the use of a Situated Analytics prototype with manual analysis. The results showed that users preferred the Situated Analytics prototype over the manual method, and that tasks were performed more quickly and accurately using the prototype.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/ElSayed et al. - 2016 - Situated Analytics Demonstrating immersive analytical tools with Augmented Reality.pdf:pdf},
  keywords     = {Augmented Reality,Information visualization,Shopping application,Situated Analytics,Visual Analytics,Visualization},
  publisher    = {Elsevier},
}

@Article{Hertzmann2001,
  author       = {Hertzmann, A.},
  year         = {2001},
  journal       = {Proceedings of Computer Graphics International Conference, CGI},
  title        = {{Paint by relaxation}},
  doi          = {10.1109/cgi.2001.934657},
  pages        = {47--54},
  abstract     = {We use relaxation to produce painted imagery from images and video. An energy function is first specified; we then search for a painting with minimal energy. The appeal of this strategy is that, ideally, we need only specify what we want, not how to directly compute it. Because the energy function is very difficult to optimize, we use a relaxation algorithm combined with search heuristics. This formulation allows us to specify painting style by varying the relative weights of energy terms. The basic energy function yields an economical style that conveys an image with few strokes. This style produces greater temporal coherence for video than previous techniques. The system allows as fine user control as desired: the user may interactively change the painting style, specify variations of style over an image, and/or add specific strokes to the painting.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hertzmann - 2001 - Paint by relaxation.pdf:pdf},
}

@Article{Meier1996,
  author       = {Meier, Barbara J.},
  year         = {1996},
  journal       = {Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1996},
  title        = {{Painterly rendering for animation}},
  doi          = {10.1145/237170.237288},
  pages        = {477--484},
  abstract     = {We present a technique for rendering animations in a painterly style. The difficulty in using existing still frame methods for animation is getting the paint to "stick" to surfaces rather than randomly change with each frame, while still retaining a hand-crafted look. We extend the still frame method to animation by solving two major specific problems of previous techniques. First our method eliminates the "shower door" effect in which an animation appears as if it were being viewed through textured glass because brush strokes stick to the viewplane not to the animating surfaces. Second, our technique provides for frame-to-frame coherence in animations so that the resulting frames do not randomly change every frame. To maintain coherence, we model surfaces as 3d particle sets which are rendered as 2d paint brush strokes in screen spacemuch like an artist lays down brush strokes on a canvas. We use geometric and lighting properties of the surfaces to control the appearanceof brush strokes. This powerful combination of using 3d particles, surface lighting information, and rendering 2d brush strokes in screen space gives us the painterly style we desire and forces the brush strokes to stick to animating surfaces. By varying lighting and choosing brush stroke parameters we can createmany varied painterly styles. We illustrate the method with images and animated sequences and present specific technical and creative suggestions for achieving different looks.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meier - 1996 - Painterly rendering for animation.pdf:pdf},
  isbn         = {0897917464},
  keywords     = {Abstract images,Non-photorealistic rendering,Painterly rendering,Painting,Particle systems},
}

@Article{Hertzmann2001a,
  author       = {Hertzmann, Aaron and Jacobs, Charles E. and Oliver, Nuria and Curless, Brian and Salesin, David H.},
  year         = {2001},
  journal       = {Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 2001},
  title        = {{Image analogies}},
  doi          = {10.1145/383259.383295},
  pages        = {327--340},
  abstract     = {This paper describes a new framework for processing images by example, called "image analogies." The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a "filtered" version of the other, is presented as "training data" and an application phase, in which the learned filter is applied to some new target image in order to create an "analogous" filtered result. Image analogies are based on a simple multi-scale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of "image filter" effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are "texturized" with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface. {\textcopyright} 2001 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hertzmann et al. - 2001 - Image analogies.pdf:pdf},
  isbn         = {158113374X},
  keywords     = {Markov random fields,autoregression,example-based rendering,non-photorealistic rendering,texture synthesis,texture transfer,texture-by-numbers},
}

@InCollection{Livingston2013,
  author    = {Livingston, Mark A and Dey, Arindam and Sandor, Christian and Thomas, Bruce H},
  booktitle = {Human Factors in Augmented Reality Environments},
  year      = {2013},
  title     = {{Pursuit of ``X-Ray Vision'' for Augmented Reality}},
  doi       = {10.1007/978-1-4614-4205-9_4},
  editor    = {Huang, Weidong and Alem, Leila and Livingston, Mark A},
  isbn      = {978-1-4614-4205-9},
  location  = {New York, NY},
  pages     = {67--107},
  publisher = {Springer New York},
  url       = {https://doi.org/10.1007/978-1-4614-4205-9_4 https://link.springer.com/chapter/10.1007/978-1-4614-4205-9_4},
  abstract  = {The ability to visualize occluded objects or people offers tremendous potential to users of augmented reality (AR). This is especially true in mobile applications for urban environments, in which navigation and other operations are hindered by the urban infrastructure. This ``X-ray vision'' feature has intrigued and challenged AR system designers for many years, with only modest progress in demonstrating a useful and usable capability. The most obvious challenge is to the human visual system, which is being asked to interpret a very unnatural perceptual metaphor. We review the perceptual background to understand how the visual system infers depth and how these assumptions are or are not met by augmented reality displays. In response to these challenges, several visualization metaphors have been proposed; we survey these in light of the perceptual background. Because augmented reality systems are user-centered, it is important to evaluate how well these visualization metaphors enable users to perform tasks that benefit from X-ray vision. We summarize studies reported in the literature. Drawing upon these analyses, we offer suggestions for future research on this tantalizing capability.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Human Factors in Augmented Reality Environments.pdf:pdf},
}

@Article{Murgia2009,
  author       = {Murgia, Alessio and Sharkey, Paul M.},
  year         = {2009},
  journal       = {International Journal of Virtual Reality},
  title        = {{Estimation of Distances in Virtual Environments Using Size Constancy}},
  doi          = {10.20870/ijvr.2009.8.1.2714},
  issn         = {1081-1451},
  number       = {1},
  pages        = {67--74},
  volume       = {8},
  abstract     = {It is reported in the literature that distances from the observer are underestimated more in virtual environments (VEs) than in physical world conditions. On the other hand estimation of size in VEs is quite accurate and follows a size-constancy law when rich cues are present. This study investigates how estimation of distance in a CAVETM environment is affected by poor and rich cue conditions, subject experience, and environmental learning when the position of the objects is estimated using an experimental paradigm that exploits size constancy. A group of 18 healthy participants was asked to move a virtual sphere controlled using the wand joystick to the position where they thought a previously-displayed virtual cube (stimulus) had appeared. Real-size physical models of the virtual objects were also presented to the participants as a reference of real physical distance during the trials. An accurate estimation of distance implied that the participants assessed the relative size of sphere and cube correctly. The cube appeared at depths between 0.6 m and 3 m, measured along the depth direction of the CAVE. The task was carried out in two environments: a poor cue one with limited background cues, and a rich cue one with textured background surfaces. It was found that distances were underestimated in both poor and rich cue conditions, with greater underestimation in the poor cue environment. The analysis also indicated that factors such as subject experience and environmental learning were not influential. However, least square fitting of Stevens' power law indicated a high degree of accuracy during the estimation of object locations. This accuracy was higher than in other studies which were not based on a size-estimation paradigm. Thus as indirect result, this study appears to show that accuracy when estimating egocentric distances may be increased using an experimental method that provides information on the relative size of the objects used.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murgia, Sharkey - 2009 - Estimation of Distances in Virtual Environments Using Size Constancy.pdf:pdf},
}

@Article{Whitlock2018,
  author       = {Whitlock, Matt and Harnner, Ethan and Brubaker, Jed R. and Kane, Shaun and Szafir, Danielle Albers},
  year         = {2018},
  journal       = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  title        = {{Interacting with Distant Objects in Augmented Reality}},
  doi          = {10.1109/VR.2018.8446381},
  pages        = {41--48},
  abstract     = {Augmented reality (AR) applications can leverage the full space of an environment to create immersive experiences. However, most empirical studies of interaction in AR focus on interactions with objects close to the user, generally within arms reach. As objects move farther away, the efficacy and usability of different interaction modalities may change. This work explores AR interactions at a distance, measuring how applications may support fluid, efficient, and intuitive interactive experiences in room-scale augmented reality. We conducted an empirical study (N = 20) to measure trade-offs between three interaction modalities-multimodal voice, embodied freehand gesture, and handhelds devices-for selecting, rotating, and translating objects at distances ranging from 8 to 16 feet (2.4m-4.9m). Though participants performed comparably with embodied freehand gestures and handheld remotes, they perceived embodied gestures as significantly more efficient and usable than device-mediated interactions. Our findings offer considerations for designing efficient and intuitive interactions in room-scale AR applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Whitlock et al. - 2018 - Interacting with Distant Objects in Augmented Reality.pdf:pdf},
  isbn         = {9781538633656},
  keywords     = {Human-centered computing-Interaction design-Intera},
}

@article{Bhatnagar,
author = {Bhatnagar, Anuj and Ramkumar, Sreesudhan},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhatnagar, Ramkumar - Unknown - Augmented Reality - Applying noise and blur to the virtual content to make it appear as real objects.pdf:pdf},
title = {{Augmented Reality - Applying noise and blur to the virtual content to make it appear as real objects}}
}

@Article{Fischer2005,
  author       = {Fischer, Jan and Bartz, Dirk and Stra{\ss}er, Wolfgang},
  year         = {2005},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Stylized augmented reality for improved immersion}},
  doi          = {10.1109/vr.2005.71},
  number       = {April 2005},
  abstract     = {The ultimate goal of augmented reality is to provide the user with a view of the surroundings enriched by virtual objects. Practically all augmented reality systems rely on standard real-time rendering methods for generating the images of virtual scene elements. Although such conventional computer graphics algorithms are fast, they often fail to produce sufficiently realistic renderings. The use of simple lighting and shading methods, as well as the lack of knowledge about actual lighting conditions in the real surroundings, cause virtual objects to appear artificial. In this paper, we propose an entirely novel approach for generating augmented reality images in video see-through systems. Our method is based on the idea of applying stylization techniques for reducing the visual realism of both the camera image and the virtual graphical objects. A special painterly image filter is applied to the camera video stream. The virtual scene elements are generated using a non-photorealistic rendering method. Since both the camera image and the virtual objects are stylized in a corresponding "cartoon-like" or "sketch-like" way, they appear very similar. As a result, the graphical objects seem to be an actual part of the real surroundings. AKWe describe both the new painterly filter for the camera image and the non-photorealistic rendering method for virtual scene elements, which has been adapted for this purpose. Both are fast enough for generating augmented reality images in real-time and are highly customizable, The results obtained using our method are very promising and show that it improves immersion in augmented reality. {\textcopyright}2005 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer, Bartz, Stra{\ss}er - 2005 - Stylized augmented reality for improved immersion.pdf:pdf},
  keywords     = {Augmented reality,Immersion,Non-photorealistic rendering,Painterly filtering},
}

@Article{Rohmer2017,
  author       = {Rohmer, Kai and Jendersie, Johannes and Grosch, Thorsten},
  year         = {2017},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Natural Environment Illumination: Coherent Interactive Augmented Reality for Mobile and Non-Mobile Devices}},
  doi          = {10.1109/TVCG.2017.2734426},
  issn         = {10772626},
  number       = {11},
  pages        = {2474--2484},
  volume       = {23},
  abstract     = {Augmented Reality offers many applications today, especially on mobile devices. Due to the lack of mobile hardware for illumination measurements, photorealistic rendering with consistent appearance of virtual objects is still an area of active research. In this paper, we present a full two-stage pipeline for environment acquisition and augmentation of live camera images using a mobile device with a depth sensor. We show how to directly work on a recorded 3D point cloud of the real environment containing high dynamic range color values. For unknown and automatically changing camera settings, a color compensation method is introduced. Based on this, we show photorealistic augmentations using variants of differential light simulation techniques. The presented methods are tailored for mobile devices and run at interactive frame rates. However, our methods are scalable to trade performance for quality and can produce quality renderings on desktop hardware.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rohmer, Jendersie, Grosch - 2017 - Natural Environment Illumination Coherent Interactive Augmented Reality for Mobile and Non-Mobile Dev.pdf:pdf},
  keywords     = {Augmented Reality,Color Compensation,Depth-Sensing,Differential Rendering,GPU-Importance Sampling,Global Illumination,Impostors Tracing,Light Estimation,Material Estimation,Mixed Reality,Mobile AR,Point Clouds,Scene Reconstruction},
  pmid         = {28809689},
}

@Article{Leyrer2011,
  author       = {Leyrer, Markus and Linkenaugery, Sally A. and B{\"{u}}lthoffz, Heinrich H. and Kloosx, Uwe and Mohler, Betty},
  year         = {2011},
  journal       = {Proceedings - APGV 2011: ACM SIGGRAPH Symposium on Applied Perception in Graphics and Visualization},
  title        = {{The influence of eye height and avatars on egocentric distance estimates in immersive virtual environments}},
  doi          = {10.1145/2077451.2077464},
  number       = {212},
  pages        = {67--74},
  volume       = {1},
  abstract     = {It is well known that eye height is an important visual cue in the perception of apparent sizes and affordances in virtual environments. However, the influence of visual eye height on egocentric distances in virtual environments has received less attention. To explore this influence, we conducted an experiment where we manipulated the virtual eye height of the user in a head-mounted display virtual environment. As a measurement we asked the participants to verbally judge egocentric distances and to give verbal estimates of the dimensions of the virtual room. In addition, we provided the participants a self-animated avatar to investigate if this virtual self-representation has an impact on the accuracy of verbal distance judgments, as recently evidenced for distance judgments accessed with an action-based measure. When controlled for ownership, the avatar had a significant influence on the verbal estimates of egocentric distances as found in previous research. Interestingly, we found that the manipulation of eye height has a significant influence on the verbal estimates of both egocentric distances and the dimensions of the room. We discuss the implications which these research results have on those interested in space perception in both immersive virtual environments and the real world. {\textcopyright} 2011 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Leyrer et al. - 2011 - The influence of eye height and avatars on egocentric distance estimates in immersive virtual environments.pdf:pdf},
  isbn         = {9781450308892},
  keywords     = {Avatars,Distance perception,Eye height,Immersive virtual environments},
}

@Article{Linton2019,
  author     = {Linton, Paul},
  year       = {2019},
  title      = {{Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and Augmented Reality?}},
  doi        = {10.31234/osf.io/cz54j},
  eprint     = {1905.10366},
  eprinttype = {arXiv},
  pages      = {0--4},
  url        = {http://arxiv.org/abs/1905.10366},
  abstract   = {Near distances are overestimated in virtual reality, and far distances are underestimated, but an explanation for these distortions remains elusive. One potential concern is that whilst the eye rotates to look at the virtual scene, the virtual cameras remain static. Could using eye-tracking to change the perspective of the virtual cameras as the eye rotates improve depth perception in virtual reality? This paper identifies 14 distinct perspective distortions that could in theory occur from keeping the virtual cameras fixed whilst the eye rotates in the context of near-eye displays. However, the impact of eye movements on the displayed image depends on the optical, rather than physical, distance of the display. Since the optical distance of most head-mounted displays is over 1m, most of these distortions will have only a negligible effect. The exception are 'gaze-contingent disparities', which will leave near virtual objects looking displaced from physical objects that are meant to be at the same distance in augmented reality.},
  arxivid    = {1905.10366},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linton - 2019 - Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and Augmented Reality.pdf:pdf},
}

@Misc{Renner2013,
  author    = {Renner, Rebekka S. and Velichkovsky, Boris M. and Helmert, Jens R.},
  year      = {2013},
  title     = {{The perception of egocentric distances in virtual environments - A review}},
  doi       = {10.1145/2543581.2543590},
  abstract  = {Over the last 20 years research has been done on the question of how egocentric distances, i.e., the subjectively reported distance from a human observer to an object, are perceived in virtual environments. This review surveys the existing literature on empirical user studies on this topic. In summary, there is amean estimation of egocentric distances in virtual environments of about 74\% of the modeled distances. Many factors possibly influencing distance estimates were reported in the literature. We arranged these factors into four groups, namely measurement methods, technical factors, compositional factors, and human factors. The research on these factors is summarized, conclusions are drawn, and promising areas for future research are outlined. {\textcopyright} 2013 ACM.},
  booktitle = {ACM Computing Surveys},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Renner, Velichkovsky, Helmert - 2013 - The perception of egocentric distances in virtual environments - A review.pdf:pdf},
  issn      = {03600300},
  keywords  = {Depth perception,Distance estimation,Perception,Virtual environments,Virtual reality},
}

@Article{Bertelsen2012,
  author       = {Bertelsen, Alvaro and Irarrazaval, Pablo and Cadiz, Rodrigo F.},
  year         = {2012},
  journal       = {Computerized Medical Imaging and Graphics},
  title        = {{Volume visualization using a spatially aware mobile display device}},
  doi          = {10.1016/j.compmedimag.2011.06.003},
  issn         = {08956111},
  number       = {1},
  pages        = {66--71},
  url          = {http://dx.doi.org/10.1016/j.compmedimag.2011.06.003},
  volume       = {36},
  abstract     = {Volume visualization is a difficult three-dimensional task and a significant amount of research is devoted to the development of a suitable computer input device for it. Most of the proposed models use fixed displays, thus rendering extracted slices in orientations unrelated to their real locations within the volume. We present a new device which takes a different approach, as it leaves the volume in a fixed location and demands the user to change his or her posture to explore it from different angles. To implement this, we built a prototype based on a mobile display equipped with sensors that allows it to track its position, which is related to the location of the slice plane within the volume. Therefore, the user can manipulate this plane by displacing and rotating the display, which is a very intuitive method with minimum learning time. Furthermore, the postural changes required to use the device add a new channel of feedback, which effectively helps to reduce the cognitive load imposed on the user. We built a prototype device and tested it with two groups of volunteers who were asked to use it in a medical imaging application. Statistical analysis of the results shows that explorations made with the proposed device were considerably faster with no penalty in precision. We believe that, with further work, the proposed device can be developed into an useful tool for radiology and neurosurgery. {\textcopyright} 2011 Elsevier Ltd.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bertelsen, Irarrazaval, Cadiz - 2012 - Volume visualization using a spatially aware mobile display device.pdf:pdf},
  keywords     = {Biomedical imaging,Computer graphics,Computer input-output,Imaging,User interfaces},
  pmid         = {21704498},
  publisher    = {Elsevier Ltd},
}

@Article{Rodriguez2012,
  author       = {Rodr{\'{i}}guez, Marcos Balsa and Alcocer, Pere Pau V{\'{a}}zquez},
  year         = {2012},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{Practical volume rendering in mobile devices}},
  doi          = {10.1007/978-3-642-33179-4_67},
  issn         = {03029743},
  number       = {PART 1},
  pages        = {708--718},
  volume       = {7431 LNCS},
  abstract     = {Volume rendering has been a relevant topic in scientific visualization for the last two decades. A decade ago the exploration of reasonably big volume datasets required costly workstations due to the high processing cost of this kind of visualization. In the last years, a high end PC or laptop was enough to be able to handle medium-sized datasets thanks specially to the fast evolution of GPU hardware. New embedded CPUs that sport powerful graphics chipsets make complex 3D applications feasible in such devices. However, besides the much marketed presentations and all its hype, no real empirical data is usually available that makes comparing absolute and relative capabilities possible. In this paper we analyze current graphics hardware in most high-end Android mobile devices and perform a practical comparison of a well-known GPU-intensive task: volume rendering. We analyze different aspects by implementing three different classical algorithms and show how the current state-of-the art mobile GPUs behave in volume rendering. {\textcopyright} 2012 Springer-Verlag.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{i}}guez, Alcocer - 2012 - Practical volume rendering in mobile devices.pdf:pdf},
  isbn         = {9783642331787},
}

@Article{Itoh2015,
  author       = {Itoh, Yuta and Klinker, Gudrun},
  year         = {2015},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Light-Field Correction for Spatial Calibration of Optical See-Through Head-Mounted Displays}},
  doi          = {10.1109/TVCG.2015.2391859},
  issn         = {10772626},
  number       = {4},
  pages        = {471--480},
  volume       = {21},
  abstract     = {A critical requirement for AR applications with Optical See-Through Head-Mounted Displays (OST-HMD) is to project 3D information correctly into the current viewpoint of the user - more particularly, according to the user's eye position. Recently-proposed interaction-free calibration methods [16], [17] automatically estimate this projection by tracking the user's eye position, thereby freeing users from tedious manual calibrations. However, the method is still prone to contain systematic calibration errors. Such errors stem from eye-/HMD-related factors and are not represented in the conventional eye-HMD model used for HMD calibration. This paper investigates one of these factors - the fact that optical elements of OST-HMDs distort incoming world-light rays before they reach the eye, just as corrective glasses do. Any OST-HMD requires an optical element to display a virtual screen. Each such optical element has different distortions. Since users see a distorted world through the element, ignoring this distortion degenerates the projection quality. We propose a light-field correction method, based on a machine learning technique, which compensates the world-scene distortion caused by OST-HMD optics. We demonstrate that our method reduces the systematic error and significantly increases the calibration accuracy of the interaction-free calibration.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Itoh, Klinker - 2015 - Light-Field Correction for Spatial Calibration of Optical See-Through Head-Mounted Displays.pdf:pdf},
  keywords     = {INDICA,OST-HMD,SPAAM,calibration,eye tracking,light field,optical see-through display,undistortion},
  pmid         = {26357097},
  publisher    = {IEEE},
}

@Article{Lee2015,
  author       = {Lee, Sangyoon and Hua, Hong},
  year         = {2015},
  journal       = {IEEE/OSA Journal of Display Technology},
  title        = {{A Robust Camera-Based Method for Optical Distortion Calibration of Head-Mounted Displays}},
  doi          = {10.1109/JDT.2014.2386216},
  issn         = {1551319X},
  number       = {10},
  pages        = {845--853},
  volume       = {11},
  abstract     = {One of the problems in using HMDs is that the virtual images shown through the HMDs are usually warped due to their optical distortions. In order to correctly compensate the optical distortions through a predistortion technique, accurate values of the distortion parameters are required. Although several distortion calibration methods have been developed in prior work, these methods have some limitations. In this paper, we propose a method for accurately estimating the optical distortion parameters of both immersive and (optical) see-through HMDs, without the limitations. The proposed method is based on photogrammetry and considers not only the radial distortion but also the tangential distortion. Its effectiveness, including the effects of different coefficient orders of radial and tangential distortions, are evaluated through an experiment conducted with two different HMDs, an optical see-through head-mounted projection display (HMPD) and a commercially available immersive HMD. According to the experimental results, the proposed method showed significantly lower reprojection and line-fitting errors than a previous method proposed by Owen , and the radial distortion coefficients estimated by the proposed method were significantly more accurate than the nominal values obtained from the HMD optics prescription. In addition, when both the radial and tangential distortions were considered, the proposed method was significantly more effective than when only the radial distortion was considered. We also discuss an OpenCV-based simple implementation of a distortion correction method.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Hua - 2015 - A Robust Camera-Based Method for Optical Distortion Calibration of Head-Mounted Displays.pdf:pdf},
  keywords     = {Geometric calibration,head-mounted display (HMD),optical distortion,photogrammetry},
  publisher    = {IEEE},
}

@Article{Blate2019,
  author       = {Blate, Alex and Whitton, Mary and Singh, Montek and Welch, Greg and State, Andrei and Whitted, Turner and Fuchs, Henry},
  year         = {2019},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Implementation and Evaluation of a 50 kHz, 28$\mu$s Motion-to-Pose Latency Head Tracking Instrument}},
  doi          = {10.1109/TVCG.2019.2899233},
  issn         = {19410506},
  number       = {5},
  pages        = {1970--1980},
  volume       = {25},
  abstract     = {This paper presents the implementation and evaluation of a 50,000-pose-sample-per-second, 6-degree-of-freedom optical head tracking instrument with motion-to-pose latency of 28$\mu$s and dynamic precision of 1-2 arcminutes. The instrument uses high-intensity infrared emitters and two duo-lateral photodiode-based optical sensors to triangulate pose. This instrument serves two purposes: it is the first step towards the requisite head tracking component in sub-100$\mu$s motion-to-photon latency optical see-through augmented reality (OST AR) head-mounted display (HMD) systems; and it enables new avenues of research into human visual perception - including measuring the thresholds for perceptible real-virtual displacement during head rotation and other human research requiring high-sample-rate motion tracking. The instrument's tracking volume is limited to about 120 × 120 × 250 but allows for the full range of natural head rotation and is sufficient for research involving seated users. We discuss how the instrument's tracking volume is scalable in multiple ways and some of the trade-offs involved therein. Finally, we introduce a novel laser-pointer-based measurement technique for assessing the instrument's tracking latency and repeatability. We show that the instrument's motion-to-pose latency is 28$\mu$s and that it is repeatable within 1-2 arcminutes at mean rotational velocities (yaw) in excess of 500°/sec.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blate et al. - 2019 - Implementation and Evaluation of a 50 kHz, 28$\mu$s Motion-to-Pose Latency Head Tracking Instrument.pdf:pdf},
  keywords     = {Tracking,augmented reality,dynamic tracking error,head tracker,lateral-effect photodiodes,low-latency augmented reality,motion tracking,perception},
  pmid         = {30843843},
}

@Article{Jota2013,
  author       = {Jota, Ricardo and Ng, Albert and Dietz, Paul and Wigdor, Daniel},
  year         = {2013},
  journal       = {Conference on Human Factors in Computing Systems - Proceedings},
  title        = {{How fast is fast enough? A study of the effects of latency in direct-touch pointing tasks}},
  doi          = {10.1145/2470654.2481317},
  number       = {1},
  pages        = {2291--2300},
  abstract     = {Although advances in touchscreen technology have provided us with more precise devices, touchscreens are still laden with latency issues. Common commercial devices present with latency up to 125ms. Although these levels have been shown to impact users' perception of the responsiveness of the system [16], relatively little is known about the impact of latency on the performance of tasks common to direct-touch interfaces, such as direct physical manipulation. In this paper, we study the effect of latency of a direct-touch pointing device on dragging tasks. Our tests show that user performance decreases as latency increases. We also find that user performance is more severely affected by latency when targets are smaller or farther away. We present a detailed analysis of users' coping mechanisms for latency, and present the results of a follow-up study demonstrating user perception of latency in the land-on phase of the dragging task. Copyright {\textcopyright} 2013 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jota et al. - 2013 - How fast is fast enough A study of the effects of latency in direct-touch pointing tasks.pdf:pdf},
  isbn         = {9781450318990},
  keywords     = {Direct input,Direct manipulations,Latency,Touch input},
}

@Article{Mania2004,
  author       = {Mania, Katerina and Adelstein, Bernard D and Ellis, Stephen R and Hill, Michael I},
  year         = {2004},
  journal       = {ACM Proceedings of the 1st Symposium on Applied perception in graphics and visualization},
  title        = {{Perceptual Sensitivity to Head Tracking Latency in Virtual Environments with Varying Degrees of Scene Complexity}},
  number       = {study 1},
  pages        = {1--10},
  url          = {papers3://publication/uuid/08D29EBD-9F54-48AF-86D8-809CB283B688},
  volume       = {1},
  abstract     = {System latency (time delay) and its visible consequences are fundamental virtual environment (VE) deficiencies that can hamper user perception and performance. The aim of this research is to quantify the role of VE scene content and resultant relative object motion on perceptual sensitivity to VE latency. Latency detection was examined by presenting observers in a head-tracked, stereoscopic head mounted display with environments having differing levels of complexity ranging from simple geometrical objects to a radiosity-rendered scene of two interconnected rooms. Latency discrimination was compared with results from a previous study in which only simple geometrical objects, without radiosity rendering or a 'real-world' setting, were used. From the results of these two studies, it can be inferred that the Just Noticeable Difference (JND) for latency discrimination by trained observers averages $\sim$15 ms or less, independent of scene complexity and real-world meaning. Such knowledge will help elucidate latency perception mechanisms and, in turn, guide VE designers in the development of latency countermeasures.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mania et al. - 2004 - Perceptual Sensitivity to Head Tracking Latency in Virtual Environments with Varying Degrees of Scene Complexity.pdf:pdf},
  isbn         = {1581139144},
  keywords     = {2004 by the association,acm acknowledges that this,co-authored by a contractor,contribution was authored or,copyright,for computing machinery,inc,latency,sensitivity thresholds,simulations},
}

@Article{Gruen2020,
  author       = {Gruen, Robert and Ofek, Eyal and Steed, Anthony and Gal, Ran and Sinclair, Mike and Gonzalez-Franco, Mar},
  year         = {2020},
  journal       = {Proceedings - 2020 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2020},
  title        = {{Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices}},
  doi          = {10.1109/VR46266.2020.1580498468656},
  pages        = {791--799},
  abstract     = {Measuring Visual Latency in VR and AR devices has become increasingly complicated as many of the components will influence others in multiple loops and ultimately affect the human cognitive and sensory perception. In this paper we present a new method based on the idea that the performance of humans on a rapid motor task will remain constant, and that any added delay will correspond to the system latency. We ask users to perform a task inside different video see-through devices and also in front of a computer. We also calculate the latency of the systems using a hardware instrumentation-based measurement technique for bench-marking. Results show that this new form of latency measurement through human cognitive performance can be reliable and comparable to hardware instrumentation-based measurement. Our method is adaptable to many forms of user interaction. It is particularly suitable for systems, such as AR and VR, where externalizing signals is difficult, or where it is important to measure latency while the system is in use by a user.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gruen et al. - 2020 - Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices.pdf:pdf},
  isbn         = {9781728156088},
  keywords     = {Computing methodologies,Human-centered computing,Perception,Virtual reality},
}

@Article{Potemin2018,
  author   = {Potemin, Igor S. and Livshits, Irina and Zhdanov, Dmitry and Zhdanov, Andrey and Bogdanov, Nikolay},
  year     = {2018},
  title    = {{An application of the virtual prototyping approach to design of VR, AR, and MR devices free from the vergence-accommodation conflict}},
  doi      = {10.1117/12.2314095},
  issn     = {1996756X},
  number   = {May 2018},
  pages    = {3},
  volume   = {1069404},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Potemin et al. - 2018 - An application of the virtual prototyping approach to design of VR, AR, and MR devices free from the vergence-ac.pdf:pdf},
  isbn     = {9781510619258},
  keywords = {augmented reality,eye,image processing,mixed reality,optical design,tracking,vergence-accommodation conflict,virtual prototyping,virtual reality},
}

@Article{Cutting1997,
  author       = {Cutting, James E},
  year         = {1997},
  journal       = {Virtual Reality},
  title        = {{How the eye measures reality and virtual reality}},
  number       = {1},
  pages        = {27--36},
  volume       = {29},
  abstract     = {If virtual reality systems are to make good on their name, designers must know how people perceive space in natural environments, in photographs, and in cinema. Perceivers understand the layout of a cluttered natural environment through the use of nine or more sources of information, each based on different assumptions--occlusion, height in the visual field, relative size, relative density, aerial per-spective, binocular disparities, accommodation, convergence, and motion perspective. The relative utility of these sources at different distances is compared, using their ordinal depth-threshold func-tions. From these, three classes of space around a moving observer are postulated: personal space, ac-tion space, and vista space. Within each, a smaller number of sources act in consort, with different rel-ative strengths. Given the general ordinality of the sources, these spaces are likely to be affine in character, stretching and collapsing with viewing conditions. One of these conditions is controlled by lens length in photography and cinematography or by field-of-view commands in computer graphics. These have striking effects on many of these sources of information and, consequently, on how the lay-out of a scene is perceived.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cutting - 1997 - HIGH-PERFORMANCE COMPUTING AND HUMAN VISION I How the eye measures reality and virtual reality THE PERCEPTION OF LAYOUT.pdf:pdf},
}

@Book{ReaEditor1205,
  author     = {{Rea Editor}, Paul M},
  year       = {1205},
  title      = {{Biomedical Visualisation}},
  isbn       = {9783030439606},
  url        = {http://www.springer.com/series/5584},
  volume     = {7},
  abstract   = {Volume 5},
  annotation = {So this is the book that was published before yours cite this Engaging with Children Using Augmented Reality on Clothing to Prevent Them from Smoking Zuzana Borovanska, Matthieu Poyade, Paul M. Rea, and Ibrahim Daniel Buksh This paper looks at using as overlays of damaged lungs as a side. I only care about the technical aspect.},
  booktitle  = {Advances in Experimental Medicine and Biology},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rea Editor - 1205 - Biomedical Visualisation.pdf:pdf;:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rea Editor - 1205 - Biomedical Visualisation.epub:epub},
}

@Article{Ko2017,
  author       = {Ko, Hyunsuk and Shim, Han Suk and Choi, Ouk and {Jay Kuo}, C. C.},
  year         = {2017},
  journal       = {Image and Vision Computing},
  title        = {{Robust uncalibrated stereo rectification with constrained geometric distortions (USR-CGD)}},
  doi          = {10.1016/j.imavis.2017.01.001},
  eprint       = {1603.09462},
  eprinttype   = {arXiv},
  issn         = {02628856},
  pages        = {98--114},
  volume       = {60},
  abstract     = {A novel algorithm for uncalibrated stereo image-pair rectification under the constraint of geometric distortion, called USR-CGD, is presented in this work. Although it is straightforward to define a rectifying transformation (or homography) given the epipolar geometry, many existing algorithms have unwanted geometric distortions as a side effect. To obtain rectified images with reduced geometric distortions while maintaining a small rectification error, we parameterize the homography by considering the influence of various kinds of geometric distortions. Next, we define several geometric measures and incorporate them into a new cost function as regularization terms for parameter optimization. Finally, we propose a constrained adaptive optimization scheme to allow a balanced performance between the rectification error and the geometric error. Extensive experimental results are provided to demonstrate the superb performance of the proposed USR-CGD method, which outperforms existing algorithms by a significant margin.},
  arxivid      = {1603.09462},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ko et al. - 2017 - Robust uncalibrated stereo rectification with constrained geometric distortions (USR-CGD).pdf:pdf},
  keywords     = {Constrained optimization,Epipolar geometry,Fundamental matrix,Geometric distortion,Homography,Projective rectification,Regularization},
}

@Article{Pantilie2011,
  author       = {Pantilie, Cosmin D. and Haller, Istvan and Drulea, Marius and Nedevschi, Sergiu},
  year         = {2011},
  journal       = {Proceedings - 2011 10th International Symposium on Parallel and Distributed Computing, ISPDC 2011},
  title        = {{Real-time image rectification and stereo reconstruction system on the GPU}},
  doi          = {10.1109/ISPDC.2011.21},
  pages        = {79--85},
  abstract     = {The increase in computational power of consumer graphic cards has successfully motivated adaptation of stereo algorithms to this kind of hardware. In order to solve the stereo correspondence problem efficiently, the images need to be rectified and lens distortions need to be removed. This paper presents an efficient two step solution for rectifying and correcting lens distortions in images captured using a pair of stereo cameras. The first step consists of a one-time, off-line calculation of a look-up table, based on the calibration parameters, for each of the two cameras. The second step computes the final pixel intensities based on the pre-calculated mappings stored in the look-up table. The GPU implementation proposed makes use of the inherent parallelism in a cost-effective manner, making the method suitable for rectifying high resolution images in real-time. Results are compared against an optimized CPU-based implementation, written in assembly language using MMX instructions, for reference. The complete stereo reconstruction system was implemented and evaluated on a current generation GPU and offers a running time of 11ms for images with resolution 512x383. {\textcopyright} 2011 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pantilie et al. - 2011 - Real-time image rectification and stereo reconstruction system on the GPU.pdf:pdf},
  isbn         = {9780769545400},
}

@article{Kramida,
author = {Kramida, Gregory and Varshney, Amitabh},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kramida, Varshney - Unknown - Resolving the Vergence-Accommodation Conflict in Head Mounted Displays.pdf:pdf},
pages = {1--17},
title = {{Resolving the Vergence-Accommodation Conflict in Head Mounted Displays}}
}

@Article{Lee2018,
  author       = {Lee, Joong Jae and Jeong, Mun Ho},
  year         = {2018},
  journal       = {Sensors (Switzerland)},
  title        = {{Stereo camera head-eye calibration based on minimum variance approach using surface normal vectors}},
  doi          = {10.3390/s18113706},
  issn         = {14248220},
  number       = {11},
  pages        = {1--19},
  volume       = {18},
  abstract     = {This paper presents a stereo camera-based head-eye calibration method that aims to find the globally optimal transformation between a robot's head and its eye. This method is highly intuitive and simple, so it can be used in a vision system for humanoid robots without any complex procedures. To achieve this, we introduce an extended minimum variance approach for head-eye calibration using surface normal vectors instead of 3D point sets. The presented method considers both positional and orientational error variances between visual measurements and kinematic data in head-eye calibration. Experiments using both synthetic and real data show the accuracy and efficiency of the proposed method.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Jeong - 2018 - Stereo camera head-eye calibration based on minimum variance approach using surface normal vectors.pdf:pdf},
  keywords     = {Head-eye calibration,Humanoid robot,Minimum variance approach,Stereo camera,Surface normal vector},
  pmid         = {30384481},
}

@Article{Zollmann2020,
  author       = {Zollmann, Stefanie and Grasset, Raphael and Langlotz, Tobias and Lo, Wei Hong and Mori, Shohei and Regenbrecht, Holger},
  year         = {2020},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Visualization Techniques in Augmented Reality: A Taxonomy, Methods and Patterns}},
  doi          = {10.1109/TVCG.2020.2986247},
  issn         = {19410506},
  number       = {c},
  pages        = {1--20},
  volume       = {2626},
  abstract     = {In recent years, the development of Augmented Reality (AR) frameworks made AR application development widely accessible to developers without AR expert background. With this development, new application fields for AR are on the rise. This comes with an increased need for visualization techniques that are suitable for a wide range of application areas. It becomes more important for a wider audience to gain a better understanding of existing AR visualization techniques. Within this work we provide a taxonomy of existing works on visualization techniques in AR. The taxonomy aims to give researchers and developers without an in-depth background in Augmented Reality the information to successively apply visualization techniques in Augmented Reality environments. We also describe required components and methods and analyze common patterns.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zollmann et al. - 2020 - Visualization Techniques in Augmented Reality A Taxonomy, Methods and Patterns.pdf:pdf},
  keywords     = {Augmented Reality,Augmented reality,Cameras,Data visualization,Information Visualization,Pipelines,Rendering (computer graphics),Taxonomy,Visualization},
  publisher    = {IEEE},
}

@Article{Mahmood2017,
  author     = {Mahmood, Razia},
  year       = {2017},
  title      = {{Edge Detection in Unorganized 3D Point Cloud}},
  annotation = {A thiesis on edge detection in point clouds},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahmood - 2017 - Edge Detection in Unorganized 3D Point Cloud.pdf:pdf},
}

@Article{Otsuki2016,
  author       = {Otsuki, Mai and Hideaki, Kuzuoka},
  year         = {2016},
  journal       = {Research report Human-computer interaction (HCI) (Research Report Human-Computer Interaction (HCI))},
  title        = {{Effect of translucent random dot mask on depth perception in stereo AR environment}},
  number       = {2},
  pages        = {1--8},
  volume       = {2016},
  abstract     = {As a practical application in the field of Augmented Reality (AR), an object inside a real object is overlaid and drawn as a virtual object on the surface of the real object. There are things that make it possible to observe the object. However, if the virtual object inside the real object is simply overlaid on the surface of the real object, the virtual object that should not be visible can be seen without being blocked by anything. There is a problem that the depth of the virtual object cannot be perceived correctly because it is perceived as "being in front of the surface of the real object". On the other hand, in the previous research, we added a random dot mask to the surface of the real object, and as a result, we asked the observer to "observe the virtual object inside through many small holes opened in the surface of the real object". We proposed a method of perceiving it. In this paper, first of all, we report the findings obtained when the technology using the proposed method was exhibited at a conference. Moreover, in the proposed method, the visibility of the internal virtual object was reduced because it was observed through the mask. Therefore, we aimed to improve visibility and depth perception by making the mask translucent. In this paper, we discuss the trade-off between the transparency of the mask and the accuracy of visibility and depth perception based on evaluation experiments.},
  annotation   = {So this paper is in japanses so I should probably get someone to check this stuff for me, What I think this paper is talking about is just apply any noise over a object can allow the user to understand depth within a object},
}

@Article{Wang2013,
  author       = {Wang, Ying and Ewert, Daniel and Schilberg, Daniel and Jeschke, Sabina},
  year         = {2013},
  journal       = {2013 10th International Conference and Expo on Emerging Technologies for a Smarter World, CEWIT 2013},
  title        = {{Edge extraction by merging 3D point cloud and 2D image data}},
  doi          = {10.1109/CEWIT.2013.6713743},
  pages        = {0--5},
  abstract     = {Edges provide important visual information by corresponding to discontinuities in the physical, photometrical and geometrical properties of scene objects, such as significant variations in the reflectance, illumination, orientation and depth of scene surfaces. The significance has drawn many people to work on the detection and extraction of edge features. The characteristics of 3D point clouds and 2D digital images are thought to be complementary, so the combined interpretation of objects with point clouds and image data is a promising approach to describe an object in computer vision area. However, the prerequisite for different levels of integrated data interpretation is the geometric referencing between the 3D point cloud and 2D image data, and a precondition for geometric referencing lies in the extraction of the corresponding features. Addressing the wide-ranged applications of edge detection in object recognition, image segmentation and pose identification, this paper presents a novel approach to extract 3D edges. The main idea is combining the edge data from a point cloud of an object and its corresponding digital images. Our approach is aimed to make use of the advantages of both edge processing and analysis of point clouds and image processing to represent the edge characteristics in 3D with increased accuracy. On the 2D image processing part, an edge extraction is applied on the image by using the Canny edge detection algorithm after the raw image data pre-processing. An easily-operating pixel data mapping mechanism is proposed in our work for corresponding 2D image pixels with 3D point cloud pixels. By referring to the correspondence map, 2D edge data are merged into 3D point cloud. On the point cloud part, the border extracting operator is performed on the range image. As a preparation work, the raw point cloud data are used to generate a range image. Edge points in the range image, points with range, are converted to 3D point type with the application of the Point Cloud Library (PCL) to define the edges in the 3D point cloud. {\textcopyright} 2013 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2013 - Edge extraction by merging 3D point cloud and 2D image data.pdf:pdf},
  isbn         = {9781479925469},
  keywords     = {2D digital images,3D point clouds,data fusion,edge detection},
  publisher    = {IEEE},
}

@Article{Ni2016,
  author       = {Ni, Huan and Lin, Xiangguo and Ning, Xiaogang and Zhang, Jixian},
  year         = {2016},
  journal       = {Remote Sensing},
  title        = {{Edge detection and feature line tracing in 3D-point clouds by analyzing geometric properties of neighborhoods}},
  doi          = {10.3390/rs8090710},
  issn         = {20724292},
  number       = {9},
  volume       = {8},
  abstract     = {This paper presents an automated and effective method for detecting 3D edges and tracing feature lines from 3D-point clouds. This method is named Analysis of Geometric Properties of Neighborhoods (AGPN), and it includes two main steps: edge detection and feature line tracing. In the edge detection step, AGPN analyzes geometric properties of each query point's neighborhood, and then combines RANdom SAmple Consensus (RANSAC) and angular gap metric to detect edges. In the feature line tracing step, feature lines are traced by a hybrid method based on region growing and model fitting in the detected edges. Our approach is experimentally validated on complex man-made objects and large-scale urban scenes with millions of points. Comparative studies with state-of-the-art methods demonstrate that our method obtains a promising, reliable, and high performance in detecting edges and tracing feature lines in 3D-point clouds. Moreover, AGPN is insensitive to the point density of the input data.},
  annotation   = {So this paper deterimines uses point cloulds to refine thier edge based blending algorithm},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ni et al. - 2016 - Edge detection and feature line tracing in 3D-point clouds by analyzing geometric properties of neighborhoods.pdf:pdf},
  keywords     = {3D edge,Angular gap,Edge detection,Feature line tracing,RANdom SAmple Consensus (RANSAC)},
}

@Article{Zollmann2014,
  author       = {Zollmann, Stefanie and Grasset, Raphael and Reitmayr, Gerhard and Langlotz, Tobias},
  year         = {2014},
  journal       = {Proceedings of the 26th Australian Computer-Human Interaction Conference, OzCHI 2014},
  title        = {{Image-based X-ray visualization techniques for spatial understanding in outdoor augmented reality}},
  doi          = {10.1145/2686612.2686642},
  pages        = {194--203},
  abstract     = {This paper evaluates different state-of-the-art approaches for implementing an X-ray view in Augmented Reality (AR). Our focus is on approaches supporting a better scene understanding and in particular a better sense of depth order between physical objects and digital objects. One of the main goals of this work is to provide effective X-ray visualization techniques that work in unprepared outdoor environments. In order to achieve this goal, we focus on methods that automatically extract depth cues from video images. The extracted depth cues are combined in ghosting maps that are used to assign each video image pixel a transparency value to control the overlay in the AR view. Within our study, we analyze three different types of ghosting maps, 1) alpha-blending which uses a uniform alpha value within the ghosting map, 2) edge-based ghosting which is based on edge extraction and 3) image-based ghosting which incorporates perceptual grouping, saliency information, edges and texture details. Our study results demonstrate that the latter technique helps the user to understand the subsurface location of virtual objects better than using alpha-blending or the edge-based ghosting.},
  annotation   = {This study focused on the improvement to depth perception caused around AR, The main technique he used was edge based ghosting. The study was foused around detemining the appoximate depth of an object. What he figered out was that Alpha blending was hardest to interperate floowed by egde based ghostings and image based ghostings were the easiest. This paper deserves more time sinse it is one of the few in the feild that has some good hard data.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zollmann et al. - 2014 - Image-based X-ray visualization techniques for spatial understanding in outdoor augmented reality.pdf:pdf},
  isbn         = {9781450306539},
  keywords     = {Augmented Reality,Evaluation,Visualization,X-ray},
}

@Article{Bazazian2015,
  author       = {Bazazian, Dena and Casas, Josep R. and Ruiz-Hidalgo, Javier},
  year         = {2015},
  journal       = {2015 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2015},
  title        = {{Fast and Robust Edge Extraction in Unorganized Point Clouds}},
  doi          = {10.1109/DICTA.2015.7371262},
  abstract     = {Edges provide important visual information in scene surfaces. The need for fast and robust feature extraction from 3D data is nowadays fostered by the widespread availability of cheap commercial depth sensors and multi-camera setups. This article investigates the challenge of detecting edges in surfaces represented by unorganized point clouds. Generally, edge recognition requires the extraction of geometric features such as normal vectors and curvatures. Since the normals alone do not provide enough information about the geometry of the cloud, further analysis of extracted normals is needed for edge extraction, such as a clustering method. Edge extraction through these techniques consists of several steps with parameters which depend on the density and the scale of the point cloud. In this paper we propose a fast and precise method to detect sharp edge features by analysing the eigenvalues of the covariance matrix that are defined by each point's k-nearest neighbors. Moreover, we evaluate quantitatively, and qualitatively the proposed methods for sharp edge extraction using several dihedral angles and well known examples of unorganized point clouds. Furthermore, we demonstrate the robustness of our approach in the noisier real-world datasets.},
  annotation   = {Just skim read this, Didn't look relivent but this paper is trying to futher refine depth perception in regards to this object.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bazazian, Casas, Ruiz-Hidalgo - 2015 - Fast and Robust Edge Extraction in Unorganized Point Clouds.pdf:pdf},
  isbn         = {9781467367950},
}

@Article{Schafer2013,
  author       = {Schafer, Henrik and Lenzen, Frank and Garbe, Christoph S.},
  year         = {2013},
  journal       = {Proceedings - 2013 International Conference on 3D Vision, 3DV 2013},
  title        = {{Depth and intensity based edge detection in time-of-flight images}},
  doi          = {10.1109/3DV.2013.23},
  pages        = {111--118},
  abstract     = {A new approach for edge detection in Time-of-Flight (ToF) depth images is presented. Especially for depth images, accurate edge detection can facilitate many image processing tasks, but rarely any methods for ToF data exist. The proposed algorithm yields highly accurate results through combining edge information both from the intensity and depth image acquired by the imager. The applicability and advantage of the new approach is demonstrated on several recorded scenes and through ToF denoising using adaptive total variation as an application. It is shown that results improve considerably compared to another state-of-the art edge detection algorithm adapted for ToF depth images. {\textcopyright} 2013 IEEE.},
  annotation   = {This paper talks about using a photo image with depth perception to get clearer images and to remove the edges detected base around depth perception. This is largly to help denoise the images for beter object detection. The citations from this paper seem to have insipred several human detection algorihtms.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schafer, Lenzen, Garbe - 2013 - Depth and intensity based edge detection in time-of-flight images.pdf:pdf},
  isbn         = {9780769550671},
  keywords     = {Time-of-Flight cameras,denoising,depth edges,depth imaging,edge detection,intensity edges,shadow removal},
}

@Article{Rompapas2020,
  author = {Rompapas, Damien Constantine},
  year   = {2020},
  title  = {{Doctor ' s Thesis Designing for Large Scale High Fidelity Collaborative Augmented Reality Experiences}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rompapas - 2020 - Doctor ' s Thesis Designing for Large Scale High Fidelity Collaborative Augmented Reality Experiences.pdf:pdf},
}

@Article{Ahmed2018,
  author       = {Ahmed, Syeda Mariam and Tan, Yan Zhi and Chew, Chee Meng and Mamun, Abdullah Al and Wong, Fook Seng},
  year         = {2018},
  journal       = {IEEE International Conference on Intelligent Robots and Systems},
  title        = {{Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding}},
  doi          = {10.1109/IROS.2018.8593910},
  eprint       = {1809.10468},
  eprinttype   = {arXiv},
  issn         = {21530866},
  pages        = {7350--7355},
  abstract     = {In this paper, we propose novel edge and corner detection algorithms for unorganized point clouds. Our edge detection method evaluates symmetry in a local neighborhood and uses an adaptive density based threshold to differentiate 3D edge points. We extend this algorithm to propose a novel corner detector that clusters curvature vectors and uses their geometrical statistics to classify a point as corner. We perform rigorous evaluation of the algorithms on RGB-D semantic segmentation and 3D washer models from the ShapeNet dataset and report higher precision and recall scores. Finally, we also demonstrate how our edge and corner detectors can be used as a novel approach towards automatic weld seam detection for robotic welding. We propose to generate weld seams directly from a point cloud as opposed to using 3D models for offline planning of welding paths. For this application, we show a comparison between Harris 3D and our proposed approach on a panel workpiece.},
  annotation   = {you only skim read this This paper is looking at edge detection using within point clouds and corner detection algorithms. This methods shows drastic improvements over others in the industry.},
  arxivid      = {1809.10468},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahmed et al. - 2018 - Edge and Corner Detection for Unorganized 3D Point Clouds with Application to Robotic Welding.pdf:pdf},
  isbn         = {9781538680940},
}

@Article{Gao2018,
  author       = {Gao, Zhongpai and Hwang, Alex and Zhai, Guangtao and Peli, Eli},
  year         = {2018},
  journal       = {PLoS ONE},
  title        = {{Correcting geometric distortions in stereoscopic 3D imaging}},
  doi          = {10.1371/journal.pone.0205032},
  issn         = {19326203},
  number       = {10},
  pages        = {1--28},
  volume       = {13},
  abstract     = {Motion in a distorted virtual 3D space may cause visually induced motion sickness. Geometric distortions in stereoscopic 3D can result from mismatches among image capture, display, and viewing parameters. Three pairs of potential mismatches are considered, including 1) camera separation vs. eye separation, 2) camera field of view (FOV) vs. screen FOV, and 3) camera convergence distance (i.e., distance from the cameras to the point where the convergence axes intersect) vs. screen distance from the observer. The effect of the viewer's head positions (i.e., head lateral offset from the screen center) is also considered. The geometric model is expressed as a function of camera convergence distance, the ratios of the three parameter-pairs, and the offset of the head position. We analyze the impacts of these five variables separately and their interactions on geometric distortions. This model facilitates insights into the various distortions and leads to methods whereby the user can minimize geometric distortions caused by some parameter-pair mismatches through adjusting of other parameter pairs. For example, in postproduction, viewers can correct for a mismatch between camera separation and eye separation by adjusting their distance from the real screen and changing the effective camera convergence distance.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2018 - Correcting geometric distortions in stereoscopic 3D imaging.pdf:pdf},
  isbn         = {1111111111},
  pmid         = {30296289},
}

@Article{Drap2016,
  author       = {Drap, Pierre and Lef{\`{e}}vre, Julien},
  year         = {2016},
  journal       = {Sensors},
  title        = {{An Exact Formula for Calculating Inverse Radial Lens Distortions}},
  doi          = {10.3390/s16060807},
  number       = {6},
  pages        = {807},
  volume       = {16},
  abstract     = {This article presents a new approach to calculating the inverse of radial distortions. The method presented here provides a model of reverse radial distortion, currently modeled by a polynomial expression, that proposes another polynomial expression where the new coefficients are a function of the original ones. After describing the state of the art, the proposed method is developed. It is based on a formal calculus involving a power series used to deduce a recursive formula for the new coefficients. We present several implementations of this method and describe the experiments conducted to assess the validity of the new approach. Such an approach, non-iterative, using another polynomial expression, able to be deduced from the first one, can actually be interesting in terms of performance, reuse of existing software, or bridging between different existing software tools that do not consider distortion from the same point of view.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Drap, Lef{\`{e}}vre - 2016 - An Exact Formula for Calculating Inverse Radial Lens Distortions.pdf:pdf},
  isbn         = {3349182852},
  keywords     = {distortion correction,power series,radial distortion},
}

@Article{Bukhari2010,
  author       = {Bukhari, Faisal and Dailey, Matthew N.},
  year         = {2010},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{Robust radial distortion from a single image}},
  doi          = {10.1007/978-3-642-17274-8_2},
  issn         = {03029743},
  number       = {PART 2},
  pages        = {11--20},
  volume       = {6454 LNCS},
  abstract     = {Many computer vision algorithms rely on the assumption of the pinhole camera model, but lens distortion with off-the-shelf cameras is significant enough to violate this assumption. Many methods for radial distortion estimation have been proposed, but they all have limitations. Robust automatic radial distortion estimation from a single natural image would be extremely useful for some applications. We propose a new method for radial distortion estimation based on the plumb-line approach. The method works from a single image and does not require a special calibration pattern. It is based on Fitzgibbon's division model, robust estimation of circular arcs, and robust estimation of distortion parameters. In a series of experiments on synthetic and real images, we demonstrate the method's ability to accurately identify distortion parameters and remove radial distortion from images. {\textcopyright} 2010 Springer-Verlag.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bukhari, Dailey - 2010 - Robust radial distortion from a single image.pdf:pdf},
  isbn         = {3642172733},
}

@Article{Watson1995,
  author       = {Watson, Benjamin A. and Hodges, Larry F.},
  year         = {1995},
  journal       = {Proceedings - Virtual Reality Annual International Symposium},
  title        = {{Using texture maps to correct for optical distortion in head-mounted displays}},
  doi          = {10.1109/vrais.1995.512493},
  pages        = {172--178},
  abstract     = {This paper describes a fast method of correcting for optical distortion in head-mounted displays (HMDs). Since the distorted display surface in an HMD is not rectilinear, the shape and location of the graphics window used with the display must be chosen carefully, and some corrections made to the predistortion model. A distortion correction might be performed with optics that reverse the distortion caused by HMD lenses, but such optics can be expensive and offer a correction for only one specific HMD. Integer incremental methods or a lookup table might be used to calculate the correction, but an I/O bottleneck makes this impractical in software. Instead, a texture map may be defined that approximates the required optical correction. Recent equipment advances allow undistorted images to be input into texture mapping hardware at interactive rates. Built in filtering handles predistortion aliasing artifacts.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Hodges - 1995 - Using texture maps to correct for optical distortion in head-mounted displays.pdf:pdf},
}

@Article{Bauer2012,
  author       = {Bauer, Aaron and Vo, Sophie and Parkins, Keith and Rodriguez, Francisco and Cakmakci, Ozan and Rolland, Jannick P.},
  year         = {2012},
  journal       = {Optics Express},
  title        = {{Computational optical distortion correction using a radial basis function-based mapping method}},
  doi          = {10.1364/oe.20.014906},
  issn         = {1094-4087},
  number       = {14},
  pages        = {14906},
  volume       = {20},
  abstract     = {A distortion mapping and computational image unwarping method based on a network interpolation that uses radial basis functions is presented. The method is applied to correct distortion in an off-axis head-worn display (HWD) presenting up to 23\% highly asymmetric distortion over a 27°x21° field of view. A 10(-5) mm absolute error of the mapping function over the field of view was achieved. The unwarping efficacy was assessed using the image-rendering feature of optical design software. Correlation coefficients between unwarped images seen through the HWD and the original images, as well as edge superimposition results, are presented. In an experiment, images are prewarped using radial basis functions for a recently built, off-axis HWD with a 20° diagonal field of view in a 4:3 ratio. Real-time video is generated by a custom application with 2 ms added latency and is demonstrated.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bauer et al. - 2012 - Computational optical distortion correction using a radial basis function-based mapping method.pdf:pdf},
}

@Article{Hoell2016,
  author     = {H{\"{o}}ll, Markus and Heran, Nikolaus and Lepetit, Vincent},
  year       = {2016},
  title      = {{Augmented Reality Oculus Rift}},
  eprint     = {1604.08848},
  eprinttype = {arXiv},
  number     = {April},
  url        = {http://arxiv.org/abs/1604.08848},
  abstract   = {This paper covers the whole process of developing an Augmented Reality Stereoscopig Render Engine for the Oculus Rift. To capture the real world in form of a camera stream, two cameras with fish-eye lenses had to be installed on the Oculus Rift DK1 hardware. The idea was inspired by Steptoe \cite{steptoe2014presence}. After the introduction, a theoretical part covers all the most neccessary elements to achieve an AR System for the Oculus Rift, following the implementation part where the code from the AR Stereo Engine is explained in more detail. A short conclusion section shows some results, reflects some experiences and in the final chapter some future works will be discussed. The project can be accessed via the git repository https://github.com/MaXvanHeLL/ARift.git.},
  arxivid    = {1604.08848},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\"{o}}ll, Heran, Lepetit - 2016 - Augmented Reality Oculus Rift.pdf:pdf},
}

@Article{Zaun2003,
  author = {Zaun, B},
  year   = {2003},
  title  = {{Calibration of virtual cameras for augmented reality}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaun - 2003 - Calibration of virtual cameras for augmented reality.pdf:pdf},
}

@Article{Fuhrmann1999,
  author       = {Fuhrmann, Anton and Schmalstieg, Dieter and Purgathofer, Werner},
  year         = {1999},
  journal       = {ACM Symposium on Virtual Reality Software and Technology, Proceedings, VRST},
  title        = {{Fast calibration for augmented reality}},
  doi          = {10.1145/323663.323692},
  number       = {September 2012},
  pages        = {166--167},
  abstract     = {Augmented Reality overlays computer generated images over the real world. These images have to be generated using transformations which correctly project a point in virtual space onto its corresponding point in the real world. We present a simple and fast calibration scheme for head-mounted displays (HMDs), which does not require additional instrumentation or complicated procedures. The user is interactively guided through the calibration process, allowing even inexperienced users to calibrate the display to their eye distance and head geometry. The calibration is stable - meaning that slight errors made by the user do not result in gross miscalibrations - and easily applicable for see-through and video-based HMDs.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuhrmann, Schmalstieg, Purgathofer - 1999 - Fast calibration for augmented reality.pdf:pdf},
}

@Article{Xia2019,
  author       = {Xia, Xinxing and Guan, Yunqing and State, Andrei and Chakravarthula, Praneeth and Rathinavel, Kishore and Cham, Tat Jen and Fuchs, Henry},
  year         = {2019},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Towards a Switchable AR/VR Near-eye Display with Accommodation-Vergence and Eyeglass Prescription Support}},
  doi          = {10.1109/TVCG.2019.2932238},
  issn         = {19410506},
  number       = {11},
  pages        = {3114--3124},
  volume       = {25},
  abstract     = {In this paper, we present our novel design for switchable AR/VR near-eye displays which can help solve the vergence-accommodation-conflict issue. The principal idea is to time-multiplex virtual imagery and real-world imagery and use a tunable lens to adjust focus for the virtual display and the see-through scene separately. With this novel design, prescription eyeglasses for near- and far-sighted users become unnecessary. This is achieved by integrating the wearer's corrective optical prescription into the tunable lens for both virtual display and see-through environment. We built a prototype based on the design, comprised of micro-display, optical systems, a tunable lens, and active shutters. The experimental results confirm that the proposed near-eye display design can switch between AR and VR and can provide correct accommodation for both.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xia et al. - 2019 - Towards a Switchable ARVR Near-eye Display with Accommodation-Vergence and Eyeglass Prescription Support.pdf:pdf},
  keywords     = {Augmented reality,Focus accommodation,Near-eye displays,Prescription correction,Virtual reality},
  pmid         = {31403422},
}

@Article{Chakravarthula2019,
  author       = {Chakravarthula, Praneeth},
  year         = {2019},
  journal       = {26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings},
  title        = {{Auto-focus augmented reality eyeglasses for both real world and virtual imagery}},
  doi          = {10.1109/VR.2019.8797780},
  pages        = {1379--1380},
  abstract     = {Near-eye displays presenting accommodation cues, thereby mitigating the vergence-accommodation conflict, have garnered interest in the recent past. However, considering that at least 40\% of US population is presbyopic 1 and similarly a sizable world population suffering other refractive errors in eye, it requires that the users wear their prescription glasses along with the AR goggles, despite focus support for virtual imagery, making the overall experience uncomfortable. In the recent work published at ISMAR-TVCG 2018, which won a Best Paper Award, my collaborators and I presented an AR display which can automatically adjust for focus for both real and virtual imagery, avoiding an extra pair of prescription correcting glasses along with AR glasses. My recent work has been on a near-eye display design which integrates with the bifocals of a presbyopic user, thereby providing depth dependent stimuli to the user who is already well adapted to bifocal lenses. A variant of these ideas are going to be presented at IEEE VR 2018 through our accepted TVCG paper. I propose that the above mentioned works combined with my future work on integrating eye trackers and depth sensors to make the display glasses more robust and completely automatic, followed by evaluating the perceptual qualities of the display are the topics of my dissertation.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chakravarthula - 2019 - Auto-focus augmented reality eyeglasses for both real world and virtual imagery.pdf:pdf},
  isbn         = {9781728113777},
}

@Article{Qian2017,
  author       = {Qian, Long and Barthel, Alexander and Johnson, Alex and Osgood, Greg and Kazanzides, Peter and Navab, Nassir and Fuerst, Bernhard},
  year         = {2017},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title        = {{Comparison of optical see-through head-mounted displays for surgical interventions with object-anchored 2D-display}},
  doi          = {10.1007/s11548-017-1564-y},
  issn         = {18616429},
  number       = {6},
  pages        = {901--910},
  volume       = {12},
  abstract     = {Purpose: Optical see-through head-mounted displays (OST-HMD) feature an unhindered and instantaneous view of the surgery site and can enable a mixed reality experience for surgeons during procedures. In this paper, we present a systematic approach to identify the criteria for evaluation of OST-HMD technologies for specific clinical scenarios, which benefit from using an object-anchored 2D-display visualizing medical information. Methods: Criteria for evaluating the performance of OST-HMDs for visualization of medical information and its usage are identified and proposed. These include text readability, contrast perception, task load, frame rate, and system lag. We choose to compare three commercially available OST-HMDs, which are representatives of currently available head-mounted display technologies. A multi-user study and an offline experiment are conducted to evaluate their performance. Results: Statistical analysis demonstrates that Microsoft HoloLens performs best among the three tested OST-HMDs, in terms of contrast perception, task load, and frame rate, while ODG R-7 offers similar text readability. The integration of indoor localization and fiducial tracking on the HoloLens provides significantly less system lag in a relatively motionless scenario. Conclusions: With ever more OST-HMDs appearing on the market, the proposed criteria could be used in the evaluation of their suitability for mixed reality surgical intervention. Currently, Microsoft HoloLens may be more suitable than ODG R-7 and Epson Moverio BT-200 for clinical usability in terms of the evaluated criteria. To the best of our knowledge, this is the first paper that presents a methodology and conducts experiments to evaluate and compare OST-HMDs for their use as object-anchored 2D-display during interventions.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian et al. - 2017 - Comparison of optical see-through head-mounted displays for surgical interventions with object-anchored 2D-display.pdf:pdf},
  keywords     = {Intervention,Mixed reality,Optical see-through head-mounted display,User study},
  pmid         = {28343301},
}

@Article{Kramida2016,
  author       = {Kramida, Gregory},
  year         = {2016},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Resolving the vergence-accommodation conflict in head-mounted displays}},
  doi          = {10.1109/TVCG.2015.2473855},
  issn         = {10772626},
  number       = {7},
  pages        = {1912--1931},
  volume       = {22},
  abstract     = {The vergence-accommodation conflict (VAC) remains a major problem in head-mounted displays for virtual and augmented reality (VR and AR). In this review, I discuss why this problem is pivotal for nearby tasks in VR and AR, present a comprehensive taxonomy of potential solutions, address advantages and shortfalls of each design, and cover various ways to better evaluate the solutions. The review describes how VAC is addressed in monocular, stereoscopic, and multiscopic HMDs, including retinal scanning and accommodation-free displays. Eye-tracking-based approaches that do not provide natural focal cues - gaze-guided blur and dynamic stereoscopy - are also covered. Promising future research directions in this area are identified.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kramida - 2016 - Resolving the vergence-accommodation conflict in head-mounted displays.pdf:pdf},
  keywords     = {Head-Mounted Displays,Vergence-Accommodation Conflict},
}

@Article{Feiner1992,
  author       = {Feiner, Steven K. and Seligmann, Dor{\'{e}}e Duncan},
  year         = {1992},
  journal       = {The Visual Computer},
  title        = {{Cutaways and ghosting: satisfying visibility constraints in dynamic 3D illustrations}},
  doi          = {10.1007/BF01897116},
  issn         = {01782789},
  number       = {5-6},
  pages        = {292--302},
  volume       = {8},
  abstract     = {For an illustration to fulfill the purposes for which it is designed, it is often important that certain objects depicted not be blocked by others. We describe an automated approach to the problem of generating illustrations that satisfy a set of visibility constraints for a given viewing specification. We introduce a family of algorithms that automatically identify potentially obscuring objects, and render them using cutaway and ghosting effects modeled after those used by illustrators. These algorithms exploit modern z-buffer-based 3D graphics hardware to make possible dynamic illustrations that maintain a set of visibility constraints as a user interactively updates the viewing specification. {\textcopyright} 1992 Springer-Verlag.},
  annotation   = {One of the first X-ray papers this one is looking just at graphics but shows how they can be used.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feiner, Seligmann - 1992 - Cutaways and ghosting satisfying visibility constraints in dynamic 3D illustrations(4).pdf:pdf},
  keywords     = {Automated picture generation,Illustration,Knowledge-based graphics,Visibility,z-buffer},
}

@Article{Feiner,
  author     = {Feiner, Steven K and Webster, Anthony C and Iii, Theodore E Krueger and Macintyre, Blair and Keller, Edward J},
  title      = {{ARCHITECTURAL ANATOMY}},
  pages      = {1--8},
  annotation = {So just some quick notes this is a old X-ray vision paper that highlights some potental usecases for this tech.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feiner et al. - Unknown - ARCHITECTURAL ANATOMY.pdf:pdf},
}

@Article{Clergeaud2017,
  author       = {Clergeaud, Damien and Roo, Joan Sol and Hachet, Martin and Guion, Pascal},
  year         = {2017},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{Towards seamless interaction between physical and virtual locations for asymmetric collaboration}},
  doi          = {10.1145/3139131.3139165},
  volume       = {Part F1319},
  abstract     = {Virtual Reality allows rapid prototyping and simulation of physical artefacts, which would be dicult and expensive to perform otherwise. On the other hand, when the design process is complex and involves multiple stakeholders, decisions are taken in meetings hosted in the physical world. In the case of aerospace industrial designs, the process is accelerated by having asymmetric collaboration between the two locations: experts discuss the possibilities in a meeting room while a technician immersed in VR tests the selected alternatives. According to experts, the current approach is not without limitations, and in this work, we present prototypes designed to tackle them. The described artefacts were created to address the main issues: awareness of the remote location, remote interaction and manipulation, and navigation between locations. First feedback from experts regarding the prototypes is also presented. The resulting design considerations can be used in other asymmetric collaborative scenarios.},
  annotation   = {This paper talks about issues regarding calibrating real world objects to specific items.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clergeaud et al. - 2017 - Towards seamless interaction between physical and virtual locations for asymmetric collaboration.pdf:pdf},
  isbn         = {9781450355483},
  keywords     = {Asymmetric collaboration,Head mounted display,Mixed reality,Spatial augmented reality,Tangible user interfaces,Virtual reality},
}

@Article{Li2014,
  author       = {Li, Jing and Koudota, Yao and Barkowsky, Marcus and Primon, Helene and {Le Callet}, Patrick},
  year         = {2014},
  journal       = {2014 6th International Workshop on Quality of Multimedia Experience, QoMEX 2014},
  title        = {{Comparing upscaling algorithms from HD to Ultra HD by evaluating preference of experience}},
  doi          = {10.1109/QoMEX.2014.6982320},
  pages        = {208--213},
  abstract     = {As the next generation of TV, Ultra High Definition Television (UHDTV) is attracting more and more people's attention as it provides a new viewing experience. Considering content delivery, due to the lack of Ultra HD resources, a direct question for the industry is that whether the state-of-the-art upscaling algorithms can be utilized to upscale the current HD or Full HD resources to UHD, gaining benefit from the higher resolution but without losing the high quality viewing experience. To investigate this, in this study, we upscaled 720p and 1080p sequences to UHD resolution by different upscaling algorithms. Paired Comparison methodology was used in the subjective experiment to evaluate their performances. The results showed that for the case of fast motion content, viewers' preference on different upscaled video sequences is not significantly different. In general conditions, the low complexity upscaling algorithms (e.g., lanczos-3) performed better than the high complexity algorithms (e.g., Robust Super Resolution algorithm). A novel upscaling algorithm adapted to UHD is recommended to be developed based on the mechanisms of human visual system.},
  annotation   = {Talks about Edge-directed interpolation/Directional_Cubic_Convolution_Interpolation},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - Comparing upscaling algorithms from HD to Ultra HD by evaluating preference of experience.pdf:pdf},
  isbn         = {9781479965366},
  keywords     = {Pair Comparison,Preference,Quality of Experience,Ultra HD,Upscaling},
}

@Article{Zhang2006a,
  author       = {Zhang, Lei and Wu, Xiaolin},
  year         = {2006},
  journal       = {IEEE Transactions on Image Processing},
  title        = {{An edge-guided image interpolation algorithm via directional filtering and data fusion}},
  doi          = {10.1109/TIP.2006.877407},
  issn         = {10577149},
  number       = {8},
  pages        = {2226--2238},
  volume       = {15},
  abstract     = {Preserving edge structures is a challenge to image interpolation algorithms that reconstruct a high-resolution image from a low-resolution counterpart. We propose a new edge-guided nonlinear interpolation technique through directional filtering and data fusion. For a pixel to be interpolated, two observation sets are defined in two orthogonal directions, and each set produces an estimate of the pixel value. These directional estimates, modeled as different noisy measurements of the missing pixel are fused by the linear minimum mean square-error estimation (LMMSE) technique into a more robust estimate, using the statistics of the two observation sets. We also present a simplified version of the LMMSE-based interpolation algorithm to reduce computational cost without sacrificing much the interpolation performance. Experiments show that the new interpolation techniques can preserve edge sharpness and reduce ringing artifacts. {\textcopyright} 2006 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Wu - 2006 - An edge-guided image interpolation algorithm via directional filtering and data fusion.pdf:pdf},
  keywords     = {Data fusion,Edge preservation,Image interpolation,Linear minimum mean square-error estimation (LMMSE},
  pmid         = {16900678},
}

@Article{Chen2020,
  author       = {Chen, Wei and Luo, Xin and Liang, Zhengfa and Li, Chen and Wu, Mingfei and Gao, Yuanming and Jia, Xiaogang},
  year         = {2020},
  journal       = {Remote Sensing},
  title        = {{A unified framework for depth prediction from a single image and binocular stereo matching}},
  doi          = {10.3390/rs12030588},
  issn         = {20724292},
  number       = {3},
  pages        = {1--13},
  volume       = {12},
  abstract     = {Depth information has long been an important issue in computer vision. The methods for this can be categorized into (1) depth prediction from a single image and (2) binocular stereo matching. However, these two methods are generally regarded as separate tasks, which are accomplished in different network architectures when using deep learning-based methods. This study argues that these two tasks can be achieved using only one network with the same weights. We modify existing networks for stereo matching to perform the two tasks. We first enable the network capable of accepting both a single image and an image pair by duplicating the left image when the right image is absent. Then, we introduce a training procedure that alternatively selects training samples of depth prediction from a single image and binocular stereo matching. In this manner, the trained network can perform both tasks and single-image depth prediction even benefits from stereo matching to achieve better performance. Experimental results on KITTI raw dataset show that our model achieves state-of-the-art performances for accomplishing depth prediction from a single image and binocular stereo matching in the same architecture.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2020 - A unified framework for depth prediction from a single image and binocular stereo matching.pdf:pdf},
  keywords     = {Binocular stereo matching,Depth prediction,Network architecture},
}

@Article{Tian2009,
  author       = {Tian, Minghui and Wan, Shouhong and Yue, Lihua},
  year         = {2009},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{A color saliency model for salient objects detection in natural scenes}},
  doi          = {10.1007/978-3-642-11301-7_26},
  issn         = {03029743},
  pages        = {240--250},
  volume       = {5916 LNCS},
  abstract     = {Detection of salient objects is very useful for object recognition, content-based image/video retrieval, scene analysis and image/video compression. In this paper, we propose a color saliency model for salient objects detection in natural scenes. In our color saliency model, different color features are extracted and analyzed. For different color features, two efficient saliency measurements are proposed to compute different saliency maps. And a feature combination strategy is presented to combine multiple saliency maps into one integrated saliency map. After that, a segmentation method is employed to locate salient objects' regions in scenes. Finally, a psychological ranking measurement is proposed for salient objects competition. In this way, we can obtain both salient objects and their rankings in one natural scene to simulate location shift in human visual attention. The experimental results indicate that our model is effective, robust and fast for salient object detection in natural scenes, also simple to implement. {\textcopyright} 2010 Springer-Verlag Berlin Heidelberg.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Wan, Yue - 2009 - A color saliency model for salient objects detection in natural scenes.pdf:pdf},
  isbn         = {3642113001},
  keywords     = {Color saliency,Natural scenes,Object detection,Visual attention},
}

@Book{Kodama2015,
  author     = {Kodama, Sachiko and Sato, Toshiki and Matsuno, Shuzo and Shida, Takahiro and Ogawa, Ryutaro and Takeno, Yasuki and Inokuchi, Kenji and Komiyama, Eitetsu},
  year       = {2015},
  title      = {{A practical ball sports platform combining dynamic body action with real-time computer graphics during ball play}},
  doi        = {10.1145/2818466.2818497},
  isbn       = {9781450339254},
  abstract   = {We developed a new entertainment platform for digital sports, composed of a shock resistant rubber ball (M-digital ball) which comprises electronic sensors, a BLE wireless module, a communication program between the ball and a computer, and real-time computer graphics/sound contents. Six sensors and a BLE module are incorporated into a substrate which attached to a rubber structure inside the ball, which was designed to protect electronic parts and electrical connections so as elasticity of rubber and layer of the air in the ball protect circuit against strong shocks. Our system can specify the state of the ball through the wireless communication, getting value of sensors in real-time. Several computer graphics and sound contents were created using the system. We realized hands free sport platform which enable ball player to organize computer graphics and sound linked to player's dynamic body actions in real-time.},
  annotation = {This is many papers look for: Please Show Me Inside: Improving the Depth Perception Using Virtual Mask in Stereoscopic AR},
  booktitle  = {SIGGRAPH Asia 2015 Emerging Technologies, SA 2015},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kodama et al. - 2015 - A practical ball sports platform combining dynamic body action with real-time computer graphics during ball play.pdf:pdf},
  keywords   = {Augmented sports,Ball,Computer graphics,Wireless},
}

@Article{Sridharan2013,
  author       = {Sridharan, Srikanth Kirshnamachari and Hincapi{\'{e}}-Ramos, Juan David and Flatla, David R. and Irani, Pourang},
  year         = {2013},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{Color correction for optical see-through displays using display color profiles}},
  doi          = {10.1145/2503713.2503716},
  pages        = {231--240},
  abstract     = {In optical see-through displays, light coming from background objects mixes with the light originating from the display, causing what is known as the color blending problem. Color blending negatively affects the usability of such displays as it impacts the legibility and color encodings of digital content. Color correction aims at reducing the impact of color blending by finding an alternative display color which, once mixed with the background, results in the color originally intended. In this paper we model color blending based on two distortions induced by the optical see-through display. The render distortion explains how the display renders colors. The material distortion explains how background colors are changed by the display material. We show the render distortion has a higher impact on color blending and propose binned-profiles (BP) - descriptors of how a display renders colors - to address it. Results show that color blending predictions using BP have a low error rate - within nine just noticeable differences (JND) in the worst case. We introduce a color correction algorithm based on predictions using BP and measure its correction capacity. Results show light display colors can be better corrected for all backgrounds. For high intensity backgrounds light colors in the neutral and CyanBlue regions perform better. Finally, we elaborate on the applicability, design and hardware implications of our approach. Copyright {\textcopyright} 2013 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sridharan et al. - 2013 - Color correction for optical see-through displays using display color profiles.pdf:pdf},
  isbn         = {9781450323796},
  keywords     = {Augmented reality,Color blending,Color correction,Display binned-profile,Interface design,Optical see-through displays},
}

@Article{Ping2020a,
  author       = {Ping, Jiamin and Thomas, Bruce H. and Baumeister, James and Guo, Jie and Weng, Dongdong and Liu, Yue},
  year         = {2020},
  journal       = {Journal of the Society for Information Display},
  title        = {{Effects of shading model and opacity on depth perception in optical see-through augmented reality}},
  doi          = {10.1002/jsid.947},
  issn         = {19383657},
  number       = {May},
  pages        = {1--13},
  abstract     = {Augmented reality (AR) technologies create an immersive environment by augmenting the real world with rendered virtual objects. One of the key requirements of an AR system is to understand how users perceive the depth of an AR object. Perceived distances to AR objects can be based on various depth cues such as rendering method used for the virtual object. The existing researches on the shading model and opacity mainly focus on the shape perception or the position relationship of several virtual objects in the virtual environment. We predicted that shading models and opacity would impact a user's estimation of a virtual object's depth in AR. We conducted two experiments to investigate the impact of color, size (Experiment 1), shading model, and opacity (Experiment 2) on the depth perception in an optical see-through head-mounted display (HMD). We found that the virtual object's color and size impact estimation efficacy. An interaction effect between color and size was also found. The results showed that shading models with specular highlights could help to improve depth perception in AR. Additionally, users had the lowest matching error when the opacity of a virtual object was 0.8.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ping et al. - 2020 - Effects of shading model and opacity on depth perception in optical see-through augmented reality.pdf:pdf},
  keywords     = {augmented reality,depth cues,distance perception,virtual environments},
}

@Article{Yukang2019,
  author       = {Yukang, YAN and Xin, YI and Chun, YU and Yuanchun, SHI},
  year         = {2019},
  journal       = {Virtual Reality {\&} Intelligent Hardware},
  title        = {{Gesture-based target acquisition in virtual and augmented reality}},
  doi          = {10.3724/sp.j.2096-5796.2019.0007},
  issn         = {2096-5796},
  number       = {3},
  pages        = {276},
  url          = {https://doi.org/10.3724/SP.J.2096-5796.2019.0007},
  volume       = {1},
  abstract     = {Background Gesture is a basic interaction channel that is frequently used by humans to communicate in daily life. In this paper, we explore to use gesture-based approaches for target acquisition in virtual and augmented reality. A typical process of gesture-based target acquisition is: when a user intends to acquire a target, she performs a gesture with her hands, head or other parts of the body, the computer senses and recognizes the gesture and infers the most possible target. Methods We build mental model and behavior model of the user to study two key parts of the interaction process. Mental model describes how user thinks up a gesture for acquiring a target, and can be the intuitive mapping between gestures and targets. Behavior model describes how user moves the body parts to perform the gestures, and the relationship between the gesture that user intends to perform and signals that computer senses. Results In this paper, we present and discuss three pieces of research that focus on the mental model and behavior model of gesture-based target acquisition in VR and AR. Conclusions We show that leveraging these two models, interaction experience and performance can be improved in VR and AR environments.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yukang et al. - 2019 - Gesture-based target acquisition in virtual and augmented reality.pdf:pdf},
  keywords     = {augmented reality,behavior model,gesture-based interaction,mental model,virtual reality},
  publisher    = {Elsevier B.V.},
}

@Article{Zhao2019,
  author       = {Zhao, Jingbo and Allison, Robert S.},
  year         = {2019},
  journal       = {Virtual Reality},
  title        = {{Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments}},
  doi          = {10.1007/s10055-019-00416-7},
  issn         = {14349957},
  number       = {November},
  abstract     = {A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and the user preference of such interfaces in the context of virtual reality (VR) have been rarely studied. In the present paper, we focused on a typical VR interaction scenario—answering Yes/No questions in VR systems to compare the performance and the user preference of three types of interfaces. These interfaces included a head gesture interface, a hand gesture interface and a conventional gamepad interface. We designed a memorization task, in which participants were asked to memorize several everyday objects presented in a virtual room and later respond to questions on whether they saw a specific object through the given interfaces when these objects were absent. The performance of the interfaces was evaluated in terms of the real-time accuracy and the response time. A user interface questionnaire was also used to reveal the user preference for these interfaces. The results showed that head gesture is a very promising interface, which can be easily added to existing VR systems for answering Yes/No questions and other binary responses in virtual environments.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Allison - 2019 - Comparing head gesture, hand gesture and gamepad interfaces for answering YesNo questions in virtual environments.pdf:pdf},
  keywords     = {Hand gesture,Head gesture,Usability,Virtual reality},
}

@Article{Zhang2016,
  author       = {Zhang, Libo and Yang, Lin and Luo, Tiejian},
  year         = {2016},
  journal       = {PLoS ONE},
  title        = {{Unified saliency detection model using color and texture features}},
  doi          = {10.1371/journal.pone.0149328},
  issn         = {19326203},
  number       = {2},
  pages        = {1--14},
  volume       = {11},
  abstract     = {Saliency detection attracted attention of many researchers and had become a very active area of research. Recently, many saliency detection models have been proposed and achieved excellent performance in various fields. However, most of these models only consider low-level features. This paper proposes a novel saliency detection model using both color and texture features and incorporating higher-level priors. The SLIC superpixel algorithm is applied to form an over-segmentation of the image. Color saliency map and texture saliency map are calculated based on the region contrast method and adaptive weight. Higher-level priors including location prior and color prior are incorporated into the model to achieve a better performance and full resolution saliency map is obtained by using the upsampling method. Experimental results on three datasets demonstrate that the proposed saliency detection model outperforms the state-of-the-art models.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Yang, Luo - 2016 - Unified saliency detection model using color and texture features.pdf:pdf},
}

@Article{Chen2017,
  author       = {Chen, Chenglizhao and Li, Shuai and Wang, Yongguang and Qin, Hong and Hao, Aimin},
  year         = {2017},
  journal       = {IEEE Transactions on Image Processing},
  title        = {{Video Saliency detection via spatial-temporal fusion and low-rank coherency diffusion}},
  doi          = {10.1109/TIP.2017.2670143},
  issn         = {10577149},
  number       = {7},
  pages        = {3156--3170},
  volume       = {26},
  abstract     = {This paper advocates a novel video saliency detection method based on the spatial-temporal saliency fusion and low-rank coherency guided saliency diffusion. In sharp contrast to the conventional methods, which conduct saliency detection locally in a frame-by-frame way and could easily give rise to incorrect low-level saliency map, in order to overcome the existing difficulties, this paper proposes to fuse the color saliency based on global motion clues in a batch-wise fashion. And we also propose low-rank coherency guided spatial-temporal saliency diffusion to guarantee the temporal smoothness of saliency maps. Meanwhile, a series of saliency boosting strategies are designed to further improve the saliency accuracy. First, the original long-term video sequence is equally segmented into many short-term frame batches, and the motion clues of the individual video batch are integrated and diffused temporally to facilitate the computation of color saliency. Then, based on the obtained saliency clues, interbatch saliency priors are modeled to guide the low-level saliency fusion. After that, both the raw color information and the fused low-level saliency are regarded as the low-rank coherency clues, which are employed to guide the spatial-temporal saliency diffusion with the help of an additional permutation matrix serving as the alternative rank selection strategy. Thus, it could guarantee the robustness of the saliency map's temporal consistence, and further boost the accuracy of the computed saliency map. Moreover, we conduct extensive experiments on five public available benchmarks, and make comprehensive, quantitative evaluations between our method and 16 state-of-the-art techniques. All the results demonstrate the superiority of our method in accuracy, reliability, robustness, and versatility.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2017 - Video Saliency detection via spatial-temporal fusion and low-rank coherency diffusion.pdf:pdf},
  keywords     = {Low-rank coherency guided saliency diffusion,Spatial-temporal saliency fusion,Video saliency,Visual saliency},
}

@Article{Sawhney2020,
  author       = {Sawhney, Rohan and Crane, Keenan},
  year         = {2020},
  journal       = {ACM Trans. Graph},
  title        = {{Monte Carlo Geometry Processing: A Grid-Free Approach to PDE-Based Methods on Volumetric Domains}},
  doi          = {10.1145/3386569.3392374},
  number       = {4},
  pages        = {18},
  url          = {https://doi.org/10.1145/3386569.3392374},
  volume       = {39},
  abstract     = {Fig. 1. Real-world geometry has not only rich surface detail (left) but also intricate internal structure (center). On such domains, FEM-based geometric algorithms struggle to mesh, setup, and solve PDEs-in this case taking more than 14 hours and 30GB of memory just for a basic Poisson equation. Our Monte Carlo solver uses about 1GB of memory and takes less than a minute to provide a preview (center right) that can then be progressively refined (far right). [Boundary mesh of Fijian strumigenys FJ13 used courtesy of the Economo Lab at OIST.] This paper explores how core problems in PDE-based geometry processing can be efficiently and reliably solved via grid-free Monte Carlo methods. Modern geometric algorithms often need to solve Poisson-like equations on geometrically intricate domains. Conventional methods most often mesh the domain, which is both challenging and expensive for geometry with fine details or imperfections (holes, self-intersections, etc.). In contrast, grid-free Monte Carlo methods avoid mesh generation entirely, and instead just evaluate closest point queries. They hence do not discretize space, time, nor even function spaces, and provide the exact solution (in expectation) even on extremely challenging models. More broadly, they share many benefits with Monte Carlo methods from photorealistic rendering: excellent scaling, trivial parallel implementation, view-dependent evaluation, and the ability to work with any kind of geometry (including implicit or procedural descriptions). We develop a complete "black box" solver that encompasses integration, variance reduction, and visualization, and explore how it can be used for various geometry processing tasks. In particular, we consider several fundamental linear elliptic PDEs with constant coefficients on solid regions of R n. Overall we find that Monte Carlo methods significantly broaden the horizons of geometry processing, since they easily handle problems of size and complexity that are essentially hopeless for conventional methods.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawhney, Crane - 2020 - Monte Carlo Geometry Processing A Grid-Free Approach to PDE-Based Methods on Volumetric Domains.pdf:pdf},
  keywords     = {numerical methods,stochastic solvers},
}

@Article{Niu2012,
  author       = {Niu, Yuzhen and Geng, Yujie and Li, Xueqing and Liu, Feng},
  year         = {2012},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title        = {{Leveraging stereopsis for saliency analysis}},
  doi          = {10.1109/CVPR.2012.6247708},
  issn         = {10636919},
  pages        = {454--461},
  abstract     = {Stereopsis provides an additional depth cue and plays an important role in the human vision system. This paper explores stereopsis for saliency analysis and presents two approaches to stereo saliency detection from stereoscopic images. The first approach computes stereo saliency based on the global disparity contrast in the input image. The second approach leverages domain knowledge in stereoscopic photography. A good stereoscopic image takes care of its disparity distribution to avoid 3D fatigue. Particularly, salient content tends to be positioned in the stereoscopic comfort zone to alleviate the vergence-accommodation conflict. Accordingly, our method computes stereo saliency of an image region based on the distance between its perceived location and the comfort zone. Moreover, we consider objects popping out from the screen salient as these objects tend to catch a viewer's attention. We build a stereo saliency analysis benchmark dataset that contains 1000 stereoscopic images with salient object masks. Our experiments on this dataset show that stereo saliency provides a useful complement to existing visual saliency analysis and our method can successfully detect salient content from images that are difficult for monocular saliency analysis methods. {\textcopyright} 2012 IEEE.},
  annotation   = {This paper looked at using two cameras that were spaced apart by a certain amount to dictate saliency. They showed very precise results. They created a saliency map based on a disparency map.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Niu et al. - 2012 - Leveraging stereopsis for saliency analysis.pdf:pdf},
  isbn         = {9781467312264},
  publisher    = {IEEE},
}

@Article{Cong2019,
  author       = {Cong, Runmin and Lei, Jianjun and Fu, Huazhu and Cheng, Ming Ming and Lin, Weisi and Huang, Qingming},
  year         = {2019},
  journal       = {IEEE Transactions on Circuits and Systems for Video Technology},
  title        = {{Review of visual saliency detection with comprehensive information}},
  doi          = {10.1109/TCSVT.2018.2870832},
  eprint       = {1803.03391},
  eprinttype   = {arXiv},
  issn         = {15582205},
  number       = {10},
  pages        = {2941--2959},
  volume       = {29},
  abstract     = {The visual saliency detection model simulates the human visual system to perceive the scene and has been widely used in many vision tasks. With the development of acquisition technology, more comprehensive information, such as depth cue, inter-image correspondence, or temporal relationship, is available to extend image saliency detection to RGBD saliency detection, co-saliency detection, or video saliency detection. The RGBD saliency detection model focuses on extracting the salient regions from RGBD images by combining the depth information. The co-saliency detection model introduces the inter-image correspondence constraint to discover the common salient object in an image group. The goal of the video saliency detection model is to locate the motion-related salient object in video sequences, which considers the motion cue and spatiotemporal constraint jointly. In this paper, we review different types of saliency detection algorithms, summarize the important issues of the existing methods, and discuss the existent problems and future works. Moreover, the evaluation datasets and quantitative measurements are briefly introduced, and the experimental analysis and discussion are conducted to provide a holistic overview of different saliency detection methods.},
  annotation   = {A survey of all the different saliency papers a good refrence of were to look for different techniques.},
  arxivid      = {1803.03391},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cong et al. - 2019 - Review of visual saliency detection with comprehensive information.pdf:pdf},
  isbn         = {6162010600},
  keywords     = {RGBD saliency detection,Salient object,co-saliency detection,depth attribute,inter-image correspondence,spatiotemporal constraint,video saliency detection},
}

@Article{Kan2019,
  author       = {K{\'{a}}n, Peter and Kafumann, Hannes},
  year         = {2019},
  journal       = {Visual Computer},
  title        = {{DeepLight: light source estimation for augmented reality using deep learning}},
  doi          = {10.1007/s00371-019-01666-x},
  issn         = {01782789},
  number       = {6-8},
  pages        = {873--883},
  url          = {https://doi.org/10.1007/s00371-019-01666-x},
  volume       = {35},
  abstract     = {This paper presents a novel method for illumination estimation from RGB-D images. The main focus of the proposed method is to enhance visual coherence in augmented reality applications by providing accurate and temporally coherent estimates of real illumination. For this purpose, we designed and trained a deep neural network which calculates a dominant light direction from a single RGB-D image. Additionally, we propose a novel method for real-time outlier detection to achieve temporally coherent estimates. Our method for light source estimation in augmented reality was evaluated on the set of real scenes. Our results demonstrate that the neural network can successfully estimate light sources even in scenes which were not seen by the network during training. Moreover, we compared our results with illumination estimates calculated by the state-of-the-art method for illumination estimation. Finally, we demonstrate the applicability of our method on numerous augmented reality scenes.},
  annotation   = {uses deep learning to determine were a light source is in a AR scene.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/K{\'{a}}n, Kafumann - 2019 - DeepLight light source estimation for augmented reality using deep learning.pdf:pdf},
  keywords     = {Augmented reality,Deep learning,Light source estimation,Photometric registration},
  publisher    = {Springer Berlin Heidelberg},
}

@Article{Pisanpeeti2017,
  author       = {Pisanpeeti, Ar Pha and Dinet, Eric},
  year         = {2017},
  journal       = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  title        = {{Transparent objects: Influence of shape and color on depth perception}},
  doi          = {10.1109/ICASSP.2017.7952480},
  issn         = {15206149},
  pages        = {1867--1871},
  abstract     = {Recovering depth information from a single still image is an important problem in computer vision. However, the problem is difficult and challenging because it has an infinite number of solutions. To address this issue, humans use numerous visual cues to infer depth. Much progress has been made towards an understanding of the visual mechanisms involved in 3D perception. Such an understanding provides relevant knowledge to design efficient approaches for computer vision. While there is much prior work on opaque objects, there has been relatively little in relation with transparency. In this paper we investigate depth estimation from single still images containing nonplanar and real transparent objects. We focused our study on two visual features: shape and color. A database of stimuli was created to carry out a psychophysical experiment with 42 na{\"{i}}ve observers.},
  annotation   = {This paper looked in to the different effects that transperent shapes and colors have on X-Ray vision. This reserach found out that users seemed to struggle quite hard determining if a object was behind or infront of a transperent object. Also it notes that using colors to determine some sense of depth is important.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pisanpeeti, Dinet - 2017 - Transparent objects Influence of shape and color on depth perception.pdf:pdf},
  isbn         = {9781509041176},
  keywords     = {Perceptual transparency,color,depth perception,psychophysical experiment,shape},
}

@Article{Kalia2019a,
  author       = {Kalia, Megha and Mathur, Prateek and Navab, Nassir and Salcudean, Septimiu E.},
  year         = {2019},
  journal       = {Healthcare Technology Letters},
  title        = {{Marker-less real-time intra-operative camera and hand-eye calibration procedure for surgical augmented reality}},
  doi          = {10.1049/htl.2019.0094},
  issn         = {20533713},
  number       = {6},
  pages        = {255--260},
  volume       = {6},
  abstract     = {Accurate medical Augmented Reality (AR) rendering requires two calibrations, a camera intrinsic matrix estimation and a hand-eye transformation. We present a unified, practical, marker-less, real-time system to estimate both these transformations during surgery. For camera calibration we perform calibrations at multiple distances from the endoscope, pre-operatively, to parametrize the camera intrinsic matrix as a function of distance from the endoscope. Then, we retrieve the camera parameters intra-operatively by estimating the distance of the surgical site from the endoscope in less than 1 s. Unlike in prior work, our method does not require the endoscope to be taken out of the patient; for the hand-eye calibration, as opposed to conventional methods that require the identification of a marker, we make use of a rendered tool-tip in 3D. As the surgeon moves the instrument and observes the offset between the actual and the rendered tool-tip, they can select points of high visual error and manually bring the instrument tip to match the virtual rendered tool tip. To evaluate the hand-eye calibration, 5 subjects carried out the hand-eye calibration procedure on a da Vinci robot. Average Target Registration Error of approximately 7mm was achieved with just three data points.},
  annotation   = {Hi Tom Saw this paper and knew you had to read it. Pls Update this},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalia et al. - 2019 - Marker-less real-time intra-operative camera and hand-eye calibration procedure for surgical augmented reality.pdf:pdf},
}

@Article{Mehrfard2019,
  author     = {Mehrfard, Arian and Fotouhi, Javad and Taylor, Giacomo and Forster, Tess and Navab, Nassir and Fuerst, Bernhard},
  year       = {2019},
  title      = {{A Comparative Analysis of Virtual Reality Head-Mounted Display Systems}},
  eprint     = {1912.02913},
  eprinttype = {arXiv},
  url        = {http://arxiv.org/abs/1912.02913},
  abstract   = {With recent advances of Virtual Reality (VR) technology, the deployment of such will dramatically increase in non-entertainment environments, such as professional education and training, manufacturing, service, or low frequency/high risk scenarios. Clinical education is an area that especially stands to benefit from VR technology due to the complexity, high cost, and difficult logistics. The effectiveness of the deployment of VR systems, is subject to factors that may not be necessarily considered for devices targeting the entertainment market. In this work, we systematically compare a wide range of VR Head-Mounted Displays (HMDs) technologies and designs by defining a new set of metrics that are 1) relevant to most generic VR solutions and 2) are of paramount importance for VR-based education and training. We evaluated ten HMDs based on various criteria, including neck strain, heat development, and color accuracy. Other metrics such as text readability, comfort, and contrast perception were evaluated in a multi-user study on three selected HMDs, namely Oculus Rift S, HTC Vive Pro and Samsung Odyssey+. Results indicate that the HTC Vive Pro performs best with regards to comfort, display quality and compatibility with glasses.},
  annotation = {This paper is looking into ways comparing the different VR headsets. I only skim read this but it could be helpful for later.},
  arxivid    = {1912.02913},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehrfard et al. - 2019 - A Comparative Analysis of Virtual Reality Head-Mounted Display Systems.pdf:pdf},
}

@Article{Fotouhi2019,
  author     = {Fotouhi, Javad and Taylor, Giacomo and Unberath, Mathias and D, Alex Johnson M and Lee, Sing Chun and D, Greg Osgood M and Armand, Mehran and Navab, Nassir},
  year       = {2019},
  title      = {{Exploring Partial Intrinsic and Extrinsic Symmetry in 3D Medical Imaging}},
  eprint     = {arXiv:2003.02294v1},
  eprinttype = {arXiv},
  number     = {Xx},
  pages      = {1--12},
  volume     = {XX},
  annotation = {This paper here is looking at the symetry (and lack of/ adjusting the symetry for thier own use) of the pelvis and the advantages to doing this in 3D rather than just 2D.},
  arxivid    = {arXiv:2003.02294v1},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fotouhi et al. - 2019 - Exploring Partial Intrinsic and Extrinsic Symmetry in 3D Medical Imaging.pdf:pdf},
}

@Article{Jud2020,
  author       = {Jud, Lukas and Fotouhi, Javad and Andronic, Octavian and Aichmair, Alexander and Osgood, Greg and Navab, Nassir and Farshad, Mazda},
  year         = {2020},
  journal       = {BMC Musculoskeletal Disorders},
  title        = {{Applicability of augmented reality in orthopedic surgery - A systematic review}},
  doi          = {10.1186/s12891-020-3110-2},
  issn         = {14712474},
  number       = {1},
  pages        = {1--13},
  volume       = {21},
  abstract     = {Background: Computer-assisted solutions are changing surgical practice continuously. One of the most disruptive technologies among the computer-integrated surgical techniques is Augmented Reality (AR). While Augmented Reality is increasingly used in several medical specialties, its potential benefit in orthopedic surgery is not yet clear. The purpose of this article is to provide a systematic review of the current state of knowledge and the applicability of AR in orthopedic surgery. Methods: A systematic review of the current literature was performed to find the state of knowledge and applicability of AR in Orthopedic surgery. A systematic search of the following three databases was performed: "PubMed", "Cochrane Library" and "Web of Science". The systematic review followed the Preferred Reporting Items on Systematic Reviews and Meta-analysis (PRISMA) guidelines and it has been published and registered in the international prospective register of systematic reviews (PROSPERO). Results: 31 studies and reports are included and classified into the following categories: Instrument / Implant Placement, Osteotomies, Tumor Surgery, Trauma, and Surgical Training and Education. Quality assessment could be performed in 18 studies. Among the clinical studies, there were six case series with an average score of 90\% and one case report, which scored 81\% according to the Joanna Briggs Institute Critical Appraisal Checklist (JBI CAC). The 11 cadaveric studies scored 81\% according to the QUACS scale (Quality Appraisal for Cadaveric Studies). Conclusion: This manuscript provides 1) a summary of the current state of knowledge and research of Augmented Reality in orthopedic surgery presented in the literature, and 2) a discussion by the authors presenting the key remarks required for seamless integration of Augmented Reality in the future surgical practice. Trial registration: PROSPERO registration number: CRD42019128569.},
  annotation   = {looked at 31 surgies that used AR on orthodic surgery. “Instrument / Implant Placement” (20 Studies) “Osteotomies” (1 Study) Surgical Training and Education” (4 Studies). “Trauma” (3 Studies) “Tumor Surgery” (3 Studies)},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jud et al. - 2020 - Applicability of augmented reality in orthopedic surgery - A systematic review.pdf:pdf},
  isbn         = {1289102031},
  keywords     = {Augmented reality,Image overlay,Orthopaedic surgery,Trauma surgery},
  pmid         = {32061248},
}

@Article{Kalia2019,
  author       = {Kalia, M. and Navab, N. and Salcudean, T.},
  year         = {2019},
  journal       = {Proceedings - IEEE International Conference on Robotics and Automation},
  title        = {{A real-time interactive augmented reality depth estimation technique for surgical robotics}},
  doi          = {10.1109/ICRA.2019.8793610},
  issn         = {10504729},
  pages        = {8291--8297},
  volume       = {2019-May},
  abstract     = {Augmented reality (AR) is a promising technology where the surgeon can see the medical abnormality in the context of the patient. It makes the anatomy of interest visible to the surgeon which otherwise is not visible. It can result in better surgical precision and therefore, potentially better surgical outcomes and faster recovery times. Despite these benefits, the current AR systems suffer from two major challenges; first, incorrect depth perception and, second, the lack of suitable evaluation systems. Therefore, in the current paper we addressed both of these problems. We proposed a color depth encoding (CDE) technique to estimate the distance between the tumor and the tissue surface using a surgical instrument. We mapped the distance between the tumor and the tissue surface to the blue-red color spectrum. For evaluation and interaction with our AR technique, we propose to use a virtual surgical instrument method using the CAD model of the instrument. The users were asked to reach the judged distance in the surgical field using the virtual tool. Realistic tool movement was simulated by collecting the forward kinematics joint encoder data. The results showed significant improvement in depth estimation, time for task completion and confidence, using our CDE technique with and without stereo versus other two cases, that are, Stereo-No CDE and No Stereo-No CDE.},
  annotation   = {This paper is has shown great progress in X-ray depth perception by using virtual tools and changing an objects color when it is being interacted with. This can be used to help the user determine how close they actually are to an object.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalia, Navab, Salcudean - 2019 - A real-time interactive augmented reality depth estimation technique for surgical robotics.pdf:pdf},
  isbn         = {9781538660263},
}

@Article{Darwish2019,
  author       = {Darwish, Saad M. and Mohallel, Aya A. and Emara, Doaa},
  year         = {2019},
  journal       = {ICENCO 2018 - 14th International Computer Engineering Conference: Secure Smart Societies},
  title        = {{An enhanced registration and display algorithm for medical augmented reality}},
  doi          = {10.1109/ICENCO.2018.8636139},
  pages        = {101--108},
  abstract     = {Augmented Reality (AR), combines real world and supplemental aids, helps in better user perception which can be very useful inaccurate decision making especially in medical applications. AR process includes registration, display, and tracking. AR system must be of a very high accuracy and should have a clear standard way of saving metadata and various information for an AR standard display. A lot of registration techniques may lack in accuracy, and display techniques may not have a standard display on different browsers and platforms. This paper handles the above shortcomings by proposing a marker-less medical AR system that uses both of intensity based detection to detect lesions in the liver and augmented reality markup language (ARML) for a standard AR display. Morphological operations are applied to make detection easier and to enhance the scene. Detection will be then followed by AR registration technique which is implemented using a mutual based information algorithm. In all system phases including preprocessing, we will consider saving metadata and information in ARML tags. Finally tracking process will be considered by applying previous steps to consequent video frames. Results of the proposed system were found to enhance physician perception for targeted objects with high accuracy. In addition; the usage of ARML for saving metadata guarantees standard AR display, it also enhances patient treatment and follow up.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Darwish, Mohallel, Emara - 2019 - An enhanced registration and display algorithm for medical augmented reality.pdf:pdf},
  isbn         = {9781538651179},
  keywords     = {Augmented Reality (AR),Augmented Reality Markup Language (ARML),Intensity-based Detection,Mutual Information based Registration},
  publisher    = {IEEE},
}

@Article{Gatys2016,
  author       = {Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
  year         = {2016},
  journal       = {Journal of Vision},
  title        = {{A Neural Algorithm of Artistic Style}},
  doi          = {10.1167/16.12.326},
  eprint       = {1508.06576},
  eprinttype   = {arXiv},
  issn         = {1534-7362},
  number       = {12},
  pages        = {326},
  volume       = {16},
  abstract     = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  arxivid      = {1508.06576},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gatys, Ecker, Bethge - 2016 - A Neural Algorithm of Artistic Style.pdf:pdf},
}

@Article{Wu2018,
  author       = {Wu, Ming Long and Chien, Jong Chih and Wu, Chieh Tsai and Lee, Jiann Der},
  year         = {2018},
  journal       = {Sensors (Switzerland)},
  title        = {{An augmented reality system using improved-iterative closest point algorithm for on-patient medical image visualization}},
  doi          = {10.3390/s18082505},
  issn         = {14248220},
  number       = {8},
  volume       = {18},
  abstract     = {In many surgery assistance systems, cumbersome equipment or complicated algorithms are often introduced to build the whole system. To build a system without cumbersome equipment or complicated algorithms, and to provide physicians the ability to observe the location of the lesion in the course of surgery, an augmented reality approach using an improved alignment method to image-guided surgery (IGS) is proposed. The system uses RGB-Depth sensor in conjunction with the Point Cloud Library (PCL) to build and establish the patient's head surface information, and, through the use of the improved alignment algorithm proposed in this study, the preoperative medical imaging information obtained can be placed in the same world-coordinates system as the patient's head surface information. The traditional alignment method, Iterative Closest Point (ICP), has the disadvantage that an ill-chosen starting position will result only in a locally optimal solution. The proposed improved para-alignment algorithm, named improved-ICP (I-ICP), uses a stochastic perturbation technique to escape from locally optimal solutions and reach the globally optimal solution. After the alignment, the results will be merged and displayed using Microsoft's HoloLens Head-Mounted Display (HMD), and allows the surgeon to view the patient's head at the same time as the patient's medical images. In this study, experiments were performed using spatial reference points with known positions. The experimental results show that the proposed improved alignment algorithm has errors bounded within 3 mm, which is highly accurate.},
  annotation   = {This},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2018 - An augmented reality system using improved-iterative closest point algorithm for on-patient medical image visualizati.pdf:pdf},
}

@PhdThesis{,
  institution = {The University of South Australia},
  title       = {{TOWARD CONSISTENT MEASUREMENT OF COGNITIVE LOAD IN AUGMENTED REALITY RESEARCH}},
  abstract    = {There are many domains of employment in which workers are re- quired to repeatedly perform a procedure or sequence of tasks, guided by some form of instruction. That instruction may be delivered by means of a paper-based manual, an instructional animation or video, or in person, to give some examples. Augmented reality has shown its efficacy for amplifying human performance on procedural tasks, such as those that may be found in manufacturing or the operation of machinery. This dissertation extends the scope of existing knowledge in this space by looking at methodologies by which the effectiveness of augmented reality for procedural task instruction can be measured. One of the key theories investigated in this dissertation is cognitive load theory. The purpose of this focus is to deeply investigate how cognitive load methods can be applied to augmented reality research and contribute to answering a broad question of why augmented re- ality instruction works. Through a series of experiments, performance on a simple procedu- ral task was measured with various methodologies. The experiments all juxtapose augmented reality and non-augmented reality display conditions for presenting instructions while performing the proce- dural task. Across all the experiments, the collected metrics include response time, error rates, questionnaire responses, secondary task performance, and neurological measures. Each of these show that augmented reality can not only lead to performance gains on a task, but that it also leads to lower cognitive load. More than this, however, the dissertation presents a whole gamut of cognitive load measures that have been validated to be sensitive to cognitive load changes in response to augmented reality instruction techniques. This gives fu- ture augmented reality researchers the opportunity to implement any individual or combination of measures to improve the quality and ex- planatory power of their experiments. Presented in this dissertation are the findings of a literature review that identified a need to provide some standard approaches for mea- suring cognitive load in augmented reality research. Secondly, a meta- analysis of all uses of the dome procedural task revealed the task to be a reliable measure of both augmented reality display performance and the cognitive load of the user. Lastly, four experiments incorpo- rated the task and the findings of the literature review to show how well each cognitive load measure performed under various experi- mental conditions. Overall, this dissertation presents an overview of how cognitive load theory can be effectively incorporated into aug- mented reality research.},
  file        = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - TOWARD CONSISTENT MEASUREMENT OF COGNITIVE LOAD IN AUGMENTED REALITY RESEARCH.pdf:pdf},
}

@Article{Xie2017,
  author       = {Xie, Tian and Islam, Mohammad M. and Lumsden, Alan B. and Kakadiaris, Ioannis A.},
  year         = {2017},
  journal       = {Adjunct Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2017},
  title        = {{Holographic iRay: Exploring Augmentation for Medical Applications}},
  doi          = {10.1109/ISMAR-Adjunct.2017.73},
  number       = {November},
  pages        = {220--222},
  abstract     = {A Holographic iRay prototype focusing on medical augmented reality is presented. The prototype is built using Microsoft HoloLens and Unity engine based on a previous iRay system built for iPad. A human subject is scanned using magnetic resonance imaging and the torso surface is pre-operatively segmented for 3D registration. The registration is performed using the iterative closest point algorithm between the pre-operative torso surface and the active torso surface mesh provided by the HoloLens spatial mapping and the gaze interaction. A scanning box is visualized at the gazing point to help the user select the targeting area. The pre-operative torso surface and the sampled active vertices within the scanning box are additionally overlaid in the box as the auxiliary information for guidance. Several simple interactions are designed to control the rendering of the inner organs after the registration. The experimental results demonstrate the potential of the Holographic iRay for medical applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xie et al. - 2017 - Holographic iRay Exploring Augmentation for Medical Applications.pdf:pdf},
  isbn         = {9780769563275},
  keywords     = {HMD,HoloLens,ICP,Markerless Registration,Medical AR,Spatial Mapping},
}

@Article{Jones2016,
  author       = {Jones, J. Adam and Edewaard, Darlene and Tyrrell, Richard A. and Hodges, Larry F.},
  year         = {2016},
  journal       = {2016 IEEE Symposium on 3D User Interfaces, 3DUI 2016 - Proceedings},
  title        = {{A schematic eye for virtual environments}},
  doi          = {10.1109/3DUI.2016.7460055},
  number       = {March 2016},
  pages        = {221--230},
  abstract     = {This paper presents a schematic eye model designed for use by virtual environments researchers and practitioners. This model, based on a combination of several ophthalmic models, attempts to very closely approximate a user's optical centers and intraocular separation using as little as a single measurement of pupillary distance (PD). Typically, these parameters are loosely approximated based on the PD of the user while converged to some known distance. However, this may not be sufficient for users to accurately perform spatially sensitive tasks in the near field. We investigate this possibility by comparing the impact of several common PD-based models and our schematic eye model on users' ability to accurately match real and virtual targets in depth. This was done using a specially designed display and robotic positioning apparatus that allowed sub-millimeter measurement of target positions and user responses. We found that the schematic eye model resulted in significantly improved real to virtual matches with average accuracy, in some cases, well under 1mm. We also present a novel, low-cost method of accurately measuring PD using an off-the-shelf trial frame and pinhole filters. We validated this method by comparing its measurements against those taken using an ophthalmic autorefractor. Significant differences were not found between the two methods.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Drouin, Kersten-Oertel, Collins - 2015 - Interaction-Based Registration Correction for Improved Augmented Reality Overlay in Neurosurger.pdf:pdf},
  isbn         = {9781509008421},
  keywords     = {Computer methodologies [Computer graphics]: Graphi,Human-centered Computing [Human computer interacti},
}

@Article{Schmidt2016,
  author       = {Schmidt, Susanne and Bruder, Gerd and Steinicke, Frank},
  year         = {2016},
  journal       = {2016 IEEE VR 2016 Workshop on Perceptual and Cognitive Issues in AR, PERCAR 2016},
  title        = {{Illusion of depth in spatial augmented reality}},
  doi          = {10.1109/PERCAR.2016.7562417},
  pages        = {1--6},
  abstract     = {Spatial augmented reality (SAR) is an emerging paradigm that differs from its origin, the traditional augmented reality (AR), in many regards. While traditional AR is a well-studied field of research, the characteristic features of SAR and their implications on the perception of spatial augmented environments have not been analyzed so far. In this paper, we present one of the first studies, which investigates the perceived spatial relationships between a user and their SAR environment. The results indicate that perceived depth of realworld objects can be manipulated by projecting illusions, such as color or blur effects, onto their surfaces. For the purpose of evaluating and comparing the illusions of interest, we developed a prototypic setup for conducting perceptual SAR experiments. Since this testing environment differs significantly from its counterparts in virtual and augmented reality, we also discuss potential challenges, which arise from the nature of SAR experiments.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt, Bruder, Steinicke - 2016 - Illusion of depth in spatial augmented reality.pdf:pdf},
  isbn         = {9781509008384},
  keywords     = {H.5.1 [Information Interfaces and Presentation]: M,and virtual realities,augmented},
  publisher    = {IEEE},
}

@Article{Dubel2015,
  author       = {Dubel, Steve and Rohlig, Martin and Schumann, Heidrun and Trapp, Matthias},
  year         = {2015},
  journal       = {2014 IEEE VIS International Workshop on 3DVis, 3DVis 2014},
  title        = {{2D and 3D presentation of spatial data: A systematic review}},
  doi          = {10.1109/3DVis.2014.7160094},
  pages        = {11--18},
  abstract     = {The question whether to use 2D or 3D for data visualization is generally difficult to decide. Two-dimensional and three-dimensional visualization techniques exhibit different advantages and disadvantages related to various perceptual and technical aspects such as occlusion, clutter, distortion, or scalability. To facilitate problem understanding and comparison of existing visualization techniques with regard to these aspects, this report introduces a systematization based on presentation characteristics. It enables a categorization with respect to combinations of static 2D and 3D presentations of attributes and their spatial reference. Further, it complements existing systematizations of data in an effort to formalize a common terminology and theoretical framework for this problem domain. We demonstrate our approach by reviewing different visualization techniques of spatial data according to the presented systematization.},
  annotation   = {Based on graphical reprentations but it looks good.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dubel et al. - 2015 - 2D and 3D presentation of spatial data A systematic review.pdf:pdf},
  isbn         = {9781479968268},
  keywords     = {H.5.2 [Information Interfaces and Presentation]: U},
}

@book{Marriott,
author = {Marriott, Kim and Schreiber, Falk and Dwyer, Tim and Klein, Karsten and Henry, Nathalie and Takayuki, Riche and Stuerzlinger, Wolfgang and Thomas, Bruce H and Hutchison, David},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marriott et al. - Unknown - Immersive Analytics 123.pdf:pdf},
isbn = {9783030013875},
title = {{Immersive Analytics 123}},
publisher = {Springer},
bookTitle = {Immersive Analytics},
year = {2018}
}

@Article{Sun2019a,
  author       = {Sun, Junwei and Stuerzlinger, Wolfgang},
  year         = {2019},
  journal       = {Proceedings - Graphics Interface},
  title        = {{Selecting and sliding hidden objects in 3D desktop environments}},
  issn         = {07135424},
  volume       = {2019-May},
  abstract     = {Selecting and positioning objects in 3D space are fundamental tasks in 3D user interfaces. We present two new techniques to improve 3D selection and positioning. We first augment 3D user interfaces with a new technique that enables users to select objects that are hidden from the current viewpoint. This layer-based technique for selecting hidden objects works for arbitrary objects and scenes. We then also extend a mouse-based sliding technique to work even if the manipulated object is hidden behind other objects, by making the manipulated object always fully visible through a transparency mask during drag-and-drop positioning. Our user study shows that with the new techniques, users can easily select hidden objects and that sliding with transparency performs faster than the common 3D widgets technique.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun, Stuerzlinger - 2019 - Selecting and sliding hidden objects in 3D desktop environments.pdf:pdf},
  isbn         = {9780994786845},
  keywords     = {3D interaction,3D positioning,Selection,Transparency},
}

@Article{Preim2018,
  author       = {Preim, Bernhard and Saalfeld, Patrick},
  year         = {2018},
  journal       = {Computers and Graphics (Pergamon)},
  title        = {{A survey of virtual human anatomy education systems}},
  doi          = {10.1016/j.cag.2018.01.005},
  issn         = {00978493},
  pages        = {132--153},
  url          = {https://doi.org/10.1016/j.cag.2018.01.005},
  volume       = {71},
  abstract     = {This survey provides an overview of visualization and interaction techniques developed for anatomy education. Besides individual techniques, the integration into virtual anatomy systems is considered. Web-based systems play a crucial role to enable learning independently at any time and space. We consider the educational background, the underlying data, the model generation as well as the incorporation of textual components, such as labels and explanations. Finally, stereoscopic devices and first immersive VR solutions are discussed. The survey comprises also evaluation studies that analyze the learning effectiveness.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preim, Saalfeld - 2018 - A survey of virtual human anatomy education systems.pdf:pdf},
  keywords     = {Medical visualization,Virtual anatomy},
  publisher    = {Elsevier Ltd},
}

@Article{Roettl2018,
  author       = {Roettl, Johanna and Terlutter, Ralf},
  year         = {2018},
  journal       = {PLoS ONE},
  title        = {{The same video game in 2D, 3D or virtual reality – How does technology impact game evaluation and brand placements?}},
  doi          = {10.1371/journal.pone.0200724},
  issn         = {19326203},
  number       = {7},
  pages        = {1--24},
  volume       = {13},
  abstract     = {Video game technology is changing from 2D to 3D and virtual reality (VR) graphics. In this research, we analyze how an identical video game that is either played in a 2D, stereoscopic 3D or Head-Mounted-Display (HMD) VR version is experienced by the players, and how brands that are placed in the video game are affected. The game related variables, which are analyzed, are presence, attitude towards the video game and arousal while playing the video game. Brand placement related variables are attitude towards the placed brands and memory (recall and recognition) for the placed brands. 237 players took part in the main study and played a jump'n'run game consisting of three levels. Results indicate that presence was higher in the HMD VR than in the stereoscopic 3D than in the 2D video game, but neither arousal nor attitude towards the video game differed. Memory for the placed brands was lower in the HMD VR than in the stereoscopic 3D than in the 2D video game, whereas attitudes towards the brands were not affected. A post hoc study (n = 53) shows that cognitive load was highest in the VR game, and lowest in the 3D game. Subjects reported higher levels of dizziness and motion-sickness in the VR game than in the 3D and in the 2D game. Limitations are addressed and implications for researchers, marketers and video game developers are outlined.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roettl, Terlutter - 2018 - The same video game in 2D, 3D or virtual reality – How does technology impact game evaluation and brand place.pdf:pdf},
  isbn         = {1111111111},
}

@Article{Moreton2001,
  author       = {Moreton, H.},
  year         = {2001},
  journal       = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
  title        = {{Watertight tessellation using forward differencing}},
  doi          = {10.1145/383507.383520},
  number       = {WORKSHOP},
  pages        = {25--32},
  abstract     = {In this paper we describe an algorithm and hardware for the tessellation of polynomial surfaces. While conventional forward difference-based tessellation is subject to round off error and cracking, our algorithm produces a bit-for-bit consistent triangle mesh across multiple independently tessellated patches. We present tessellation patterns that exploit the efficiency of iterative evaluation techniques while delivering a defect free adaptive tessellation with continuous level-of-detail. We also report the rendering performance of the resulting physical hardware implementation.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moreton - 2001 - Watertight tessellation using forward differencing.pdf:pdf},
  isbn         = {158113407X},
  keywords     = {CAD,Curves \& Surfaces,Geometric Modeling,Graphics Hardware,Hardware Systems,Rendering Hardware},
}

@Article{Macedo2014,
  author       = {Macedo, Marcio C.F. and Apolinario, Antonio L.},
  year         = {2014},
  journal       = {Brazilian Symposium of Computer Graphic and Image Processing},
  title        = {{Improving On-Patient Medical Data Visualization in a Markerless Augmented Reality Environment by Volume Clipping}},
  doi          = {10.1109/SIBGRAPI.2014.33},
  issn         = {15301834},
  pages        = {149--156},
  volume       = {Cli},
  abstract     = {To improve the human perception of an augmented reality scene, its virtual and real entities can be rendered according to the focus+context visualization. This paradigm is specially important in the field of on-patient medical data visualization, as it provides insight to the physicians about the spatial relations between the patient's anatomy (focus region) and his entire body (context region). However, the current existing methods proposed in this field do not give special treatment to the effect of volume clipping, which can open new ways for physicians to explore and understand the entire scene. In this paper we introduce an on-patient focus+context medical data visualization based on volume clipping. It is proposed in a markerless augmented reality environment. From the estimated camera pose, the volumetric medical data can be displayed to a physician inside the patient's anatomy at the location of the real anatomy. To improve the visual quality of the final scene, three methods based on volume clipping are proposed to allow new focus+context visualizations. Moreover, the whole solution supports occlusion handling. From the evaluation of the proposed techniques, the results obtained highlight that these methods improve the visual quality of the final rendering. Furthermore, the application still runs in realtime.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Macedo, Apolinario - 2014 - Improving On-Patient Medical Data Visualization in a Markerless Augmented Reality Environment by Volume Clip.pdf:pdf},
  isbn         = {9781479942602},
  keywords     = {Augmented Reality,Focus+Context Visualization,Volume Clipping,Volume Rendering},
}

@Article{Tachikawa2012,
  author       = {Tachikawa, Tomoya and Hara, Takenori and Toyono, Chiho and Motai, Goro and Iwazaki, Karin and Shuto, Keisuke and Uchiyama, Hiroko and Yoshimura, Sakuji},
  year         = {2012},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{Semi-transparent augmented reality system}},
  doi          = {10.1007/978-3-642-34292-9-61},
  issn         = {03029743},
  pages        = {569--572},
  volume       = {7624 LNCS},
  abstract     = {We have developed a new Semi-Transparent Augmented Reality (AR) system that displays the inner structures of objects by making their surface semi-transparent In this system we combine the live video of the object of interest and 3D computer graphics (3DCG) models with appropriate transparency and in proper order using AR technology This system shows the 3DCG models of inner structures as if they existed inside the object {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tachikawa et al. - 2012 - Semi-transparent augmented reality system.pdf:pdf},
  isbn         = {9783642342912},
  keywords     = {AR,Exhibition,Transparency},
}

@Article{Kitajima2015,
  author       = {Kitajima, Yuki and Ikeda, Sei and Sato, Kosuke},
  year         = {2015},
  journal       = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015},
  title        = {{[POSTER] Vergence-based AR X-ray vision}},
  doi          = {10.1109/ISMAR.2015.58},
  pages        = {188--189},
  abstract     = {The ideal AR x-ray vision should enable users to clearly observe and grasp not only occludees, but also occluders. We propose a novel selective visualization method of both occludee and oc-cluder layers with dynamic opacity depending on the user's gaze depth. Using the gaze depth as a trigger to select the layers has a essential advantage over using other gestures or spoken commands in the sense of avoiding collision between user's intentional commands and unintentional actions. Our experiment by a visual paired-comparison task shows that our method has achieved a 20\% higher success rate, and significantly reduced 30\% of the average task completion time than a non-selective method using a constant and half transparency.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitajima, Ikeda, Sato - 2015 - POSTER Vergence-based AR X-ray vision.pdf:pdf},
  isbn         = {9781467376600},
  keywords     = {Augmented reality,Gaze-contingent display,Ghosted views,Vergence,Visibility,X-ray vision},
  publisher    = {IEEE},
}

@Article{Dey2014,
  author       = {Dey, Arindam and Sandor, Christian},
  year         = {2014},
  journal       = {International Journal of Human Computer Studies},
  title        = {{Lessons learned: Evaluating visualizations for occluded objects in handheld augmented reality}},
  doi          = {10.1016/j.ijhcs.2014.04.001},
  issn         = {10959300},
  number       = {10-11},
  pages        = {704--716},
  url          = {http://dx.doi.org/10.1016/j.ijhcs.2014.04.001},
  volume       = {72},
  abstract     = {Handheld devices like smartphones and tablets have emerged as one of the most promising platforms for Augmented Reality (AR). The increased usage of these portable handheld devices has enabled handheld AR applications to reach the end-users; hence, it is timely and important to seriously consider the user experience of such applications. AR visualizations for occluded objects enable an observer to look through objects. AR visualizations have been predominantly evaluated using Head-Worn Displays (HWDs), handheld devices have rarely been used. However, unless we gain a better understanding of the perceptual and cognitive effects of handheld AR systems, effective interfaces for handheld devices cannot be designed. Similarly, human perception of AR systems in outdoor environments, which provide a higher degree of variation than indoor environments, has only been insufficiently explored. In this paper, we present insights acquired from five experiments we performed using handheld devices in outdoor locations. We provide design recommendations for handheld AR systems equipped with visualizations for occluded objects. Our key conclusions are the following: (1) Use of visualizations for occluded objects improves the depth perception of occluded objects akin to non-occluded objects. (2) To support different scenarios, handheld AR systems should provide multiple visualizations for occluded objects to complement each other. (3) Visual clutter in AR visualizations reduces the visibility of occluded objects and deteriorates depth judgment; depth judgment can be improved by providing clear visibility of the occluded objects. (4) Similar to virtual reality interfaces, both egocentric and exocentric distances are underestimated in handheld AR. (5) Depth perception will improve if handheld AR systems can dynamically adapt their geometric field of view (GFOV) to match the display field of view (DFOV). (6) Large handheld displays are hard to carry and use; however, they enable users to better grasp the depth of multiple graphical objects that are presented simultaneously. {\textcopyright} 2014 Elsevier Ltd.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey, Sandor - 2014 - Lessons learned Evaluating visualizations for occluded objects in handheld augmented reality.pdf:pdf},
  keywords     = {Augmented reality,Experiments,Handheld devices,Outdoor environments,Visualizations for occluded objects},
  publisher    = {Elsevier},
}

@Article{Ahn2018,
  author       = {Ahn, Euijai and Lee, Sungkil and Kim, Gerard Jounghyun},
  year         = {2018},
  journal       = {Virtual Reality},
  title        = {{Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality}},
  doi          = {10.1007/s10055-017-0319-y},
  issn         = {14349957},
  number       = {3},
  pages        = {245--262},
  volume       = {22},
  abstract     = {Augmented reality (AR) “augments” virtual information over the real-world medium and is emerging as an important type of an information visualization technique. As such, the visibility and readability of the augmented information must be as high as possible amidst the dynamically changing real-world surrounding and background. In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real-world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object's representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making real-time computation possible. While the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the “optimal” ground truth in terms of correctly perceiving and recognizing the augmented information. In addition, we also present another experiment that explores in what fashion the proposed algorithm can be applied in actual AR applications. The results suggested that the users clearly preferred the automatic contrast modulation upon large movements in the scenery.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahn, Lee, Kim - 2018 - Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality.pdf:pdf},
  keywords     = {Augmented reality,Contrast,Human perception and performance,Saliency,See-through display},
  publisher    = {Springer London},
}

@Article{Wu2018a,
  author       = {Wu, Meng Lin and Popescu, Voicu},
  year         = {2018},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Efficient VR and AR Navigation Through Multiperspective Occlusion Management}},
  doi          = {10.1109/TVCG.2017.2778249},
  issn         = {19410506},
  number       = {12},
  pages        = {3069--3080},
  volume       = {24},
  abstract     = {Immersive navigation in virtual reality (VR) and augmented reality (AR) leverages physical locomotion through pose tracking of the head-mounted display. While this navigation modality is intuitive, regions of interest in the scene may suffer from occlusion and require significant viewpoint translation. Moreover, limited physical space and user mobility need to be taken into consideration. Some regions of interest may require viewpoints that are physically unreachable without less intuitive methods such as walking in-place or redirected walking. We propose a novel approach for increasing navigation efficiency in VR and AR using multiperspective visualization. Our approach samples occluded regions of interest from additional perspectives, which are integrated seamlessly into the user's perspective. This approach improves navigation efficiency by bringing simultaneously into view multiple regions of interest, allowing the user to explore more while moving less. We have conducted a user study that shows that our method brings significant performance improvement in VR and AR environments, on tasks that include tracking, matching, searching, and ambushing objects of interest.},
  annotation   = {So this paper is focusing on how to occulde virtual objects in AR not so much X-ray vision but it does make a good case for areas of use of X-ray vision},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Popescu - 2018 - Efficient VR and AR Navigation Through Multiperspective Occlusion Management.pdf:pdf},
  keywords     = {Augmented reality,depth cues,multiperspective visualization,navigation,occlusion management,virtual reality},
  publisher    = {IEEE},
}

@Book{McLaughlin2018,
  author    = {McLaughlin, Anne Collins and Matalenas, Laura A. and Coleman, Maribeth Gandy},
  year      = {2018},
  title     = {{Design of human centered augmented reality for managing chronic health conditions}},
  doi       = {10.1016/B978-0-12-811272-4.00011-7},
  isbn      = {9780128112731},
  pages     = {261--296},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/B978-0-12-811272-4.00011-7},
  abstract  = {Augmented reality (AR) has the potential to support older people with chronic health conditions. However, despite the engineering advances, more attention is needed for the human-computer interaction (HCI) and human factors aspects of AR. We highlight one health area of particular complexity and importance for older adults, type 2 diabetes, and show how AR could be applied to assist older patients beyond the reach of current health technologies. We draw from the literature on the HCI of AR and combine it with the well-established literature of age-related changes in perception, cognition, and movement-control to produce draft principles for the design of AR to benefit health at older ages.},
  booktitle = {Aging, Technology and Health},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McLaughlin, Matalenas, Coleman - 2018 - Design of human centered augmented reality for managing chronic health conditions.pdf:pdf},
  keywords  = {Age-related change,Augmented reality,HCI,Health self-management,Mixed reality,Simulations},
}

@Article{DeyArindamandJarvisGraemeandSandorChristianandWibowoAriawanandVilleVeikko2011,
  author       = {{Dey, Arindam and Jarvis, Graeme and Sandor, Christian and Wibowo, Ariawan and Ville-Veikko}, Mattila},
  year         = {2011},
  journal       = {In Proceedings of International Conference on Artificial Reality and Telexistence},
  title        = {{An Evaluation of Augmented Reality X-Ray Vision for Outdoor Navigation}},
  pages        = {28--32},
  abstract     = {During the last decade, pedestrian navigation applications on mobile phones have become commonplace; most of them provide a birds-eye view of the environment. Recently, mobile Augmented Reality (AR) browsers have become popular, providing a complementary, egocentric view of where points of interest are located in the environment. As points of interest are often occluded by realworld objects, we previously developed a mobile AR X-ray system, which enables users to look through occluders. We present an evaluation that compares it with two standard pedestrian navigation applications (North-up and View-up map). Participants had to walk a 900 meter route with three checkpoints along the path. Our main findings are based on the analysis of recorded videos. We could show that the number of context switches is significantly lowest in the AR X-ray condition. We believe that this finding provides useful design constraints for any developer of mobile navigation applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey, Arindam and Jarvis, Graeme and Sandor, Christian and Wibowo, Ariawan and Ville-Veikko - 2011 - An Evaluation of Augmented Reality X.pdf:pdf},
  keywords     = {aug-,augmented reality,evaluation,map,mented reality x-ray,mobile phone,navigation,visualization},
}

@Article{Young2016,
  author       = {Young, Gareth and Kehoe, Aidan and Murphy, David},
  year         = {2016},
  journal       = {Games User Research},
  title        = {{Usability Testing of Video Game Controllers: A Case Study}},
  doi          = {10.1201/b21564-8},
  number       = {May},
  pages        = {145--188},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Young, Kehoe, Murphy - 2016 - Usability Testing of Video Game Controllers A Case Study.pdf:pdf},
}

@Article{Computing2017,
  author = {Computing, Mobile and Praveena, P},
  year   = {2017},
  title  = {{V6I2201709}},
  number = {2},
  pages  = {41--50},
  volume = {6},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Computing, Praveena - 2017 - V6I2201709.pdf:pdf},
}

@Article{Ткач,
  author = {Ткач, М},
  title  = {{A Method for Registration of 3D-D Shapes}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Besl, Mckay - 1992 - A method for registration of 3-D shapes.pdf:pdf},
}

@Article{Hinkle1989,
  author       = {Hinkle, Steve and Taylor, Laurie A. and Fox‐Cardamone, D. Lee and Crook, Kimberly F.},
  year         = {1989},
  journal       = {British Journal of Social Psychology},
  title        = {{Intragroup identification and intergroup differentiation: A multicomponent approach}},
  doi          = {10.1111/j.2044-8309.1989.tb00874.x},
  issn         = {20448309},
  number       = {4},
  pages        = {305--317},
  volume       = {28},
  abstract     = {The factor structure of an intragroup identification scale was examined. Subjects participating in small groups worked on a decision‐making task, completed the identification measure and made in‐group and out‐group evaluations. Three factors reflecting emotional, cognitive and individual/group interdependence aspects of identification accounted for 70.5 percent of the scale's total variance. Based upon these results, the identification scale was decomposed into three subscales. Correlations between subscale scores and differentiation were positive, but small in magnitude (range 0.23–0.28). In contrast, subscale scores were more strongly associated with the in‐group aspect of differentiation (rs from 0.37 to 0.62). Implications of the findings for social identity theory are discussed along with the value of multicomponent conceptualizations and methodologies. 1989 The British Psychological Society},
  annotation   = {So this paper talks about},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinkle et al. - 1989 - Intragroup identification and intergroup differentiation A multicomponent approach.pdf:pdf},
}

@article{Prof,
author = {Prof, Supervisor and Billinghurst, Mark and Lee, Co-supervisor Gun and Jing, Allison},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Prof et al. - Unknown - Exploration of Gaze Cues in Mixed Reality Remote Collaboration.pdf:pdf},
title = {{Exploration of Gaze Cues in Mixed Reality Remote Collaboration}}
}

@Article{Lewis2018,
  author       = {Lewis, James R},
  year         = {2018},
  journal       = {International Journal of Human-Computer Interaction},
  title        = {{The System Usability Scale: Past, Present, and Future}},
  doi          = {10.1080/10447318.2018.1455307},
  issn         = {1044-7318},
  number       = {7},
  pages        = {577},
  volume       = {34},
  abstract     = {The System Usability Scale (SUS) is the most widely used standardized questionnaire for the assessment of perceived usability. This review of the SUS covers its early history from inception in the 1980s through recent research and its future prospects. From relatively inauspicious beginnings,...},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewis - 2018 - The System Usability Scale Past, Present, and Future.pdf:pdf},
  keywords     = {Computer Science,Perceived Usability,Standardized Usability Scale,Sus,System Usability},
}

@Article{Maruhn2019,
  author       = {Maruhn, Philipp and Schneider, Sonja and Bengler, Klaus},
  year         = {2019},
  journal       = {PLoS ONE},
  title        = {{Measuring egocentric distance perception in virtual reality: Influence of methodologies, locomotion and translation gains}},
  doi          = {10.1371/journal.pone.0224651},
  issn         = {19326203},
  number       = {10},
  pages        = {1--24},
  url          = {http://dx.doi.org/10.1371/journal.pone.0224651},
  volume       = {14},
  abstract     = {Virtual reality has become a popular means to study human behavior in a wide range of settings, including the role of pedestrians in traffic research. To understand distance perception in virtual environments is thereby crucial to the interpretation of results, as reactions to complex and dynamic traffic scenarios depend on perceptual processes allowing for the correct anticipation of future events. A number of approaches have been suggested to quantify perceived distances. While previous studies imply that the selected method influences the estimates' accuracy, it is unclear how the respective estimates depend on depth information provided by different perceptual modalities. In the present study, six methodological approaches were compared in a virtual city scenery. The respective influence of visual and non-visual cues was investigated by manipulating the ratio between visually perceived and physically walked distances. In a repeated measures design with 30 participants, significant differences between methods were observed, with the smallest error occurring for visually guided walking and verbal estimates. A linear relation emerged between the visual-to-physical ratio and the extent of underestimation, indicating that non-visual cues during walking affected distance estimates. This relationship was mainly evident for methods building on actual or imagined walking movements and verbal estimates.},
  annotation   = {This paper notes that movement may play a huge role in this work A major finding was that VR users walk less then they think they are.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maruhn, Schneider, Bengler - 2019 - Measuring egocentric distance perception in virtual reality Influence of methodologies, locomotion a.pdf:pdf},
  isbn         = {1111111111},
}

@PhdThesis{Hamadouche2018a,
  author   = {Hamadouche, Ilyas},
  year     = {2018},
  title    = {{Ilyas Hamadouche Augmented Reality X-ray vision on Optical See-Through Head Mounted}},
  pages    = {58},
  url      = {https://pdfs.semanticscholar.org/de04/a705f11ebdb9a7e913e5dd86914d346a9444.pdf},
  abstract = {In this thesis, we present the development and evaluation of an augmented reality X-ray system on optical see-through head-mounted displays. Augmented reality X-ray vision allows users to see through solid surfaces such as walls and facades,by augmenting the real view with virtual images representing the hidden objects. Our system is developed based on the optical see-through mixed reality headset Microsoft Hololens. We have developed an X-ray cutout algorithm that uses thegeometric data of the environment and enables seeing through surfaces. We have developed four different visualizations as well based on the algorithm. The first visualization renders simply the X-ray cutout without displaying any information about the occluding surface. The other three visualizations display features extracted from the occluder surface to help the user to get better depth perceptionof the virtual objects. We have used Sobel edge detection to extract the information. The three visualizations differ in the way to render the extracted features. A subjective experiment is conducted to test and evaluate the visualizations and to compare them with each other. The experiment consists of two parts; depthestimation task and a questionnaire. Both the experiment and its results are pre-sented in the thesis.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamadouche - 2018 - Ilyas Hamadouche Augmented Reality X-ray vision on Optical See-Through Head Mounted.pdf:pdf},
  keywords = {augmented reality,mixed reality,optical see-through,x-ray vision},
  number   = {June},
}

@Article{Mori2018,
  author       = {Mori, Shohei and Saito, Hideo},
  year         = {2018},
  journal       = {APSIPA Transactions on Signal and Information Processing},
  title        = {{An overview of augmented visualization: Observing the real world as desired}},
  doi          = {10.1017/ATSIP.2018.13},
  issn         = {20487703},
  number       = {2018},
  volume       = {7},
  abstract     = {Over 20 years have passed since a free-viewpoint video technology has been proposed with which a user's viewpoint can be freely set up in a reconstructed three-dimensional space of a target scene photographed by multi-view cameras. This technology allows us to capture and reproduce the real world as recorded. Once we capture the world in a digital form, we can modify it as augmented reality (i.e., placing virtual objects in the digitized real world). Unlike this concept, the augmented world allows us to see through real objects by synthesizing the backgrounds that cannot be observed in our raw perspective directly. The key idea is to generate the background image using multi-view cameras, observing the backgrounds at different positions and seamlessly overlaying the recovered image in our digitized perspective. In this paper, we review such desired view-generation techniques from the perspective of free-view point image generation and discuss challenges and open problems through a case study of our implementations.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mori, Saito - 2018 - An overview of augmented visualization Observing the real world as desired.pdf:pdf},
  keywords     = {Augmented reality,Diminished reality,Free-view point image generation,Human-computer interaction,Overview},
}

@Article{Kytoe2018,
  author       = {Kyt{\"{o}}, Mikko and Ens, Barrett and Piumsomboon, Thammathip and Lee, Gun A. and Billinghurst, Mark},
  year         = {2018},
  journal       = {Conference on Human Factors in Computing Systems - Proceedings},
  title        = {{Pinpointing: Precise head- and eye-based target selection for augmented reality}},
  doi          = {10.1145/3173574.3173655},
  pages        = {1--14},
  volume       = {2018-April},
  abstract     = {Head and eye movement can be leveraged to improve the user's interaction repertoire for wearable displays. Head movements are deliberate and accurate, and provide the current state-of-the-art pointing technique. Eye gaze can potentially be faster and more ergonomic, but suffers from low accuracy due to calibration errors and drift of wearable eye-tracking sensors. This work investigates precise, multimodal selection techniques using head motion and eye gaze. A comparison of speed and pointing accuracy reveals the relative merits of each method, including the achievable target size for robust selection. We demonstrate and discuss example applications for augmented reality, including compact menus with deep structure, and a proof-of-concept method for on-line correction of calibration drift.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kyt{\"{o}} et al. - 2018 - Pinpointing Precise head- and eye-based target selection for augmented reality.pdf:pdf},
  isbn         = {9781450356206},
  keywords     = {Augmented reality,Eye tracking,Gaze interaction,Head-worn display,Refinement techniques,Target selection},
}

@Article{Collins2017,
  author       = {Collins, Jonny and Regenbrecht, Holger and Langlotz, Tobias},
  year         = {2017-02},
  journal       = {Presence: Teleoperators and Virtual Environments},
  title        = {{Visual Coherence in Mixed Reality: A Systematic Enquiry}},
  doi          = {10.1162/PRES_a_00284},
  number       = {1},
  pages        = {16--41},
  url          = {https://doi.org/10.1162/PRES_a_00284},
  volume       = {26},
  abstract     = {Virtual and augmented reality, and other forms of mixed reality (MR), have become a focus of attention for companies and researchers. Before they can become successful in the market and in society, those MR systems must be able to deliver a convincing, novel experience for the users. By definition, the experience of mixed reality relies on the perceptually successful blending of reality and virtuality. Any MR system has to provide a sensory, in particular visually coherent, set of stimuli. Therefore, issues with visual coherence, that is, a discontinued experience of a MR environment, must be avoided. While it is very easy for a user to detect issues with visual coherence, it is very difficult to design and implement a system for coherence. This article presents a framework and exemplary implementation of a systematic enquiry into issues with visual coherence and possible solutions to address those issues. The focus is set on head-mounted display-based systems, notwithstanding its applicability to other types of MR systems. Our framework, together with a systematic discussion of tangible issues and solutions for visual coherence, aims at guiding developers of mixed reality systems for better and more effective user experiences.},
  annotation   = {doi: 10.1162/PRES_a_00284 This paper has some usful notes on the current perfromace of visual effects in AR},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collins, Regenbrecht, Langlotz - 2017 - Visual Coherence in Mixed Reality A Systematic Enquiry.pdf:pdf},
  publisher    = {MIT Press},
}

@Article{Rodr2006,
  author       = {Rodr, Evelio and Kypson, Alan P and Moten, Simon C and Nifong, L Wiley and Jr, W Randolph Chitwood},
  year         = {2006},
  journal       = {International Journal},
  title        = {{Video see-through augmented reality for oral andmaxillofacial surgery}},
  doi          = {10.1002/rcs},
  number       = {April},
  pages        = {211--215},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Video see-through augmented reality for oral and maxillofacial surgery.pdf:pdf},
  keywords     = {3d-2d image registration,and,and maxillofacial surgery,augmented reality,automation,beihang university,object tracking,oral,school of mechanical engineering},
}

@Article{Wang2017,
  author       = {Wang, Junchen and Suenaga, Hideyuki and Yang, Liangjing and Kobayashi, Etsuko and Sakuma, Ichiro},
  year         = {2017-06},
  journal       = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  title        = {{Video see-through augmented reality for oral and maxillofacial surgery}},
  doi          = {10.1002/rcs.1754},
  issn         = {1478-5951},
  number       = {2},
  pages        = {e1754},
  url          = {https://doi.org/10.1002/rcs.1754},
  volume       = {13},
  abstract     = {Abstract Background Oral and maxillofacial surgery has not been benefitting from image guidance techniques owing to the limitations in image registration. Methods A real-time markerless image registration method is proposed by integrating a shape matching method into a 2D tracking framework. The image registration is performed by matching the patient's teeth model with intraoperative video to obtain its pose. The resulting pose is used to overlay relevant models from the same CT space on the camera video for augmented reality. Results The proposed system was evaluated on mandible/maxilla phantoms, a volunteer and clinical data. Experimental results show that the target overlay error is about 1?mm, and the frame rate of registration update yields 3?5 frames per second with a 4?K camera. Conclusions The significance of this work lies in its simplicity in clinical setting and the seamless integration into the current medical procedure with satisfactory response time and overlay accuracy. Copyright ? 2016 John Wiley \& Sons, Ltd.},
  annotation   = {doi: 10.1002/rcs.1754},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Video see-through augmented reality for oral and maxillofacial surgery.pdf:pdf},
  keywords     = {3D-2D image registration,augmented reality,object tracking,oral and maxillofacial surgery},
  publisher    = {John Wiley {\&} Sons, Ltd},
}

@Article{Gao2019,
  author       = {Gao, Yuan and Liu, Yue and Normand, Jean Marie and Moreau, Guillaume and Gao, Xue and Wang, Yongtian},
  year         = {2019},
  journal       = {Journal of the Society for Information Display},
  title        = {{A study on differences in human perception between a real and an AR scene viewed in an OST-HMD}},
  doi          = {10.1002/jsid.752},
  issn         = {19383657},
  number       = {3},
  pages        = {155--171},
  volume       = {27},
  abstract     = {With the recent growth in the development of augmented reality (AR) technologies, it is becoming important to study human perception of AR scenes. In order to detect whether users will suffer more from visual and operator fatigue when watching virtual objects through optical see-through head-mounted displays (OST-HMDs), compared with watching real objects in the real world, we propose a comparative experiment including a virtual magic cube task and a real magic cube task. The scores of the subjective questionnaires (SQ) and the values of the critical flicker frequency (CFF) were obtained from 18 participants. In our study, we use several electrooculogram (EOG) and heart rate variability (HRV) measures as objective indicators of visual and operator fatigue. Statistical analyses were performed to deal with the subjective and objective indicators in the two tasks. Our results suggest that participants were very likely to suffer more from visual and operator fatigue when watching virtual objects presented by the OST-HMD. In addition, the present study provides hints that HRV and EOG measures could be used to explore how visual and operator fatigue are induced by AR content. Finally, three novel HRV measures are proposed to be used as potential indicators of operator fatigue.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2019 - A study on differences in human perception between a real and an AR scene viewed in an OST-HMD.pdf:pdf},
  keywords     = {ECG,EOG,augmented reality,human perception,optical see-through HMD,visual fatigue},
}

@Article{Gibson1951,
  author       = {Gibson, James J.},
  year         = {1951},
  journal      = {The Philosophical Review},
  title        = {{The Perception of the Visual World.}},
  doi          = {10.2307/2181436},
  issn         = {00318108},
  number       = {4},
  pages        = {594},
  volume       = {60},
  abstract     = {Relative to low scorers, high scorers on the autism-spectrum quotient (AQ) show enhanced performance on the embedded figures test and the radial frequency search task (RFST), which has been attributed to both enhanced local processing and differences in combining global percepts. We investigate the role of local and global processing further using the RFST in four experiments. High AQ adults maintained a consistent advantage in search speed across diverse target-distracter stimulus conditions. This advantage may reflect enhanced local processing of curvature in early stages of the form vision pathway and superior global detection of shape primitives. However, more probable is the presence of a superior search process that enables a consistent search advantage at both levels of processing.},
  annotation   = {This is the first time the theory that we use the distance of things traveling across the ground. Basicly because we evolved with gravity being a constant we understand it naturally but the absence of it confuses our under},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibson - 1951 - The Perception of the Visual World.pdf:pdf},
  publisher    = "Duke University Press"
}

@Article{Heinrich2019b,
  author       = {Heinrich, Florian and Bornemann, Kai and Lawonn, Kai and Hansen, Christian},
  year         = {2019},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{Depth perception in projective augmented reality: An evaluation of advanced visualization techniques}},
  doi          = {10.1145/3359996.3364245},
  abstract     = {Augmented reality (AR) is a promising tool to convey useful information at the place where it is needed. However, perceptual issues with augmented reality visualizations affect the estimation of distances and depth and thus can lead to critically wrong assumptions. These issues have been successfully investigated for video see-through modalities. Moreover, advanced visualization methods encoding depth information by displaying additional depth cues were developed. In this work, state-of-the-art visualization concepts were adopted for a projective AR setup. We conducted a user study to assess the concepts' suitability to convey depth information. Participants were asked to sort virtual cubes by using the provided depth cues. The investigated visualization concepts consisted of conventional Phong shading, a virtual mirror, depth-encoding silhouettes, pseudo-chromadepth rendering and an illustrative visualization using supporting line depth cues. Besides different concepts, we altered between a monoscopic and a stereoscopic display mode to examine the effects of stereopsis. Consistent results across variables show a clear ranking of examined concepts.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinrich et al. - 2019 - Depth perception in projective augmented reality An evaluation of advanced visualization techniques.pdf:pdf},
  isbn         = {9781450370011},
  keywords     = {Depth Perception,Distance Estimation,Projective Augmented Reality,Visualization},
  publisher    = {Association of Computing Machines}
}

@Misc{Gibby2019,
  author    = {Gibby, Jacob T. and Swenson, Samuel A. and Cvetko, Steve and Rao, Raj and Javan, Ramin},
  year      = {2019},
  title     = {{Head-mounted display augmented reality to guide pedicle screw placement utilizing computed tomography}},
  doi       = {10.1007/s11548-018-1814-7},
  abstract  = {Purpose: Augmented reality has potential to enhance surgical navigation and visualization. We determined whether head-mounted display augmented reality (HMD-AR) with superimposed computed tomography (CT) data could allow the wearer to percutaneously guide pedicle screw placement in an opaque lumbar model with no real-time fluoroscopic guidance. Methods: CT imaging was obtained of a phantom composed of L1–L3 Sawbones vertebrae in opaque silicone. Preprocedural planning was performed by creating virtual trajectories of appropriate angle and depth for ideal approach into the pedicle, and these data were integrated into the Microsoft HoloLens using the Novarad OpenSight application allowing the user to view the virtual trajectory guides and CT images superimposed on the phantom in two and three dimensions. Spinal needles were inserted following the virtual trajectories to the point of contact with bone. Repeat CT revealed actual needle trajectory, allowing comparison with the ideal preprocedural paths. Results: Registration of AR to phantom showed a roughly circular deviation with maximum average radius of 2.5 mm. Users took an average of 200 s to place a needle. Extrapolation of needle trajectory into the pedicle showed that of 36 needles placed, 35 (97\%) would have remained within the pedicles. Needles placed approximated a mean distance of 4.69 mm in the mediolateral direction and 4.48 mm in the craniocaudal direction from pedicle bone edge. Conclusion: To our knowledge, this is the first peer-reviewed report and evaluation of HMD-AR with superimposed 3D guidance utilizing CT for spinal pedicle guide placement for the purpose of cannulation without the use of fluoroscopy.},
  booktitle = {International Journal of Computer Assisted Radiology and Surgery},
  issn      = {18616429},
  number    = {3},
  volume    = {14},
}

@Article{Williams2017,
  author       = {Williams, George},
  year         = {2017-05},
  journal       = {Retina (Philadelphia, Pa.)},
  title        = {{Surgical Viewing; Do You See What I See?}},
  doi          = {10.1097/IAE.0000000000001721},
  url          = {https://plosjournal.deepdyve.com/lp/wolters-kluwer-health/surgical-viewing-do-you-see-what-i-see-h2nnmK01di?impressionId=5d9d6c9617c8c&i_medium=docview&i_campaign=recommendations&i_source=recommendations},
  volume       = {37},
}

@Article{Ma2019,
  author       = {Ma, Cong and Chen, Guowen and Zhang, Xinran and Ning, Guochen and Liao, Hongen},
  year         = {2019},
  journal       = {IEEE Journal of Biomedical and Health Informatics},
  title        = {{Moving-Tolerant Augmented Reality Surgical Navigation System Using Autostereoscopic Three-Dimensional Image Overlay}},
  doi          = {10.1109/JBHI.2018.2885378},
  issn         = {21682208},
  number       = {6},
  pages        = {2483--2493},
  volume       = {23},
  abstract     = {Augmented reality (AR) surgical navigation systems based on image overlay have been used in minimally invasive surgery. However, conventional systems still suffer from a limited viewing zone, a shortage of intuitive three-dimensional (3D) image guidance and cannot be moved freely. To fuse the 3-D overlay image with the patient in situ, it is essential to track the overlay device while it is moving. A direct line-of-sight should be maintained between the optical markers and the tracker camera. In this study, we propose a moving-tolerant AR surgical navigation system using autostereoscopic image overlay, which can avoid the use of the optical tracking system during the intraoperative period. The system captures binocular image sequences of environmental change in the operation room to locate the overlay device, rather than tracking the device directly. Therefore, it is no longer required to maintain a direct line-of-sight between the tracker and the tracked devices. The movable range of the system is also not limited by the scope of the tracker camera. Computer simulation experiments demonstrate the reliability of the proposed moving-tolerant AR surgical navigation system. We also fabricate a computer-generated integral photography-based 3-D overlay AR system to validate the feasibility of the proposed moving-tolerant approach. Qualitative and quantitative experiments demonstrate that the proposed system can always fuse the 3-D image with the patient, thus, increasing the feasibility and reliability of traditional 3-D overlay image AR surgical navigation systems.},
  annotation   = {A silvered mirror based AR system. Some of the mechanics for this are quite interesting but it proberbly is too far unrealted from your system to be of too much use.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ma et al. - 2019 - Moving-Tolerant Augmented Reality Surgical Navigation System Using Autostereoscopic Three-Dimensional Image Overlay.pdf:pdf},
  keywords     = {Minimally invasive surgery,augmented reality,integral photography,moving-tolerant},
  publisher    = {IEEE},
}

@Article{Dunn2017,
  author       = {Dunn, David and Tippets, Cary and Torell, Kent and Kellnhofer, Petr and Aksit, Kaan and Didyk, Piotr and Myszkowski, Karol and Luebke, David and Fuchs, Henry},
  year         = {2017},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Wide Field of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors}},
  doi          = {10.1109/TVCG.2017.2657058},
  issn         = {10772626},
  number       = {4},
  pages        = {1275--1284},
  volume       = {23},
  abstract     = {Accommodative depth cues, a wide field of view, and ever-higher resolutions all present major hardware design challenges for near-eye displays. Optimizing a design to overcome one of these challenges typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution - a new wide field of view, gaze-tracked near-eye display for augmented reality applications. The key component of our solution is the use of a single see-through, varifocal deformable membrane mirror for each eye reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane which is determined by the gaze tracker. The benefits of using the membranes include wide field of view (100° diagonal) and fast depth switching (from 20 cm to infinity within 300 ms). Our subjective experiment verifies the prototype and demonstrates its potential benefits for near-eye see-through displays.},
  annotation   = {This attempts to create the hardware to place an object at over the images.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dunn et al. - 2017 - Wide Field of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors.pdf:pdf},
  keywords     = {Augmented reality,displays,focus accommodation,perception,user study},
}

@Article{Kaneko2017,
  author       = {Kaneko, Naoki and Tsunoda, Mayumi and Mitsuhashi, Masatsugu and Okubo, Keisuke and Takeshima, Taro and Sehara, Yoshihide and Nagai, Mutsumi and Kawai, Kensuke},
  year         = {2017},
  journal       = {Journal of Ultrasound in Medicine},
  title        = {{Ultrasound-Guided Fine-Needle Aspiration in the Neck Region Using an Optical See-Through Head-Mounted Display: A Randomized Controlled Trial}},
  doi          = {10.1002/jum.14237},
  issn         = {15509613},
  number       = {10},
  pages        = {2071--2077},
  volume       = {36},
  abstract     = {Objectives: The purpose of this study was to examine the feasibility of an optical see-through head-mounted display (OST-HMD) to improve ergonomics during ultrasound-guided fine-needle aspiration (FNA) in the neck region. Methods: This randomized controlled study compared an OST-HMD with a normal ultrasound monitor during an ultrasound-guided FNA in the neck region. Patients with a neck tumor were recruited and randomized into one of two groups. Two practitioners performed ultrasound-guided FNA with or without the HMD, as indicated. An independent researcher measured the procedure time, the number and time of head movements, as well as the number of needle redirections. In addition, practitioners completed questionnaires after performing the FNA on each patient. Results: In 93\% of the sessions with the OST-HMD, practitioners performed ultrasound-guided FNA without turning the patients' heads. There was no difference in procedural time and number of needle redirections between the two groups. Results from the questionnaire revealed not only good wearability and low fatigue, but also the practitioners' preference for the HMD. Conclusions: The OST-HMD improved the practitioners' ergonomics and can be adopted for performing ultrasound-guided interventional procedures in clinical settings.},
  annotation   = {So this paper isn't all that novel but it does put note some issues with just using raw out put with Hololens from scan data. This paper presented a hololnes application that took ultrasound data in real time and diplayed it to the users in thier vision as a 2D image. The comberance of the Hololens and the lag between the device and the users were seens as issues. Various the study is also proberbly worth tryiing to copy.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaneko et al. - 2017 - Ultrasound-Guided Fine-Needle Aspiration in the Neck Region Using an Optical See-Through Head-Mounted Display A R.pdf:pdf},
  keywords     = {fine needle biopsy,head-mounted display,optical see-through head-mounted display,ultrasound-guided interventional procedures},
}

@Article{Chen2015,
  author       = {Chen, Xiaojun and Xu, Lu and Wang, Yiping and Wang, Huixiang and Wang, Fang and Zeng, Xiangsen and Wang, Qiugen and Egger, Jan},
  year         = {2015},
  journal       = {Journal of Biomedical Informatics},
  title        = {{Development of a surgical navigation system based on augmented reality using an optical see-through head-mounted display}},
  doi          = {10.1016/j.jbi.2015.04.003},
  issn         = {15320464},
  pages        = {124--131},
  volume       = {55},
  abstract     = {The surgical navigation system has experienced tremendous development over the past decades for minimizing the risks and improving the precision of the surgery. Nowadays, Augmented Reality (AR)-based surgical navigation is a promising technology for clinical applications. In the AR system, virtual and actual reality are mixed, offering real-time, high-quality visualization of an extensive variety of information to the users (Moussa et al., 2012) [1]. For example, virtual anatomical structures such as soft tissues, blood vessels and nerves can be integrated with the real-world scenario in real time. In this study, an AR-based surgical navigation system (AR-SNS) is developed using an optical see-through HMD (head-mounted display), aiming at improving the safety and reliability of the surgery. With the use of this system, including the calibration of instruments, registration, and the calibration of HMD, the 3D virtual critical anatomical structures in the head-mounted display are aligned with the actual structures of patient in real-world scenario during the intra-operative motion tracking process. The accuracy verification experiment demonstrated that the mean distance and angular errors were respectively 0.809. ±. 0.05. mm and 1.038°. ±. 0.05°, which was sufficient to meet the clinical requirements.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2015 - Development of a surgical navigation system based on augmented reality using an optical see-through head-mounted di.pdf:pdf},
  keywords     = {Augmented reality,Intra-operative motion tracking,Optical see-through HMD,Surgical navigation},
}

@InProceedings{Shinohara2017,
  author     = {Shinohara, Kazuhiko},
  year       = {2017},
  title      = {{Prospects and Problems of Smart Glasses as Tools for Surgery BT - Advances in Human Factors and Ergonomics in Healthcare}},
  editor     = {Duffy, Vincent G and Lightner, Nancy},
  isbn       = {978-3-319-41652-6},
  location   = {Cham},
  pages      = {39--45},
  publisher  = {Springer International Publishing},
  abstract   = {The feasibility and problems of using smart glasses (SMG) in surgery were investigated in this study. The prospects and challenges of SMG in surgery are as follows. The advantages of SMG are that various kinds of medical information can be provided, including surgical navigation and consultation with other physicians. Since SMG are hands-free, they are clean and safe to use in surgical practice. The challenges and unresolved problems of SMG are ergonomic problems such as eyestrain and fatigue, and technical problems such as Internet security and electromagnetic interference. Further collaborative investigation and development of the technology and ergonomics of SMG are important for their successful application in surgery.},
  annotation = {Note pdf exists but is the complete book which would get confusion look this up in the university system if you need help.},
}

@Article{Bimber2006,
  author       = {Bimber, Oliver and Raskar, Ramesh},
  year         = {2006},
  journal       = {SIGGRAPH 2006 - ACM SIGGRAPH 2006 Courses},
  title        = {{Modern approaches to augmented reality}},
  doi          = {10.1145/1185657.1185796},
  abstract     = {This tutorial discusses the Spatial Augmented Reality (SAR) concept, its advantages and limitations. It will present examples of state-of-the-art display configurations, appropriate real-time rendering techniques, details about hardware and software implementations, and current areas of application. Specifically, it will describe techniques for optical combination using single/multiple spatially aligned mirror-beam splitters, image sources, transparent screens and optical holograms. Furthermore, it presents techniques for projector-based augmentation of geometrically complex and textured display surfaces, and (along with optical combination) methods for achieving consistent illumination and occlusion effects. Emerging technologies that have the potential of enhancing future augmented reality displays will be surveyed.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bimber, Raskar - 2006 - Modern approaches to augmented reality.pdf:pdf},
  isbn         = {1595933646},
}

@Article{Gruenefeld2019,
  author       = {Gruenefeld, Uwe and Pr{\"{a}}del, Lars and Heuten, Wilko},
  year         = {2019},
  journal       = {ACM International Conference Proceeding Series},
  title        = {{Locating nearby physical objects in augmented reality}},
  doi          = {10.1145/3365610.3365620},
  abstract     = {Locating objects in physical environments can be an exhausting and frustrating task, particularly when these objects are out of the user's view or occluded by other objects. With recent advances in Augmented Reality (AR), these environments can be augmented to visualize objects for which the user searches. However, it is currently unclear which visualization strategy can best support users in locating these objects. In this paper, we compare a printed map to three different AR visualization strategies: (1) in-view visualization, (2) out-of-view visualization, and (3) the combination of in-view and out-of-view visualizations. Our results show that in-view visualization reduces error rates for object selection accuracy, while additional out-of-view object visualization improves users' search time performance. However, combining in-view and out-of-view visualizations leads to visual clutter, which distracts users.},
  annotation   = {So this paper sites Lilija's paper and it does a similar task but it is proberbly looking more into comparing how people can work out these things. I have simmed this paper but it seems to be more concerend with how people find objects that the visulizations that are used.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gruenefeld, Pr{\"{a}}del, Heuten - 2019 - Locating nearby physical objects in augmented reality.pdf:pdf},
  isbn         = {9781450376242},
  keywords     = {Augmented reality,Head-mounted,In-view,Occlusion,Out-of-view},
}

@InProceedings{Kiyokawa2003,
  author     = {Kiyokawa, K and Billinghurst, M and Campbell, B and Woods, E},
  booktitle  = {The Second IEEE and ACM International Symposium on Mixed and Augmented Reality, 2003. Proceedings.},
  year       = {2003},
  title      = {{An occlusion capable optical see-through head mount display for supporting co-located collaboration}},
  doi        = {10.1109/ISMAR.2003.1240696},
  isbn       = {null VO -},
  pages      = {133--141},
  abstract   = {An ideal augmented reality (AR) display for multi-user co-located collaboration should have following three features: 1) any virtual object should be able to be shown at any arbitrary position, e.g. a user can see a virtual object in front of other users' faces. 2) Correct occlusion of virtual and real objects should be supported. 3) The real world should be naturally and clearly visible, which is important for face-to-face conversation. We have been developing an optical see-through display, ELMO (Enhanced see-through display using an LCD panel for Mutual Occlusion), that satisfies these three requirements. While previous prototype systems were not practical due to their size and weight, we have come up with an improved optics design which has reduced size and is lightweight enough to wear. In this paper, the characteristics of typical multi-user three-dimensional displays are summarized and the design details of the latest optics are then described. Finally, a collaborative AR application employing the new display and its user experience are explained.},
  annotation = {An early AR paper showing occulsions between real world objects and virtual ones. Uses a huge AR HMD apperatist.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kiyokawa et al. - 2003 - An occlusion capable optical see-through head mount display for supporting co-located collaboration.pdf:pdf},
  keywords   = {AR display,Augmented reality,Collaboration,Collaborative work,ELMO,Enhanced see-through display using an LCD panel fo,Face,Humans,Laboratories,Layout,Liquid crystal displays,Optical design,Three dimensional displays,arbitrary position,augmented reality,co-located collaboration,collaboration support,collaborative AR application,computer displays,computer games,face-to-face conversation,groupware,head mount display,helmet mounted displays,hidden feature removal,lightweight,multiuser collaboration,occlusion,optical see-through display,optics design,prototype systems,reduced size,three-dimensional displays,typical multiuser displays,user interfaces,user modelling,virtual object},
}

@Article{FernandezCarames2018,
  author       = {Fern{\'{a}}ndez-Caram{\'{e}}s, Tiago M. and Fraga-Lamas, Paula and Su{\'{a}}rez-Albela, Manuel and Vilar-Montesinos, Miguel},
  year         = {2018},
  journal       = {Sensors (Switzerland)},
  title        = {{A fog computing and cloudlet based augmented reality system for the industry 4.0 shipyard}},
  doi          = {10.3390/s18061798},
  issn         = {14248220},
  number       = {6},
  pages        = {0--18},
  volume       = {18},
  abstract     = {Augmented Reality (AR) is one of the key technologies pointed out by Industry 4.0 as a tool for enhancing the next generation of automated and computerized factories. AR can also help shipbuilding operators, since they usually need to interact with information (e.g., product datasheets, instructions, maintenance procedures, quality control forms) that could be handled easily and more efficiently through AR devices. This is the reason why Navantia, one of the 10 largest shipbuilders in the world, is studying the application of AR (among other technologies) in different shipyard environments in a project called “Shipyard 4.0”. This article presents Navantia's industrial AR (IAR) architecture, which is based on cloudlets and on the fog computing paradigm. Both technologies are ideal for supporting physically-distributed, low-latency and QoS-aware applications that decrease the network traffic and the computational load of traditional cloud computing systems. The proposed IAR communications architecture is evaluated in real-world scenarios with payload sizes according to demanding Microsoft HoloLens applications and when using a cloud, a cloudlet and a fog computing system. The results show that, in terms of response delay, the fog computing system is the fastest when transferring small payloads (less than 128 KB), while for larger file sizes, the cloudlet solution is faster than the others. Moreover, under high loads (with many concurrent IAR clients), the cloudlet in some cases is more than four times faster than the fog computing system in terms of response delay.},
  annotation   = {Industry Augmented Reality paper. This paper spends most of its time talking about. This paper talks about the things that could be needed for designing techonology for inustrial purposes and it has quite comprensive Intros and Backgrounds. Works on a differnt type of cloud computing technolgy},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fern{\'{a}}ndez-Caram{\'{e}}s et al. - 2018 - A fog computing and cloudlet based augmented reality system for the industry 4.0 shipyard.pdf:pdf},
  keywords     = {Augmented reality,Cloudlet,Fog computing,IIoT,Industrial augmented reality,Industrial operator support,Industry 4.0,Microsoft HoloLens,Shipyard},
}

@Book{Huang2015,
  year       = {2015},
  title      = {{Human Factors in Augmented Reality Environments}},
  editor     = {Huang, Weidong and Alem, Leila and Livingston, Mark A.},
  isbn       = {9781461442042},
  publisher  = {Springer},
  url        = {https://link-springer-com.access.library.unisa.edu.au/content/pdf/10.1007\%2F978-1-4614-4205-9.pdf https://link-springer-com.access.library.unisa.edu.au/chapter/10.1007\%2F978-1-4614-4205-9_4},
  annotation = {A Book chapter with a chapter called the pursuit of X-Ray Vision starting on page 67 One whole part is dedicated to Perception and Cognition},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2015 - Human Factors in Augmented Reality Environments.pdf:pdf},
}

@Article{Grossman2006,
  author       = {Grossman, Tovi and Balakrishnan, Ravin},
  year         = {2006},
  journal       = {Proceedings of the Workshop on Advanced Visual Interfaces},
  title        = {{An evaluation of depth perception on volumetric displays}},
  doi          = {10.1145/1133265.1133305},
  pages        = {193--200},
  volume       = {2006},
  abstract     = {We present an experiment that compares volumetric displays to existing 3D display techniques in three tasks that require users to perceive depth in 3D scenes. Because they generate imagery in true 3D space, volumetric displays allow viewers to use their natural physiological mechanisms for depth perception, without requiring special hardware such as head trackers or shutter glasses. However, it is unclear from the literature as to whether these displays are actually better than the status-quo for enabling the perception of 3D scenes, thus motivating the present study. Our results show that volumetric displays enable significantly better user performance in a simple depth judgment task, and better performance in a collision judgment task, but in its current form does not enhance user comprehension of more complex 3D scenes. Copyright 2006 ACM.},
  annotation   = {so this paper looks at using depth perception using a actual 3D depth display. These deisplays use a native voxel based resultion and there is a good chance that results found with this techonolgy. Has lots of good info regarding Variuos real world settings.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grossman, Balakrishnan - 2006 - An evaluation of depth perception on volumetric displays.pdf:pdf},
  isbn         = {1595933530},
  keywords     = {Depth perception,Evaluation,Volumetric display},
}

@Article{Chen2007,
  author       = {Chen, Jessie Y.C. and Thropp, Jennifer E.},
  year         = {2007},
  journal       = {IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans},
  title        = {{Review of low frame rate effects on human performance}},
  doi          = {10.1109/TSMCA.2007.904779},
  issn         = {10834427},
  number       = {6},
  pages        = {1063--1076},
  volume       = {37},
  abstract     = {In this paper, we conducted a comprehensive survey of the effects of different frame rates (FRs) on human performance and reviewed more than 50 studies and summarized them in the areas of psychomotor performance, perceptual performance, behavioral effects, and subjective perception. Overall, there seems to be strong support for a threshold of around 15 Hz for many tasks, including those that are psychomotor and perceptual in nature. Less impressive yet acceptable performance may be accomplished at around 10 Hz for many tasks. Subjective reactions to the quality and watchability of videos seem to support rates of 5 Hz, although videos presented at 15 Hz and above are generally more widely preferred. These generalizations regarding superior and acceptable FRs may also be subject to the effects of several moderating factors such as display characteristics, nature of the tasks, viewing condition, additional cues, and user experience. {\textcopyright} 2007 IEEE.},
  annotation   = {This is kind of a bible of Frame rates if you ever need to get some exact details about frame rates look here.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Thropp - 2007 - Review of low frame rate effects on human performance.pdf:pdf},
  keywords     = {Frame rate (FR),Human factors,Human performance,Video quality,Virtual reality,Visual perception},
}

@Article{Deng2018,
  author       = {Deng, Kai and Wei, Bo and Chen, Mo and Huang, Zhiyin and Wu, Hao},
  year         = {2018},
  journal       = {Scientific Reports},
  title        = {{Realization of real-time X-ray stereoscopic vision during interventional procedures}},
  doi          = {10.1038/s41598-018-34153-9},
  issn         = {20452322},
  number       = {1},
  pages        = {1--10},
  volume       = {8},
  abstract     = {During interventional procedures, the deficiencies of nonstereoscopic vision increase the difficulty of identifying the anteroposterior direction and pathways of vessels. Therefore, achieving real-time stereoscopic vision during interventional procedures is meaningful. Pairs of X-ray images were captured with identical parameter settings, except for different rotation angles (represented as the $\alpha$ angle). The resulting images at these $\alpha$ angles were used as left-eye and right-eye views and were horizontally merged into single left-right 3D images. Virtual reality (VR) glasses were used for achieving stereo vision. Pairs of X-ray images from four angiographies with different $\alpha$ angles (1.8–3.4°) were merged into left-right 3D images. Observation with VR glasses can produce realistic stereo views of vascular anatomical structure. The results showed that the optimal $\alpha$ angles accepted by the brain for generating stereo vision were within a narrow range (approximately 1.4–4.1°). Subsequent tests showed that during transcatheter arterial chemoembolization, 3D X-ray stereoscopic images provided significantly improved spatial discrimination and convenience for identifying the supply vessels of a liver tumor and its anteroposterior direction compared with plain X-ray images (all P < 0.01). Real-time X-ray stereoscopic vision can be easily achieved via the straightforward method described herein and has the potential to benefit patients during interventional procedures.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deng et al. - 2018 - Realization of real-time X-ray stereoscopic vision during interventional procedures.pdf:pdf},
}

@InProceedings{Perkins2017,
  author    = {Perkins, S L and Lin, M A and Srinivasan, S and Wheeler, A J and Hargreaves, B A and Daniel, B L},
  booktitle = {2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)},
  year      = {2017},
  title     = {{A Mixed-Reality System for Breast Surgical Planning}},
  doi       = {10.1109/ISMAR-Adjunct.2017.92},
  isbn      = {null VO -},
  pages     = {269--274},
  abstract  = {One quarter of women who undergo breast lumpectomy to treat early-stage breast cancer in the United States undergo a repeat surgery due to concerns that residual tumor was left behind. This has led to a significant increase in women choosing mastectomy operations in the United States. We have developed a mixed-reality system that projects a 3D “hologram” of images from a breast MRI onto a patient using the Microsoft HoloLens. The goal of this system is to reduce the number of repeated surgeries by improving surgeons' ability to determine tumor extent. We are conducting a pilot study in patients with palpable tumors that tests a surgeon's ability to accurately identify the tumor location via mixed-reality visualization during surgical planning. Although early results are promising, it is critical but not straightforward to align holograms to the breast and to account for tissue deformations. More work is needed to improve the registration and holographic display at arm's-length working distance. Nonetheless, first results from breast cancer surgeries have shown that mixed-reality guidance can indeed provide information about tumor location, and that this exciting new use for AR has the potential to improve the lives of many patients.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perkins et al. - 2017 - A Mixed-Reality System for Breast Surgical Planning.pdf:pdf},
  keywords  = {3D hologram,Augmented reality,Breast,Planning,Shape,Surgery,Tumors,Virtual reality,augmented reality,biological organs,biological tissues,biomedical MRI,breast MRI,breast cancer,breast cancer surgeries,breast lumpectomy,breast surgical planning,breast-conserving surgery,cancer,data visualisation,magnetic resonance imaging,mastectomy operations,medical image processing,microsoft hololens,mixed reality,mixed-reality system,mixed-reality visualization,palpable tumors,surgery,surgical planning,tissue deformations,tumor location,tumours},
}

@Article{Raisamo2019,
  author       = {Raisamo, Roope and Rakkolainen, Ismo and Majaranta, P{\"{a}}ivi and Salminen, Katri and Rantala, Jussi and Farooq, Ahmed},
  year         = {2019},
  journal       = {International Journal of Human-Computer Studies},
  title        = {{Human augmentation: Past, present and future}},
  doi          = {https://doi.org/10.1016/j.ijhcs.2019.05.008},
  issn         = {1071-5819},
  pages        = {131--143},
  url          = {http://www.sciencedirect.com/science/article/pii/S1071581919300576},
  volume       = {131},
  abstract     = {Human augmentation is a field of research that aims to enhance human abilities through medicine or technology. This has historically been achieved by consuming chemical substances that improve a selected ability or by installing implants which require medical operations. Both of these methods of augmentation can be invasive. Augmented abilities have also been achieved with external tools, such as eyeglasses, binoculars, microscopes or highly sensitive microphones. Lately, augmented reality and multimodal interaction technologies have enabled non-invasive ways to augment human. In this article, we first discuss the field and related terms. We provide relevant definitions based on the present understanding of the field. This is followed by a summary of existing work in augmented senses, action, and cognition. Our contribution to the future includes a model for wearable augmentation. In addition, we present a call for research to realize this vision. Then, we discuss future human abilities. Wearable technologies may act as mediators for human augmentation, in the same manner as eyeglasses once revolutionized human vision. Non-invasive and easy-to-use wearable extensions will enable lengthening the active life for aging citizens or supporting the full inclusion of people with special needs in society, but there are also potential problems. Therefore, we conclude by discussing ethical and societal issues: privacy, social manipulation, autonomy and side effects, accessibility, safety and balance, and unpredictable future.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raisamo et al. - 2019 - Human augmentation Past, present and future.pdf:pdf},
  keywords     = {Augmented action,Augmented cognition,Augmented reality,Augmented senses,Crossmodal interaction,Human augmentation,Interaction paradigms,Multimodal interaction,Wearable computing},
}

@Article{Gabbard2013,
  author       = {Gabbard, Joseph L. and Swan, J. Edward and Zarger, Adam},
  year         = {2013},
  journal       = {Proceedings - IEEE Virtual Reality},
  title        = {{Color blending in outdoor optical see-through AR: The effect of real-world backgrounds on user interface color}},
  doi          = {10.1109/VR.2013.6549410},
  pages        = {157--158},
  volume       = {2010},
  abstract     = {When viewing augmented reality (AR) through an optical see-through head-mounted display (oHMD), the colors of AR elements become washed out as the luminance of the background increases. In addition, the colors of AR elements can shift and change depending on the color of the background that the AR elements are seen against. Both of these phenomena result from the way the color of an AR element blends with the color of the real-world background that the AR element is seen against. Although the fact that this color blending occurs is generally known, to date it has not been systematically studied or quantified. In this work, we sought to quantify this color blending by building an apparatus that allows precise and repeatable color measurements, and then conducting an empirical engineering study. {\textcopyright} 2013 IEEE.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabbard, Swan, Zarger - 2013 - Color blending in outdoor optical see-through AR The effect of real-world backgrounds on user interface c.pdf:pdf},
  isbn         = {9781467347952},
  publisher    = {IEEE},
}

@Article{Bruno1988,
  author       = {Bruno, Nicola and Cutting, James},
  year         = {1988-07},
  journal       = {Journal of experimental psychology. General},
  title        = {{Minimodularity and the Perception of Layout}},
  doi          = {10.1037//0096-3445.117.2.161},
  pages        = {161--170},
  volume       = {117},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bruno, Cutting - 1988 - Minimodularity and the Perception of Layout.pdf:pdf},
}

@Article{Zorzal2019,
  author       = {Zorzal, Ezequiel R. and Sousa, Maur{\'{i}}cio and Mendes, Daniel and dos Anjos, Rafael Kuffner and Medeiros, Daniel and Paulo, Soraia Figueiredo and Rodrigues, Pedro and Mendes, Jos{\'{e}} Jo{\~{a}}o and Delmas, Vincent and Uhl, Jean Francois and Mogorr{\'{o}}n, Jos{\'{e}} and Jorge, Joaquim Armando and Lopes, Daniel Sim{\~{o}}es},
  year         = {2019},
  journal       = {Computers and Graphics (Pergamon)},
  title        = {{Anatomy Studio: A tool for virtual dissection through augmented 3D reconstruction}},
  doi          = {10.1016/j.cag.2019.09.006},
  issn         = {00978493},
  pages        = {74--84},
  volume       = {85},
  abstract     = {3D reconstruction from anatomical slices allows anatomists to create three dimensional depictions of real structures by tracing organs from sequences of cryosections. However, conventional user interfaces rely on single-user experiences and mouse-based input to create content for education or training purposes. In this work, we present Anatomy Studio, a collaborative Mixed Reality tool for virtual dissection that combines tablets with styli and see-through head-mounted displays to assist anatomists by easing manual tracing and exploring cryosection images. We contribute novel interaction techniques intended to promote spatial understanding and expedite manual segmentation. By using mid-air interactions and interactive surfaces, anatomists can easily access any cryosection and edit contours, while following other user's contributions. A user study including experienced anatomists and medical professionals, conducted in real working sessions, demonstrates that Anatomy Studio is appropriate and useful for 3D reconstruction. Results indicate that Anatomy Studio encourages closely-coupled collaborations and group discussion, to achieve deeper insights.},
  annotation   = {This paper is looking at changing the scan data in a collaborative environment. Methods This section was quite detailed however it did. There wasn't any special about this creation tablets and M2s were used for both the participants. Results The participants noted that working together helped them solve tasks sooner. Challenges noted included Stylus was too thick Orientation needs to be consistent. The advantage of this use of technology is that it can help students understand a patient's anatomy. Things that still need to be done. Improve user perception, tactile sensitivity and spatial correlation between physical and virtual objects. It also notes that this tech could introduce new teaching technologies.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zorzal et al. - 2019 - Anatomy Studio A tool for virtual dissection through augmented 3D reconstruction.pdf:pdf},
  keywords     = {3D reconstruction,Collaboration,Medical image segmentation,Mixed reality,Tablet},
  publisher    = {Elsevier Ltd},
}

@Article{Wang2017a,
  author       = {Wang, Rong and Geng, Zheng and Zhang, Zhaoxing and Pei, Renjing and Meng, Xiangbing},
  year         = {2017},
  journal       = {Displays},
  title        = {{Autostereoscopic augmented reality visualization for depth perception in endoscopic surgery}},
  doi          = {10.1016/j.displa.2017.03.003},
  issn         = {01419382},
  pages        = {50--60},
  url          = {http://dx.doi.org/10.1016/j.displa.2017.03.003},
  volume       = {48},
  abstract     = {Augmented reality (AR) has received increasing attention in minimally invasive surgery (MIS) applications. The goal of applying AR techniques to MIS is to enhance a surgeon's perception of the spatial relationship by overlaying invisible structures (e.g. tumor or vessels) onto the in vivo endoscopic video acquired during the surgery. One of primary issues of AR visualization is to provide correct depth perception for visible and invisible structures. In this paper, we present a video-based AR system consisting of functional modules for real-time 3D surface capture, reconstruction, and registration with pre-operative segmented CT model. The real-time 3D registration allows precise overlay of invisible structures onto 2D video for AR visualization. The AR overlay result is displayed on a multi-view autostereoscopic lenticular LCD. To study and compare the efficacy of AR visualization techniques, we investigated five different AR visualization modes. Both simulated and in vivo experiments were carried out and autostereoscopic AR visualization results were given. Evaluation and comparison for depth perception between five AR visualization modes are presented. Finally, we conclude the characteristics of these visualization modes. The novelty of our work lies in successful implementation of an end-to-end 3D autostereoscopic AR system from real-time reconstruction and registration with our multi-channel 3D endoscope, and systematic evaluation and comparison of five different visualization modes for depth perception.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Autostereoscopic augmented reality visualization for depth perception in endoscopic surgery.pdf:pdf},
  keywords     = {Augmented reality,Autostereoscopic 3D display,Depth perception,Visualization},
  publisher    = {Elsevier B.V.},
}

@Article{Cutolo2017,
  author       = {Cutolo, Fabrizio and Meola, Antonio and Carbone, Marina and Sinceri, Sara and Cagnazzo, Federico and Denaro, Ennio and Esposito, Nicola and Ferrari, Mauro and Ferrari, Vincenzo},
  year         = {2017},
  journal       = {Computer Assisted Surgery},
  title        = {{A new head-mounted display-based augmented reality system in neurosurgical oncology: a study on phantom}},
  doi          = {10.1080/24699322.2017.1358400},
  issn         = {24699322},
  number       = {1},
  pages        = {39--53},
  url          = {https://doi.org/10.1080/24699322.2017.1358400},
  volume       = {22},
  abstract     = {Purpose: Benefits of minimally invasive neurosurgery mandate the development of ergonomic paradigms for neuronavigation. Augmented Reality (AR) systems can overcome the shortcomings of commercial neuronavigators. The aim of this work is to apply a novel AR system, based on a head-mounted stereoscopic video see-through display, as an aid in complex neurological lesion targeting. Effectiveness was investigated on a newly designed patient-specific head mannequin featuring an anatomically realistic brain phantom with embedded synthetically created tumors and eloquent areas. Materials and methods: A two-phase evaluation process was adopted in a simulated small tumor resection adjacent to Broca's area. Phase I involved nine subjects without neurosurgical training in performing spatial judgment tasks. In Phase II, three surgeons were involved in assessing the effectiveness of the AR-neuronavigator in performing brain tumor targeting on a patient-specific head phantom. Results: Phase I revealed the ability of the AR scene to evoke depth perception under different visualization modalities. Phase II confirmed the potentialities of the AR-neuronavigator in aiding the determination of the optimal surgical access to the surgical target. Conclusions: The AR-neuronavigator is intuitive, easy-to-use, and provides three-dimensional augmented information in a perceptually-correct way. The system proved to be effective in guiding skin incision, craniotomy, and lesion targeting. The preliminary results encourage a structured study to prove clinical effectiveness. Moreover, our testing platform might be used to facilitate training in brain tumour resection procedures.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cutolo et al. - 2017 - A new head-mounted display-based augmented reality system in neurosurgical oncology a study on phantom.pdf:pdf},
  keywords     = {Augmented reality,depth perception,head phantom,neuronavigation,surgical planning},
  publisher    = {Informa UK Ltd.},
}

@Article{Choi2016,
  author       = {Choi, Hyunseok and Cho, Byunghyun and Masamune, Ken and Hashizume, Makoto and Hong, Jaesung},
  year         = {2016-03},
  journal       = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  title        = {{An effective visualization technique for depth perception in augmented reality-based surgical navigation}},
  doi          = {10.1002/rcs.1657},
  issn         = {1478-5951},
  number       = {1},
  pages        = {62--72},
  url          = {https://doi.org/10.1002/rcs.1657},
  volume       = {12},
  abstract     = {Abstract Background Depth perception is a major issue in augmented reality (AR)-based surgical navigation. We propose an AR and virtual reality (VR) switchable visualization system with distance information, and evaluate its performance in a surgical navigation set-up. Methods To improve depth perception, seamless switching from AR to VR was implemented. In addition, the minimum distance between the tip of the surgical tool and the nearest organ was provided in real time. To evaluate the proposed techniques, five physicians and 20 non-medical volunteers participated in experiments. Results Targeting error, time taken, and numbers of collisions were measured in simulation experiments. There was a statistically significant difference between a simple AR technique and the proposed technique. Conclusions We confirmed that depth perception in AR could be improved by the proposed seamless switching between AR and VR, and providing an indication of the minimum distance also facilitated the surgical tasks. Copyright ? 2015 John Wiley \& Sons, Ltd.},
  annotation   = {doi: 10.1002/rcs.1657},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2016 - An effective visualization technique for depth perception in augmented reality-based surgical navigation.pdf:pdf},
  keywords     = {augmented reality,depth perception,surgical navigation,virtual reality},
  publisher    = {John Wiley {\&} Sons, Ltd},
}

@Article{Behrens1998,
  author       = {Behrens, Uwe and Ratering, Ralf},
  year         = {1998},
  journal       = {Proceedings of the 1998 IEEE Symposium on Volume Visualization, VVS 1998},
  title        = {{Adding shadows to a texture-based volume renderer}},
  doi          = {10.1145/288126.288149},
  pages        = {39--46},
  abstract     = {Texture-based volume rendering is a technique to efficiently visualize volumetric data using texture mapping hardware. In this paper we present an algorithm, that extends mis approach to render shadows for the volume. The algorithm takes advantage of fast frame-buffer operations modern graphics hardware offers, but does not depend on any special purpose hardware. The visual impression of the final image is significantly improved by bringing more structure and three-dimensional information into the often foggyish appearance of texture-based volume renderings. Although the algorithm does not perform lighting calculations, the resulting image has a shaded appearance, which is a further visual cue to spatial understanding of the data and lets the images appear more realistic. As calculating the shadows is independent of the visualization process it can be applied to any form of volume visualization, though volume rendering based on two- or three-dimensional texture mapping hardware makes the most sense. Compared to unshadowed texture-based volume rendering, performance decreases by less than 50\%, which is still sufficient to guarantee interactive manipulation of the volume data. In the special case where only the camera is moving with the light position fixed to the scene there is no performance decrease at all, because recalculation has only to be done if the position of the light source with respect to the volume changes.},
  annotation   = {This paper explains clearly how to add shadows to a volume texture with some ease. Used for cinimatic rendering},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Behrens, Ratering - 1998 - Adding shadows to a texture-based volume renderer.pdf:pdf},
  isbn         = {1581131054},
  keywords     = {Image compositing,Shadow algorithms,Texture-based volume rendering,Volume rendering},
}

@InProceedings{Vasco2018,
  author     = {Vasco, Pires and Miguel, Belo and Carlos, Sousa and Joaquim, Jorge and Daniel, Sim{\~{o}}es Lopes},
  booktitle  = {Interactive Tablets for 3D Medical Image Exploration},
  year       = {2018},
  title      = {{Interactive Tablets for 3D Medical Image Exploration. In: Tavares J., Natal Jorge R. (eds) VipIMAGE 2017. ECCOMAS 2017. Lecture Notes in Computational Vision and Biomechanics, vol 27. Springer, Cham}},
  url        = {https://link.springer.com/chapter/10.1007/978-3-319-68195-5_62#citeas},
  annotation = {This was just a prototype paper for a simple voxelatization framework.},
}

@Article{Mandalika2018,
  author       = {Mandalika, Veera Bhadra Harish and Chernoglazov, Alexander I. and Billinghurst, Mark and Bartneck, Christoph and Hurrell, Michael A. and Ruiter, Nielsde and Butler, Anthony P.H. and Butler, Philip H.},
  year         = {2018},
  journal       = {Journal of Digital Imaging},
  title        = {{A Hybrid 2D/3D User Interface for Radiological Diagnosis}},
  doi          = {10.1007/s10278-017-0002-6},
  issn         = {1618727X},
  number       = {1},
  pages        = {56--73},
  url          = {http://dx.doi.org/10.1007/s10278-017-0002-6},
  volume       = {31},
  abstract     = {This paper presents a novel 2D/3D desktop virtual reality hybrid user interface for radiology that focuses on improving 3D manipulation required in some diagnostic tasks. An evaluation of our system revealed that our hybrid interface is more efficient for novice users and more accurate for both novice and experienced users when compared to traditional 2D only interfaces. This is a significant finding because it indicates, as the techniques mature, that hybrid interfaces can provide significant benefit to image evaluation. Our hybrid system combines a zSpace stereoscopic display with 2D displays, and mouse and keyboard input. It allows the use of 2D and 3D components interchangeably, or simultaneously. The system was evaluated against a 2D only interface with a user study that involved performing a scoliosis diagnosis task. There were two user groups: medical students and radiology residents. We found improvements in completion time for medical students, and in accuracy for both groups. In particular, the accuracy of medical students improved to match that of the residents.},
  annotation   = {So your proberbly going to tear this paper apart but this paper has found workarounds to most of your inovations thus far. They have managed to get create a creative sytle for a dual screen display with a sudo stylus based pen.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mandalika et al. - 2018 - A Hybrid 2D3D User Interface for Radiological Diagnosis.pdf:pdf},
  keywords     = {3D input,Diagnostic radiology,Hybrid user interface,Medical visualization,User interface},
  publisher    = {Journal of Digital Imaging},
}

@InCollection{Douglas2018,
  author     = {Douglas, David B. and Venets, Demetri and Wilke, Cliff and Gibson, David and Liotta, Lance and Petricoin, Emanuel and Beck, Buddy and Douglas, Robert},
  booktitle  = {Intechopen},
  year       = {2018},
  title      = {{Augmented Reality and Virtual Reality: Initial Successes in Diagnostic Radiology, State of the Art Virtual Reality and Augmented Reality Knowhow, Nawaz Mohamudally}},
  chapter    = {2},
  doi        = {10.5772/intechopen.74317},
  pages      = {5--24},
  url        = {https://www.intechopen.com/books/advanced-biometric-technologies/liveness-detection-in-biometrics https://www.intechopen.com/books/state-of-the-art-virtual-reality-and-augmented-reality-knowhow/augmented-reality-and-virtual-reality-initial-successes-in-di},
  abstract   = {In this chapter, we will review the applications of augmented reality (AR) and virtual reality (VR) in diagnostic radiology. We begin the chapter by discussing state of the art medical imaging techniques to include scanner hardware, spatial resolution, and presen- tation methods to the radiologist or other medical professionals. We continue by discuss- ing the methodology of a technique called Depth-3-Dimensional (D3D) imaging, which transforms cross-sectional medical imaging datasets into a left and right eye images that that can be viewed with state of the art AR or VR headsets. We include results of the D3D processed AR/VR imaging and conclude with a discussion of the path forward.},
  annotation = {Good paper regarding the current state of AR shows an interesting read.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Douglas et al. - 2018 - Augmented Reality and Virtual Reality Initial Successes in Diagnostic Radiology, State of the Art Virtual Realit.pdf:pdf},
}

@Article{Tomandl2001,
  author       = {Tomandl, Bernd F. and Hastreiter, Peter and Rezk-Salama, Christof and Engel, Klaus and Ertl, Thomas and Huk, Walter J. and Naraghi, Ramin and Ganslandt, Oliver and Nimsky, Christopher and Eberhardt, Knut E.W.},
  year         = {2001},
  journal       = {Radiographics},
  title        = {{Local and remote visualization techniques for interactive direct volume rendering in neuroradiology}},
  doi          = {10.1148/radiographics.21.6.g01nv241561},
  issn         = {02715333},
  number       = {6},
  pages        = {1561--1572},
  volume       = {21},
  abstract     = {The increasing capabilities of magnetic resonance (MR) imaging and multisection spiral computed tomography (CT) to acquire volumetric data with near-isotropic voxels make three-dimensional (3D) postprocessing a necessity, especially in studies of complex structures like intracranial vessels. Since most modern CT and MR imagers provide limited postprocessing capabilities, 3D visualization with interactive direct volume rendering requires expensive graphics workstations that are not available at many institutions. An approach has been developed that combines fast visualization on a low-cost PC system with high-quality visualization on a high-end graphics workstation that is directly accessed and remotely controlled from the PC environment via the Internet by using a Java client. For comparison of quality, both techniques were applied to several neuroradiologic studies: visualization of structures related to the inner ear, intracranial aneurysms, and the brainstem and surrounding neurovascular structures. The results of pure PC-based visualization were comparable with those of many commercially available volume-rendering systems. In addition, the high-end graphics workstation with 3D texture-mapping capabilities provides visualization results of the highest quality. Combining local and remote 3D visualization allows even small radiologic institutions to achieve low-cost but high-quality 3D visualization of volumetric data.},
  annotation   = {This explains the tech that was used in the cinimatic rendering demo that you used while you were in germany},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tomandl et al. - 2001 - Local and remote visualization techniques for interactive direct volume rendering in neuroradiology.pdf:pdf},
  keywords     = {Computed tomography (CT), image processing,Computed tomography (CT), three-dimensional, 10.12,Computed tomography (CT), volume rendering,Computers,Magnetic resonance (MR), image processing,Magnetic resonance (MR), three-dimensional, 10.121},
}

@Article{Bajaj1999,
  author = {Bajaj, Chandrajit and Ihm, Insung and Park, Sanghun},
  year   = {1999-08},
  title  = {{Making 3D Textures Practical}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bajaj, Ihm, Park - 1999 - Making 3D Textures Practical.pdf:pdf},
}

@Book{Shirazi2019,
  author = {Shirazi, Seyed Yahya and Huang, Helen},
  year   = {2019-02},
  title  = {{More Reliable EEG Electrode Digitizing Methods Can Reduce Source Estimation Uncertainty, But Current Methods Already Accurately Identify Brodmann Areas}},
  doi    = {10.1101/557074},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shirazi, Huang - 2019 - More Reliable EEG Electrode Digitizing Methods Can Reduce Source Estimation Uncertainty, But Current Methods Alr.pdf:pdf},
}

@Article{Sparks2017,
  author       = {Sparks, Rachel and Zombori, Gergely and Rodionov, Roman and Nowell, Mark and Vos, Sjoerd B. and Zuluaga, Maria A. and Diehl, Beate and Wehner, Tim and Miserocchi, Anna and McEvoy, Andrew W. and Duncan, John S. and Ourselin, Sebastien},
  year         = {2017},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title        = {{Automated multiple trajectory planning algorithm for the placement of stereo-electroencephalography (SEEG) electrodes in epilepsy treatment}},
  doi          = {10.1007/s11548-016-1452-x},
  issn         = {18616429},
  number       = {1},
  pages        = {123--136},
  volume       = {12},
  abstract     = {Purpose: About one-third of individuals with focal epilepsy continue to have seizures despite optimal medical management. These patients are potentially curable with neurosurgery if the epileptogenic zone (EZ) can be identified and resected. Stereo-electroencephalography (SEEG) to record epileptic activity with intracranial depth electrodes may be required to identify the EZ. Each SEEG electrode trajectory, the path between the entry on the skull and the cerebral target, must be planned carefully to avoid trauma to blood vessels and conflicts between electrodes. In current clinical practice trajectories are determined manually, typically taking 2–3 h per patient (15 min per electrode). Manual planning (MP) aims to achieve an implantation plan with good coverage of the putative EZ, an optimal spatial resolution, and 3D distribution of electrodes. Computer-assisted planning tools can reduce planning time by quantifying trajectory suitability. Methods: We present an automated multiple trajectory planning (MTP) algorithm to compute implantation plans. MTP uses dynamic programming to determine a set of plans. From this set a depth-first search algorithm finds a suitable plan. We compared our MTP algorithm to (a) MP and (b) an automated single trajectory planning (STP) algorithm on 18 patient plans containing 165 electrodes. Results: MTP changed all 165 trajectories compared to MP. Changes resulted in lower risk (122), increased grey matter sampling (99), shorter length (92), and surgically preferred entry angles (113). MTP changed 42 \% (69/165) trajectories compared to STP. Every plan had between 1 to 8 (median 3.5) trajectories changed to resolve electrode conflicts, resulting in surgically preferred plans. Conclusion: MTP is computationally efficient, determining implantation plans containing 7–12 electrodes within 1 min, compared to 2–3 h for MP.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sparks et al. - 2017 - Automated multiple trajectory planning algorithm for the placement of stereo-electroencephalography (SEEG) electr.pdf:pdf},
  keywords     = {Computer-assisted planning,Epilepsy,Image-guided neurosurgery,Neurosurgery},
}

@Article{Xu2014,
  author       = {Xu, Ziyue and Bagci, Ulas and Jonsson, Colleen and Jain, Sanjay and Mollura, Daniel J.},
  year         = {2014},
  journal       = {2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC 2014},
  title        = {{Efficient ribcage segmentation from CT scans using shape features}},
  doi          = {10.1109/EMBC.2014.6944229},
  number       = {January 2015},
  pages        = {2899--2902},
  abstract     = {Rib cage structure and morphology is important for anatomical analysis of chest CT scans. A fundamental challenge in rib cage extraction is varying intensity levels and connection with adjacent bone structures including shoulder blade and sternum. In this study, we present a fully automated 3-D algorithm to segment the rib cage by detection and separation of other bone structures. The proposed approach consists of four steps. First, all high-intensity bone structures are segmented. Second, multi-scale Hessian analysis is performed to capture plateness and vesselness information. Third, with the plate/vessel features, bone structures other than rib cage are detected. Last, the detected bones are separated from rib cage with iterative relative fuzzy connectedness method. The algorithm was evaluated using 400 human CT scans and 100 small animal images with various resolution. The results suggested that the percent accuracy of rib cage extraction is over 95\% with the proposed algorithm.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2014 - Efficient ribcage segmentation from CT scans using shape features.pdf:pdf},
  isbn         = {9781424479290},
  keywords     = {Rib cage segmentation,iterative relative fuzzy connectedness,multi-scale Hessian analysis},
}

@Article{Zhang2015a,
  author   = {Zhang, Li and Li, Xiaodong and Hu, Qingmao},
  year     = {2015},
  title    = {{Automatic Rib Segmentation in Chest CT Volume Data Automatic Rib Segmentation in Chest CT Volume Data}},
  doi      = {10.1109/iCBEB.2012.89},
  number   = {May 2012},
  volume   = {D},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Li, Hu - 2015 - Automatic Rib Segmentation in Chest CT Volume Data Automatic Rib Segmentation in Chest CT Volume Data.pdf:pdf},
  isbn     = {9780769547060},
  keywords = {Rib Segmentation, Computed Tomography, Threshold,},
}

@Article{Schaefer2005,
  author       = {Schaefer, Scott and Warren, Joe},
  year         = {2005},
  journal       = {Computer Graphics Forum},
  title        = {{Dual marching cubes: Primal contouring of dual grids}},
  doi          = {10.1111/j.1467-8659.2005.00843.x},
  issn         = {14678659},
  number       = {2},
  pages        = {195--201},
  volume       = {24},
  abstract     = {We present a method for contouring an implicit function using a grid topologically dual to structured grids such as octrees. By aligning the vertices of the dual grid with the features of the implicit function, we are able to reproduce thin features of the extracted surface without excessive subdivision required by methods such as Marching Cubes or Dual Contouring. Dual Marching Cubes produces a crack-free, adaptive polygonalization of the surface that reproduces sharp features. Our approach maintains the advantage of using structured grids for operations such as CSG while being able to conform to the relevant features of the implicit function yielding much sparser polygonalizations than has been possible using structured grids. {\textcopyright} IEEE Proceedings of Pacific Graphics 2004.},
  annotation   = {Aims to use an oct tree with marching cubes. it notices no real difference. The overhead of the data stucture seems to make up for the time it would take to render everything else.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaefer, Warren - 2005 - Dual marching cubes Primal contouring of dual grids.pdf:pdf},
  keywords     = {CSG,Contouring,Implicit functions,Marching cubes,Octree,QEF},
}

@Article{Custodio2019,
  author       = {Custodio, Lis and Pesco, Sinesio and Silva, Claudio},
  year         = {2019},
  journal       = {Journal of the Brazilian Computer Society},
  title        = {{An extended triangulation to the Marching Cubes 33 algorithm}},
  doi          = {10.1186/s13173-019-0086-6},
  issn         = {16784804},
  number       = {1},
  volume       = {25},
  abstract     = {The Marching Cubes algorithm is arguably the most popular isosurface extraction algorithm. Since its inception, two problems have lingered, namely, triangle quality and topology correctness. Although there is an extensive literature to solve them, topology correctness is achieved in detriment of triangle quality and vice versa. In this paper, we present an extended version of the Marching Cubes 33 algorithm (a variation of the Marching Cubes algorithm which guarantees topological correctness), called Extended Marching Cubes 33. In the proposed algorithm, the grid vertex are labeled with “+,” “ −,” and “=,” according to the relationship between its scalar field value and the isovalue. The inclusion of the “=” grid vertex label naturally avoids degenerate triangles. As an application of our method, we use the proposed triangulation to improve the quality of the triangles in the generated mesh while preserving its topology as much as possible.},
  annotation   = {A extended model on the marching cubes 33 algorithm. Good to note were the algorithm is these days},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Custodio, Pesco, Silva - 2019 - An extended triangulation to the Marching Cubes 33 algorithm.pdf:pdf},
  isbn         = {1317301900},
  keywords     = {Extended lookup table,Isosurface extraction,Marching Cubes 33,Mesh quality},
  publisher    = {Journal of the Brazilian Computer Society},
}

@Article{Alliez2007,
  author       = {Alliez, Pierre and Cohen-Steiner, David and Tong, Yiying and Desbrun, Mathieu},
  year         = {2007},
  journal       = {Symposium on Geometry Processing},
  title        = {{Voronoi-based Variational Reconstruction of Unoriented Point Sets}},
  doi          = {10.2312/SGP/SGP07/039-048},
  issn         = {15240703},
  number       = {5},
  pages        = {39--48},
  volume       = {67},
  abstract     = {We introduce an algorithm for reconstructing watertight surfaces from unoriented point sets. Using the Voronoi diagram of the input point set, we deduce a tensor field whose principal axes and eccentricities locally represent respectively the most likely direction of the normal to the surface, and the confidence in this direction estimation. An implicit function is then computed by solving a generalized eigenvalue problem such that its gradient is most aligned with the principal axes of the tensor field, providing a best-fitting isosurface reconstruction. Our approach possesses a number of distinguishing features. In particular, the implicit function optimization provides resilience to noise, adjustable fitting to the data, and controllable smoothness of the reconstructed surface. Finally, the use of simplicial meshes (possibly restricted to a thin crust around the input data) and (an)isotropic Laplace operators renders the numerical treatment simple and robust.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alliez et al. - 2007 - Voronoi-based Variational Reconstruction of Unoriented Point Sets.pdf:pdf},
  keywords     = {"Computer Graphics Forum,EUROGRAPHICS"},
}

@Article{Park2019,
  author     = {Park, Brian J. and Hunt, Stephen and Nadolski, Gregory J. and Gade, Terence P.},
  year       = {2019},
  title      = {{Registration methods to enable augmented reality-assisted 3D image-guided interventions}},
  doi        = {10.1117/12.2533787},
  issn       = {1996756X},
  number     = {May 2019},
  pages      = {11},
  abstract   = {Augmented reality (AR) can be used to visualize virtual 3D models of medical imaging in actual 3D physical space. Accurate registration of these models onto patients will be essential for AR-assisted image-guided interventions. In this study, registration methods were developed, and registration times for aligning a virtual 3D anatomic model of patient imaging onto a CT grid commonly used in CT-guided interventions were compared. The described methodology enabled automated and accurate registration within seconds using computer vision detection of the CT grid as compared to minutes using user-interactive registration methods. Simple, accurate, and near instantaneous registration of virtual 3D models onto CT grids will facilitate the use of AR for real-time procedural guidance and combined virtual/actual 3D navigation during image-guided interventions.},
  annotation = {A fairly basic system that used Vuforia to detect were a image was and displayed medical data over the top. The intial test was around how comfactable they were using this technology but I don't belive they created a base test between how much they used a xbox controler vs a hololens},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2019 - Registration methods to enable augmented reality-assisted 3D image-guided interventions.pdf:pdf},
  isbn       = {9781510628373},
}

@Article{Newman2006,
  author       = {Newman, Timothy S. and Yi, Hong},
  year         = {2006},
  journal       = {Computers and Graphics (Pergamon)},
  title        = {{A survey of the marching cubes algorithm}},
  doi          = {10.1016/j.cag.2006.07.021},
  issn         = {00978493},
  number       = {5},
  pages        = {854--879},
  volume       = {30},
  abstract     = {A survey of the development of the marching cubes algorithm [W. Lorensen, H. Cline, Marching cubes: a high resolution 3D surface construction algorithm. Computer Graphics 1987; 21(4):163-9], a well-known cell-by-cell method for extraction of isosurfaces from scalar volumetric data sets, is presented. The paper's primary aim is to survey the development of the algorithm and its computational properties, extensions, and limitations (including the attempts to resolve its limitations). A rich body of publications related to this aim are included. Representative applications and spin-off work are also considered and related techniques are briefly discussed. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Newman, Yi - 2006 - A survey of the marching cubes algorithm.pdf:pdf},
  keywords     = {Indirect volume rendering,Isosurface extraction,Marching cubes,Scientific visualization,Volume visualization},
}

@Article{Lehmann2018,
  author       = {Lehmann, T and Sloboda, R and Usmani, N and Tavakoli, M},
  year         = {2018},
  journal       = {IEEE Robotics and Automation Letters},
  title        = {{Model-Based Needle Steering in Soft Tissue via Lateral Needle Actuation}},
  doi          = {10.1109/LRA.2018.2858001},
  issn         = {2377-3774 VO - 3},
  number       = {4},
  pages        = {3930--3936},
  volume       = {3},
  abstract     = {Needle insertion is a minimally invasive procedure finding applications in biopsy, ablation, drug delivery, and cancer therapy. Prostate brachytherapy is a therapeutic procedure where radioactive agents are implanted within the prostate using long needles for the purpose of cancer irradiation. During insertion, each inserted needle needs to remain on a straight path due to dosage distribution requirements. This is difficult to achieve as the beveled needle tip causes needle deflection. A common method for the surgeon to steer beveled-tip needles manually is intermittent axial rotation. Over the last decade, substantial advancements have been made in robotic assistance during needle steering. The most common steering input has, however, been axial needle rotation. A novel needle steering input proposed and investigated recently by our research group is lateral needle actuation near the needle's entry point into tissue. The investigations have yielded promising results with respect to the overall steering potential and improving the steerability of the needle. In this letter, a model-based control algorithm is presented that uses only lateral needle actuation to minimize needle deflection at the final insertion depth. The algorithm consists of two stages: an off-line trajectory planning stage and an on-line trajectory adjustment stage. Insertion experiments into phantom tissue were carried out to validate the proposed control algorithm. The results show that the algorithm is able to steer the needle toward the desired target whereas only using lateral needle actuation as steering input.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehmann et al. - 2018 - Model-Based Needle Steering in Soft Tissue via Lateral Needle Actuation.pdf:pdf},
  keywords     = {Brachytherapy,Cancer,Force,Load modeling,Medical robots and systems,Needles,Robots,Trajectory,actuators,been axial needle rotation,beveled-tip needles,biological tissues,cancer,force control,lateral needle actuation,medical robotics,model-based control algorithm,model-based needle steering,needle deflection,needle insertion,needle steering input,needles,off-line trajectory planning stage,on-line trajectory adjustment stage,phantom tissue,phantoms,soft tissue,surgery,surgical robotics: Steerable catheters/needles},
}

@Article{Chentanez2009,
  author       = {Chentanez, Nuttapong and Alterovitz, Ron and Ritchie, Daniel and Cho, Lita and Hauser, Kris K. and Goldberg, Ken and Shewchuk, Jonathan R. and O'Brien, James F.},
  year         = {2009},
  journal       = {ACM Transactions on Graphics},
  title        = {{Interactive simulation of surgical needle insertion and steering}},
  doi          = {10.1145/1531326.1531394},
  issn         = {07300301},
  number       = {3},
  pages        = {1--10},
  volume       = {28},
  abstract     = {We present algorithms for simulating and visualizing the insertion and steering of needles through deformable tissues for surgical training and planning. Needle insertion is an essential component of many clinical procedures such as biopsies, injections, neurosurgery, and brachytherapy cancer treatment. The success of these procedures depends on accurate guidance of the needle tip to a clinical target while avoiding vital tissues. Needle insertion deforms body tissues, making accurate placement difficult. Our interactive needle insertion simulator models the coupling between a steerable needle and deformable tissue. We introduce (1) a novel algorithm for local remeshing that quickly enforces the conformity of a tetrahedral mesh to a curvilinear needle path, enabling accurate computation of contact forces, (2) an efficient method for coupling a 3D finite element simulation with a 1D inextensible rod with stick-slip friction, and (3) optimizations that reduce the computation time for physically based simulations. We can realistically and interactively simulate needle insertion into a prostate mesh of 13,375 tetrahedra and 2,763 vertices at a 25 Hz frame rate on an 8-core 3.0 GHz Intel Xeon PC. The simulation models prostate brachytherapy with needles of varying stiffness, steering needles around obstacles, and supports motion planning for robotic needle insertion. We evaluate the accuracy of the simulation by comparing against real-world experiments in which flexible, steerable needles were inserted into gel tissue phantoms. {\textcopyright} 2009 ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chentanez et al. - 2009 - Interactive simulation of surgical needle insertion and steering.pdf:pdf},
  keywords     = {Coupled simulation,Needle insertion,Real-time finite element methods,Surgical simulation},
}

@Article{Handschuh2010,
  author       = {Handschuh, Stephan and Schwaha, Thomas and Metscher, Brian D.},
  year         = {2010},
  journal       = {BMC Developmental Biology},
  title        = {{Showing their true colors: A practical approach to volume rendering from serial sections}},
  doi          = {10.1186/1471-213X-10-41},
  issn         = {1471213X},
  volume       = {10},
  abstract     = {Background. In comparison to more modern imaging methods, conventional light microscopy still offers a range of substantial advantages with regard to contrast options, accessible specimen size, and resolution. Currently, tomographic image data in particular is most commonly visualized in three dimensions using volume rendering. To date, this method has only very rarely been applied to image stacks taken from serial sections, whereas surface rendering is still the most prevalent method for presenting such data sets three-dimensionally. The aim of this study was to develop standard protocols for volume rendering of image stacks of serial sections, while retaining the benefits of light microscopy such as resolution and color information. Results. Here we provide a set of protocols for acquiring high-resolution 3D images of diverse microscopic samples through volume rendering based on serial light microscopical sections using the 3D reconstruction software Amira (Visage Imaging Inc.). We overcome several technical obstacles and show that these renderings are comparable in quality and resolution to 3D visualizations using other methods. This practical approach for visualizing 3D micro-morphology in full color takes advantage of both the sub-micron resolution of light microscopy and the specificity of histological stains, by combining conventional histological sectioning techniques, digital image acquisition, three-dimensional image filtering, and 3D image manipulation and visualization technologies. Conclusions. We show that this method can yield "true"-colored high-resolution 3D views of tissues as well as cellular and sub-cellular structures and thus represents a powerful tool for morphological, developmental, and comparative investigations. We conclude that the presented approach fills an important gap in the field of micro-anatomical 3D imaging and visualization methods by combining histological resolution and differentiation of details with 3D rendering of whole tissue samples. We demonstrate the method on selected invertebrate and vertebrate specimens, and propose that reinvestigation of historical serial section material may be regarded as a special benefit. {\textcopyright} 2010 Handschuh et al; licensee BioMed Central Ltd.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Handschuh, Schwaha, Metscher - 2010 - Showing their true colors A practical approach to volume rendering from serial sections.pdf:pdf},
}

@Book{Anagnostopoulos2016,
  author     = {Anagnostopoulos, Ioannis and Pătrăucean, Viorica and Brilakis, Ioannis and Vela, Patricio},
  year       = {2016-05},
  title      = {{Detection of Walls, Floors, and Ceilings in Point Cloud Data}},
  doi        = {10.1061/9780784479827.229},
  pages      = {2302--2311},
  annotation = {This paper just looks at the ablity to work out if you can detect walls and ceilings from threasholds. The short part of it detecting the floor and ceiling was simple but detecting walls seemed quite hard due to stuff being on them. The algorithm they use seemed to be quite algorithmicly based.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anagnostopoulos et al. - 2016 - Detection of Walls, Floors, and Ceilings in Point Cloud Data.pdf:pdf},
}

@Article{Fichtinger2005a,
  author       = {Fichtinger, Gabor and Deguet, Anton and Fischer, Gregory and Iordachita, Iulian and Balogh, Emese and Masamune, Ken and Taylor, Russell H and Fayad, Laura M and {De Oliveira}, Michelle and Zinreich, S James},
  year         = {2005-01},
  journal       = {Computer Aided Surgery},
  title        = {{Image overlay for CT-guided needle insertions}},
  doi          = {10.3109/10929080500230486},
  issn         = {1092-9088},
  number       = {4},
  pages        = {241--255},
  url          = {https://doi.org/10.3109/10929080500230486},
  volume       = {10},
  annotation   = {doi: 10.3109/10929080500230486},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fichtinger et al. - 2005 - Image overlay for CT-guided needle insertions.pdf:pdf},
  publisher    = {Taylor {\&} Francis},
}

@Article{Citardi2016,
  author       = {Citardi, Martin J. and Agbetoba, Abib and Bigcas, Jo Lawrence and Luong, Amber},
  year         = {2016-05},
  journal       = {International Forum of Allergy and Rhinology},
  title        = {{Augmented reality for endoscopic sinus surgery with surgical navigation: A cadaver study}},
  doi          = {10.1002/alr.21702},
  issn         = {20426984},
  number       = {5},
  pages        = {523--528},
  volume       = {6},
  abstract     = {Background: Augmented reality (AR) fuses computer-generated images of preoperative imaging data with real-time views of the surgical field. Scopis Hybrid Navigation (Scopis GmbH, Berlin, Germany) is a surgical navigation system with AR capabilities for endoscopic sinus surgery (ESS). Methods: Predissection planning was performed with Scopis Hybrid Navigation software followed by ESS dissection on 2 human specimens using conventional ESS instruments. Results: Predissection planning included creating models of relevant frontal recess structures and the frontal sinus outflow pathway on orthogonal computed tomography (CT) images. Positions of the optic nerve and internal carotid artery were marked on the CT images. Models and annotations were displayed as an overlay on the endoscopic images during the dissection, which was performed with electromagnetic surgical navigation. The accuracy of the AR images relative to underlying anatomy was better than 1.5 mm. The software's trajectory targeting tool was used to guide instrument placement along the frontal sinus outflow pathway. AR imaging of the optic nerve and internal carotid artery served to mark the positions of these structures during the dissection. Conclusion: Surgical navigation with AR was easily deployed in this cadaveric model of ESS. This technology builds upon the positive impact of surgical navigation during ESS, particularly during frontal recess surgery. Instrument tracking with this technology facilitates identifying and cannulation of the frontal sinus outflow pathway without dissection of the frontal recess anatomy. AR can also highlight "anti-targets" (ie, structures to be avoided), such as the optic nerve and internal carotid artery, and thus reduce surgical complications and morbidity.},
  keywords     = {Augmented reality,Cadaveric model,Computer-aided surgery,Endoscopic sinus surgery,Image-guided surgery,Visualization technology},
  publisher    = {John Wiley and Sons Inc.},
}

@Article{Marchal2017,
  author   = {Marchal, Damien and Moerman, Cl{\'{e}}ment and Casiez, G{\'{e}}ry and Roussel, Nicolas and Marchal, Damien and Moerman, Cl{\'{e}}ment and Casiez, G{\'{e}}ry and Roussel, Nicolas and Intuitive, Designing and Techniques, Navigation and Kotz{\'{e}}, Paula and Marsden, Gary and Lindgaard, Gitte and Wesson, Janet},
  year     = {2017},
  title    = {{Designing Intuitive Multi-touch 3D Navigation Techniques To cite this version : HAL Id : hal-00813232 Designing Intuitive Multi-touch 3D Navigation Techniques}},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marchal et al. - 2017 - Designing Intuitive Multi-touch 3D Navigation Techniques To cite this version HAL Id hal-00813232 Designing In.pdf:pdf},
  isbn     = {9783642404832},
  keywords = {3D navigation, multi-touch, interaction technique,},
}

@Article{Roberts2016,
  author       = {Roberts, Jess P. and Fisher, Thomas R. and Trowbridge, Matthew J. and Bent, Christine},
  year         = {2016},
  journal       = {Healthcare},
  title        = {{A design thinking framework for healthcare management and innovation}},
  doi          = {10.1016/j.hjdsi.2015.12.002},
  issn         = {22130772},
  number       = {1},
  pages        = {11--14},
  volume       = {4},
  abstract     = {The business community has learned the value of design thinking as a way to innovate in addressing people's needs - and health systems could benefit enormously from doing the same. This paper lays out how design thinking applies to healthcare challenges and how systems might utilize this proven and accessible problem-solving process. We show how design thinking can foster new approaches to complex and persistent healthcare problems through human-centered research, collective and diverse teamwork and rapid prototyping. We introduce the core elements of design thinking for a healthcare audience and show how it can supplement current healthcare management, innovation and practice.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roberts et al. - 2016 - A design thinking framework for healthcare management and innovation.pdf:pdf},
  keywords     = {Design thinking,Human-centered,Innovation,Systems thinking},
}

@Article{Altman2018,
  author       = {Altman, Myra and Huang, Terry T K and Breland, Jessica Y},
  year         = {2018-09},
  journal       = {Preventing chronic disease},
  title        = {{Design Thinking in Health Care}},
  doi          = {10.5888/pcd15.180128},
  issn         = {1545-1151},
  language     = {eng},
  pages        = {E117--E117},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/30264690 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6178900/},
  volume       = {15},
  abstract     = {INTRODUCTION: Applying Design Thinking to health care could enhance innovation, efficiency, and effectiveness by increasing focus on patient and provider needs. The objective of this review is to determine how Design Thinking has been used in health care and whether it is effective. METHODS: We searched online databases (PubMed, Medline, Web of Science, CINAHL, and PyscINFO) for articles published through March 31, 2017, using the terms "health," "health care," or "healthcare"; and "Design Thinking," "design science," "design approach," "user centered design," or "human centered design." Studies were included if they were written in English, were published in a peer-reviewed journal, provided outcome data on a health-related intervention, and used Design Thinking in intervention development, implementation, or both. Data were collected on target users, health conditions, intervention, Design Thinking approach, study design or sample, and health outcomes. Studies were categorized as being successful (all outcomes improved), having mixed success (at least one outcome improved), or being not successful (no outcomes improved). RESULTS: Twenty-four studies using Design Thinking were included across 19 physical health conditions, 2 mental health conditions, and 3 systems processes. Twelve were successful, 11 reported mixed success, and one was not successful. All 4 studies comparing Design Thinking interventions to traditional interventions showed greater satisfaction, usability, and effectiveness. CONCLUSION: Design Thinking is being used in varied health care settings and conditions, although application varies. Design Thinking may result in usable, acceptable, and effective interventions, although there are methodological and quality limitations. More research is needed, including studies to isolate critical components of Design Thinking and compare Design Thinking-based interventions with traditionally developed interventions.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Altman, Huang, Breland - 2018 - Design Thinking in Health Care.pdf:pdf},
  keywords     = {*Delivery of Health Care,*Research Design,Humans,Randomized Controlled Trials as Topic},
  publisher    = {Centers for Disease Control and Prevention},
}

@Article{Safi2018,
  author       = {Safi, Sabur and Thiessen, Thomas and Schmailzl, Kurt J G},
  year         = {2018-12},
  journal       = {JMIR Res Protoc},
  title        = {{Acceptance and Resistance of New Digital Technologies in Medicine: Qualitative Study}},
  doi          = {10.2196/11072},
  issn         = {1929-0748},
  number       = {12},
  pages        = {e11072},
  url          = {http://www.ncbi.nlm.nih.gov/pubmed/30514693},
  volume       = {7},
  abstract     = {Background: This study discusses the acceptance of new medical technologies in health care settings and resistance to these technologies from hospitals, doctors' surgical centers, electronic health (eHealth) centers, and related institutions. We suggest a novel method of identifying factors that influence the acceptance of, and resistance to, new technologies by medical staff and patients. Objective: The objective of this study was to determine and evaluate the factors that influence acceptance and resistance to achieve a successful implementation of new technologies. Methods: The target group was patients residing in Brandenburg and major stakeholders in the local health care structure, for instance, medical institutions and medical professionals. The process relies on 3 models: the technology acceptance model, the unified technology acceptance and use of technology model, and the theory of technical innovation diffusion. Qualitative methodology was employed in this study, and an exploratory design was adopted to gain new insights into a poorly understood phenomenon in the German context. This enabled the researcher to take a flexible approach toward exploring a wide range of secondary data and to choose a different approach when unexpected information emerged. Content analysis was used to identify and interpret the data, and the researcher assured that the meaning associated with the information has concurred with that of the original source. Results: This study confirmed that adoption of new technologies in health care depended on individual opinions of the factors relating to them. Some medical professionals believed that technology would interfere with their ability to make independent diagnoses and their relationships with patients. Doctors also feared that technology was a means of management control. In contrast, other medical staff welcomed technology because it provided them with more opportunities to interact with patients and their carers. Generally, patients were more enthusiastic about technology than medical professionals and health care managers because it allowed them to have greater autonomy in selecting health care options. The need for all groups to be involved in the development of the new health care approach was an important outcome, otherwise resistance to it was likely to be greater. In other words, the strategy for change management was the indicator of success or failure. Therefore, following our analysis, a number of practical precepts emerged that could facilitate user acceptance of digital solutions and innovative medical technologies. Conclusions: The acceptance of digital solutions and innovative medical technology by patients and professionals relies on understanding their anxieties and feelings of insecurity. The process will take time because individuals accept change at different rates. Hence, the development of an extensive user community to fully and successfully implement eHealth is less likely in the short term; however, this should not prevent the push for changes in health care technology.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Safi, Thiessen, Schmailzl - 2018 - Acceptance and Resistance of New Digital Technologies in Medicine Qualitative Study.pdf:pdf},
  keywords     = {health care innovation,information and technology communication,innovation diffusion,telematics infrastructure},
}

@Article{Laerum2004,
  author       = {L{\ae}rum, Hallvard and Faxvaag, Arild},
  year         = {2004},
  journal       = {BMC Medical Informatics and Decision Making},
  title        = {{Task-oriented evaluation of electronic medical records systems: development and validation of a questionnaire for physicians}},
  doi          = {10.1186/1472-6947-4-1},
  issn         = {1472-6947},
  number       = {1},
  pages        = {1},
  url          = {https://doi.org/10.1186/1472-6947-4-1},
  volume       = {4},
  abstract     = {Evaluation is a challenging but necessary part of the development cycle of clinical information systems like the electronic medical records (EMR) system. It is believed that such evaluations should include multiple perspectives, be comparative and employ both qualitative and quantitative methods. Self-administered questionnaires are frequently used as a quantitative evaluation method in medical informatics, but very few validated questionnaires address clinical use of EMR systems.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/L{\ae}rum, Faxvaag - 2004 - Task-oriented evaluation of electronic medical records systems development and validation of a questionnaire for.pdf:pdf},
}

@Article{Heinrich2019a,
  author       = {Heinrich, Florian and Schwenderling, Luisa and Becker, Mathias and Skalej, Martin and Hansen, Christian},
  year         = {2019-10},
  journal       = {Healthcare Technology Letters},
  title        = {{HoloInjection: Augmented Reality Support for CT-guided spinal Needle Injections}},
  doi          = {10.1049/htl.2019.0062},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinrich et al. - 2019 - HoloInjection Augmented Reality Support for CT-guided spinal Needle Injections.pdf:pdf},
}

@Article{Zhu2012,
  author       = {Zhu, Yang Ming and Cochoff, Steven M. and Sukalac, Ronald},
  year         = {2012},
  journal       = {Journal of Digital Imaging},
  title        = {{Automatic patient table removal in CT images}},
  doi          = {10.1007/s10278-012-9454-x},
  issn         = {08971889},
  number       = {4},
  pages        = {480--485},
  volume       = {25},
  abstract     = {In many medical imaging applications, it is desirable and important to localize and remove the patient table from CT images. However, existing methods often require user interactions to define the table and sometimes make inaccurate assumptions about the table shape. Due to different patient table designs, shapes, and characteristics, these methods are not robust in identifying and removing the patient table. This paper proposes a new automatic approach which first identifies and locates the patient table in the sagittal planes and then removes it from the axial planes. The method has been tested successfully against different tables in different products from multiple vendors, showing it is both a versatile and robust technique for patient table removal. {\textcopyright} 2011 Society for Imaging Informatics in Medicine.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Cochoff, Sukalac - 2012 - Automatic patient table removal in CT images.pdf:pdf},
  keywords     = {Computed tomography,Hough transform,Patient table},
}

@Article{Schmiedl2015,
  author     = {Schmiedl, Felix},
  year       = {2015},
  title      = {{Shape Matching and Mesh Segmentation}},
  annotation = {This is just a thesis I grabed to gain an understanding of},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmiedl - 2015 - Shape Matching and Mesh Segmentation.pdf:pdf},
  keywords   = {Largest Common Point Set, Gromov-Hausdorff Distanc},
}

@Article{Marani2016,
  author       = {Marani, Roberto and Ren{\`{o}}, Vito and Nitti, Massimiliano and D'Orazio, Tiziana and Stella, Ettore},
  year         = {2016},
  journal       = {Computer-Aided Civil and Infrastructure Engineering},
  title        = {{A Modified Iterative Closest Point Algorithm for 3D Point Cloud Registration}},
  doi          = {10.1111/mice.12184},
  issn         = {14678667},
  number       = {7},
  pages        = {515--534},
  volume       = {31},
  abstract     = {In this article, an accurate method for the registration of point clouds returned by a 3D rangefinder is presented. The method modifies the well-known iterative closest point (ICP) algorithm by introducing the concept of deletion mask. This term is defined starting from virtual scans of the reconstructed surfaces and using inconsistencies between measurements. In this way, spatial regions of implicit ambiguities, due to edge effects or systematical errors of the rangefinder, are automatically found. Several experiments are performed to compare the proposed method with three ICP variants. Results prove the capability of deletion masks to aid the point cloud registration, lowering the errors of the other ICP variants, regardless the presence of artifacts caused by small changes of the sensor view-point and changes of the environment.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marani et al. - 2016 - A Modified Iterative Closest Point Algorithm for 3D Point Cloud Registration.pdf:pdf},
}

@InCollection{Scholl2018,
  author = {Scholl, Ingrid and Suder, Sebastian and Schiffer, Stefan},
  year   = {2018-01},
  title  = {{Direct Volume Rendering in Virtual Reality}},
  doi    = {10.1007/978-3-662-56537-7_79},
  isbn   = {978-3-662-56536-0},
  pages  = {297--302},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scholl, Suder, Schiffer - 2018 - Direct Volume Rendering in Virtual Reality.pdf:pdf},
}

@Article{Lim2013,
  author       = {Lim, Gi Hyun and Kim, Kun Woo and Suh, Hyowon and Suh, Il Hong and Beetz, Michael},
  year         = {2013},
  journal       = {Autonomous Learning workshop, ICRA},
  title        = {{Knowledge-based Incremental Bayesian Learning for Object Recognition}},
  abstract     = {Some of object recognition approaches are very effective in environments such as industrial settings, where the position and orientation of object could be controlled. However, in everyday human environments, objects are not located in the same place at all times; rather, they are cluttered in such a way that some of them are visually occluded. Thus, this paper proposes a method of robust object recognition combing ontology and probabilistic inference. The basic idea even in a human environment there is organizational principles},
  annotation   = {This is a good paper for proberble template matching in a large area. I didn't get around to reading it all since this is going to require a fair amount of work.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lim et al. - 2013 - Knowledge-based Incremental Bayesian Learning for Object Recognition.pdf:pdf},
}

@Article{Thrun2002,
  author       = {Thrun, Sebastian},
  year         = {2002},
  journal       = {Science},
  title        = {{Robotic Mapping: A Survey}},
  doi          = {10.1126/science.298.5594.699f},
  eprint       = {1004.4027},
  eprinttype   = {arXiv},
  issn         = {00368075},
  number       = {February},
  pages        = {1--35},
  volume       = {298},
  abstract     = {This article provides a comprehensive introduction into the field of robotic mapping, with a focus on indoor mapping. It describes and compares various probabilistic techniques, as they are presently being applied to a vast array of mobile robot mapping problems. The history of robotic mapping is also described, along with an extensive list of open research problems.},
  annotation   = {Good paper about robotics, I orginally read this to learn how to tell appart different objects in an enviorment. This papper is much to broad for that though. Good read if you ever have a spaital awareness issue you should revisit.},
  arxivid      = {1004.4027},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thrun - 2002 - Robotic Mapping A Survey.pdf:pdf},
  isbn         = {9781558608115},
  keywords     = {bayes filters,expectation maximization algorithm,exploration,filters,kalman,mobile robots,robotic mapping},
  pmid         = {634412},
}

@Article{Heinrich2019c,
  author       = {Heinrich, Florian and Joeres, Fabian and Lawonn, Kai and Hansen, Christian},
  year         = {2019},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Comparison of Projective Augmented Reality Concepts to Support Medical Needle Insertion}},
  doi          = {10.1109/TVCG.2019.2903942},
  issn         = {19410506},
  number       = {6},
  pages        = {2157--2167},
  volume       = {25},
  abstract     = {Augmented reality (AR) is a promising tool to improve instrument navigation in needle-based interventions. Limited research has been conducted regarding suitable navigation visualizations. In this work, three navigation concepts based on existing approaches were compared in a user study using a projective AR setup. Each concept was implemented with three different scales for accuracy-to-color mapping and two methods of navigation indicator scaling. Participants were asked to perform simulated needle insertion tasks with each of the resulting 18 prototypes. Insertion angle and insertion depth accuracies were measured and analyzed, as well as task completion time and participants' subjectively perceived task difficulty. Results show a clear ranking of visualization concepts across variables. Less consistent results were obtained for the color and indicator scaling factors. Results suggest that logarithmic indicator scaling achieved better accuracy, but participants perceived it to be more difficult than linear scaling. With specific results for angle and depth accuracy, our study contributes to the future composition of improved navigation support and systems for precise needle insertion or similar applications.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinrich et al. - 2019 - Comparison of Projective Augmented Reality Concepts to Support Medical Needle Insertion.pdf:pdf},
  keywords     = {Visualization,augmented reality,evaluation,instrument guidance,medical navigation systems,needle placement},
}

@Article{Buell2008,
  author       = {Buell, Joseph F. and Thomas, Mark T. and Rudich, Steven and Marvin, Michael and Nagubandi, Ravi and Ravindra, Kadiyala V. and Brock, Guy and McMasters, Kelly M.},
  year         = {2008},
  journal       = {Annals of Surgery},
  title        = {{Experience with more than 500 minimally invasive hepatic procedures}},
  doi          = {10.1097/SLA.0b013e318185e647},
  issn         = {00034932},
  number       = {3},
  pages        = {475--485},
  volume       = {248},
  abstract     = {objective: To evaluate our experience with more than 500 minimally invasive hepatic procedures. Summary Background Data: Recent data have confirmed the safety and efficacy of minimally invasive liver surgery. Despite these reports, no programmatic approach to minimally invasive liver surgery has been proposed. Methods: We retrospectively reviewed all patients who underwent a minimally invasive procedure for the management of hepatic tumors between January 2001 and April 2008. Patients were divided into 3 groups: laparoscopy with intraoperative ultrasound and biopsy only, laparoscopic radiofrequency ablation (RFA), and minimally invasive resection. To compare the various forms of surgery, we analyzed the incidence of complications, tumor recurrence, mortality, and cost. Statistical analysis was performed using $\chi$ analysis, Student t test, Kaplan-Meier survival analysis with the log-rank test, and multivariable Cox models. Results: A total of 590 minimally invasive hepatic procedures were performed during 489 operative interventions. The representative tumor histologies were: hepatocellular carcinoma (HCC; N = 210), colorectal carcinoma (N = 40), miscellaneous liver metastases (N = 42), biliary cancer (N = 20), and benign tumors (N = 176). Thirty-five patients underwent laparoscopic ultrasound and confirmatory biopsy alone; 201 patients underwent 240 laparoscopic RFAs, and 253 patients underwent 306 minimally invasive resections. Conversion rates to open surgery for the RFA and resection group were 2\% overall. One hundred ninety-nine (40.6\%) patients were cirrhotic; 31 resections were performed in cirrhotic patients. Complication and mortality rates for RFA and resection were comparable (11\% vs. 16\%, and 1.5\% vs. 1.6\%). However, complication rates (14\% vs. 29\%; P = 0.02) and mortality (0.3\% vs. 9.7\%; P = 0.006) rates were higher in the cirrhotic versus noncirrhotic resection group. Overall recurrence rates for RFA and resection groups were 24\% and 23\%, respectively. Local recurrence rates were higher in the RFA group (6.3\% versus 1.5\%; P < 0.06). Overall patient survival differed between HCC patients receiving RFA alone and those receiving RFA and OLT (P < 0.0001). Overall survival for cancer patients receiving RFA versus resection differed significantly when unadjusted for other covariates (P = 0.01), and remained marginally significant in a multivariable model (P = 0.056). Conclusions: Minimally invasive hepatic surgery has become a viable alternative to open hepatic surgery. Our present data are equivalent or superior to those encountered in any large open series. Our experience with RFA confirms a low local recurrence rate and an excellent technique for bridging patients to transplantation. Morbidity and mortality rates for minimally invasive hepatic resections in cirrhotics, is similar to other reported open resection series. This series confirmed excellent interim survival rates after laparoscopic HR and superiority over RFA in the treatment of cancer, with significantly lower local tumor recurrence rate. {\textcopyright} 2008 Lippincott Williams \& Wilkins.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buell et al. - 2008 - Experience with more than 500 minimally invasive hepatic procedures.pdf:pdf},
}

@Article{Ko2013,
  author       = {Ko, Sang Min and Chang, Won Suk and Ji, Yong Gu},
  year         = {2013},
  journal       = {International Journal of Human-Computer Interaction},
  title        = {{Usability Principles for Augmented Reality Applications in a Smartphone Environment}},
  doi          = {10.1080/10447318.2012.722466},
  issn         = {10447318},
  number       = {8},
  pages        = {501--515},
  volume       = {29},
  abstract     = {Through the rapid spread of smartphones, users have access to many types of applications similar to those on desktop computer systems. Smartphone applications using augmented reality (AR) technology make use of users' location information. As AR applications will require new evaluation methods, improved usability and user convenience should be developed. The purpose of the current study is to develop usability principles for the development and evaluation of smartphone applications using AR technology. We develop usability principles for smartphone AR applications by analyzing existing research about heuristic evaluation methods, design principles for AR systems, guidelines for handheld mobile device interfaces, and usability principles for the tangible user interface. We conducted a heuristic evaluation for three popularly used smartphone AR applications to identify usability problems. We suggested new design guidelines to solve the identified problems. Then, we developed an improved AR application prototype of an Android-based smartphone, which later was conducted a usability testing to validate the effects of usability principles. {\textcopyright} 2013 Copyright Taylor and Francis Group, LLC.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ko, Chang, Ji - 2013 - Usability Principles for Augmented Reality Applications in a Smartphone Environment.pdf:pdf},
}

@Article{Nasser2019,
  author     = {Nasser, Bilal and Rabani, Amir},
  year       = {2019},
  title      = {{An advanced method for matching partial 3D point clouds to free-form CAD models for in-situ inspection and repair}},
  doi        = {10.1117/12.2525135},
  issn       = {1996756X},
  number     = {June},
  pages      = {5},
  volume     = {1106105},
  annotation = {This is a good paper for suggesting a method to futher robust the ICP algorithm.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nasser, Rabani - 2019 - An advanced method for matching partial 3D point clouds to free-form CAD models for in-situ inspection and repai.pdf:pdf},
  isbn       = {9781510628014},
  keywords   = {iterative closest point,point cloud registration,procrustes},
}

@Article{Nysjoe2017,
  author       = {Nysj{\"{o}}, Fredrik and Olsson, Pontus and Malmberg, Filip and Carlbom, Ingrid B. and Nystr{\"{o}}m, Ingela},
  year         = {2017},
  journal       = {Journal of WSCG},
  title        = {{Using anti-aliased signed distance fields for generating surgical guides and plates from CT images}},
  issn         = {12136964},
  number       = {1},
  pages        = {11--20},
  volume       = {25},
  abstract     = {We present a method for generating shell-like objects such as surgical guides and plates from segmented computed tomography (CT) images, using signed distance fields and constructive solid geometry (CSG). We develop a userfriendly modeling tool which allows a user to quickly design such models with the help of stereo graphics, six degrees-of-freedom input, and haptic feedback, in our existing software for virtual cranio-maxiollofacial surgery planning, HASP. To improve the accuracy and precision of the modeling, we use an anti-aliased distance transform to compute signed distance field values from fuzzy coverage representations of the bone. The models can be generated within a few minutes, with only a few interaction steps, and are 3D printable. The tool has potential to be used by the surgeons themselves, as an alternative to traditional surgery planning services.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nysj{\"{o}} et al. - 2017 - Using anti-aliased signed distance fields for generating surgical guides and plates from CT images.pdf:pdf},
  keywords     = {CT,Distance fields,Implicit modeling,Shells,Virtual surgery planning},
}

@Article{Cheng2018,
  author       = {Cheng, Chih Hao and Chang, Chia Chi and Chen, Ying Hsuan and Lin, Ying Li and Huang, Jing Yuan and Han, Ping Hsuan and Ko, Ju Chun and Lee, Lai Chung},
  year         = {2018},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{GravityCup: A liquid-based haptics for simulating dynamic weight in virtual reality}},
  doi          = {10.1145/3281505.3281569},
  abstract     = {During interaction in a virtual environment, haptic displays provide users with sensations such as vibration, texture simulation, and electrical muscle stimulation. However, as humans perceive object weights naturally in daily life, objects picked up in virtual reality feel unrealistically light. To create an immersive experience in virtual reality that includes weight sensation, we propose GravityCup, a liquid-based haptic feedback device that simulates realistic object weights and inertia when moving virtual handheld objects. In different scenarios, GravityCup uses liquid to provide users with a dynamic weight sensation experience that enhances interaction with handheld objects in virtual reality.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2018 - GravityCup A liquid-based haptics for simulating dynamic weight in virtual reality.pdf:pdf},
  isbn         = {9781450360869},
  keywords     = {Haptics,Liquid-based,Virtual reality,Weight simulation},
}

@Article{HamzaLup2011,
  author       = {Hamza-Lup, Felix G. and Bogdan, Crenguta M. and Popovici, Dorin M. and Costea, Ovidiu D.},
  year         = {2011},
  journal       = {eLmL - International Conference on Mobile, Hybrid, and On-line Learning},
  title        = {{A survey of visuo-haptic simulation in surgical training}},
  issn         = {23084367},
  pages        = {57--62},
  abstract     = {Surgeons must accomplish complex technical and intellectual tasks that can generate unexpected and serious challenges with little or no room for error. In the last decade, computer simulations have played an increasing role in surgical training, pre-operative planning, and biomedical research. Specifically, visuo-haptic simulations have been the focus of research to develop advanced e-Learning systems facilitating surgical training. The cost of haptic hardware was reduced through mass scale production and as haptics gained popularity in the gaming industry. Visuo-haptic simulations combine the tactile sense with visual information and provide training scenarios with a high degree of reality. For surgical training, such scenarios can be used as ways to gain, improve, and assess resident and expert surgeons' skills and knowledge.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamza-Lup et al. - 2011 - A survey of visuo-haptic simulation in surgical training.pdf:pdf},
  isbn         = {9781612081205},
  keywords     = {Haptics,Laparoscopy,Surgical training},
}

@Article{Vespa2018,
  author       = {Vespa, Emanuele and Nikolov, Nikolay and Grimm, Marius and Nardi, Luigi and Kelly, Paul H.J. and Leutenegger, Stefan},
  year         = {2018},
  journal       = {IEEE Robotics and Automation Letters},
  title        = {{Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping}},
  doi          = {10.1109/LRA.2018.2792537},
  issn         = {23773766},
  number       = {2},
  pages        = {1144--1151},
  volume       = {3},
  abstract     = {We present a dense volumetric simultaneous localisation and mapping (SLAM) framework that uses an octree representation for efficient fusion and rendering of either a truncated signed distance field (TSDF) or an occupancy map. The primary aim of this letter is to use one single representation of the environment that can be used not only for robot pose tracking and high-resolution mapping, but seamlessly for planning. We show that our highly efficient octree representation of space fits SLAM and planning purposes in a real-time control loop. In a comprehensive evaluation, we demonstrate dense SLAM accuracy and runtime performance on-par with flat hashing approaches when using TSDF-based maps, and considerable speed-ups when using occupancy mapping compared to standard occupancy maps frameworks. Our SLAM system can run at 10-40 Hz on a modern quadcore CPU, without the need for massive parallelization on a GPU. We, furthermore, demonstrate a probabilistic occupancy mapping as an alternative to TSDF mapping in dense SLAM and show its direct applicability to online motion planning, using the example of informed rapidly-exploring random trees (RRT∗).},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vespa et al. - 2018 - Efficient Octree-Based Volumetric SLAM Supporting Signed-Distance and Occupancy Mapping.pdf:pdf},
  keywords     = {Mapping,simultaneous localisation and mapping (SLAM),visual-based navigation},
}

@InProceedings{Kutter,
  author     = {Kutter, Oliver and Aichert, Andr{\'{e}} and Bichlmeier, Christoph and Traub, J{\"{o}}rg and Euler, Ekkehard and Navab, Nassir},
  title      = {{Real-time volume rendering for high quality visualization in augmented reality}},
  annotation = {Hevely cited in your circles you need to read this.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kutter et al. - Unknown - Real-time volume rendering for high quality visualization in augmented reality.pdf:pdf},
}

@Article{Grubert2017,
  author        = {Grubert, Jens and Itoh, Yuta and Moser, Kenneth and Swan, J Edward},
  year          = {2017},
  journaltitle  = {IEEE Transactions on Visualization and Computer Graphics},
  title         = {{A Survey of Calibration Methods for Optical See-Though Head-Mounted Displays}},
  eprint        = {arXiv:1709.04299v1},
  eprinttype    = {arXiv},
  number        = {9},
  pages         = {2649--2662},
  volume        = {24},
  abstract      = {Optical see-through head-mounted displays (OST HMDs) are a major output medium for Augmented Reality, which have seen significant growth in popularity and usage among the general public due to the growing release of consumer-oriented models, such as the Microsoft Hololens. Unlike Virtual Reality headsets, OST HMDs inherently support the addition of computer-generated graphics directly into the light path between a user's eyes and their view of the physical world. As with most Augmented and Virtual Reality systems, the physical position of an OST HMD is typically determined by an external or embedded 6-Degree-of-Freedom tracking system. However, in order to properly render virtual objects, which are perceived as spatially aligned with the physical environment, it is also necessary to accurately measure the position of the user's eyes within the tracking system's coordinate frame. For over 20 years, researchers have proposed various calibration methods to determine this needed eye position. However, to date, there has not been a comprehensive overview of these procedures and their requirements. Hence, this paper surveys the field of calibration methods for OST HMDs. Specifically, it provides insights into the fundamentals of calibration techniques, and presents an overview of both manual and automatic approaches, as well as evaluation methods and metrics. Finally, it also identifies opportunities for future research.},
  arxivid       = {arXiv:1709.04299v1},
  file          = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grubert et al. - 2017 - A Survey of Calibration Methods for Optical See-Though Head-Mounted Displays.pdf:pdf},
  keywords      = {Calibration,Optical See Though Displays},
  mendeley-tags = {Calibration,Optical See Though Displays},
}

@Book{Preim2019,
  author = {Preim, Bernhard and Meuschke, Monique},
  year   = {2019-08},
  title  = {{Medical Animations: A Survey and a Research Agenda}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preim, Meuschke - 2019 - Medical Animations A Survey and a Research Agenda.pdf:pdf},
}

@Article{Graphics2019,
  author     = {Graphics, Volume and Zheng, Lei},
  year       = {2019},
  title      = {{Instance Segmentation of Fibers from Low Resolution CT Scans via 3D Deep Embedding Learning}},
  eprint     = {arXiv:1901.01034v1},
  eprinttype = {arXiv},
  pages      = {1--12},
  arxivid    = {arXiv:1901.01034v1},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graphics, Zheng - 2019 - Instance Segmentation of Fibers from Low Resolution CT Scans via 3D Deep Embedding Learning.pdf:pdf},
}

@Article{Theisel2002,
  author       = {Theisel, Holger},
  year         = {2002},
  journal       = {Computer Graphics Forum},
  title        = {{Exact Isosurfaces for Marching Cubes}},
  number       = {1},
  pages        = {19--31},
  volume       = {21},
  abstract     = {In this paper we study the exact contours ofa piecewise trilinear scalar field. We show how to represent these contours exactly as trimmed surfaces of triangular rational cubic B´ezier patches. As part of this, we introduce an extension ofthe marching cubes algorithm which gives a topologically exact triangular approximation ofthe contours for any case. Finally, we modify the exact contours to be globally G1 continuous without changing their topologies. We test the algorithm on both theoretical and practical data sets.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Theisel - 2002 - Exact Isosurfaces for Marching Cubes.pdf:pdf},
  keywords     = {3,4,5 computational geometry and,8 scene analysis,acm css,and object representations,cubic bezier patches,curve,i,iso-surface extraction,marching cubes,object modeling,solid,surface,surface fitting,topological correctness},
}

@Article{Luebke1976,
  author       = {Luebke, David and Carolina, North and Hill, Chapel},
  year         = {1976},
  journal       = {Communications},
  title        = {{A Survey of Polygonal Simplification Algorithms UNC Technical Report TR97-045}},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luebke, Carolina, Hill - 1976 - A Survey of Polygonal Simplification Algorithms UNC Technical Report TR97-045.pdf:pdf},
}

@Article{Rossignac1993,
  author = {Rossignac, Jarek and Borrel, Paul},
  year   = {1993-01},
  title  = {{Multi-resolution 3D approximation for rendering complex scenes}},
  doi    = {10.1007/978-3-642-78114-8_29},
  pages  = {455--465},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rossignac, Borrel - 1993 - Multi-resolution 3D approximation for rendering complex scenes.pdf:pdf},
}

@Article{GonzalezFranco2017,
  author       = {Gonzalez-Franco, Mar and Lanier, Jaron},
  year         = {2017},
  journal       = {Frontiers in Psychology},
  title        = {{Model of Illusions and Virtual Reality}},
  doi          = {10.3389/fpsyg.2017.01125},
  issn         = {1664-1078},
  pages        = {1125},
  url          = {https://www.frontiersin.org/article/10.3389/fpsyg.2017.01125},
  volume       = {8},
  abstract     = {In Virtual Reality (VR) it is possible to induce illusions in which users report and behave as if they have entered into altered situations and identities. The effect can be robust enough for participants to respond “realistically,” meaning behaviors are altered as if subjects had been exposed to the scenarios in reality. The circumstances in which such VR illusions take place were first introduced in the 80's. Since then, rigorous empirical evidence has explored a wide set of illusory experiences in VR. Here, we compile this research and propose a neuroscientific model explaining the underlying perceptual and cognitive mechanisms that enable illusions in VR. Furthermore, we describe the minimum instrumentation requirements to support illusory experiences in VR, and discuss the importance and shortcomings of the generic model.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gonzalez-Franco, Lanier - 2017 - Model of Illusions and Virtual Reality.pdf:pdf},
}

@Article{Manteaux2017,
  author       = {Manteaux, P. L. and Wojtan, Christopher and Narain, Rahul and Redon, Stephane and Faure, Fran{\c{c}}ois and Cani, M. P.},
  year         = {2017},
  journal       = {Computer Graphics Forum},
  title        = {{Adaptive Physically Based Models in Computer Graphics}},
  doi          = {10.1111/cgf.12941},
  issn         = {14678659},
  number       = {6},
  pages        = {312--337},
  volume       = {36},
  abstract     = {One of the major challenges in physically based modelling is making simulations efficient. Adaptive models provide an essential solution to these efficiency goals. These models are able to self-adapt in space and time, attempting to provide the best possible compromise between accuracy and speed. This survey reviews the adaptive solutions proposed so far in computer graphics. Models are classified according to the strategy they use for adaptation, from time-stepping and freezing techniques to geometric adaptivity in the form of structured grids, meshes and particles. Applications range from fluids, through deformable bodies, to articulated solids.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manteaux et al. - 2017 - Adaptive Physically Based Models in Computer Graphics.pdf:pdf},
  keywords     = {I.3.7 Computer GraphicsThree-Dimensional Graphics,adaptivity,physically based animation},
}

@Article{Batteau2004,
  author       = {Batteau, Lukas M. and Liu, Alan and {Antoine Maintz}, J. B. and Bhasin, Yogendra and Bowyer, Mark W.},
  year         = {2004},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  title        = {{A study on the perception of haptics in surgical simulation}},
  doi          = {10.1007/978-3-540-25968-8_21},
  issn         = {03029743},
  pages        = {185--192},
  volume       = {3078},
  abstract     = {Physically accurate modeling of human soft-tissue is an active research area in surgical simulation. The challenge is compounded by the need for real-time feedback. A good understanding of human haptic interaction can facilitate tissue modeling research, as achieving accuracy beyond perception may be counterproductive. This paper studies human sensitivity to haptic feedback. Specifically, the ability of individuals to consistently recall specific haptic experience, and their ability to perceive latency in haptic feedback. Results suggest that individual performance varies widely, and that this ability is not correlated with clinical experience. A surprising result was the apparent insensitivity of test subjects to significant latency in haptic feedback. The implications of our findings to the design and development of surgical simulators are discussed. {\textcopyright} Springer-Verlag 2004.},
  annotation   = {In complete paper found at Siemens need to see if the uni has access},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Batteau et al. - 2004 - A study on the perception of haptics in surgical simulation.pdf:pdf},
}

@Article{Narain2013,
  author       = {Narain, Rahul and Pfaff, Tobias and O'Brien, James F.},
  year         = {2013},
  journal       = {ACM Transactions on Graphics},
  title        = {{Folding and crumpling adaptive sheets}},
  doi          = {10.1145/2461912.2462010},
  issn         = {07300301},
  number       = {4},
  url          = {http://graphics.berkeley.edu/papers/Narain-FCA-2013-07/},
  volume       = {32},
  abstract     = {We present a technique for simulating plastic deformation in sheets of thin materials, such as crumpled paper, dented metal, and wrinkled cloth. Our simulation uses a framework of adaptive mesh refinement to dynamically align mesh edges with folds and creases. This framework allows efficient modeling of sharp features and avoids bend locking that would be otherwise caused by stiff in-plane behavior. By using an explicit plastic embedding space we prevent remeshing from causing shape diffusion. We include several examples demonstrating that the resulting method realistically simulates the behavior of thin sheets as they fold and crumple. Copyright {\textcopyright} ACM 2013.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Narain, Pfaff, O'Brien - 2013 - Folding and crumpling adaptive sheets.pdf:pdf},
  keywords     = {Bending,Cloth simulation,Paper,Plastic deformation,Thin sheets},
}

@article{Anatole,
author = {Anatole, L and Coquillart, Sabine and Coiffet, Philippe},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anatole, Coquillart, Coiffet - Unknown - 2000 - Pseudo-Haptic Feedback Can Isometric Input Devices Simulate Force Feedback - L{\'{e}}cuyer et.pdf:pdf},
title = {{2000 - Pseudo-Haptic Feedback Can Isometric Input Devices Simulate Force Feedback - L{\'{e}}cuyer et al. - Proceedings of the IEEE Virtual Rea.pdf}}
}

@Article{Rietzler2018,
  author       = {Rietzler, Michael and Geiselhart, Florian and Frommel, Julian and Rukzio, Enrico},
  year         = {2018},
  journal       = {Conference on Human Factors in Computing Systems - Proceedings},
  title        = {{Conveying the perception of kinesthetic feedback in virtual reality using state-of-the-art hardware}},
  doi          = {10.1145/3173574.3174034},
  volume       = {2018-April},
  abstract     = {Including haptic feedback in current consumer VR applications is frequently challenging, since technical possibilities to create haptic feedback in consumer-grade VR are limited. While most systems include and make use of the possibility to create tactile feedback through vibration, kinesthetic feedback systems almost exclusively rely on external mechanical hardware to induce actual sensations so far. In this paper, we describe an approach to create a feeling of such sensations by using unmodified off-the-shelf hardware and a software solution for a multi-modal pseudo-haptics approach. We first explore this design space by applying user-elicited methods, and afterwards evaluate our refined solution in a user study. The results show that it is indeed possible to communicate kinesthetic feedback by visual and tactile cues only and even induce its perception. While visual clipping was generally unappreciated, our approach led to significant increases of enjoyment and presence.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rietzler et al. - 2018 - Conveying the perception of kinesthetic feedback in virtual reality using state-of-the-art hardware.pdf:pdf},
  isbn         = {9781450356206},
  keywords     = {Kinesthetic feedback,Pseudo haptics,Virtual reality},
}

@Article{Lopes2017,
  author       = {Lopes, Pedro and You, Sijing and Cheng, Lung Pan and Marwecki, Sebastian and Baudisch, Patrick},
  year         = {2017},
  journal       = {Conference on Human Factors in Computing Systems - Proceedings},
  title        = {{Providing haptics to walls \& heavy objects in virtual reality by means of electrical muscle stimulation}},
  doi          = {10.1145/3025453.3025600},
  pages        = {1471--1482},
  volume       = {2017-May},
  abstract     = {We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor. In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on "prevented me from passing through" as well as "realistic." In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles. Copyright is held by the owner/author(s) Publication rights licensed to ACM.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lopes et al. - 2017 - Providing haptics to walls \& heavy objects in virtual reality by means of electrical muscle stimulation.pdf:pdf},
  isbn         = {9781450346559},
  keywords     = {EMS,Force feedback,Muscle interfaces,Virtual reality},
}

@Article{Elkhamisy2018,
  author   = {El-khamisy, Nashaat and Amer, Khaled and Riad, Alaa and Mamdouh, Rafeek},
  year     = {2018},
  title    = {{Survey on Virtual Reality , Augmented Reality and Mixed Reality Techniques for Liver Surgical Operations and Training A Survey on Virtual Reality , Augmented Reality and Mixed Reality Techniques for Liver Surgical Operations and Training}},
  number   = {September},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/El-khamisy et al. - 2018 - Survey on Virtual Reality , Augmented Reality and Mixed Reality Techniques for Liver Surgical Operations and.pdf:pdf},
  keywords = {ar,ct,ldlt,mr,spm,train surgeons,vr},
}

@Article{Elanattil2019,
  author     = {Elanattil, Shafeeq and Moghadam, Peyman},
  year       = {2019},
  title      = {{Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction}},
  doi        = {10.25919/5c495488b0f4e},
  eprint     = {1903.02679},
  eprinttype = {arXiv},
  pages      = {1--9},
  url        = {http://arxiv.org/abs/1903.02679\%0Ahttp://dx.doi.org/10.25919/5c495488b0f4e},
  abstract   = {We introduce a synthetic dataset for evaluating non-rigid 3D human reconstruction based on conventional RGB-D cameras. The dataset consist of seven motion sequences of a single human model. For each motion sequence per-frame ground truth geometry and ground truth skeleton are given. The dataset also contains skinning weights of the human model. More information about the dataset can be found at: https://research.csiro.au/robotics/our-work/databases/synthetic-human-model-dataset/},
  arxivid    = {1903.02679},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elanattil, Moghadam - 2019 - Synthetic Human Model Dataset for Skeleton Driven Non-rigid Motion Tracking and 3D Reconstruction.pdf:pdf},
}

@Article{Faieghi2018,
  author       = {Faieghi, Mohammadreza and Tutunea-Fatan, O. Remus and Eagleson, Roy},
  year         = {2018},
  journal       = {Computer-Aided Design and Applications},
  title        = {{Fast and cross-vendor OpenCL-based implementation for voxelization of triangular mesh models}},
  doi          = {10.1080/16864360.2018.1486961},
  issn         = {16864360},
  number       = {6},
  pages        = {852--862},
  volume       = {15},
  annotation   = {This is a paper that is based on the transfomation of poligonal shapes to voxlized shapes. The concept seems simple but if you ever need to do this there is at least a base model for it. This model here notes advantages with memory and so on compared to other models.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Faieghi, Tutunea-Fatan, Eagleson - 2018 - Fast and cross-vendor OpenCL-based implementation for voxelization of triangular mesh models.pdf:pdf},
  keywords     = {GPGPU,GPU Computing,OpenCL,Voxelization},
}

@Article{Jablonski2016,
  author       = {Jab{\l}o{\'{n}}ski, Szymon and Martyn, Tomasz},
  year         = {2016},
  journal       = {24th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2016 - Full Papers Proceedings},
  title        = {{Real-time voxel rendering algorithm based on Screen Space Billboard Voxel Buffer with Sparse Lookup Textures}},
  pages        = {27--36},
  abstract     = {In this paper, we present a novel approach to efficient real-time rendering of numerous high-resolution voxelized objects. We present a voxel rendering algorithm based on triangle rasterization pipeline with screen space rendering computational complexity. In order to limit the number of vertex shader invocations, voxel filtering algorithm with fixed size voxel data buffer was developed. Voxelized objects are represented by sparse voxel octree (SVO) structure. Using sparse texture available in modern graphics APIs, we create a 3D lookup table for voxel ids. Voxel filtering algorithm is based on 3D sparse texture ray marching approach. Screen Space Billboard Voxel Buffer is filled by voxels from visible voxels point cloud. Thanks to using 3D sparse textures, we are able to store high-resolution objects in VRAM memory. Moreover, sparse texture mipmaps can be used to control object level of detail (LOD). The geometry of a voxelized object is represented by a collection of points extracted from object SVO. Each point is defined by position, normal vector and texture coordinates. We also showhowto take advantage of programmable geometry shaders in order to store voxel objects with extremely lowmemory requirements and to perform real-time visualization. Moreover, geometry shaders are used to generate billboard quads from the point cloud and to perform fast face culling. As a result, we obtained comparable or even better performance results in comparison to SVO ray tracing approach. The number of rendered voxels is limited to defined Screen Space Billboard Voxel Buffer resolution. Last but not least, thanks to graphics card adapter support, developed algorithm can be easily integrated with any graphics engine using triangle rasterization pipeline.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jab{\l}o{\'{n}}ski, Martyn - 2016 - Real-time voxel rendering algorithm based on Screen Space Billboard Voxel Buffer with Sparse Lookup Textures.pdf:pdf},
  keywords     = {Billboarding,Computer graphics,Geometry shader,Point cloud,Sparse texture,Sparse voxel octree,Voxel rendering},
}

@article{Risholm,
author = {Risholm, Petter and Bornik, Alexander and Schmalstieg, Dieter and Samset, Eigil},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Risholm et al. - Unknown - Integrated Surgical Workflow for Augmented Reality Applications.pdf:pdf},
keywords = {augmented reality,medical applications of augmented,medical augmented reality,reality,software solutions for},
title = {{Integrated Surgical Workflow for Augmented Reality Applications}}
}

@Article{Gabeur2019,
  author     = {Gabeur, Valentin and Franco, Jean-Sebastien and Martin, Xavier and Schmid, Cordelia and Rogez, Gregory},
  year       = {2019},
  title      = {{Moulding Humans: Non-parametric 3D Human Shape Estimation from Single Images}},
  eprint     = {1908.00439},
  eprinttype = {arXiv},
  url        = {http://arxiv.org/abs/1908.00439},
  abstract   = {In this paper, we tackle the problem of 3D human shape estimation from single RGB images. While the recent progress in convolutional neural networks has allowed impressive results for 3D human pose estimation, estimating the full 3D shape of a person is still an open issue. Model-based approaches can output precise meshes of naked under-cloth human bodies but fail to estimate details and un-modelled elements such as hair or clothing. On the other hand, non-parametric volumetric approaches can potentially estimate complete shapes but, in practice, they are limited by the resolution of the output grid and cannot produce detailed estimates. In this work, we propose a non-parametric approach that employs a double depth map to represent the 3D shape of a person: a visible depth map and a "hidden" depth map are estimated and combined, to reconstruct the human 3D shape as done with a "mould". This representation through 2D depth maps allows a higher resolution output with a much lower dimension than voxel-based volumetric representations. Additionally, our fully derivable depth-based model allows us to efficiently incorporate a discriminator in an adversarial fashion to improve the accuracy and "humanness" of the 3D output. We train and quantitatively validate our approach on SURREAL and on 3D-HUMANS, a new photorealistic dataset made of semi-synthetic in-house videos annotated with 3D ground truth surfaces.},
  arxivid    = {1908.00439},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gabeur et al. - 2019 - Moulding Humans Non-parametric 3D Human Shape Estimation from Single Images.pdf:pdf},
}

@Article{Zhu2019a,
  author     = {Zhu, Hao and Zuo, Xinxin and Wang, Sen and Cao, Xun and Yang, Ruigang},
  year       = {2019},
  title      = {{Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation}},
  eprint     = {1904.10506},
  eprinttype = {arXiv},
  pages      = {4491--4500},
  url        = {http://arxiv.org/abs/1904.10506},
  abstract   = {This paper presents a novel framework to recover detailed human body shapes from a single image. It is a challenging task due to factors such as variations in human shapes, body poses, and viewpoints. Prior methods typically attempt to recover the human body shape using a parametric based template that lacks the surface details. As such the resulting body shape appears to be without clothing. In this paper, we propose a novel learning-based framework that combines the robustness of parametric model with the flexibility of free-form 3D deformation. We use the deep neural networks to refine the 3D shape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the constraints from body joints, silhouettes, and per-pixel shading information. We are able to restore detailed human body shapes beyond skinned models. Experiments demonstrate that our method has outperformed previous state-of-the-art approaches, achieving better accuracy in terms of both 2D IoU number and 3D metric distance. The code is available in https://github.com/zhuhao-nju/hmd.git},
  arxivid    = {1904.10506},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2019 - Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation.pdf:pdf},
}

@Article{Olajos2016,
  author = {Olajos, Rikard},
  year   = {2016},
  title  = {{Real-Time Rendering of Volumetric Clouds}},
  issn   = {1650-2884},
  url    = {http://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=8893256&fileOId=8893258},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olajos - 2016 - Real-Time Rendering of Volumetric Clouds.pdf:pdf},
}

@Article{Schneider2015,
  author       = {Schneider, Andrew},
  year         = {2015},
  journal       = {Siggraph Course: Advances in Real-Time Rendering in Games},
  title        = {{The Real-time Volumetric Cloudscapes of Horizon: Zero Dawn}},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schneider - 2015 - The Real-time Volumetric Cloudscapes of Horizon Zero Dawn.pdf:pdf},
}

@Article{Buffet2019,
  author       = {Buffet, Thomas and Rohmer, Damien and Barthe, Lo{\"{i}}c and Boissieux, Laurence and Cani, Marie-Paule},
  year         = {2019-07},
  journal       = {ACM Transactions on Graphics},
  title        = {{Implicit Untangling: A Robust Solution for Modeling Layered Clothing}},
  doi          = {10.1145/3306346.3323010},
  number       = {4},
  pages        = {Article No. 120:1--12},
  series       = {Proc. ACM SIGGRAPH},
  url          = {https://hal.archives-ouvertes.fr/hal-02129156},
  volume       = {38},
  keywords     = {Collision Detection ; Collision processing ; Cloth},
  publisher    = {Association for Computing Machinery},
}

@Article{Zhang2018,
  author     = {Zhang, Wenhui},
  year       = {2018},
  title      = {{Medical Volume Reconstruction Techniques}},
  eprint     = {1802.07710},
  eprinttype = {arXiv},
  pages      = {1--33},
  url        = {http://arxiv.org/abs/1802.07710},
  abstract   = {Medical visualization is the use of computers to create 3D images from medical imaging data sets, almost all surgery and cancer treatment in the developed world relies on it.Volume visualization techniques includes iso-surface visualization, mesh visualization and point cloud visualization techniques, these techniques have revolutionized medicine. Much of modern medicine relies on the 3D imaging that is possible with magnetic resonance imaging (MRI) scanners, functional magnetic resonance imaging (fMRI)scanners, positron emission tomography (PET) scanners, ultrasound imaging (US) scanners, X-Ray scanners, bio-marker microscopy imaging scanners and computed tomography (CT) scanners, which make 3D images out of 2D slices. The primary goal of this report is the application-oriented optimization of existing volume rendering methods providing interactive frame rates. Techniques are presented for traditional alpha-blending rendering, surface-shaded display, maximum intensity projection (MIP), and fast previewing with fully interactive parameter control. Different preprocessing strategies are proposed for interactive iso-surface rendering and fast previewing, such as the well-known marching cube algorithm.},
  arxivid    = {1802.07710},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2018 - Medical Volume Reconstruction Techniques.pdf:pdf},
  keywords   = {alpha-blending rendering,iso-surface visualization,marching cube,volume rendering},
}

@InProceedings{Gouis2017,
  author    = {Gouis, B Le and Lehericey, F and Marchai, M and Arnaldi, B and Gouranton, V and L{\'{e}}cuyer, A},
  booktitle = {2017 IEEE World Haptics Conference (WHC)},
  year      = {2017},
  title     = {{Haptic rendering of FEM-based tearing simulation using clusterized collision detection}},
  doi       = {10.1109/WHC.2017.7989936},
  isbn      = {VO  -},
  pages     = {406--411},
  abstract  = {Haptic rendering of deformable phenomena remains computationally-demanding, especially when topology modifications are simulated. Within this context, the haptic rendering of tearing phenomena is under-explored as of today. In this paper we propose a fully-functional interaction pipeline for physically-based simulation of deformable surfaces tearing allowing to reach a haptic interactive rate. It relies on a high efficiency collision detection algorithm for deformable surface meshes, combined with an efficient FEM-based simulation of deformable surfaces enabling tearing process. We especially introduce a novel formulation based on clusters for the collision detection to improve the computation time performances. Our approach is illustrated through interactive use-cases of tearing phenomena with haptic feedback, showing its ability to handle realistic rendering of deformable surface tearing on consumergrade haptic devices.},
  keywords  = {Collision avoidance,Computational modeling,Deformable models,Displacement measurement,FEM-based tearing simulation haptic rendering,Haptic interfaces,Pipelines,Rendering (computer graphics),clusterized collision detection,computation time performance improvement,deformable phenomena haptic rendering,deformable surfaces FEM-based simulation,deformation,finite element analysis,fully-functional interaction pipeline,haptic interactive rate,haptic interfaces,rendering (computer graphics),tearing phenomena haptic rendering},
}

@Article{Lehericey2015,
  author       = {Lehericey, Fran{\c{c}}ois and Gouranton, Val{\'{e}}rie and Arnaldi, Bruno},
  year         = {2015},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{GPU ray-traced collision detection for cloth simulation}},
  doi          = {10.1145/2821592.2821615},
  pages        = {47--50},
  volume       = {13-15-Nove},
  abstract     = {{\textcopyright} 2015 ACM. We propose a method to perform collision detection with cloths with ray-tracing. Our method is able to perform collision detection between cloths and volumetric objects (rigid or deformable) as well as collision detection between cloths (including auto-collision). Our method casts rays between objects to perform collision detection, and an inversion-handling algorithm is introduced to correct errors introduced by discrete simulations. GPU computing is used to improve the performances. Our implementation handles scenes containing deformable objects at an interactive frame-rate, with collision detection lasting a few milliseconds.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehericey, Gouranton, Arnaldi - 2015 - GPU ray-traced collision detection for cloth simulation.pdf:pdf},
  isbn         = {9781450339902},
  keywords     = {Collision detection,Narrow-phase,Ray-tracing},
}

@Article{Mahmood2018,
  author       = {Mahmood, Faisal and Chen, Richard and Sudarsky, Sandra and Yu, Daphne and Durr, Nicholas J.},
  year         = {2018},
  journal       = {Physics in Medicine and Biology},
  title        = {{Deep learning with cinematic rendering: Fine-tuning deep neural networks using photorealistic medical images}},
  doi          = {10.1088/1361-6560/aada93},
  eprint       = {arXiv:1805.08400v3},
  eprinttype   = {arXiv},
  issn         = {13616560},
  number       = {18},
  pages        = {1--17},
  volume       = {63},
  abstract     = {Deep learning has emerged as a powerful artificial intelligence tool to interpret medical images for a growing variety of applications. However, the paucity of medical imaging data with high-quality annotations that is necessary for training such methods ultimately limits their performance. Medical data is challenging to acquire due to privacy issues, shortage of experts available for annotation, limited representation of rare conditions and cost. This problem has previously been addressed by using synthetically generated data. However, networks trained on synthetic data often fail to generalize to real data. Cinematic rendering simulates the propagation and interaction of light passing through tissue models reconstructed from CT data, enabling the generation of photorealistic images. In this paper, we present one of the first applications of cinematic rendering in deep learning, in which we propose to fine-tune synthetic data-driven networks using cinematically rendered CT data for the task of monocular depth estimation in endoscopy. Our experiments demonstrate that: (a) Convolutional Neural Networks (CNNs) trained on synthetic data and fine-tuned on photorealistic cinematically rendered data adapt better to real medical images and demonstrate more robust performance when compared to networks with no fine-tuning, (b) these fine-tuned networks require less training data to converge to an optimal solution, and (c) fine-tuning with data from a variety of photorealistic rendering conditions of the same scene prevents the network from learning patient-specific information and aids in generalizability of the model. Our empirical evaluation demonstrates that networks fine-tuned with cinematically rendered data predict depth with 56.87\% less error for rendered endoscopy images and 27.49\% less error for real porcine colon endoscopy images.},
  arxivid      = {arXiv:1805.08400v3},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahmood et al. - 2018 - Deep learning with cinematic rendering Fine-tuning deep neural networks using photorealistic medical images.pdf:pdf},
  keywords     = {Deep learning,cinematic rendering,convolutional neural networks,endoscopy,fne tuning,photorealistic rendering,synthetic medical images},
}

@Article{Fellner2016,
  author       = {Fellner, Franz A.},
  year         = {2016},
  journal       = {Journal of Biomedical Science and Engineering},
  title        = {{Introducing Cinematic Rendering: A Novel Technique for Post-Processing Medical Imaging Data}},
  doi          = {10.4236/jbise.2016.93013},
  issn         = {1937-6871},
  number       = {03},
  pages        = {170--175},
  volume       = {09},
  abstract     = {Since the 1980s, various techniques have been used in the field of medicine for the post-processing of medical imaging data from computed tomography (CT) and magnetic resonance (MR). They include multiplanar reformations (MPR), maximum intensity projection (MIP) and Volume Rendering (VR). This paper presents the prototype of a new means of post-processing radiological examinations such as CT and MR, a technique that, for the first time, provides photorealistic visua-lizations of the human body. This new procedure was inspired by the quality of images achieved by animation software such as programs used in the entertainment industry, particularly to produce animated films. Thus, the name: Cinematic Rendering. It is already foreseeable that this new method of depiction will quickly be incorporated into the set of instruments employed in so-called virtual anatomy (teaching anatomy through the use of radiological depictions of the human body via X-ray, CT and MR in addition to the use of computer animation programs designed especially for human anatomy). Its potential for medical applications will have to be evaluated by future scientific investigations.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fellner - 2016 - Introducing Cinematic Rendering A Novel Technique for Post-Processing Medical Imaging Data.pdf:pdf},
  keywords     = {Cinematic Rendering, Volume Rendering, Virtual Ana,cinematic rendering,computed tomography,ct,magnetic,virtual anatomy,volume rendering},
}

@Article{Preim2018a,
  author = {Preim, Bernhard and Ropinski, Timo and Isenberg, Petra},
  year   = {2018},
  title  = {{A Critical Analysis of the Evaluation Practice in Medical Visualization To cite this version : Proc . of Eurographics Workshop on Visual Computing for Biology and Medicine ( EG A Critical Analysis of the Evaluation Practice in Medical Visualization}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preim, Ropinski, Isenberg - 2018 - A Critical Analysis of the Evaluation Practice in Medical Visualization To cite this version Proc ..pdf:pdf},
}

@Article{Aldoihi2018,
  author       = {Aldoihi, Saad and Hammami, Omar},
  year         = {2018},
  journal       = {CITS 2018 - 2018 International Conference on Computer, Information and Telecommunication Systems},
  title        = {{Evaluation of CT scan usability for Saudi Arabian users}},
  doi          = {10.1109/CITS.2018.8440165},
  pages        = {3--7},
  abstract     = {Like consumer electronic products, medical devices are becoming more complicated, with performance doubling every two years. With multiple commands and systems to negotiate, cognitive load can make it difficult for users to execute commands effectively. In the case of medical devices, which use advanced technology and require multidisciplinary inputs for design and development, cognitive workload is a significant factor. As these devices are very expensive and operators require specialized training, effective and economical methods are needed to evaluate the user experience. Heuristic evaluation is an effective method of identifying major usability problems and related issues. This study used heuristic evaluation to assess the usability of a CT scan and associated physical and mental loads for Saudi Arabian users. The findings indicate a gender difference in terms of consistency, flexibility, and document attributes, with a statistically significant gender difference in mental load.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aldoihi, Hammami - 2018 - Evaluation of CT scan usability for Saudi Arabian users.pdf:pdf},
  isbn         = {9781538640753},
  keywords     = {CT scan Heuristic Evaluation,Human-computer Interaction,Usability},
}

@Article{Fujinawa2017,
  author       = {Fujinawa, Eisuke and Yoshida, Shigeo and Koyama, Yuki and Narumi, Takuji and Tanikawa, Tomohiro and Hirose, Michitaka},
  year         = {2017},
  journal       = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
  title        = {{Computational design of hand-held VR controllers using haptic shape illusion}},
  doi          = {10.1145/3139131.3139160},
  volume       = {Part F1319},
  abstract     = {{\textcopyright} 2017 Copyright held by the owner/author(s). Humans are capable of haptically perceiving the shape of an object by simply wielding it, even without seeing it. On the other hand, typical hand-held controllers for virtual reality (VR) applications are pre-designed for general applications, and thus not capable of providing appropriate haptic shape perception when wielding specic virtual objects. Contradiction between haptic and visual shape perception causes a lack of immersion and leads to inappropriate object handling in VR. To solve this problem, we propose a novel method for designing hand-held VR controllers which illusorily represent haptic equivalent of visual shape in VR. In ecological psychology, it has been suggested that the perceived shape can be modeled using the limited mass properties of wielded objects. Based on this suggestion, we built a shape perception model using a data-driven approach; we aggregated data of perceived shapes against various hand-held VR controllers with dierent mass properties, and derived the model using regression techniques. We implemented a design system which enables automatic design of hand-held VR controllers whose actual shapes are smaller than target shapes while maintaining their haptic shape perception. We veried that controllers designed with our system can present aimed shape perception irrespective of their actual shapes.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fujinawa et al. - 2017 - Computational design of hand-held VR controllers using haptic shape illusion.pdf:pdf},
  isbn         = {9781450355483},
  keywords     = {computational design,data-driven,perception,virtual reality},
}

@Article{George2019,
  author = {George, Ceenu and Janssen, Philipp and Heuss, David and Alt, Florian and Munich, L M U},
  year   = {2019},
  title  = {{Should I Interrupt or Not ? Understanding Interruptions in Head-Mounted Display Settings}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/George et al. - 2019 - Should I Interrupt or Not Understanding Interruptions in Head-Mounted Display Settings.pdf:pdf},
  isbn   = {1234567245},
}

@Article{Choi2018,
  author       = {Choi, Inrak and Ofek, Eyal and Benko, Hrvoje and Sinclair, Mike and Holz, Christian},
  year         = {2018},
  journal       = {Conference on Human Factors in Computing Systems - Proceedings},
  title        = {{CLAW: A multifunctional handheld haptic controller for grasping, touching, and triggering in virtual reality}},
  doi          = {10.1145/3173574.3174228},
  volume       = {2018-April},
  abstract     = {{\textcopyright} 2018 Copyright held by the owner/author(s). CLAW is a handheld virtual reality controller that augments the typical controller functionality with force feedback and actuated movement to the index finger. Our controller enables three distinct interactions (grasping virtual object, touching virtual surfaces, and triggering) and changes its corresponding haptic rendering by sensing the differences in the user's grasp. A servo motor coupled with a force sensor renders controllable forces to the index finger during grasping and touching. Using position tracking, a voice coil actuator at the index fingertip generates vibrations for various textures synchronized with finger movement. CLAWalso supports a haptic force feedback in the trigger mode when the user holds a gun. We describe the design considerations for CLAW and evaluate its performance through two user studies. The first study obtained qualitative user feedback on the naturalness, effectiveness, and comfort when using the device. The second study investigated the ease of the transition between grasping and touching when using our device.},
  annotation   = {This paper dosn't do a bad job at explaining puesdo haptics it seems to be a very early one but gets the basics quite right.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2018 - CLAW A multifunctional handheld haptic controller for grasping, touching, and triggering in virtual reality.pdf:pdf},
  isbn         = {9781450356206},
  keywords     = {Controller design,Force feedback,Grasping,Haptics,Texture,Touching,Trigger,Virtual reality},
}

@Article{Blackledge2012,
  author       = {Blackledge, J M and Blackledge, M D and Courtney, J N},
  year         = {2012},
  journal       = {International Society for Advanced Science and Technology (ISAST)-Transaction on Computing and Intelligent Systems},
  title        = {{Non-Gaussian Anisotropic Diffusion for Medical Image Processing using the OsiriX DICOM}},
  number       = {1},
  pages        = {24--31},
  volume       = {4},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blackledge, Blackledge, Courtney - 2012 - Non-Gaussian Anisotropic Diffusion for Medical Image Processing using the OsiriX DICOM.pdf:pdf},
}

@Article{Laurent2010,
  author       = {Laurent, Remy and Henriet, Julien and Makovicka, Libor},
  year         = {2010},
  journal       = {Acta Polytechnica},
  title        = {{A morphing technique applied to lung motion in radiotherapy: preliminary results}},
  number       = {n°6},
  volume       = {50},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laurent, Henriet, Makovicka - 2010 - A morphing technique applied to lung motion in radiotherapy preliminary results.pdf:pdf},
  keywords     = {4dct,morphing,organ motion},
}

@Article{Bermejo2017,
  author     = {Bermejo, Carlos and Hui, Pan},
  year       = {2017},
  title      = {{A survey on haptic technologies for mobile augmented reality}},
  eprint     = {1709.00698},
  eprinttype = {arXiv},
  pages      = {1--24},
  url        = {http://arxiv.org/abs/1709.00698},
  abstract   = {Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have gained much research and industry attention these days. The mobile nature of MAR applications limits users' interaction capabilities such as inputs, and haptic feedbacks. This survey reviews current research issues in the area of human computer interaction for MAR and haptic devices. The survey first presents human sensing capabilities and their applicability in AR applications. We classify haptic devices into two groups according to the triggered sense: cutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic: manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR applications, we mainly focus our study on wearable haptic devices for each category and their AR possibilities. To conclude, we discuss the future paths that haptic feedbacks should follow for MAR applications and their challenges.},
  arxivid    = {1709.00698},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bermejo, Hui - 2017 - A survey on haptic technologies for mobile augmented reality.pdf:pdf},
}

@Article{Choi2017,
  author       = {Choi, Inrak and Culbertson, Heather and Miller, Mark R. and Olwal, Alex and Follmer, Sean},
  year         = {2017},
  journal       = {UIST Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
  title        = {{Grabity}},
  doi          = {10.1145/3126594.3126599},
  number       = {17},
  pages        = {119--130},
  url          = {https://www.youtube.com/watch?v=Vj79OLcxnDk},
  abstract     = {Figure 1. Grabity is a novel, unified design based on the combination of vibrotactile feedback, uni-directional brakes, and asymmetric skin stretch. The gripper-style haptic device can simulate grasping motions with a real object (top), in Virtual Reality (bottom). Gravity provides vibrotactile feedback during contact, high stiffness force feedback during grasping, and weight force feedback during lifting. ABSTRACT Ungrounded haptic devices for virtual reality (VR) applica-tions lack the ability to convincingly render the sensations of a grasped virtual object's rigidity and weight. We present Gra-bity, a wearable haptic device designed to simulate kinesthetic pad opposition grip forces and weight for grasping virtual objects in VR. The device is mounted on the index finger and thumb and enables precision grasps with a wide range of motion. A unidirectional brake creates rigid grasping force feedback. Two voice coil actuators create virtual force tangen-tial to each finger pad through asymmetric skin deformation. These forces can be perceived as gravitational and inertial forces of virtual objects. The rotational orientation of the voice coil actuators is passively aligned with the real direction of gravity through a revolute joint, causing the virtual forces to always point downward. This paper evaluates the performance of Grabity through two user studies, finding promising ability to simulate different levels of weight with convincing object rigidity. The first user study shows that Grabity can convey various magnitudes of weight and force sensations to users by manipulating the amplitude of the asymmetric vibration. The second user study shows that users can differentiate different weights in a virtual environment using Grabity.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2017 - Grabity.pdf:pdf},
  isbn         = {9781450349819},
}

@Article{Dachselt2007,
  author       = {Dachselt, Raimund and H{\"{u}}bner, Anett},
  year         = {2007},
  journal       = {Computers and Graphics (Pergamon)},
  title        = {{Three-dimensional menus: A survey and taxonomy}},
  doi          = {10.1016/j.cag.2006.09.006},
  issn         = {00978493},
  number       = {1},
  pages        = {53--65},
  volume       = {31},
  abstract     = {Various interaction techniques have been developed in the field of virtual and augmented reality. Whereas techniques for object selection, manipulation, travel, and wayfinding have already been covered in existing taxonomies in some detail, application control techniques have not yet been sufficiently considered. However, they are needed by almost every mixed reality application, e.g. for choosing from alternative objects or options. For this purpose a great variety of distinct three-dimensional (3D) menu selection techniques is available. This paper surveys existing 3D menus from the corpus of literature and classifies them according to various criteria. The taxonomy introduced here assists developers of interactive 3D applications to better evaluate their options when choosing, optimizing, and implementing a 3D menu technique. Since the taxonomy spans the design space for 3D menu solutions, it also aids researchers in identifying opportunities to improve or create novel virtual menu techniques. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dachselt, H{\"{u}}bner - 2007 - Three-dimensional menus A survey and taxonomy.pdf:pdf},
  keywords     = {3D user interfaces,3D widgets,Augmented reality,Desktop VR,Interaction techniques,Virtual reality},
}

@Article{Triepels2019,
  author       = {Triepels, Charlotte P.R. and Smeets, Carlijn F.A. and Notten, Kim J.B. and Kruitwagen, Roy F.P.M. and Futterer, Jurgen J. and Vergeldt, Tineke F.M. and {Van Kuijk}, Sander M.J.},
  year         = {2019},
  journal       = {Clinical Anatomy},
  title        = {{Does three-dimensional anatomy improve student understanding?}},
  doi          = {10.1002/ca.23405},
  issn         = {10982353},
  number       = {May},
  abstract     = {We aim to provide an overview of the various digital three-dimensional visualizations used for learning anatomy and to assess whether these improve medical students' understanding of anatomy compared to traditional learning methods. Furthermore, we evaluate the attitudes of the users of three-dimensional visualizations. We included articles that compared advanced newer three-dimensional anatomy visualization methods (i.e., virtual reality, augmented reality, and computer-based three-dimensional visualizations) to traditional methods that have been used for a long time (i.e., cadaver and textbooks) with regard to users' understanding of anatomy. Of the 1,148 articles identified, 21 articles reported data on the effectiveness of using three-dimensional visualization methods compared to two-dimensional methods. Twelve articles found that three-dimensional visualization is a significantly more effective learning method compared to traditional methods, whereas nine articles did not find that three-dimensional visualization was a significantly more effective method. In general, based on these articles, medical students prefer to use three-dimensional visualizations to learn anatomy. In most of the articles, using three-dimensional visualization was shown to be a more effective method to gain anatomical knowledge compared to traditional methods. Besides that, students are motivated and interested in using these new visualization methods for learning anatomical structures. Clin. Anat., 2019. ? 2019 Wiley Periodicals, Inc.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Triepels et al. - 2019 - Does three-dimensional anatomy improve student understanding.pdf:pdf},
  keywords     = {anatomy,review,students,three dimensional,traditional methods},
}

@Article{Karambakhsh2019,
  author       = {Karambakhsh, Ahmad and Kamel, Aouaidjia and Sheng, Bin and Li, Ping and Yang, Po and Feng, David Dagan},
  year         = {2019},
  journal       = {International Journal of Information Management},
  title        = {{Deep gesture interaction for augmented anatomy learning}},
  doi          = {10.1016/j.ijinfomgt.2018.03.004},
  issn         = {02684012},
  number       = {March},
  pages        = {328--336},
  volume       = {45},
  abstract     = {Augmented reality is very useful in medical education because of the problem of having body organs in a regular classroom. In this paper, we propose to apply augmented reality to improve the way of teaching in medical schools and institutes. We propose a novel convolutional neural network (CNN) for gesture recognition, which recognizes the human's gestures as a certain instruction. We use augmented reality technology for anatomy learning, which simulates the scenarios where students can learn Anatomy with HoloLens instead of rare specimens. We have used the mesh reconstruction to reconstruct the 3D specimens. A user interface featured augment reality has been designed which fits the common process of anatomy learning. To improve the interaction services, we have applied gestures as an input source and improve the accuracy of gestures recognition by an updated deep convolutional neural network. Our proposed learning method includes many separated train procedures using cloud computing. Each train model and its related inputs have been sent to our cloud and the results are returned to the server. The suggested cloud includes windows and android devices, which are able to install deep convolutional learning libraries. Compared with previous gesture recognition, our approach is not only more accurate but also has more potential for adding new gestures. Furthermore, we have shown that neural networks can be combined with augmented reality as a rising field, and the great potential of augmented reality and neural networks to be employed for medical learning and education systems.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karambakhsh et al. - 2019 - Deep gesture interaction for augmented anatomy learning.pdf:pdf},
  keywords     = {3D reconstruction,Augmented reality,Medical education,Mobile cloud,Neural network},
}

@Article{Gill2015,
  author       = {Gill, Gurman and Beichel, Reinhard R.},
  year         = {2015},
  journal       = {International Journal of Biomedical Imaging},
  title        = {{Lung Segmentation in 4D CT Volumes Based on Robust Active Shape Model Matching}},
  doi          = {10.1155/2015/125648},
  issn         = {16874196},
  volume       = {2015},
  abstract     = {Dynamic and longitudinal lung CT imaging produce 4D lung image data sets, enabling applications like radiation treatment planning or assessment of response to treatment of lung diseases. In this paper, we present a 4D lung segmentation method that mutually utilizes all individual CT volumes to derive segmentations for each CT data set. Our approach is based on a 3D robust active shape model and extends it to fully utilize 4D lung image data sets. This yields an initial segmentation for the 4D volume, which is then refined by using a 4D optimal surface finding algorithm. The approach was evaluated on a diverse set of 152 CT scans of normal and diseased lungs, consisting of total lung capacity and functional residual capacity scan pairs. In addition, a comparison to a 3D segmentation method and a registration based 4D lung segmentation approach was performed. The proposed 4D method obtained an average Dice coefficient of 0.9773±0.0254 , which was statistically significantly better ( p value ≪0.001 ) than the 3D method ( 0.9659±0.0517 ). Compared to the registration based 4D method, our method obtained better or similar performance, but was 58.6\% faster. Also, the method can be easily expanded to process 4D CT data sets consisting of several volumes.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gill, Beichel - 2015 - Lung Segmentation in 4D CT Volumes Based on Robust Active Shape Model Matching.pdf:pdf},
  publisher    = {Hindawi Publishing Corporation},
}

@Article{Chiao2013,
  author       = {Chiao, F B and Resta-Flarer, F and Lesser, J and Ng, J and Ganz, A and Pino-Luey, D and Bennett, H and Perkins, C and Witek, B},
  year         = {2013},
  journal       = {British Journal of Anaesthesia},
  title        = {{Vein visualization: patient characteristic factors and efficacy of a new infrared vein finder technology†}},
  doi          = {https://doi.org/10.1093/bja/aet003},
  issn         = {0007-0912},
  number       = {6},
  pages        = {966--971},
  url          = {http://www.sciencedirect.com/science/article/pii/S000709121753857X},
  volume       = {110},
  abstract     = {Background We investigated the patient characteristic factors that correlate with identification of i.v. cannulation sites with normal eyesight. We evaluated a new infrared vein finding (VF) technology device in identifying i.v. cannulation sites. Methods Each subject underwent two observations: one using the conventional method (CM) of normal, unassisted eyesight and the other with the infrared VF device, VueTek's Veinsite™ (VF). A power analysis for moderate effect size ($\beta$=0.95) required 54 samples for within-subject differences. Results Patient characteristic profiles were obtained from 384 subjects (768 observations). Our sample population exhibited an overall average of 5.8 [95\% confidence interval (CI) 5.4–6.2] veins using CM. As a whole, CM vein visualization were less effective among obese [4.5 (95\% CI 3.8–5.3)], African-American [4.6 (95\% CI 3.6–5.5 veins)], and Asian [5.1 (95\% CI 4.1–6.0)] subjects. Next, the VF technology identified an average of 9.1 (95\% CI 8.6–9.5) possible cannulation sites compared with CM [average of 5.8 (95\% CI 5.4–6.2)]. Seventy-six obese subjects had an average of 4.5 (95\% CI 3.8–5.3) and 8.2 (95\% CI 7.4–9.1) veins viewable by CM and VF, respectively. In dark skin subjects, 9.1 (95\% CI 8.3–9.9) veins were visible by VF compared with 5.4 (95\% CI 4.8–6.0) with CM. Conclusions African-American or Asian ethnicity, and obesity were associated with decreased vein visibility. The visibility of veins eligible for cannulation increased for all subgroups using a new infrared device.},
  keywords     = {catheterization,veins},
}

@Article{Fukuroku2016,
  author       = {Fukuroku, Keiko and Narita, Yugo and Taneda, Yukari and Kobayashi, Shinji and Gayle, Alberto A},
  year         = {2016},
  journal       = {Nurse Education in Practice},
  title        = {{Does infrared visualization improve selection of venipuncture sites for indwelling needle at the forearm in second-year nursing students?}},
  doi          = {https://doi.org/10.1016/j.nepr.2016.02.005},
  issn         = {1471-5953},
  pages        = {1--9},
  url          = {http://www.sciencedirect.com/science/article/pii/S1471595316000275},
  volume       = {18},
  abstract     = {Objectives To evaluate the effectiveness of a vein visualization display system using near-infrared light (“Vein Display”) for the safe and proper selection of venipuncture sites for indwelling needle placement in the forearm. Methods Ten second year nursing students were recruited to apply an indwelling needle line with and without Vein Display. Another ten participants were recruited from various faculty to serve as patients. The quality of the venipuncture procedure at various selected sites was evaluated according to a scale developed by the authors. Time, scores and patterns of puncture-site selection were compared with respect to three different methods: [1] attempt 1 (tourniquet only), [2] attempt 2 (Vein Display only) and [3] attempt 3 (both). To validate the effectiveness of Vein Display, 52 trials were conducted in total. Results We found that venipuncture site selection time was significantly improved with the Vein Display, particularly in the case of difficult to administer venipuncture sites. Overall, we found no significant difference with respect to venipuncture quality, as determined by our scale. Conclusion These results suggest that equipment such as the Vein Display can contribute immensely to the improvement of practical skills, such as venipuncture, especially in the context of elderly patients.},
  keywords     = {Forearm,Indwelling needle line administration,Infrared visualization,Nursing students,Venipuncture site},
}

@Article{Speksnijder2012,
  author       = {Speksnijder, L. and Rousian, M. and Steegers, E. A.P. and {Van Der Spek}, P. J. and Koning, A. H.J. and Steensma, A. B.},
  year         = {2012},
  journal       = {Ultrasound in Obstetrics and Gynecology},
  title        = {{Agreement and reliability of pelvic floor measurements during contraction using three-dimensional pelvic floor ultrasound and virtual reality}},
  doi          = {10.1002/uog.10129},
  issn         = {09607692},
  number       = {1},
  pages        = {87--92},
  volume       = {40},
  abstract     = {Objectives: Virtual reality is a novel method of visualizing ultrasound data with the perception of depth and offers possibilities for measuring non-planar structures. The levator ani hiatus has both convex and concave aspects. The aim of this study was to compare levator ani hiatus volume measurements obtained with conventional three-dimensional (3D) ultrasound and with a virtual reality measurement technique and to establish their reliability and agreement. Methods: 100 symptomatic patients visiting a tertiary pelvic floor clinic with a normal intact levator ani muscle diagnosed on translabial ultrasound were selected. Datasets were analyzed using a rendered volume with a slice thickness of 1.5 cm at the level of minimal hiatal dimensions during contraction. The levator area (in cm 2) was measured and multiplied by 1.5 to get the levator ani hiatus volume in conventional 3D ultrasound (in cm 3). Levator ani hiatus volume measurements were then measured semi-automatically in virtual reality (cm 3) using a segmentation algorithm. An intra- and interobserver analysis of reliability and agreement was performed in 20 randomly chosen patients. Results: The mean difference between levator ani hiatus volume measurements performed using conventional 3D ultrasound and virtual reality was 0.10 (95\% CI, - 0.15 to 0.35) cm 3. The intraclass correlation coefficient (ICC) comparing conventional 3D ultrasound with virtual reality measurements was > 0.96. Intra- and interobserver ICCs for conventional 3D ultrasound measurements were > 0.94 and for virtual reality measurements were > 0.97, indicating good reliability for both. Conclusion: Levator ani hiatus volume measurements performed using virtual reality were reliable and the results were similar to those obtained with conventional 3D ultrasonography. Copyright {\textcopyright} 2012 ISUOG.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Speksnijder et al. - 2012 - Agreement and reliability of pelvic floor measurements during contraction using three-dimensional pelvic flo.pdf:pdf},
  keywords     = {3D ultrasound,levator hiatus,pelvic floor muscles,reliability and agreement,virtual reality},
}

@Article{GarciaVazquez2018,
  author       = {Garc{\'{i}}a-V{\'{a}}zquez, Ver{\'{o}}nica and von Haxthausen, Felix and J{\"{a}}ckle, Sonja and Schumann, Christian and Kuhlemann, Ivo and Bouchagiar, Juljan and H{\"{o}}fer, Anna-Catharina and Matysiak, Florian and H{\"{u}}ttmann, Gereon and Goltz, Jan Peter and Kleemann, Markus and Ernst, Floris and Horn, Marco},
  year         = {2018},
  journal       = {Innovative Surgical Sciences},
  title        = {{Navigation and visualisation with HoloLens in endovascular aortic repair}},
  doi          = {10.1515/iss-2018-2001},
  number       = {3},
  pages        = {167--177},
  volume       = {3},
  abstract     = {Introduction: Endovascular aortic repair (EVAR) is a min- imal-invasive technique that prevents life-threatening rupture in patients with aortic pathologies by implanta- tion of an endoluminal stent graft. During the endovas- cular procedure, device navigation is currently performed by fluoroscopy in combination with digital subtraction angiography. This study presents the current iterative process of biomedical engineering within the disrup- tive interdisciplinary project Nav EVAR, which includes advanced navigation, image techniques and augmented reality with the aim of reducing side effects (namely radia- tion exposure and contrast agent administration) and optimising visualisation during EVAR procedures. This article describes the current prototype developed in this project and the experiments conducted to evaluate it. Methods: The current approach of the Nav EVAR project is guiding EVAR interventions in real-time with an elec- tromagnetic tracking system after attaching a sensor on the catheter tip and displaying this information on Micro- soft HoloLens glasses. This augmented reality technology enables the visualisation of virtual objects superimposed on the real environment. These virtual objects include three-dimensional (3D) objects (namely 3D models of the skin and vascular structures) and two-dimensional (2D) objects [namely orthogonal views of computed tomogra- phy (CT) angiograms, 2D images of 3D vascular models, and 2D images of a new virtual angioscopy whose appear- ance of the vessel wall follows that shown in ex vivo and in vivo angioscopies]. Specific external markers were designed to be used as landmarks in the registration process to map the tracking data and radiological data into a common space. In addition, the use of real-time 3D ultrasound (US) is also under evaluation in the Nav EVAR project for guiding endovascular tools and updat- ing navigation with intraoperative imaging. US volumes are streamed from the US system to HoloLens and visu- alised at a certain distance from the probe by tracking augmented reality markers. A human model torso that includes a 3D printed patient-specific aortic model was built to provide a realistic test environment for evalua- tion of technical components in the Nav EVAR project. The solutions presented in this study were tested by using an US training model and the aortic-aneurysm phantom. Results: During the navigation of the catheter tip in the US training model, the 3D models of the phantom surface and vessels were visualised on HoloLens. In addition, a virtual angioscopy was also built from a CT scan of the aortic-aneurysm phantom. The external markers designed for this study were visible in the CT scan and the electro- magnetically tracked pointer fitted in each marker hole. US volumes of the US training model were sent from the US system to HoloLens in order to display them, showing a latency of 259 ± 86 ms (mean ± standard deviation). Conclusion: The Nav EVAR project tackles the problem of radiation exposure and contrast agent administration during EVAR interventions by using a multidisciplinary approach to guide the endovascular tools. Its current state presents several limitations such as the rigid alignment between preoperative data and the simulated patient. Nevertheless, the techniques shown in this study in com- bination with fibre Bragg gratings and optical coherence tomography are a promising approach to overcome the problems of EVAR interventions.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a-V{\'{a}}zquez et al. - 2018 - Navigation and visualisation with HoloLens in endovascular aortic repair.pdf:pdf},
}

@InProceedings{Eck2014,
  author    = {Eck, U and Pankratz, F and Sandor, C and Klinker, G and Laga, H},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  title     = {{Comprehensive workspace calibration for visuo-haptic augmented reality}},
  doi       = {10.1109/ISMAR.2014.6948417},
  isbn      = {VO  -},
  pages     = {123--128},
  abstract  = {Visuo-haptic augmented reality systems enable users to see and touch digital information that is embedded in the real world. Precise co-location of computer graphics and the haptic stylus is necessary to provide a realistic user experience. PHANToM haptic devices are often used in such systems to provide haptic feedback. They consist of two interlinked joints, whose angles define the position of the haptic stylus and three sensors at the gimbal to sense its orientation. Previous work has focused on calibration procedures that align the haptic workspace within a global reference coordinate system and developing algorithms that compensate the non-linear position error, caused by inaccuracies in the joint angle sensors. In this paper, we present an improved workspace calibration that additionally compensates for errors in the gimbal sensors. This enables us to also align the orientation of the haptic stylus with high precision. To reduce the required time for calibration and to increase the sampling coverage, we utilize time-delay estimation to temporally align external sensor readings. This enables users to continuously move the haptic stylus during the calibration process, as opposed to commonly used point and hold processes. We conducted an evaluation of the calibration procedure for visuo-haptic augmented reality setups with two different PHANToMs and two different optical trackers. Our results show a significant improvement of orientation alignment for both setups over the previous state of the art calibration procedure. Improved position and orientation accuracy results in higher fidelity visual and haptic augmentations, which is crucial for fine-motor tasks in areas including medical training simulators, assembly planning tools, or rapid prototyping applications. A user friendly calibration procedure is essential for real-world applications of VHAR.},
  keywords  = {Calibration,H.5.1. [Information Interfaces and Presentation]:,H.5.2. [Information Interfaces and Presentation],Haptic interfaces,Joints,PHANToM haptic devices,Phantoms,Sensors,Target tracking,User Interfaces &#x2014; [Haptic I/O],VHAR,Visualization,assembly planning tools,augmented and virtual realities],augmented reality,calibration,calibration procedures,calibration process,comprehensive workspace calibration,computer graphics,error compensation,gimbal sensors,global reference coordinate system,haptic augmentations,haptic feedback,haptic interfaces,haptic stylus,haptic workspace,improved workspace calibration,joint angle sensors,medical training simulators,nonlinear position error,optical trackers,optical tracking,orientation accuracy,point and hold processes,position accuracy,rapid prototyping applications,realistic user experience,sampling coverage,sampling methods,time-delay estimation,touch digital information,user friendly calibration procedure,visual augmentations,visuo-haptic augmented reality systems},
}

@Article{Kim2017,
  author       = {Kim, Donghoon and Kim, Yujin and Yoon, Siyeop and Lee, Deukhee},
  year         = {2017-02},
  journal       = {Sensors (Basel, Switzerland)},
  title        = {{Preliminary Study for Designing a Novel Vein-Visualizing Device}},
  doi          = {10.3390/s17020304},
  issn         = {1424-8220},
  language     = {eng},
  number       = {2},
  pages        = {304},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/28178227 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5336071/},
  volume       = {17},
  abstract     = {Venipuncture is an important health diagnosis process. Although venipuncture is one of the most commonly performed procedures in medical environments, locating the veins of infants, obese, anemic, or colored patients is still an arduous task even for skilled practitioners. To solve this problem, several devices using infrared light have recently become commercially available. However, such devices for venipuncture share a common drawback, especially when visualizing deep veins or veins of a thick part of the body like the cubital fossa. This paper proposes a new vein-visualizing device applying a new penetration method using near-infrared (NIR) light. The light module is attached directly on to the declared area of the skin. Then, NIR beam is rayed from two sides of the light module to the vein with a specific angle. This gives a penetration effect. In addition, through an image processing procedure, the vein structure is enhanced to show it more accurately. Through a phantom study, the most effective penetration angle of the NIR module is decided. Additionally, the feasibility of the device is verified through experiments in vivo. The prototype allows us to visualize the vein patterns of thicker body parts, such as arms.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim et al. - 2017 - Preliminary Study for Designing a Novel Vein-Visualizing Device.pdf:pdf},
  keywords     = {*Veins,Arm,Elbow,Humans,Infrared Rays,Phlebotomy,image processing,near-infrared light,penetration,vein-visualizing device,venipuncture},
  publisher    = {MDPI},
}

@Book{Gupta2018,
  author    = {Gupta, Nishant},
  year      = {2018-11},
  title     = {{CT and MR imaging of the upper extremity vasculature: pearls, pitfalls, and challenges}},
  doi       = {10.21037/cdt.2018.09.15},
  volume    = {9},
  booktitle = {Cardiovascular Diagnosis and Therapy},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta - 2018 - CT and MR imaging of the upper extremity vasculature pearls, pitfalls, and challenges.pdf:pdf},
}

@Article{Si2018,
  author       = {Si, W and Liao, X and Qian, Y and Wang, Q},
  year         = {2018},
  journal       = {IEEE Access},
  title        = {{Mixed Reality Guided Radiofrequency Needle Placement: A Pilot Study}},
  doi          = {10.1109/ACCESS.2018.2843378},
  issn         = {2169-3536 VO - 6},
  pages        = {31493--31502},
  volume       = {6},
  abstract     = {This paper presents a novel mixed reality (MR) guidance method for liver tumors radiofrequency ablation (RFA). Compared with traditional computed tomography (CT)-guided method, our system can provide a more natural and intuitive surgical mode for surgeons. In essence, our system is a holographic navigation platform, which projects a MR overlay onto the patient via HoloLens during RFA. We first reconstruct the patient-specific anatomy structure from the CT images of abdominal phantom. Then, a tailored precise registration method is employed to map the virtual-real spatial information. In addition, considering that tumor shifting during biopsy severely impacts the accuracy of RFA, our guidance system involves a motion compensation computation through data-driven physically-based modeling in holographic environment. In experiments, we conduct a user study on the comparison trial between MR-guided and CT-guided biopsy. User feedback demonstrates that our MR guidance method for needle placement procedure has the potential to simplify the operation, reduce the operation difficulty, shorten the operation time, and raise the operation precision.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Si et al. - 2018 - Mixed Reality Guided Radiofrequency Needle Placement A Pilot Study.pdf:pdf},
  keywords     = {CT images,CT-guided biopsy,Computational modeling,HoloLens,Liver,MR guidance method,Mixed reality,Needles,Phantoms,RFA,Surgery,Tumors,Virtual reality,abdominal phantom,augmented reality,biomedical MRI,cancer,computed tomography,computerised tomography,holographic navigation platform,image reconstruction,image registration,intuitive surgical mode,liver,liver tumors radiofrequency ablation,medical image processing,mixed reality guided radiofrequency needle placeme,motion compensation,motion compensation computation,natural surgical mode,needle placement,needle placement procedure,needles,novel mixed reality guidance method,patient-specific anatomy structure,patient-specific anatomy structure reconstruction,phantoms,surgery,tailored precise registration method,tumours,user feedback,virtual-real spatial information},
}

@Article{Edgcumbe2018,
  author       = {Edgcumbe, Philip and Singla, Rohit and Pratt, Philip and Schneider, Caitlin and Nguan, Christopher and Rohling, Robert},
  year         = {2018-04},
  journal       = {Journal of medical imaging (Bellingham, Wash.)},
  title        = {{Follow the light: projector-based augmented reality intracorporeal system for laparoscopic surgery}},
  doi          = {10.1117/1.JMI.5.2.021216},
  issn         = {2329-4302},
  language     = {eng},
  number       = {2},
  pages        = {21216},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/29487888 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5812432/},
  volume       = {5},
  abstract     = {A projector-based augmented reality intracorporeal system (PARIS) is presented that includes a miniature tracked projector, tracked marker, and laparoscopic ultrasound (LUS) transducer. PARIS was developed to improve the efficacy and safety of laparoscopic partial nephrectomy (LPN). In particular, it has been demonstrated to effectively assist in the identification of tumor boundaries during surgery and to improve the surgeon's understanding of the underlying anatomy. PARIS achieves this by displaying the orthographic projection of the cancerous tumor on the kidney's surface. The performance of PARIS was evaluated in a user study with two surgeons who performed 32 simulated robot-assisted partial nephrectomies. They performed 16 simulated partial nephrectomies with PARIS for guidance and 16 simulated partial nephrectomies with only an LUS transducer for guidance. With PARIS, there was a significant reduction [30\% ([Formula: see text])] in the amount of healthy tissue excised and a trend toward a more accurate dissection around the tumor and more negative margins. The combined point tracking and reprojection root-mean-square error of PARIS was 0.8 mm. PARIS' proven ability to improve key metrics of LPN surgery and qualitative feedback from surgeons about PARIS supports the hypothesis that it is an effective surgical navigation tool.},
  edition      = {2018/02/14},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edgcumbe et al. - 2018 - Follow the light projector-based augmented reality intracorporeal system for laparoscopic surgery.pdf:pdf},
  keywords     = {augmented reality,computer vision,image-guided procedure,intraoperative ultrasound imaging,laparoscopic surgery,projector,surgical navigation},
  publisher    = {Society of Photo-Optical Instrumentation Engineers},
}

@Article{Liu2018,
  author       = {Liu, Xiaoming and Guo, Shuxu and Yang, Bingtao and Ma, Shuzhi and Zhang, Huimao and Li, Jing and Sun, Changjian and Jin, Lanyi and Li, Xueyan and Yang, Qi and Fu, Yu},
  year         = {2018-10},
  journal       = {Journal of Digital Imaging},
  title        = {{Automatic Organ Segmentation for CT Scans Based on Super-Pixel and Convolutional Neural Networks}},
  doi          = {10.1007/s10278-018-0052-4},
  issn         = {1618-727X},
  number       = {5},
  pages        = {748--760},
  url          = {https://doi.org/10.1007/s10278-018-0052-4},
  volume       = {31},
  abstract     = {Accurate segmentation of specific organ from computed tomography (CT) scans is a basic and crucial task for accurate diagnosis and treatment. To avoid time-consuming manual optimization and to help physicians distinguish diseases, an automatic organ segmentation framework is presented. The framework utilized convolution neural networks (CNN) to classify pixels. To reduce the redundant inputs, the simple linear iterative clustering (SLIC) of super-pixels and the support vector machine (SVM) classifier are introduced. To establish the perfect boundary of organs in one-pixel-level, the pixels need to be classified step-by-step. First, the SLIC is used to cut an image into grids and extract respective digital signatures. Next, the signature is classified by the SVM, and the rough edges are acquired. Finally, a precise boundary is obtained by the CNN, which is based on patches around each pixel-point. The framework is applied to abdominal CT scans of livers and high-resolution computed tomography (HRCT) scans of lungs. The experimental CT scans are derived from two public datasets (Sliver 07 and a Chinese local dataset). Experimental results show that the proposed method can precisely and efficiently detect the organs. This method consumes 38 s/slice for liver segmentation. The Dice coefficient of the liver segmentation results reaches to 97.43\%. For lung segmentation, the Dice coefficient is 97.93\%. This finding demonstrates that the proposed framework is a favorable method for lung segmentation of HRCT scans.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2018 - Automatic Organ Segmentation for CT Scans Based on Super-Pixel and Convolutional Neural Networks.pdf:pdf},
}

@Article{Frantz2018,
  author       = {Frantz, Taylor and Jansen, Bart and Duerinck, Johnny and Vandemeulebroucke, Jef},
  year         = {2018},
  journal       = {Healthcare Technology Letters},
  title        = {{Augmenting microsoft's HoloLens with vuforia tracking for neuronavigation}},
  doi          = {10.1049/htl.2018.5079},
  issn         = {20533713},
  number       = {5},
  pages        = {221--225},
  volume       = {5},
  abstract     = {Major hurdles for Microsoft's HoloLens as a tool in medicine have been access to tracking data, as well as a relatively high localization error of the displayed information; cumulatively resulting in its limited use and minimal quantification. The following work investigates the augmentation of HoloLens with the proprietary image processing SDK Vuforia, allowing integration of data from its front-facing RGB camera to provide more spatially stable holograms for neuronavigational use. Continuous camera tracking was able to maintain hologram registration with a mean perceived drift of 1.41mm, as well as a mean sub two-millimeter surface point localization accuracy of 53\%, all while allowing the researcher to walk about a test area. This represents a 68\% improvement for the later and a 34\% improvement for the former compared with a typical HoloLens deployment used as a control. Both represent a significant improvement on hologram stability given the current state of the art, and to the best of our knowledge are the first example of quantified measurements when augmenting hologram stability using data from the RGB sensor.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frantz et al. - 2018 - Augmenting microsoft's HoloLens with vuforia tracking for neuronavigation.pdf:pdf},
}

@Article{Sutherland2019,
  author       = {Sutherland, Justin and Belec, Jason and Sheikh, Adnan and Chepelev, Leonid and Althobaity, Waleed and Chow, Benjamin J.W. and Mitsouras, Dimitrios and Christensen, Andy and Rybicki, Frank J. and {La Russa}, Daniel J.},
  year         = {2019},
  journal       = {Journal of Digital Imaging},
  title        = {{Applying Modern Virtual and Augmented Reality Technologies to Medical Images and Models}},
  doi          = {10.1007/s10278-018-0122-7},
  issn         = {1618727X},
  number       = {1},
  pages        = {38--53},
  volume       = {32},
  abstract     = {Recent technological innovations have created new opportunities for the increased adoption of virtual reality (VR) and augmented reality (AR) applications in medicine. While medical applications of VR have historically seen greater adoption from patient-as-user applications, the new era of VR/AR technology has created the conditions for wider adoption of clinician-as-user applications. Historically, adoption to clinical use has been limited in part by the ability of the technology to achieve a sufficient quality of experience. This article reviews the definitions of virtual and augmented reality and briefly covers the history of their development. Currently available options for consumer-level virtual and augmented reality systems are presented, along with a discussion of technical considerations for their adoption in the clinical environment. Finally, a brief review of the literature of medical VR/AR applications is presented prior to introducing a comprehensive conceptual framework for the viewing and manipulation of medical images in virtual and augmented reality. Using this framework, we outline considerations for placing these methods directly into a radiology-based workflow and show how it can be applied to a variety of clinical scenarios.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutherland et al. - 2019 - Applying Modern Virtual and Augmented Reality Technologies to Medical Images and Models.pdf:pdf},
  keywords     = {Augmented reality,Radiology,Virtual reality,Visualization},
  publisher    = {Journal of Digital Imaging},
}

@Article{Alterovitz2009,
  author       = {Alterovitz, Ron and Goldberg, Kenneth Y and Pouliot, Jean and Hsu, I-Chow Joe},
  year         = {2009-03},
  journal       = {IEEE transactions on information technology in biomedicine : a publication of the IEEE Engineering in Medicine and Biology Society},
  title        = {{Sensorless motion planning for medical needle insertion in deformable tissues}},
  doi          = {10.1109/TITB.2008.2008393},
  issn         = {1558-0032},
  language     = {eng},
  number       = {2},
  pages        = {217--225},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/19126473 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2822650/},
  volume       = {13},
  abstract     = {Minimally invasive medical procedures such as biopsies, anesthesia drug injections, and brachytherapy cancer treatments require inserting a needle to a specific target inside soft tissues. This is difficult because needle insertion displaces and deforms the surrounding soft tissues causing the target to move during the procedure. To facilitate physician training and preoperative planning for these procedures, we develop a needle insertion motion planning system based on an interactive simulation of needle insertion in deformable tissues and numerical optimization to reduce placement error. We describe a 2-D physically based, dynamic simulation of needle insertion that uses a finite-element model of deformable soft tissues and models needle cutting and frictional forces along the needle shaft. The simulation offers guarantees on simulation stability for mesh modifications and achieves interactive, real-time performance on a standard PC. Using texture mapping, the simulation provides visualization comparable to ultrasound images that the physician would see during the procedure. We use the simulation as a component of a sensorless planning algorithm that uses numerical optimization to compute needle insertion offsets that compensate for tissue deformations. We apply the method to radioactive seed implantation during permanent seed prostate brachytherapy to minimize seed placement error.},
  annotation   = {Not really in my space but a good read for needle insertions},
  edition      = {2008/12/31},
  keywords     = {*Brachytherapy/instrumentation/methods,*Punctures/instrumentation/methods,Algorithms,Computer Simulation,Elastic Modulus,Humans,Male,Models, Anatomic,Needles,Poisson Distribution,Prostate,Prostatic Neoplasms/radiotherapy/therapy,Robotics/instrumentation/methods},
}

@Article{Pfeiffer2018,
  author       = {Pfeiffer, Micha and Kenngott, Hannes and Preukschas, Anas and Huber, Matthias and Bettscheider, Lisa and M{\"{u}}ller-Stich, Beat and Speidel, Stefanie},
  year         = {2018},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title        = {{IMHOTEP: virtual reality framework for surgical applications}},
  doi          = {10.1007/s11548-018-1730-x},
  issn         = {18616429},
  number       = {5},
  pages        = {741--748},
  url          = {https://doi.org/10.1007/s11548-018-1730-x},
  volume       = {13},
  abstract     = {{\textcopyright} 2018 CARS Purpose: The data which is available to surgeons before, during and after surgery is steadily increasing in quantity as well as diversity. When planning a patient's treatment, this large amount of information can be difficult to interpret. To aid in processing the information, new methods need to be found to present multimodal patient data, ideally combining textual, imagery, temporal and 3D data in a holistic and context-aware system. Methods: We present an open-source framework which allows handling of patient data in a virtual reality (VR) environment. By using VR technology, the workspace available to the surgeon is maximized and 3D patient data is rendered in stereo, which increases depth perception. The framework organizes the data into workspaces and contains tools which allow users to control, manipulate and enhance the data. Due to the framework's modular design, it can easily be adapted and extended for various clinical applications. Results: The framework was evaluated by clinical personnel (77 participants). The majority of the group stated that a complex surgical situation is easier to comprehend by using the framework, and that it is very well suited for education. Furthermore, the application to various clinical scenarios—including the simulation of excitation propagation in the human atrium—demonstrated the framework's adaptability. As a feasibility study, the framework was used during the planning phase of the surgical removal of a large central carcinoma from a patient's liver. Conclusion: The clinical evaluation showed a large potential and high acceptance for the VR environment in a medical context. The various applications confirmed that the framework is easily extended and can be used in real-time simulation as well as for the manipulation of complex anatomical structures.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pfeiffer et al. - 2018 - IMHOTEP virtual reality framework for surgical applications.pdf:pdf},
  keywords     = {Advanced medical visualization,Surgical planning,Virtual reality},
  publisher    = {Springer International Publishing},
}

@Article{Agten2018,
  author       = {Agten, Christoph A. and Dennler, Cyrill and Rosskopf, Andrea B. and Jaberg, Laurenz and Pfirrmann, Christian W.A. and Farshad, Mazda},
  year         = {2018},
  journal       = {Investigative Radiology},
  title        = {{Augmented Reality-Guided Lumbar Facet Joint Injections}},
  doi          = {10.1097/RLI.0000000000000478},
  issn         = {15360210},
  number       = {8},
  pages        = {495--498},
  volume       = {53},
  abstract     = {Objectives The aim of this study was to assess feasibility and accuracy of augmented reality-guided lumbar facet joint injections. Materials and Methods A spine phantom completely embedded in hardened opaque agar with 3 ring markers was built. A 3-dimensional model of the phantom was uploaded to an augmented reality headset (Microsoft HoloLens). Two radiologists independently performed 20 augmented reality-guided and 20 computed tomography (CT)-guided facet joint injections each: for each augmented reality-guided injection, the hologram was manually aligned with the phantom container using the ring markers. The radiologists targeted the virtual facet joint and tried to place the needle tip in the holographic joint space. Computed tomography was performed after each needle placement to document final needle tip position. Time needed from grabbing the needle to final needle placement was measured for each simulated injection. An independent radiologist rated images of all needle placements in a randomized order blinded to modality (augmented reality vs CT) and performer as perfect, acceptable, incorrect, or unsafe. Accuracy and time to place needles were compared between augmented reality-guided and CT-guided facet joint injections. Results In total, 39/40 (97.5\%) of augmented reality-guided needle placements were either perfect or acceptable compared with 40/40 (100\%) CT-guided needle placements (P = 0.5). One augmented reality-guided injection missed the facet joint space by 2 mm. No unsafe needle placements occurred. Time to final needle placement was substantially faster with augmented reality guidance (mean 14 6 seconds vs 39 +/- 15 seconds, P < 0.001 for both readers). Conclusions Augmented reality-guided facet joint injections are feasible and accurate without potentially harmful needle placement in an experimental setting.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agten et al. - 2018 - Augmented Reality-Guided Lumbar Facet Joint Injections.pdf:pdf},
  isbn         = {0000000000000},
  keywords     = {augmented reality,injections,lumber fact joints,mixed reality,phantom,spine},
}

@Article{Knodel2018,
  author       = {Knodel, Markus M. and Lemke, Babett and Lampe, Michael and Hoffer, Michael and Gillmann, Clarissa and Uder, Michael and Hillenga{\ss}, Jens and Wittum, Gabriel and B{\"{a}}uerle, Tobias},
  year         = {2018},
  journal       = {Computing and Visualization in Science},
  title        = {{Virtual reality in advanced medical immersive imaging: a workflow for introducing virtual reality as a supporting tool in medical imaging}},
  doi          = {10.1007/s00791-018-0292-3},
  issn         = {14330369},
  number       = {6},
  pages        = {203--212},
  url          = {https://doi.org/10.1007/s00791-018-0292-3},
  volume       = {18},
  abstract     = {{\textcopyright} 2018, Springer-Verlag GmbH Germany, part of Springer Nature. Radiologic evaluation of images from computed tomography (CT) or magnetic resonance imaging for diagnostic purposes is based on the analysis of single slices, occasionally supplementing this information with 3D reconstructions as well as surface or volume rendered images. However, due to the complexity of anatomical or pathological structures in biomedical imaging, innovative visualization techniques are required to display morphological characteristics three dimensionally. Virtual reality is a modern tool of representing visual data, The observer has the impression of being “inside” a virtual surrounding, which is referred to as immersive imaging. Such techniques are currently being used in technical applications, e.g. in the automobile industry. Our aim is to introduce a workflow realized within one simple program which processes common image stacks from CT, produces 3D volume and surface reconstruction and rendering, and finally includes the data into a virtual reality device equipped with a motion head tracking cave automatic virtual environment system. Such techniques have the potential to augment the possibilities in non-invasive medical imaging, e.g. for surgical planning or educational purposes to add another dimension for advanced understanding of complex anatomical and pathological structures. To this end, the reconstructions are based on advanced mathematical techniques and the corresponding grids which we can export are intended to form the basis for simulations of mathematical models of the pathogenesis of different diseases.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Knodel et al. - 2018 - Virtual reality in advanced medical immersive imaging a workflow for introducing virtual reality as a supporting.pdf:pdf},
  keywords     = {Cave automatic virtual environment,Imaging,Postprocessing,Surface reconstruction,Surface rendering,Virtual reality,Volume rendering},
  publisher    = {Springer Berlin Heidelberg},
}

@Article{Rynio2019,
  author       = {Rynio, Pawe{\l} and Witowski, Jan and Kami{\'{n}}ski, Jakub and Serafin, Jakub and Kazimierczak, Arkadiusz and Gutowski, Piotr},
  year         = {2019},
  journal       = {Journal of Endovascular Therapy},
  title        = {{Holographically-Guided Endovascular Aneurysm Repair}},
  doi          = {10.1177/1526602819854468},
  issn         = {15451550},
  pages        = {2--5},
  abstract     = {Purpose: To demonstrate the feasibility of augmented reality visualization in planning and navigating endovascular aortic repair. Technique: A 77-year-old patient with abdominal aortic aneurysm was treated with endovascular repair. An augmented reality head-mounted display was used during the procedure. The aneurysm and bones were projected as 3-dimensional holograms. The operator controlled the device with gestures and voice commands (movement, rotation, cutting through, and zooming). Moreover, the hologram was placed in front of the angiography monitor and manually registered with fluoroscopy. Conclusion: Augmented reality with holographic rendering is feasible and helpful during endovascular aortic repair. Its routine use could possibly lead to shorter operating time, reduced contrast volume, and lower radiation dose; however, larger studies are required to obtain statistically significant results on the outcomes.},
  annotation   = {CarnaLife Holo software was a program that allowed these guys to place volume renderings on to the hololens},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rynio et al. - 2019 - Holographically-Guided Endovascular Aneurysm Repair.pdf:pdf},
  keywords     = {abdominal aortic aneurysm,augmented reality,computed tomography angiography,endovascular aneurysm repair,hologram,image fusion,mixed reality,three-dimensional imaging},
}

@Article{Alismail2019,
  author       = {Alismail, Abdullah and Thomas, Jonathan and Daher, Noha S and Cohen, Avi and Almutairi, Waleed and Terry, Michael H and Huang, Cynthia and Tan, Laren},
  year         = {2019},
  journal       = {Advances in Medical Education and Practice},
  title        = {{<p>Augmented reality glasses improve adherence to evidence-based intubation practice</p>}},
  doi          = {10.2147/amep.s201640},
  pages        = {279--286},
  volume       = {Volume 10},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alismail et al. - 2019 - pAugmented reality glasses improve adherence to evidence-based intubation practicep.pdf:pdf},
  keywords     = {augmented reality,intubation,medical education,simulation},
}

@Article{FuertesMunoz2019,
  author       = {{Fuertes Mu{\~{n}}oz}, Gabriel and Mollineda, Ram{\'{o}}n A and {Gallardo Casero}, Jes{\'{u}}s and Pla, Filiberto},
  year         = {2019},
  journal       = {Sensors},
  title        = {{A RGBD-Based Interactive System for Gaming-Driven Rehabilitation of Upper Limbs}},
  doi          = {10.3390/s19163478},
  issn         = {1424-8220},
  number       = {16},
  url          = {https://www.mdpi.com/1424-8220/19/16/3478},
  volume       = {19},
  abstract     = {Current physiotherapy services may not be effective or suitable for certain patients due to lack of motivation, poor adherence to exercises, insufficient supervision and feedback or, in the worst case, refusal to continue with the rehabilitation plan. This paper introduces a novel approach for rehabilitation of upper limbs through KineActiv, a platform based on Microsoft Kinect v2 and developed in Unity Engine. KineActiv proposes exergames to encourage patients to perform rehabilitation exercises prescribed by a specialist, controls the patient&prime;s performance, and corrects execution errors on the fly. KineActiv comprises a web platform where the physiotherapist can review session results, monitor patient health, and adjust rehabilitation routines. We recruited 10 patients for assessing the system usability as well as the system performance. Results show that KineActiv is a usable, enjoyable and reliable system, that does not cause any negative feelings.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fuertes Mu{\~{n}}oz et al. - 2019 - A RGBD-Based Interactive System for Gaming-Driven Rehabilitation of Upper Limbs.pdf:pdf},
}

@Article{Hiranaka2017,
  author       = {Hiranaka, Takafumi and Nakanishi, Yuta and Fujishiro, Takaaki and Hida, Yuichi and Tsubosaka, Masanori and Shibata, Yosaku and Okimura, Kenjiro and Uemoto, Harunobu},
  year         = {2017-01},
  journal       = {Surgical Innovation},
  title        = {{The Use of Smart Glasses for Surgical Video Streaming}},
  doi          = {10.1177/1553350616685431},
  issn         = {1553-3506},
  number       = {2},
  pages        = {151--154},
  url          = {https://doi.org/10.1177/1553350616685431},
  volume       = {24},
  abstract     = {Observation of surgical procedures performed by experts is extremely important for acquisition and improvement of surgical skills. Smart glasses are small computers, which comprise a head-mounted monitor and video camera, and can be connected to the internet. They can be used for remote observation of surgeries by video streaming. Although Google Glass is the most commonly used smart glasses for medical purposes, it is still unavailable commercially and has some limitations. This article reports the use of a different type of smart glasses, InfoLinker, for surgical video streaming. InfoLinker has been commercially available in Japan for industrial purposes for more than 2 years. It is connected to a video server via wireless internet directly, and streaming video can be seen anywhere an internet connection is available. We have attempted live video streaming of knee arthroplasty operations that were viewed at several different locations, including foreign countries, on a common web browser. Although the quality of video images depended on the resolution and dynamic range of the video camera, speed of internet connection, and the wearer?s attention to minimize image shaking, video streaming could be easily performed throughout the procedure. The wearer could confirm the quality of the video as the video was being shot by the head-mounted display. The time and cost for observation of surgical procedures can be reduced by InfoLinker, and further improvement of hardware as well as the wearer?s video shooting technique is expected. We believe that this can be used in other medical settings.},
  annotation   = {doi: 10.1177/1553350616685431},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hiranaka et al. - 2017 - The Use of Smart Glasses for Surgical Video Streaming.pdf:pdf},
  publisher    = {SAGE Publications Inc},
}

@Article{Hoe2019,
  author       = {Hoe, Zheng-Yu and Lee, I-Jui and Chen, Chien-Hsu and Chang, Kuo-Ping},
  year         = {2019-06},
  journal       = {Universal Access in the Information Society},
  title        = {{Using an augmented reality-based training system to promote spatial visualization ability for the elderly}},
  doi          = {10.1007/s10209-017-0597-x},
  issn         = {1615-5297},
  number       = {2},
  pages        = {327--342},
  url          = {https://doi.org/10.1007/s10209-017-0597-x},
  volume       = {18},
  abstract     = {The physical condition and cognitive ability of older adults tends to decline. This study focused on the development of an augmented reality (AR)-based rehabilitation training system to improve the spatial visualization and mental rotation abilities of elderly people. Using one's imagination to manipulate objects is common in everyday life. However, training tasks for the elderly are still presented in two-dimension, which research indicates generates a cognitive load that reduces the participants' interest and diminishes the effects of training. AR can effectively reduce cognitive load, improve one's sense of spatial direction, and increase participants' interest in training. Therefore, this study used AR technology, combined with a tangible user interface as a manual controller, to allow participants to directly manipulate a virtual three-dimensional model that used a cube to conduct mental rotation tasks (MRT) for the elderly to improve their mental rotation ability. After 6 weeks of intervention, we used an ABA (reversal) design and paired-sample t tests in SPSS to compare the learning effects on the experimental group's pre- and posttests. The participants' error rates significantly declined and their reaction times significantly improved during the MRT test.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoe et al. - 2019 - Using an augmented reality-based training system to promote spatial visualization ability for the elderly.pdf:pdf},
}

@Article{VidalSicart2018,
  author       = {Vidal-Sicart, S and Olmos, R Vald{\'{e}}s and Nieweg, O E and Faccini, R and Grootendorst, M R and Wester, H J and Navab, N and Vojnovic, B and van der Poel, H and Mart{\'{i}}nez-Rom{\'{a}}n, S and Klode, J and Wawroschek, F and van Leeuwen, F W B},
  year         = {2018},
  journal       = {Revista Espa{\~{n}}ola de Medicina Nuclear e Imagen Molecular (English Edition)},
  title        = {{From interventionist imaging to intraoperative guidance: New perspectives by combining advanced tools and navigation with radio-guided surgery}},
  doi          = {https://doi.org/10.1016/j.remnie.2017.10.009},
  issn         = {2253-8089},
  number       = {1},
  pages        = {28--40},
  url          = {http://www.sciencedirect.com/science/article/pii/S2253808917301222},
  volume       = {37},
  abstract     = {The integration of medical imaging technologies into diagnostic and therapeutic approaches can provide a preoperative insight into both anatomical (e.g. using computed tomography (CT), magnetic resonance (MR) imaging, or ultrasound (US)), as well as functional aspects (e.g. using single photon emission computed tomography (SPECT), positron emission tomography (PET), lymphoscintigraphy, or optical imaging). Moreover, some imaging modalities are also used in an interventional setting (e.g. CT, US, gamma or optical imaging) where they provide the surgeon with real-time information during the procedure. Various tools and approaches for image-guided navigation in cancer surgery are becoming feasible today. With the development of new tracers and portable imaging devices, these advances will reinforce the role of interventional molecular imaging. Resumen La integraci{\'{o}}n de tecnolog{\'{i}}as de imagen m{\'{e}}dica en los enfoques diagn{\'{o}}sticos y terap{\'{e}}uticos puede proporcionar una perspectiva preoperatoria tanto en los aspectos anat{\'{o}}micos (tomograf{\'{i}}a computarizada, resonancia magn{\'{e}}tica o ecograf{\'{i}}a) como funcional (tomograf{\'{i}}a computarizada de emisi{\'{o}}n de fot{\'{o}}n {\'{u}}nico, tomograf{\'{i}}a por emisi{\'{o}}n de positrones, linfogammagraf{\'{i}}a o imagen {\'{o}}ptica). Adem{\'{a}}s, algunas modalidades de imagen se utilizan tambi{\'{e}}n en un entorno intervencionista (tomograf{\'{i}}a computarizada, ecograf{\'{i}}a, im{\'{a}}genes gammagr{\'{a}}ficas o im{\'{a}}genes {\'{o}}pticas), donde proporcionan al cirujano informaci{\'{o}}n en tiempo real durante el procedimiento. En la actualidad, son factibles diversas herramientas y enfoques metodol{\'{o}}gicos para la navegaci{\'{o}}n guiada por im{\'{a}}genes en la cirug{\'{i}}a del c{\'{a}}ncer. Con el desarrollo de nuevos trazadores y dispositivos port{\'{a}}tiles de imagen, estos avances reforzar{\'{a}}n el papel de la imagen molecular intervencionista.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vidal-Sicart et al. - 2018 - From interventionist imaging to intraoperative guidance New perspectives by combining advanced tools and na.pdf:pdf},
  keywords     = {Cirug{\'{i}}a radioguiada,Ganglio centinela,Imagen molecular,Imagen {\'{o}}ptica,Molecular imaging,Optical image,Radioguided surgery,Sentinel node},
}

@Article{Abeywardena2019,
  author       = {Abeywardena, Sajeeva and Yuan, Qiaodi and Tzemanaki, Antonia and Psomopoulou, Efi and Droukas, Leonidas and Melhuish, Chris and Dogramadzi, Sanja},
  year         = {2019},
  journal       = {Frontiers in Robotics and AI},
  title        = {{Estimation of Tool-Tissue Forces in Robot-Assisted Minimally Invasive Surgery Using Neural Networks}},
  doi          = {10.3389/frobt.2019.00056},
  issn         = {2296-9144},
  pages        = {56},
  url          = {https://www.frontiersin.org/article/10.3389/frobt.2019.00056},
  volume       = {6},
  abstract     = {A new algorithm is proposed to estimate the tool-tissue force interaction in robot-assisted minimally invasive surgery which does not require the use of external force sensing. The proposed method utilises the current of the motors of the surgical instrument and neural network methods to estimate the force interaction. Offline and online testing is conducted to assess the feasibility of the developed algorithm. Results showed that the developed method has promise in allowing online estimation of tool-tissue force and could thus enable haptic feedback in robotic surgery to be provided.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abeywardena et al. - 2019 - Estimation of Tool-Tissue Forces in Robot-Assisted Minimally Invasive Surgery Using Neural Networks.pdf:pdf},
}

@Article{Kasprzak2019,
  author       = {Kasprzak, Jaroslaw D and Pawlowski, Jaroslaw and Peruga, Jan Z and Kaminski, Jakub and Lipiec, Piotr},
  year         = {2019-04},
  journal       = {European Heart Journal},
  title        = {{First-in-man experience with real-time holographic mixed reality display of three-dimensional echocardiography during structural intervention: balloon mitral commissurotomy}},
  doi          = {10.1093/eurheartj/ehz127},
  issn         = {0195-668X},
  url          = {https://doi.org/10.1093/eurheartj/ehz127},
  abstract     = {Three-dimensional transoesophageal echocardiography (3DTOE) is widely used as an aid during structural percutaneous cardiac interventions. However, displaying 3D information on standard flat monitors is limited in this setting. Therefore, we developed the method of real-time streaming of 3DTOE data into head-mounted mixed-reality holographic display allowing for touchless control and data sharing within cath-lab. The method was tested for the first time in human during percutaneous mitral balloon commissurotomy (PMC). PMC was performed using Inoue balloon in a 74-year-old woman with New York Heart Association (NYHA) class III rheumatic mitral stenosis, 43 years after surgical valvuloplasty (area 1.3 cm2, mean gradient 8 mmHg, conscious sedation with midazolam/fentanyl) and monitored with real-time 3DTOE (Vivid e95, GE Healthcare, Horten, Norway). Raw 3D data were streamed from standard echocardiograph using custom connection to 3D DICOM viewer workstation (CarnaLife Holo, MedApp, Krakow, Poland) for real-time, dynamic 3D rendering and wirelessly transferred into HoloLens mixed reality display (Microsoft, Redmond, USA) to overlay non-obstructive 3D data hologram upon reality view. Data were visible as a semitransparent holographic cube positioned in a convenient sector of visual field of echocardiographist and shared by interventional cardiologist (Figure). Hologram could be controlled (e.g. zoom, position, cropping) in a touchless manner by voice commands and gestures recognized by HoloLens without perceptible delay. Specific views were used to assess preprocedural anatomy, support transseptal puncture, positioning of wires, orifice crossing, and balloon positioning for inflation, and later, to verify outcomes. The procedure was successfully completed within 60 min with 81 mGy irradiation, with 1.8 cm2 final mitral valve area.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kasprzak et al. - 2019 - First-in-man experience with real-time holographic mixed reality display of three-dimensional echocardiography.pdf:pdf},
}

@Article{Roy2019,
  author       = {Roy, Susmita and Gr{\"{u}}nwald, Alexander T. D. and Alves-Pinto, Ana and Maier, Robert and Cremers, Daniel and Pfeiffer, Daniela and Lampe, Ren{\'{e}}e},
  year         = {2019-05},
  journal       = {BioMed Research International},
  title        = {{A Noninvasive 3D Body Scanner and Software Tool towards Analysis of Scoliosis}},
  doi          = {10.1155/2019/4715720},
  issn         = {2314-6133},
  number       = {c},
  pages        = {1--15},
  url          = {https://www.hindawi.com/journals/bmri/2019/4715720/},
  volume       = {2019},
  abstract     = {Purpose. Children with neurological disorders, such as cerebral palsy (CP), have a high risk of developing scoliosis during growth. The fast progression of scoliosis implies in several cases frequent clinical and X-ray examinations. We present an ionizing radiation-free, noncontacting method to estimate the trajectory of the vertebral column and to potentially facilitate medical diagnosis in cases where an X-ray examination is not indicated. Methods. A body scanner and corresponding analysis software tools have been developed to get 3D surface scans of patient torsos and to analyze their spinal curvatures. The trajectory of the vertebral column has been deduced from the body contours at different transverse sectional planes along the vertical torso axis. In order to verify the present methods, we have analyzed twenty-five torso contours, extracted from computer tomography (CT) images of patients who had a CT scan for other medical reasons, but incidentally also showed a scoliosis. The software tools therefore process data from the body scanner as well as X-ray or CT images. Results. The methods presented show good results in the estimations of the lateral deviation of the spine for mild and moderate scoliosis. The partial mismatch for severe cases is associated with a less accurate estimation of the rotation of the vertebrae around the vertical body axis in these cases. In addition, distinct torso contour shapes, in the transverse sections, have been characterized according to the severity of the scoliosis. Conclusion. The hardware and software tools are a first step towards an ionizing radiation-free analysis of progression of scoliosis. However, further improvements of the analysis methods and tests on a larger number of data sets with diverse types of scoliosis are necessary, before its introduction into clinical application as a supplementary tool to conventional examinations.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roy et al. - 2019 - A Noninvasive 3D Body Scanner and Software Tool towards Analysis of Scoliosis.pdf:pdf},
}

@Article{Abiri2019,
  author       = {Abiri, Ahmad and Pensa, Jake and Tao, Anna and Ma, Ji and Juo, Yen-Yi and Askari, Syed J and Bisley, James and Rosen, Jacob and Dutson, Erik P and Grundfest, Warren S},
  year         = {2019},
  journal       = {Scientific Reports},
  title        = {{Multi-Modal Haptic Feedback for Grip Force Reduction in Robotic Surgery}},
  doi          = {10.1038/s41598-019-40821-1},
  issn         = {2045-2322},
  number       = {1},
  pages        = {5016},
  url          = {https://doi.org/10.1038/s41598-019-40821-1},
  volume       = {9},
  abstract     = {Minimally invasive robotic surgery allows for many advantages over traditional surgical procedures, but the loss of force feedback combined with a potential for strong grasping forces can result in excessive tissue damage. Single modality haptic feedback systems have been designed and tested in an attempt to diminish grasping forces, but the results still fall short of natural performance. A multi-modal pneumatic feedback system was designed to allow for tactile, kinesthetic, and vibrotactile feedback, with the aims of more closely imitating natural touch and further improving the effectiveness of HFS in robotic surgical applications and tasks such as tissue grasping and manipulation. Testing of the multi-modal system yielded very promising results with an average force reduction of nearly 50\% between the no feedback and hybrid (tactile and kinesthetic) trials (p < 1.0E-16). The multi-modal system demonstrated an increased reduction over single modality feedback solutions and indicated that the system can help users achieve average grip forces closer to those normally possible with the human hand.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abiri et al. - 2019 - Multi-Modal Haptic Feedback for Grip Force Reduction in Robotic Surgery.pdf:pdf},
}

@Book{Drouin2015,
  author   = {Drouin, Simon and Kersten-Oertel, Marta and Collins, Louis},
  year     = {2015-10},
  title    = {{Interaction-Based Registration Correction for Improved Augmented Reality Overlay in Neurosurgery}},
  doi      = {10.1007/978-3-319-24601-7_3},
  abstract = {In image-guided neurosurgery the patient is registered with the reference of a tracking system and preoperative data before sterile draping. Due to several factors extensively reported in the literature, the accuracy of this registration can be much deteriorated after the initial phases of the surgery. In this paper, we present a simple method that allows the surgeon to correct the initial registration by tracing corresponding features in the real and virtual parts of an augmented reality view of the surgical field using a tracked pointer. Results of a preliminary study on a phantom yielded a target registration error of 4.06 ± 0.91 mm, which is comparable to results for initial landmark registration reported in the literature.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Drouin, Kersten-Oertel, Collins - 2015 - Interaction-Based Registration Correction for Improved Augmented Reality Overlay in Neurosurger.pdf:pdf},
}

@Book{Oguma2014,
  author   = {Oguma, Ryo and Nakaguchi, Toshiya and Nakamura, Ryoichi and Yamaguchi, Tadashi and Kawahira, Hiroshi and Haneishi, Hideaki},
  year     = {2014},
  title    = {{Ultrasound Image Overlay onto Endoscopic Image by Fusing 2D-3D Tracking of Laparoscopic Ultrasound Probe}},
  doi      = {10.1007/978-3-319-10437-9_2},
  isbn     = {9783319246000},
  pages    = {14--22},
  abstract = {{\textcopyright} Springer International Publishing Switzerland 2014.In laparoscopic surgery, to identify the location of lesions and blood vessels inside organs, an ultrasound probe which can be inserted through small incision is used. However, since surgeons must observe the laparoscopic and ultrasound images both at the same time, it is difficult to understand the correspondence between the ultrasound image and real space. Therefore, to recognize the correspondence between these two images intuitively, we developed a system for overlaying ultrasound image on the laparoscopic image. Since the tip of the probe is flexed freely, we acquired the probe tip position and orientation using a method for detecting the probe angle from laparoscopic image and information obtained from optical tracking sensor. As a result of an experiment using a wire phantom, overlaying error of ultrasound images was found to be 0.97 mm. Furthermore, the rate of probe angle detection was evaluated through an animal experiment to be 83.1\%.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oguma et al. - 2014 - Ultrasound Image Overlay onto Endoscopic Image by Fusing 2D-3D Tracking of Laparoscopic Ultrasound Probe.pdf:pdf},
}

@Article{Cutolo2016,
  author       = {Cutolo, Fabrizio and Freschi, Cinzia and Mascioli, Stefano and Parchi, Paolo and Ferrari, Mauro and Ferrari, Vincenzo},
  year         = {2016},
  journal       = {Electronics},
  title        = {{Robust and Accurate Algorithm for Wearable Stereoscopic Augmented Reality with Three Indistinguishable Markers}},
  doi          = {10.3390/electronics5030059},
  number       = {4},
  pages        = {59},
  volume       = {5},
  abstract     = {In the context of surgical navigation systems based on augmented reality (AR), the key challenge is to ensure the highest degree of realism in merging computer-generated elements with live views of the surgical scene. This paper presents an algorithm suited for wearable stereoscopic augmented reality video see-through systems for use in a clinical scenario. A video-based tracking solution is proposed that relies on stereo localization of three monochromatic markers rigidly constrained to the scene. A PnP-based optimization step is introduced to refine separately the pose of the two cameras. Video-based tracking methods using monochromatic markers are robust to non-controllable and/or inconsistent lighting conditions. The two-stage camera pose estimation algorithm provides sub-pixel registration accuracy. From a technological and an ergonomic standpoint, the proposed approach represents an effective solution to the implementation of wearable AR-based surgical navigation systems wherever rigid anatomies are involved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cutolo et al. - 2016 - Robust and Accurate Algorithm for Wearable Stereoscopic Augmented Reality with Three Indistinguishable Markers.pdf:pdf},
  keywords     = {augmented reality,image-guided surgery,machine vision,wearable displays},
}

@Article{Unger2019,
  author       = {Unger, Michael and Black, David and Fischer, Nele M and Neumuth, Thomas and Glaser, Bernhard},
  year         = {2019},
  journal       = {The International Journal of Medical Robotics and Computer Assisted Surgery},
  title        = {{Design and evaluation of an eye tracking support system for the scrub nurse}},
  doi          = {10.1002/rcs.1954},
  number       = {1},
  pages        = {e1954},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rcs.1954},
  volume       = {15},
  abstract     = {Abstract Background The availability of an increasing number of medical devices in the digital operating room has led to increased interaction demands of the surgical staff. To counteract the risk of bacterial contamination induced by device interactions, touchless interaction techniques are required. Support systems based on eye tracking enable interaction while maintaining sterility and freeing the hands to manipulate surgical instruments. Methods A system using eye tracking glasses was developed. In an evaluation, participants completed tasks using gaze gestures. Three use cases were evaluated in an intraoperative setup. System performance, user acceptance, and workload were measured. Results The system was evaluated in a laboratory environment with 26 participants. The precision of the gaze gesture recognition is 97.9\%, and the true positive rate is 98.5\%. The participants rated the system useful and were satisfied. Conclusions Touchless interaction ensures sterility, although the increasing availability of medical devices in the operating room.},
  annotation   = {e1954 RCS-18-0105.R1},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unger et al. - 2019 - Design and evaluation of an eye tracking support system for the scrub nurse.pdf:pdf},
  keywords     = {digital OR,eye tracking,interaction,operating room,scrub nurse,support system,touchless},
}

@Article{Shu2018,
  author       = {Shu, Jiayu and Kosta, Sokol and Zheng, Rui and Hui, Pan},
  year         = {2018},
  journal       = {2018 IEEE International Conference on Pervasive Computing and Communications, PerCom 2018},
  title        = {{Talk2Me: A Framework for Device-To-Device Augmented Reality Social Network}},
  doi          = {10.1109/PERCOM.2018.8444578},
  abstract     = {The continuous proliferation of mobile and wearable smart devices, together with their increasing computational power and multitude of sensors, has given birth to innovative applications that enhance the world with virtual layers of processed information. In this paper, we present Talk2Me, an augmented reality social network framework that enables users to disseminate information in a distributed way and view others' information instantly. Talk2Me advertises users' messages, together with their face-signature, to every nearby device in a Device-to–Device fashion. When a user looks at nearby persons through her camera-enabled wearable devices (e.g., Google Glass), the framework automatically extracts the face-signature of the person of interest, compares it with the previously captured signatures, and presents the information shared by this person to the user. We design and implement Talk2Me to be lightweight, given that it runs on mobile devices with limited power. We analyze different content dissemination strategies to find the best protocol that yields reliable and fast information-spreading, while reducing the number of packets and containing the energy consumption on the devices. We design a novel face recognition algorithm for this specific scenario with a small number of face features and limited computing capability. Evaluation results of the prototype with real users and extensive simulations validate the performance and usability of our design, showing the potentials of the augmented reality social network framework in real-world scenarios.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shu et al. - 2018 - Talk2Me A Framework for Device-To-Device Augmented Reality Social Network.pdf:pdf},
  isbn         = {9781538632246},
}

@Article{Peng2018,
  author       = {Peng, Huaishu and Briggs, Jimmy and Wang, Cheng-Yao and Guo, Kevin and Kider, Joseph and Mueller, Stefanie and Baudish, Patrick and Guimbreti{\`{e}}re, Fran{\c{c}}ois},
  year         = {2018},
  journal       = {Proc. of CHI},
  title        = {{RoMA: Interactive Fabrication with Augmented Reality and a Robotic 3D Printer}},
  doi          = {10.1145/3173574.3174153},
  pages        = {1--12},
  url          = {http://dl.acm.org/citation.cfm?doid=3173574.3174153\%0Ahttp://www.huaishu.me/projects/roma.pdf},
  abstract     = {We present the Robotic Modeling Assistant (RoMA), an interactive fabrication system providing a fast, precise, hands-on and in-situ modeling experience. As a designer creates a new model using RoMA AR CAD editor, features are constructed concurrently by a 3D printing robotic arm sharing the same design volume. The partially printed physical model then serves as a tangible reference for the designer as she adds new elements to her design. RoMA's proxemics-inspired handshake mechanism between the designer and the 3D printing robotic arm allows the designer to quickly interrupt printing to access a printed area or to indicate that the robot can take full control of the model to finish printing. RoMA lets users integrate real-world constraints into a design rapidly, allowing them to create well-proportioned tangible artifacts or to extend existing objects. We conclude by presenting the strengths and limitations of our current design.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peng et al. - 2018 - RoMA Interactive Fabrication with Augmented Reality and a Robotic 3D Printer.pdf:pdf},
  isbn         = {9781450356206},
  keywords     = {Augmented Reality,Author Keywords 3D printing,CAD,Interactive Fabrication,Rapid Prototyping,User Interfaces},
}

@Book{Nakajima2019,
  author    = {Nakajima, Yoshikatsu and Saito, Hideo},
  year      = {2019-01},
  title     = {{Efficient Object-oriented Semantic Mapping with Object Detector}},
  doi       = {10.1109/ACCESS.2018.2887022},
  pages     = {3206--3213},
  volume    = {7},
  abstract  = {Incrementally building a 3D map in which object instances are semantically annotated has a wide range of applications, including scene understanding, human-robot interactions, and simultaneous localization and mapping (SLAM) extensions. Although researchers are developing efficient and accurate systems, these methods still face a critical issue: real-time processing, because the task requires a series of heavy processing components, e.g., camera pose estimation, 3D map reconstruction, and especially recognition. In this paper, we propose a novel object-oriented semantic mapping approach aiming at overcoming such issues by introducing highly accurate object-oriented semantic scene reconstruction in real-time. For high efficiency, the proposed method employs a fast and scalable object detection algorithm for exploiting semantic information from the incoming frames. These outputs are integrated into geometric regions of the 3D map, which are carried by the geometric-based incremental segmentation method. The strategy of assigning class probabilities to each segmented region, not each element (e.g., surfels and voxels), notably reduces the computational cost, as well as the memory footprint. In addition to efficiency, by geometrically segmenting the 3D map first, clear boundaries between objects appear. We complementarily improve geometric-based segmentation results beyond the geometric only to the semanticaware representation. We validate the proposed method's accuracy and computational efficiency through experiments in a common office scene.},
  booktitle = {IEEE Access},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nakajima, Saito - 2019 - Efficient Object-oriented Semantic Mapping with Object Detector.pdf:pdf},
}

@Book{Garon2016,
  author     = {Garon, Mathieu and Boulet, Pierre-Olivier and Doironz, Jean-Philippe and Beaulieu, Luc and Lalonde, Jean-Fran{\c{c}}ois},
  year       = {2016-09},
  title      = {{Real-Time High Resolution 3D Data on the HoloLens}},
  doi        = {10.1109/ISMAR-Adjunct.2016.0073},
  pages      = {189--191},
  abstract   = {The recent appearance of augmented reality headsets, such as the Microsoft HoloLens, is a marked move from traditional 2D screen to 3D hologram-like interfaces. Striving to be completely portable, these devices unfortunately suffer multiple limitations, such as the lack of real-time, high quality depth data, which severely restricts their use as research tools. To mitigate this restriction, we provide a simple method to augment a HoloLens headset with much higher resolution depth data. To do so, we calibrate an external depth sensor connected to a computer stick that communicates with the HoloLens headset in real-time. To show how this system could be useful to the research community, we present an implementation of small object detection on HoloLens device.},
  annotation = {I didn't read this paper thougherly but it does state that calibaration with just the original hololens wasn't really possible in a 3D enviroment. I think it could be a useful reference.},
}

@Article{Grinshpoon2018,
  author       = {Grinshpoon, Alon and Sadri, Shirin and Loeb, Gabrielle J. and Elvezio, Carmine and Feiner, Steven K.},
  year         = {2018},
  journal       = {25th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2018 - Proceedings},
  title        = {{Hands-Free Interaction for Augmented Reality in Vascular Interventions}},
  doi          = {10.1109/VR.2018.8446259},
  number       = {May},
  pages        = {751--752},
  abstract     = {Vascular interventions are minimally invasive surgical procedures in which a physician navigates a catheter through a patient's vasculature to a desired destination in the patient's body. Since perception of relevant patient anatomy is limited in procedures of this sort, virtual reality and augmented reality systems have been developed to assist in 3D navigation. These systems often require user interaction, yet both of the physician's hands may already be busy performing the procedure. To address this need, we demonstrate hands-free interaction techniques that use voice and head tracking to allow the physician to interact with 3D virtual content on a head-worn display while making both hands available intraoperatively. Our approach supports rotation and scaling of 3D anatomical models that appear to reside in the surrounding environment through small head rotations using first-order control, and rigid body transformation of those models using zero-order control. This allows the physician to easily manipulate a model while it stays close to the center of their field of view.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grinshpoon et al. - 2018 - Hands-Free Interaction for Augmented Reality in Vascular Interventions.pdf:pdf},
  isbn         = {9781538633656},
  keywords     = {Hands-free interaction,augmented reality,head tracking,head-worn display,vascular interventions},
}

@Book{Jorge2016,
  author = {Jorge, Nelson and Morgado, Lina and Gaspar, Pedro},
  year   = {2016-06},
  title  = {{Augmented Learning Environment for wound care simulation}},
}

@Article{Cheng2018a,
  author       = {Cheng, Qiangqiang and Liu, Peter X. and Lai, Pinhua and Xu, Shaoping and Zou, Yanni},
  year         = {2018},
  journal       = {Journal of Healthcare Engineering},
  title        = {{A Novel Haptic Interactive Approach to Simulation of Surgery Cutting Based on Mesh and Meshless Models}},
  doi          = {10.1155/2018/9204949},
  issn         = {2040-2295},
  pages        = {1--16},
  volume       = {2018},
  abstract     = {In the present work, the majority of implemented virtual surgery simulation systems have been based on either a mesh or meshless strategy with regard to soft tissue modelling. To take full advantage of the mesh and meshless models, a novel coupled soft tissue cutting model is proposed. Specifically, the reconstructed virtual soft tissue consists of two essential components. One is associated with surface mesh that is convenient for surface rendering and the other with internal meshless point elements that is used to calculate the force feedback during cutting. To combine two components in a seamless way, virtual points are introduced. During the simulation of cutting, the Bezier curve is used to characterize smooth and vivid incision on the surface mesh. At the same time, the deformation of internal soft tissue caused by cutting operation can be treated as displacements of the internal point elements. Furthermore, we discussed and proved the stability and convergence of the proposed approach theoretically. The real biomechanical tests verified the validity of the introduced model. And the simulation experiments show that the proposed approach offers high computational efficiency and good visual effect, enabling cutting of soft tissue with high stability.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng et al. - 2018 - A Novel Haptic Interactive Approach to Simulation of Surgery Cutting Based on Mesh and Meshless Models.pdf:pdf},
}

@Article{Marker2017,
  author       = {Marker, David R. and U-Thainual, Paweena and Ungi, Tamas and Flammang, Aaron J. and Fichtinger, Gabor and Iordachita, Iulian I. and Carrino, John A. and Fritz, Jan},
  year         = {2017},
  journal       = {Diagnostic and Interventional Radiology},
  title        = {{1.5 T augmented reality navigated interventional MRI: Paravertebral sympathetic plexus injections}},
  doi          = {10.5152/dir.2017.16323},
  issn         = {13053612},
  number       = {3},
  pages        = {227--232},
  volume       = {23},
  abstract     = {{\textcopyright} Turkish Society of Radiology 2017. The high contrast resolution and absent ionizing radiation of interventional magnetic resonance imaging (MRI) can be advantageous for paravertebral sympathetic nerve plexus injections. We assessed the feasibility and technical performance of MRI-guided paravertebral sympathetic injections utilizing augmented reality navigation and 1.5 T MRI scanner. METHODS A total of 23 bilateral injections of the thoracic (8/23, 35\%), lumbar (8/23, 35\%), and hypogastric (7/23, 30\%) paravertebral sympathetic plexus were prospectively planned in twelve human cadavers using a 1.5 Tesla (T) MRI scanner and augmented reality navigation system. MRI-conditional needles were used. Gadolinium-DTPA-enhanced saline was injected. Outcome variables included the number of control magnetic resonance images, target error of the needle tip, punctures of critical nontarget structures, distribution of the injected fluid, and  procedure length. RESULTS Augmented-reality navigated MRI guidance at 1.5 T provided detailed anatomical visualization for successful targeting of the paravertebral space, needle placement, and perineural paravertebral injections in 46 of 46 targets (100\%). A mean of 2 images (range, 1–5 images) were required to control needle placement. Changes of the needle trajectory occurred in 9 of 46 targets (20\%) and changes of needle advancement occurred in 6 of 46 targets (13\%), which were statistically not related to spinal regions (P = 0.728 and P = 0.86, respectively) and cadaver sizes (P = 0.893 and P = 0.859, respectively). The mean error of the needle tip was 3.9±1.7 mm. There were no punctures of critical nontarget structures. The mean procedure length was 33±12 min. CONCLUSION 1.5 T augmented reality-navigated interventional MRI can provide accurate imaging guidance for perineural injections of the thoracic, lumbar, and hypogastric sympathetic plexus.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marker et al. - 2017 - 1.5 T augmented reality navigated interventional MRI Paravertebral sympathetic plexus injections.pdf:pdf},
}

@Article{Kowshalya2017,
  author       = {Kowshalya, A. Meena and Valarmathi, M. L.},
  year         = {2017},
  journal       = {Journal of Applied Engineering Science},
  title        = {{Think smart, think social! the road map from smarter objects to social objects in social internet of things - A survey}},
  doi          = {10.5937/jaes15-12161},
  issn         = {18213197},
  number       = {2},
  pages        = {149--154},
  volume       = {15},
  abstract     = {Internet of Things, the one paradigm many vision idea is ruling the world. By 2025 over trillions and trillions of objects will be connected to the internet. Social networking concepts are revolutions beyond IoT. One of the many visions of IoT is to make objects not only smarter but also socially conscious. A new paradigm named Social Internet of Things evolved which integrated two technologies namely Internet of Things and Social Networking. A SIoT comprises of socially aware smart objects that can autonomously establish and enable collaboration with other smart objects that are friends. In this paper we study the role, characteristics of social objects and their relationships. Five kinds of relationships are identified. These relationship and characteristics helps in revealing the level of trust between objects. Experiments were conducted for 85 social objects in an office environment and the types of objects, their relationships, interest, activities etc were discovered.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kowshalya, Valarmathi - 2017 - Think smart, think social! the road map from smarter objects to social objects in social internet of thin.pdf:pdf},
  keywords     = {Internet of Things,Object relationships,Smart objects,Social Internet of Things,Social awareness},
}

@Article{Wei2018,
  author   = {Wei, Dawei and Ning, Huansheng and Qian, Yuke and Zhu, Tao},
  year     = {2018},
  title    = {{Social relationship for physical objects}},
  doi      = {10.1177/1550147718754968},
  number   = {1},
  volume   = {14},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wei et al. - 2018 - Social relationship for physical objects.pdf:pdf},
  keywords = {18 march 2017,28 november 2017,accepted,date received,handling editor,internet of things,michele amoretti,physical object,search engine,social relationship,spatial-temporal attributes},
}

@Article{Liao2018,
  author       = {Liao, Yiyi and Donne, Simon and Geiger, Andreas},
  year         = {2018},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  title        = {{Deep Marching Cubes: Learning Explicit Surface Representations}},
  doi          = {10.1109/CVPR.2018.00308},
  issn         = {10636919},
  pages        = {2916--2925},
  abstract     = {Existing learning based solutions to 3D surface predic-tion cannot be trained end-to-end as they operate on inter-mediate representations (e.g., TSDF) from which 3D sur-face meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm). In this paper, we investigate the problem of end-to-end 3D surface predic-tion. We first demonstrate that the marching cubes algo-rithm is not differentiable and propose an alternative differ-entiable formulation which we insert as a final layer into a 3D convolutional neural network. We further propose a set of loss functions which allow for training our model with sparse point supervision. Our experiments demon-strate that the model allows for predicting sub-voxel accu-rate 3D shapes of arbitrary topology. Additionally, it learns to complete shapes and to separate an object's inside from its outside even in the presence of sparse and incomplete ground truth. We investigate the benefits of our approach on the task of inferring shapes from 3D point clouds. Our model is flexible and can be combined with a variety of shape encoder and shape inference techniques.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liao, Donne, Geiger - 2018 - Deep Marching Cubes Learning Explicit Surface Representations.pdf:pdf},
  isbn         = {9781538664209},
}

@article{Ienaga,
author = {Ienaga, Naoto and Kawai, Wataru and Fujita, Koji and Miyata, Natsuki},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ienaga et al. - Unknown - A Thumb Tip Wearable Device Consisting of Multiple Cameras to Measure Thumb Posture.pdf:pdf},
keywords = {human computer interaction,pose esti-,wearable device},
pages = {1--7},
title = {{A Thumb Tip Wearable Device Consisting of Multiple Cameras to Measure Thumb Posture}},
volume = {1}
}

@InProceedings{Tsai2013,
  author    = {Tsai, Chi Yi and Wang, Chuan Wei and Wang, Wei Yi},
  booktitle = {2013 CACS International Automatic Control Conference, CACS 2013 - Conference Digest},
  year      = {2013},
  title     = {{Design and implementation of a RANSAC RGB-D mapping algorithm for multi-view point cloud registration}},
  doi       = {10.1109/CACS.2013.6734162},
  isbn      = {9781479923847},
  abstract  = {This paper addresses multi-view point cloud registration, which is one of the major techniques to develop a robust visual tracking system. The proposed algorithm first uses Kinect camera to capture point clouds of an object-of-interest or a complex environment with different view points. Then, a novel RANSAC RGB-D mapping algorithm is developed to efficiently and accurately align the multiple point clouds. Based on the aligned point cloud of the object, a CAD model can possibly be created to achieve the purpose of three-dimensional object pose estimation, which helps improving the robustness of visual tracking systems.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsai, Wang, Wang - 2013 - Design and implementation of a RANSAC RGB-D mapping algorithm for multi-view point cloud registration.pdf:pdf},
  keywords  = {Multi-view point cloud registration,RANSAC algorithm,RGB-D mapping algorithm,optimization algorithm},
}

@InProceedings{Huang2013,
  author    = {Huang, Zheng Yang and Huang, Jung Tang and Hsu, Chih Ming},
  booktitle = {Proceedings - 2013 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2013},
  year      = {2013},
  title     = {{A case study of object identification using a kinect sensor}},
  doi       = {10.1109/SMC.2013.300},
  isbn      = {9780769551548},
  abstract  = {The RGB camera and LIDAR have been applied in object identification for robots. However, Kinect has changed this situation. Because of the wide use of three-dimensional (3D) processing, the point cloud library (PCL) now enables people to easily use in a 3D environment. Iterative closest point (ICP) is an ideal method for identifying a target, but it involves first reconstructing an object. Therefore, this paper presents an alternative method using geometry of surface for identifying the features of an object and differentiating it from other objects.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Huang, Hsu - 2013 - A case study of object identification using a kinect sensor.pdf:pdf},
  keywords  = {Component,Kinect,Point cloud library (PCL)},
}

@Article{Wang2018,
  author       = {Wang, Yue and Zhang, Shusheng and Wan, Bile and He, Weiping and Bai, Xiaoliang},
  year         = {2018},
  journal       = {International Journal of Advanced Manufacturing Technology},
  title        = {{Point cloud and visual feature-based tracking method for an augmented reality-aided mechanical assembly system}},
  doi          = {10.1007/s00170-018-2575-8},
  issn         = {14333015},
  abstract     = {To improve the applicability and robustness of the three-dimensional tracking method of an augmented reality-aided assembly guiding system for mechanical products, a tracking method based on the combination of point cloud and visual feature is proposed. First, the tracking benchmark coordinate system is defined using a reference model point cloud to determine the position of the virtual assembly guiding information. Then a camera tracking algorithm combining visual feature matching and point cloud alignment is implemented. To obtain enough matching points of visual features in a textureless assembly environment, a novel ORB feature-matching strategy based on the consistency of direction vectors is presented. The experimental results show that the proposed method has good robust stability and tracking accuracy in an assembly environment that lacks both visual and depth features, and it can also achieve good real-time results. Its comprehensive performance is better than the point cloud-based KinectFusion tracking method.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2018 - Point cloud and visual feature-based tracking method for an augmented reality-aided mechanical assembly system.pdf:pdf},
  keywords     = {Augmented reality,Mechanical assembly,Point cloud,Tracking,Visual feature},
}

@InProceedings{Sano2015,
  author    = {Sano, Masayuki and Matsumoto, Kazuki and Thomas, Bruce H. and Saito, Hideo},
  booktitle = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015},
  year      = {2015},
  title     = {{[POSTER] Rubix: Dynamic spatial augmented reality by extraction of plane regions with a RGB-D camera}},
  doi       = {10.1109/ISMAR.2015.43},
  isbn      = {9781467376600},
  abstract  = {Dynamic spatial augmented reality requires accurate real-time 3D pose information of the physical objects that are to be projected onto. Previous depth-based methods for tracking objects required strong features to enable recognition; making it difficult to estimate an accurate 6DOF pose for physical objects with a small set of recognizable features (such as a non-textured cube). We propose a more accurate method with fewer limitations for the pose estimation of a tangible object that has known planar faces and using depth data from an RGB-D camera only. In this paper, the physical object's shape is limited to cubes of different sizes. We apply this new tracking method to achieve dynamic projections onto these cubes. In our method, 3D points from an RGB-D camera are divided into a cluster of planar regions, and the point cloud inside each face of the object is fitted to an already-known geometric model of a cube. With the 6DOF pose of the physical object, SAR generated imagery is then projected correctly onto the physical object. The 6DOF tracking is designed to support tangible interactions with the physical object. We implemented example interactive applications with one or multiple cubes to show the capability of our method.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sano et al. - 2015 - POSTER Rubix Dynamic spatial augmented reality by extraction of plane regions with a RGB-D camera.pdf:pdf},
  keywords  = {RGB-D Camera,Six Degree of Freedom Tracking,Spatial Augmented Reality},
}

@Article{Zeynalov2013,
  author       = {Zeynalov, R. Sh. and Yakubenko, A. A. and Konushin, A. S.},
  year         = {2013},
  journal       = {Pattern Recognition and Image Analysis},
  title        = {{The matching of infrared markers for tracking objects using stereo pairs}},
  doi          = {10.1134/s1054661813040196},
  issn         = {1054-6618},
  abstract     = {This paper presents a new algorithm for obtaining inter-frame and inter-view (inter-camera) correspondences to solve the problem of tracking an object labeled with infrared markers using a stereo pair taken simultaneously in the infrared region. In practice it is often necessary to track an object when it is impossible to have contact with it, for example, the tracking of facial movements using the motion capture technique (Motion Capture, [9, 10] ) to create realistic animation or the tracking of object movements when interacting with the augmented reality. In such cases contactless object tracking methods are used. In the classic version of this problem, two or more cameras are used to capture the object of interest. In order to restore three-dimensional coordinates of object points, it is necessary to triangulate the received projections of the points. In the case of the visible range, the problem of finding and matching points on the object can be solved using interest point descriptors [2]. However, there are situations in which it is impossible to use the visible range data, for example, a uniformly colored or regularly textured object, which makes it senseless to use interest point descriptors. Thus, the use of interest point descriptors significantly limits the scope of application of algorithms because of the imposition of severe restrictions on the class of tracked objects, objects should have uneven texture. In turn, the motion capture method implies the tracking of an object of a predetermined shape, when it is not always possible to establish its shape. In this work, an alternative approach is proposed, i.e., the use of infrared markers and cameras that capture frames in the infrared range which makes the task of finding critical points irrelevant. On the other hand, infrared markers on cameras that operate in the infrared range are indistinguishable from each other. Therefore, there is the problem of finding correspondences which in the case of interest point descriptors is solved by the very nature of the descriptors. In this paper, we describe algorithms that make it possible to restore point correspondences by a sequence of stereo pair images (Fig. 1) and its calibrations. In this case, epipolar constraints and a voting scheme based on the greedy algorithm are used. {\textcopyright} 2013 Pleiades Publishing, Ltd.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeynalov, Yakubenko, Konushin - 2013 - The matching of infrared markers for tracking objects using stereo pairs.pdf:pdf},
}

@Article{Gao2018a,
  author       = {Gao, Qing Hong and Wan, Tao Ruan and Tang, Wen and Chen, Long},
  year         = {2018},
  journal       = {Multimedia Tools and Applications},
  title        = {{Object registration in semi-cluttered and partial-occluded scenes for augmented reality}},
  doi          = {10.1007/s11042-018-6905-5},
  issn         = {15737721},
  abstract     = {{\textcopyright} 2018, The Author(s). This paper proposes a stable and accurate object registration pipeline for markerless augmented reality applications. We present two novel algorithms for object recognition and matching to improve the registration accuracy from model to scene transformation via point cloud fusion. Whilst the first algorithm effectively deals with simple scenes with few object occlusions, the second algorithm handles cluttered scenes with partial occlusions for robust real-time object recognition and matching. The computational framework includes a locally supported Gaussian weight function to enable repeatable detection of 3D descriptors. We apply a bilateral filtering and outlier removal to preserve edges of point cloud and remove some interference points in order to increase matching accuracy. Extensive experiments have been carried to compare the proposed algorithms with four most used methods. Results show improved performance of the algorithms in terms of computational speed, camera tracking and object matching errors in semi-cluttered and partial-occluded scenes.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2018 - Object registration in semi-cluttered and partial-occluded scenes for augmented reality.pdf:pdf},
  keywords     = {3D object recognition and matching,3D point clouds,Augmented reality,SLAM algorithm},
}

@Article{Bae2015,
  author       = {Bae, Hyojoon and White, Jules and Golparvar-Fard, Mani and Pan, Yao and Sun, Yu},
  year         = {2015},
  journal       = {Personal and Ubiquitous Computing},
  title        = {{Fast and scalable 3D cyber-physical modeling for high-precision mobile augmented reality systems}},
  doi          = {10.1007/s00779-015-0892-6},
  issn         = {16174909},
  abstract     = {{\textcopyright} 2015, Springer-Verlag London.Mobile augmented reality is an emerging technique which allows users to use a mobile device's camera to capture real-world imagery and view real-world physical objects and their associated cyber-information overlaid on top of imagery of them. One key challenge for mobile augmented reality is the fast and precisely localization of a user in order to determine what is visible in their camera view. Recent advances in Structure-from-Motion (SfM) enable the creation of 3D point clouds of physical objects from an unordered set of photographs taken by commodity digital cameras. The generated 3D point cloud can be used to identify the location and orientation of the camera relative to the point cloud. While this SfM-based approach provides complete pixel-accurate camera pose estimation in 3D without relying on external GPS or geomagnetic sensors, the preparation of initial 3D point cloud typically takes from hours to a day, making it difficult to use in mobile augmented reality applications. Furthermore, creating 3D cyber-information and associating it with the 3D point cloud is also a challenge of using SfM-based approach for mobile augmented reality. To overcome these challenges in 3D point cloud creation and cyber-physical content authoring, the paper presents a new SfM framework that is optimized for mobile augmented reality and rapidly generates a complete 3D point cloud of a target scene up to 28 times faster than prior approaches. Key improvements in the proposed SfM framework stem from the use of (1) state-of-the-art binary feature descriptors, (2) new filtering approach for accurate 3D modeling, (3) optimized point cloud structure for augmented reality, and (4) hardware/software parallelism. The paper also provides a new image-based 3D content authoring method designed specifically for the limited user interfaces of mobile devices. The proposed content authoring method generates 3D cyber-information from a single 2D image and automatically associates it with the 3D point cloud.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bae et al. - 2015 - Fast and scalable 3D cyber-physical modeling for high-precision mobile augmented reality systems.pdf:pdf},
  keywords     = {Image-based modeling,Mobile augmented reality,Structure-from-Motion},
}

@Article{Weinmann2017,
  author       = {Weinmann, Martin and Weinmann, Michael and Mallet, Cl{\'{e}}ment and Br{\'{e}}dif, Mathieu},
  year         = {2017},
  journal       = {Remote Sensing},
  title        = {{A classification-segmentation framework for the detection of individual trees in dense MMS point cloud data acquired in urban areas}},
  doi          = {10.3390/rs903277},
  issn         = {20724292},
  abstract     = {{\textcopyright} 2017 by the authors. In this paper, we present a novel framework for detecting individual trees in densely sampled 3D point cloud data acquired in urban areas. Given a 3D point cloud, the objective is to assign point-wise labels that are both class-aware and instance-aware, a task that is known as instance-level segmentation. To achieve this, our framework addresses two successive steps. The first step of our framework is given by the use of geometric features for a binary point-wise semantic classification with the objective of assigning semantic class labels to irregularly distributed 3D points, whereby the labels are defined as "tree points" and "other points". The second step of our framework is given by a semantic segmentation with the objective of separating individual trees within the "tree points". This is achieved by applying an efficient adaptation of the mean shift algorithm and a subsequent segment-based shape analysis relying on semantic rules to only retain plausible tree segments. We demonstrate the performance of our framework on a publicly available benchmark dataset, which has been acquired with a mobile mapping system in the city of Delft in the Netherlands. This dataset contains 10.13 M labeled 3D points among which 17.6\% are labeled as "tree points". The derived results clearly reveal a semantic classification of high accuracy (up to 90.77\%) and an instance-level segmentation of high plausibility, while the simplicity, applicability and efficiency of the involved methods even allow applying the complete framework on a standard laptop computer with a reasonable processing time (less than 2.5 h).},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinmann et al. - 2017 - A classification-segmentation framework for the detection of individual trees in dense MMS point cloud data acq.pdf:pdf},
  keywords     = {3D point cloud,Feature extraction,Feature selection,Instance-level segmentation,Mobile mapping systems,Semantic classification,Semantic segmentation,Tree-like objects},
}

@Article{Wolf2016,
  author       = {Wolf, Daniel and Prankl, Johann and Vincze, Markus},
  year         = {2016},
  journal       = {IEEE Robotics and Automation Letters},
  title        = {{Enhancing semantic segmentation for robotics: The power of 3-D entangled forests}},
  doi          = {10.1109/LRA.2015.2506118},
  issn         = {23773766},
  abstract     = {— We present a novel, fast and compact method to improve semantic segmentation of 3D point clouds, which is able to learn and exploit common contextual relations between observed structures and objects. Introducing 3D Entangled Forests (3DEF), we extend the concept of entangled features for decision trees to 3D point clouds, enabling the classifier not only to learn which labels are likely to occur close to each other, but also in which specific geometric configuration. Operating on a plane-based representation of a point cloud, our method does not require a final smoothing step and achieves state-of-the-art results on the NYU Depth Dataset in a single inference step. This compactness in turn allows for fast processing times, a crucial factor to consider for online applications on robotic platforms. In a thorough evaluation, we demonstrate the expressiveness of our new 3D entangled feature set and the importance of spatial context in the scope of semantic segmentation.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolf, Prankl, Vincze - 2016 - Enhancing semantic segmentation for robotics The power of 3-D entangled forests.pdf:pdf},
  keywords     = {Entangled fores,Semantic segmentation},
}

@Article{Tsuboi2013,
  author       = {Tsuboi, Kazuna and Oyamada, Yuji and Sugimoto, Maki and Saito, Hideo},
  year         = {2013},
  journal       = {Proceedings of the Fourteenth Australasian User Interface Conference - Volume 139},
  title        = {{3D object surface tracking using partial shape templates trained from a depth camera for spatial augmented reality environments}},
  abstract     = {We present a 3D object tracking method using a single depth camera for Spatial Augmented Reality (SAR). The drastic change of illumination in a SAR environment makes object tracking difficult. Our method uses a depth camera to train and track the 3D physical object. The training allows maker-less tracking of the moving object under illumination changes. The tracking is a combination of feature based matching and frame sequential matching of point clouds. Our method allows users to adapt 3D objects of their choice into a dynamic SAR environment. .},
  isbn         = {978-1-921770-24-1},
  keywords     = {3D object,depth camera,point cloud,real-time,spatial augmented reality,tracking},
}

@InProceedings{Ishikawa2019,
  author    = {Ishikawa, Y and Hachiuma, R and Ienaga, N and Kuno, W and Sugiura, Y and Saito, H},
  booktitle = {2019 12th Asia Pacific Workshop on Mixed and Augmented Reality (APMAR)},
  year      = {2019},
  title     = {{Semantic Segmentation of 3D Point Cloud to Virtually Manipulate Real Living Space}},
  doi       = {10.1109/APMAR.2019.8709156},
  isbn      = {VO -},
  pages     = {1--7},
  abstract  = {This paper presents a method for the virtual manipulation of real living space using semantic segmentation of a 3D point cloud captured in the real world. We applied PointNet to segment each piece of furniture from the point cloud of a real indoor environment captured by moving a RGB-D camera. For semantic segmentation, we focused on local geometric information not used in PointNet, and we proposed a method to refine the class probability of labels attached to each point in PointNet's output. The effectiveness of our method was experimentally confirmed. We then created 3D models of real-world furniture using a point cloud with corrected labels, and we virtually manipulated real living space using Dollhouse VR, a layout system.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishikawa et al. - 2019 - Semantic Segmentation of 3D Point Cloud to Virtually Manipulate Real Living Space.pdf:pdf;:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishikawa et al. - 2019 - Semantic Segmentation of 3D Point Cloud to Virtually Manipulate Real Living Space(2).pdf:pdf},
  keywords  = {3D point cloud,Dollhouse VR,Image color analysis,Layout,Legged locomotion,PointNet,RGB-D camera,Semantics,Shape,Solid modeling,Three-dimensional displays,cameras,class probability,furniture,furniture arrangement,image colour analysis,image segmentation,living space,local geometric information,real indoor environment,semantic segmentation,solid modelling,virtual manipulation,virtual reality},
}

@Article{Masala2013,
  author       = {Masala, G L and Golosio, B and Oliva, P},
  year         = {2013},
  journal       = {Computer Physics Communications},
  title        = {{An improved Marching Cube algorithm for 3D data segmentation}},
  doi          = {https://doi.org/10.1016/j.cpc.2012.09.030},
  issn         = {0010-4655},
  number       = {3},
  pages        = {777--782},
  url          = {http://www.sciencedirect.com/science/article/pii/S0010465512003281},
  volume       = {184},
  abstract     = {The marching cube algorithm is one of the most popular algorithms for isosurface triangulation. It is based on a division of the data volume into elementary cubes, followed by a standard triangulation inside each cube. In the original formulation, the marching cube algorithm is based on 15 basic triangulations and a total of 256 elementary triangulations are obtained from the basic ones by rotation, reflection, conjugation, and combinations of these operations. The original formulation of the algorithm suffers from well-known problems of connectivity among triangles of adjacent cubes, which has been solved in various ways. We developed a variant of the marching cube algorithm that makes use of 21 basic triangulations. Triangles of adjacent cubes are always well connected in this approach. The output of the code is a triangulated model of the isosurface in raw format or in VRML (Virtual Reality Modelling Language) format. Program summary Program title: TRIANGOLATE Catalogue identifier: AENS_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AENS_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: Standard CPC licence, http://cpc.cs.qub.ac.uk/licence/licence.html No. of lines in distributed program, including test data, etc.: 147558 No. of bytes in distributed program, including test data, etc.: 26084066 Distribution format: tar.gz Programming language: C. Computer: Pentium 4, CPU 3.2 GHz and 3.24 GB of RAM (2.77 GHz). Operating system: Tested on several Linux distribution, but generally works in all Linux-like platforms. RAM: Approximately 2 MB Classification: 6.5. Nature of problem: Given a scalar field $\mu$(x,y,z) sampled on a 3D regular grid, build a discrete model of the isosurface associated to the isovalue $\mu$Iso, which is defined as the set of points that satisfy the equation $\mu$(x,y,z)=$\mu$Iso. Solution method: The proposed solution is an improvement of the Marching Cube algorithm, which approximates the isosurface using a set of triangular facets. The data volume is divided into logical volumes where the topology of the triangulation is selected through a look-up table, while the metric is computed by linear interpolation. Running time: It is dependent on the input data, but the test provided takes 8 seconds.},
  keywords     = {3D imaging,Surface triangulation},
}

@Article{Radkowski2016,
  author       = {Radkowski, Rafael},
  year         = {2016-01},
  journal       = {Journal of Computing and Information Science in Engineering},
  title        = {{Object Tracking With a Range Camera for Augmented Reality Assembly Assistance}},
  issn         = {1530-9827},
  number       = {1},
  pages        = {11004--11008},
  url          = {http://dx.doi.org/10.1115/1.4031981},
  volume       = {16},
  abstract     = {This paper introduces a 3D object tracking method for an augmented reality (AR) assembly assistance application. The tracking method relies on point clouds; it uses 3D feature descriptors and point cloud matching with the iterative closest points (ICP) algorithm. The feature descriptors identify an object in a point cloud; ICP align a reference object with this point cloud. The challenge is to achieve high fidelity while maintaining camera frame rates. The point cloud and reference object sampling density are one of the key factors to meet this challenge. In this research, three-point sampling methods and two-point cloud search algorithms were compared to assess their fidelity when tracking typical products of mechanical engineering. The results indicate that a uniform sampling maintains the best fidelity at camera frame rates.},
  annotation   = {10.1115/1.4031981},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radkowski - 2016 - Object Tracking With a Range Camera for Augmented Reality Assembly Assistance.pdf:pdf},
  publisher    = {ASME},
}

@InProceedings{Radkowski2016a,
  author = {Radkowski, Rafael and Garrett, Timothy and Ingebrand, Jarid and Wehr, David},
  year   = {2016},
  title  = {{TrackingExpert: A Versatile Tracking Toolbox for Augmented Reality}},
  doi    = {10.1115/DETC2016-60401},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radkowski et al. - 2016 - TrackingExpert A Versatile Tracking Toolbox for Augmented Reality.pdf:pdf},
}

@Article{Zhang2015,
  author       = {Zhang, Jinghe and Welch, Greg and Ramakrishnan, Naren and Rahman, Saifur},
  year         = {2015-06},
  journal       = {Intelligent Industrial Systems},
  title        = {{Kalman Filters for Dynamic and Secure Smart Grid State Estimation}},
  doi          = {10.1007/s40903-015-0009-6},
  issn         = {2199-854X},
  number       = {1},
  pages        = {29--36},
  url          = {https://doi.org/10.1007/s40903-015-0009-6},
  volume       = {1},
  abstract     = {Combining dynamic state estimation methods such as Kalman filters with real-time data generated/collected by digital meters such as phasor measurement units (PMU) can lead to advanced techniques for improving the quality of monitoring and controllability in smart grids. Classic Kalman filters achieve optimal performance with ideal system models, which are usually hard to obtain in practice with unexpected disturbances, device failures, and malicious data attacks. In this work, we introduce and compare a novel method, viz. adaptive Kalman Filter with inflatable noise variances, against a variety of classic Kalman filters. Extensive simulation studies demonstrate the powerful ability of our proposed algorithm under suboptimal conditions such as wrong system modeling, sudden disturbance and bad data injection.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - Kalman Filters for Dynamic and Secure Smart Grid State Estimation.pdf:pdf},
}

@Article{Wiederhold2015,
  author = {Wiederhold, Brenda Kay and Riva, Giuseppe and Wiederhold, Mark D},
  year   = {2015},
  title  = {{Annual Review of CyberTherapy and Telemedicine: Virtual Reality in Healthcare: Medical Simulation and Experiential Interface Communicating body perception in CRPS View project Virtual Reality and Pain View project}},
  number = {May 2017},
  url    = {https://www.researchgate.net/publication/291165363},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiederhold, Riva, Wiederhold - 2015 - Annual Review of CyberTherapy and Telemedicine Virtual Reality in Healthcare Medical Simulation an.pdf:pdf},
}

@Article{Riva2015,
  author       = {Riva, G and Wiederhold, B K},
  year         = {2015},
  journal       = {Studies in Health Technology and Informatics},
  title        = {{The New Dawn of Virtual Reality in Health Care: Medical Simulation and Experiential Interface}},
  doi          = {10.3233/978-1-61499-595-1-3},
  issn         = {09269630},
  pages        = {3--6},
  volume       = {219},
  keywords     = {Anxiety Disorders,Cardboard,Experiential Interface,Health Care,Medical Simulation,Smartphones,Virtual Reality},
}

@InProceedings{Datcu2013,
  author    = {Datcu, Dragos and Lukosch, Stephan},
  booktitle = {Proceedings of the 1st Symposium on Spatial User Interaction},
  year      = {2013},
  title     = {{Free-hands Interaction in Augmented Reality}},
  doi       = {10.1145/2491367.2491370},
  isbn      = {978-1-4503-2141-9},
  location  = {New York, NY, USA},
  pages     = {33--40},
  publisher = {ACM},
  series    = {SUI '13},
  url       = {http://doi.acm.org/10.1145/2491367.2491370},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Datcu, Lukosch - 2013 - Free-hands Interaction in Augmented Reality.pdf:pdf},
  keywords  = {augmented reality,crime scene investigation,hands free interaction},
}

@InProceedings{daSilvaSousa2007,
  author    = {{da Silva Sousa}, Jo{\~{a}}o Rodrigo Ferreira and Silva, Arist{\'{o}}fanes Corr{\^{e}}a and de Paiva, Anselmo},
  booktitle = {Progress in Pattern Recognition, Image Analysis and Applications},
  year      = {2007},
  title     = {{Lung Structure Classification Using 3D Geometric Measurements and SVM}},
  editor    = {Rueda, Luis and Mery, Domingo and Kittler, Josef},
  isbn      = {978-3-540-76725-1},
  location  = {Berlin, Heidelberg},
  pages     = {783--792},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {In this paper, a set of three features for aiding classification of lung nodule bearing candidates based upon morphological characteristics is proposed. Metrics were validated using Support Vector Machine (SVM) technique as classifier. Preliminary results indicate the efficiency of the adopted measurements, taking into account the sensitivity and specificity high rates obtained from the studied samplings.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/da Silva Sousa, Silva, de Paiva - 2007 - Lung Structure Classification Using 3D Geometric Measurements and SVM.pdf:pdf},
}

@Article{Grauer2009,
  author       = {Grauer, Dan and Cevidanes, Lucia S H and Proffit, William R},
  year         = {2009},
  journal       = {American Journal of Orthodontics and Dentofacial Orthopedics},
  title        = {{Working with DICOM craniofacial images}},
  doi          = {https://doi.org/10.1016/j.ajodo.2009.04.016},
  issn         = {0889-5406},
  number       = {3},
  pages        = {460--470},
  url          = {http://www.sciencedirect.com/science/article/pii/S0889540609005964},
  volume       = {136},
  abstract     = {The increasing use of cone-beam computed tomography (CBCT) requires changes in our diagnosis and treatment planning methods as well as additional training. The standard for digital computed tomography images is called digital imaging and communications in medicine (DICOM). In this article we discuss the following concepts: visualization of CBCT images in orthodontics, measurement in CBCT images, creation of 2-dimensional radiographs from DICOM files, segmentation engines and multimodal images, registration and superimposition of 3-dimensional (3D) images, special applications for quantitative analysis, and 3D surgical prediction. CBCT manufacturers and software companies are continually working to improve their products to help clinicians diagnose and plan treatment using 3D craniofacial images.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grauer, Cevidanes, Proffit - 2009 - Working with DICOM craniofacial images.pdf:pdf},
}

@Article{Vandenberghe2018,
  author       = {Vandenberghe, Michel E and Souedet, Nicolas and H{\'{e}}rard, Anne-Sophie and Ayral, Anne-Marie and Letronne, Florent and Balbastre, Ya{\"{e}}l and Sadouni, Elmahdi and Hantraye, Philippe and Dhenain, Marc and Frouin, Fr{\'{e}}d{\'{e}}rique and Lambert, Jean-Charles and Delzescaux, Thierry},
  year         = {2018},
  journal       = {Frontiers in Neuroscience},
  title        = {{Voxel-Based Statistical Analysis of 3D Immunostained Tissue Imaging}},
  doi          = {10.3389/fnins.2018.00754},
  issn         = {1662-453X},
  pages        = {754},
  url          = {https://www.frontiersin.org/article/10.3389/fnins.2018.00754},
  volume       = {12},
  abstract     = {Recently developed techniques to visualize immunostained tissues in 3D and in large samples have expanded the scope of microscopic investigations at the level of the whole brain. Here, we propose to adapt voxel-wise statistical analysis to 3D high-resolution images of the immunostained rodent brain. The proposed approach was first validated with a simulation dataset with known cluster locations. Then, it was applied to characterize the effect of ADAM30, a gene involved in the metabolism of the amyloid precursor protein, in a mouse model of the Alzheimer's disease. This work introduces voxel-wise analysis of 3D immunostained microscopic brain images and, therefore, opens the door to unprecedented exploratory investigation of pathological markers and cellular alterations.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vandenberghe et al. - 2018 - Voxel-Based Statistical Analysis of 3D Immunostained Tissue Imaging.pdf:pdf},
}

@Article{Strzelecki2013,
  author       = {Strzelecki, Michal and Szczypinski, Piotr and Materka, Andrzej and Klepaczko, Artur},
  year         = {2013},
  journal       = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  title        = {{A software tool for automatic classification and segmentation of 2D/3D medical images}},
  doi          = {https://doi.org/10.1016/j.nima.2012.09.006},
  issn         = {0168-9002},
  pages        = {137--140},
  url          = {http://www.sciencedirect.com/science/article/pii/S0168900212010212},
  volume       = {702},
  abstract     = {Modern medical diagnosis utilizes techniques of visualization of human internal organs (CT, MRI) or of its metabolism (PET). However, evaluation of acquired images made by human experts is usually subjective and qualitative only. Quantitative analysis of MR data, including tissue classification and segmentation, is necessary to perform e.g. attenuation compensation, motion detection, and correction of partial volume effect in PET images, acquired with PET/MR scanners. This article presents briefly a MaZda software package, which supports 2D and 3D medical image analysis aiming at quantification of image texture. MaZda implements procedures for evaluation, selection and extraction of highly discriminative texture attributes combined with various classification, visualization and segmentation tools. Examples of MaZda application in medical studies are also provided.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Strzelecki et al. - 2013 - A software tool for automatic classification and segmentation of 2D3D medical images.pdf:pdf},
  keywords     = {Data classification,Image analysis and processing software,Magnetic resonance imaging,Texture analysis},
}

@InProceedings{McAuliffe2001,
  author    = {McAuliffe, M J and Lalonde, F M and McGarry, D and Gandler, W and Csaky, K and Trus, B L},
  booktitle = {Proceedings 14th IEEE Symposium on Computer-Based Medical Systems. CBMS 2001},
  year      = {2001},
  title     = {{Medical Image Processing, Analysis and Visualization in clinical research}},
  doi       = {10.1109/CBMS.2001.941749},
  isbn      = {1063-7125 VO -},
  pages     = {381--386},
  abstract  = {Imaging has become an essential component in many fields of medical and laboratory research and clinical practice. Biologists study cells and generate 3D confocal microscopy data sets; virologists generate 3D reconstructions of viruses from micrographs; radiologists identify and quantify tumors from MRI and CT scans; and neuroscientists detect regional metabolic brain activity from PET and functional MRI scans. Analysis of these diverse image types requires sophisticated computerized quantification and visualization tools. Until recently, 3D visualization of images and quantitative analysis could only be performed using expensive UNIX workstations and customized software. Today, much of the visualization and analysis can be performed on an inexpensive desktop computer equipped with the appropriate graphics hardware and software. This paper introduces an extensible, platform-independent, general-purpose image processing and visualization program specifically designed to meet the needs of an Internet-linked medical research community. The application, named MIPAV (Medical Image Processing, Analysis and Visualization), enables clinical and quantitative analysis of medical images over the Internet. Using MIPAV's standard user interface and analysis tools, researcher and clinicians at remote sites can easily share research data and analyses, thereby enhancing their ability to study, diagnose, monitor and treat medical disorders.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McAuliffe et al. - 2001 - Medical Image Processing, Analysis and Visualization in clinical research.pdf:pdf},
  keywords  = {3D confocal microscopy data sets,3D medical data visualization,3D reconstructions,Biomedical image processing,Biomedical imaging,CT scans,Data visualization,Image analysis,Internet,Internet-linked medical research community,Laboratories,MIPAV,MRI scans,Magnetic resonance imaging,Medical diagnostic imaging,PET scans,Performance analysis,Software performance,UNIX workstations,biology,biomedical communication,cells,clinical research,computer graphics software,computerized quantification tools,customized software,data sharing,data visualisation,desktop computer,extensible platform-independent general-purpose im,graphics hardware,image types,laboratory research,medical disorders,medical image analysis,medical image processing,micrographs,neuroscience,quantitative analysis,radiology,regional metabolic brain activity,remote sites,tumors,user interface,virology,viruses,visual communication},
}

@InProceedings{Jackson2018,
  author     = {Jackson, Jesse Colin},
  booktitle  = {Proceedings of the Twelfth International Conference on Tangible, Embedded, and Embodied Interaction},
  year       = {2018},
  title      = {{Marching Cubes Made Tangible}},
  doi        = {10.1145/3173225.3173313},
  isbn       = {978-1-4503-5568-1},
  location   = {New York, NY, USA},
  pages      = {592--597},
  publisher  = {ACM},
  series     = {TEI '18},
  url        = {http://doi.acm.org/10.1145/3173225.3173313},
  annotation = {I only skimmed this paper. It seems to be focuing on using a 3D version of marching blocks at least as a prototype to show what is possible with the tech.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jackson - 2018 - Marching Cubes Made Tangible.pdf:pdf},
  keywords   = {additive manufacturing,critical design,embodied interaction,materiality,tangibility},
}

@Article{Scheiblauer2014,
  author = {Scheiblauer, Claus and der Arbeit, Verfassung and Wimmer, Michael and Gervautz, Michael and gang Knecht, Wolf- and Marek, Stefan and Hebart, Gabriele and Gschwantner, Fritz-Michael and Zimmer, Norbert and Mayer, Irmengard and Fugger, Verena and Barsukov, Yaroslav and Preiner, Reinhold and Mayer, Julian and Pregesbauer, Michael and Tragust, Markus and Arikan, Murat},
  year   = {2014},
  title  = {{Interactions with Gigantic Point Clouds}},
  url    = {https://www.cg.tuwien.ac.at/research/publications/2014/scheiblauer-thesis/scheiblauer-thesis-thesis.pdf},
  volume = {2014},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheiblauer et al. - 2014 - Interactions with Gigantic Point Clouds.pdf:pdf},
}

@Article{Scheiblauer2013,
  author       = {Scheiblauer, C. and Wimmer, M.},
  year         = {2013},
  journal       = {21st International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2013 - Full Papers Proceedings},
  title        = {{Analysis of interactive editing operations for out-of-core point-cloud hierarchies}},
  pages        = {123--132},
  url          = {http://hdl.handle.net/11025/10602 https://dspace5.zcu.cz/handle/11025/10602 http://wscg.zcu.cz/WSCG2013/!_2013-WSCG-Full-proceedings.pdf\%0Ahttp://hdl.handle.net/11025/10602},
  abstract     = {In this paper we compare the time and space complexity of editing operations on two data structures which are suitable for visualizing huge point clouds. The first data structure was introduced by Scheiblauer and Wimmer [SW11] and uses only the original points from a source data set for building a level-of-detail hierarchy that can be used for rendering points clouds. The second data structure introduced by Wand et al. [WBB+07] requires additional points for the level-of-detail hierarchy and therefore needs more memory when stored on disk. Both data structures are based on an octree hierarchy and allow for deleting and inserting points. Besides analyzing and comparing these two data structures we also introduce an improvement to the points deleting algorithm for the data structure of Wand et al. [WBB+07], which thus allows for a more efficient node loading strategy during rendering.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scheiblauer, Wimmer - 2013 - Analysis of interactive editing operations for out-of-core point-cloud hierarchies.pdf:pdf},
  isbn         = {9788086943749},
  keywords     = {Complexity analysis,Data structures,Point clouds,Viewing algorithms},
}

@Article{Teschner2005,
  author       = {Teschner, M and Kimmerle, S and Heidelberger, B and Zachmann, G and Raghupathi, L and Fuhrmann, A and Cani, M.-P. and Faure, F and Magnenat-Thalmann, N and Strasser, W and Volino, P},
  year         = {2005},
  journal       = {Computer Graphics Forum},
  title        = {{Collision Detection for Deformable Objects}},
  doi          = {10.1111/j.1467-8659.2005.00829.x},
  number       = {1},
  pages        = {61--81},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2005.00829.x},
  volume       = {24},
  abstract     = {Abstract Interactive environments for dynamically deforming objects play an important role in surgery simulation and entertainment technology. These environments require fast deformable models and very efficient collision handling techniques. While collision detection for rigid bodies is well investigated, collision detection for deformable objects introduces additional challenging problems. This paper focuses on these aspects and summarizes recent research in the area of deformable collision detection. Various approaches based on bounding volume hierarchies, distance fields and spatial partitioning are discussed. In addition, image-space techniques and stochastic methods are considered. Applications in cloth modeling and surgical simulation are presented.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teschner et al. - 2005 - Collision Detection for Deformable Objects.pdf:pdf},
  keywords     = {I.3.5 Computer Graphics: Computational Geometry an,I.3.7 Computer Graphics: Three-Dimensional Graphic,bounding-volume hierarchy,continuous collision detection,deformable collision detection,distance field,image-space collision detection,self-collision detection,spatial subdivision,stochastic collision detection},
}

@Article{Shamir2008,
  author       = {Shamir, Ariel},
  year         = {2008},
  journal       = {Computer Graphics Forum},
  title        = {{A survey on Mesh Segmentation Techniques}},
  doi          = {10.1111/j.1467-8659.2007.01103.x},
  number       = {6},
  pages        = {1539--1556},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8659.2007.01103.x},
  volume       = {27},
  abstract     = {Abstract We present a review of the state of the art of segmentation and partitioning techniques of boundary meshes. Recently, these have become a part of many mesh and object manipulation algorithms in computer graphics, geometric modelling and computer aided design. We formulate the segmentation problem as an optimization problem and identify two primarily distinct types of mesh segmentation, namely part segmentation and surface-patch segmentation. We classify previous segmentation solutions according to the different segmentation goals, the optimization criteria and features used, and the various algorithmic techniques employed. We also present some generic algorithms for the major segmentation techniques.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shamir - 2008 - A survey on Mesh Segmentation Techniques.pdf:pdf},
  keywords     = {1.3.5 Computing Methodologies: Computer Graphics C,I.3.6 Computing Methodologies: Computer Graphics M,clustering,mesh partioning,mesh segmentation},
}

@Article{Cui2011,
  author       = {Cui, Juncheng and Chow, Y.W. and Zhang, Minjie},
  year         = {2011},
  journal       = {International Journal of Computer Science and Network Security},
  title        = {{A voxel-based octree construction approach for procedural cave generation}},
  number       = {6},
  pages        = {160--168},
  url          = {http://paper.ijcsns.org/07_book/201106/20110624.pdf},
  volume       = {11},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cui, Chow, Zhang - 2011 - A voxel-based octree construction approach for procedural cave generation.pdf:pdf},
  keywords     = {caves,octree,procedural content generation,voxel},
}

@Article{Hadwiger2012,
  author       = {Hadwiger, M and Beyer, J and Jeong, W and Pfister, H},
  year         = {2012},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Interactive Volume Exploration of Petascale Microscopy Data Streams Using a Visualization-Driven Virtual Memory Approach}},
  doi          = {10.1109/TVCG.2012.240},
  issn         = {1077-2626 VO - 18},
  number       = {12},
  pages        = {2285--2294},
  volume       = {18},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hadwiger et al. - 2012 - Interactive Volume Exploration of Petascale Microscopy Data Streams Using a Visualization-Driven Virtual Memory.pdf:pdf},
  keywords     = {2D microscope image tiles,3D blocks,3D multiresolution representation,Algorithms,Animals,Artificial Intelligence,Cerebral Cortex,Computer-Assisted,Data visualization,Databases,Electron,Factual,Graphics processing unit,Hippocampus,Image Processing,Image resolution,Mice,Microscopy,Models,Neuroscience,Octrees,Petascale volume exploration,Rendering (computer graphics),Theoretical,anisotropic petascale volume,best-of-breed system,cache misses,continuous stream,data acquisition,data visualisation,decouples construction,electron microscopes,high resolution electron microscopy image,high-resolution microscopy,high-throughput imaging,interactive volume exploration,microscopes,microscopy,multiresolution hierarchy,multiresolution virtual memory architecture,neuroscience,octree,petascale microscopy data streams,petascale volumes,ray-casting,real microscopy data,system design,virtual storage,visible volume data,visualization-driven virtual memory,volume ray casting,volume visualization system},
}

@Article{Seiler2011,
  author       = {Seiler, Martin and Steinemann, Denis and Spillmann, Jonas and Harders, Matthias},
  year         = {2011-06},
  journal       = {The Visual Computer},
  title        = {{Robust interactive cutting based on an adaptive octree simulation mesh}},
  doi          = {10.1007/s00371-011-0561-3},
  issn         = {1432-2315},
  number       = {6},
  pages        = {519--529},
  url          = {https://doi.org/10.1007/s00371-011-0561-3},
  volume       = {27},
  abstract     = {We present an adaptive octree based approach for interactive cutting of deformable objects. Our technique relies on efficient refine- and node split-operations. These are sufficient to robustly represent cuts in the mechanical simulation mesh. A high-resolution surface embedded into the octree is employed to represent a cut visually. Model modification is performed in the rest state of the object, which is accomplished by back-transformation of the blade geometry. This results in an improved robustness of our approach. Further, an efficient update of the correspondences between simulation elements and surface vertices is proposed. The robustness and efficiency of our approach is underlined in test examples as well as by integrating it into a prototype surgical simulator.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Seiler et al. - 2011 - Robust interactive cutting based on an adaptive octree simulation mesh.pdf:pdf},
}

@Article{Figueiredo2018,
  author       = {Figueiredo, Lucas and Rodrigues, Eduardo and Teixeira, Jo{\~{a}}o and Teichrieb, Veronica},
  year         = {2018},
  journal       = {Computers {\&} Graphics},
  title        = {{A comparative evaluation of direct hand and wand interactions on consumer devices}},
  doi          = {https://doi.org/10.1016/j.cag.2018.10.006},
  issn         = {0097-8493},
  pages        = {108--121},
  url          = {http://www.sciencedirect.com/science/article/pii/S0097849318301675},
  volume       = {77},
  abstract     = {Along with the popularization of VR Head Mounted Displays, there is an increasing demand for understanding how to use these devices within VR applications. This work evaluates the use of two input techniques for VR applications: wands and hands. We perform experiments using consumer devices (Leap Motion Controller and HTC Vive), aiming at understanding how popular hardware respond to users' needs. Five distinct scenarios were tested, exploring both near and far object interaction. The evaluation happens in three steps: user profile evaluation, system performance evaluation, and the System Usability Scale questionnaire. The results showed that even with a lower task accuracy, natural interaction provided by using a hand representation in the virtual world gained user's preference when interacting with virtual elements that were close to the user. For distant object interaction, it still needs some improvement.},
  keywords     = {Hand and wand interaction,Input devices evaluation,Ray casting},
}

@Misc{Nielson1991,
  author = {Nielson, G M and Hamann, B},
  year   = {1991},
  title  = {{The asymptotic decider: Resolving the ambiguity in marching cubes}},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nielson, Hamann - 1991 - The asymptotic decider Resolving the ambiguity in marching cubes.pdf:pdf},
  pages  = {83--413},
}

@Article{Ju2002,
  author   = {Ju, Tao and Losasso, Frank and Schaefer, Scott and Warren, Joe},
  year     = {2002},
  title    = {{339-346-juF.pdf}},
  pages    = {339--346},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ju et al. - 2002 - Dual contouring of hermite data.pdf:pdf},
  isbn     = {1581135211},
  keywords = {contouring,crack prevention,implicit functions,polyhedral simplification,quadratic error functions},
}

@Article{Ju2002a,
  author       = {Ju, Tao and Losasso, Frank and Schaefer, Scott and Warren, Joe},
  year         = {2002},
  journal       = {ACM Transactions on Graphics (TOG)},
  title        = {{Dual contouring of hermite data}},
  doi          = {10.1145/566654.566586},
  number       = {3},
  pages        = {339--346},
  volume       = {21},
  abstract     = {This paper describes a new method for contouring a signed grid whose edges are tagged by Hermite data (i.e},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ju et al. - 2002 - Dual contouring of hermite data.pdf:pdf},
  keywords     = {Contouring,Crack Prevention,Engineering,Implicit Functions,Polyhedral Simplification,Quadratic Error Functions},
}

@InProceedings{Schroeder2015,
  author    = {Schroeder, W and Maynard, R and Geveci, B},
  booktitle = {2015 IEEE 5th Symposium on Large Data Analysis and Visualization (LDAV)},
  year      = {2015},
  title     = {{Flying edges: A high-performance scalable isocontouring algorithm}},
  doi       = {10.1109/LDAV.2015.7348069},
  isbn      = {VO -},
  pages     = {33--40},
  abstract  = {Isocontouring remains one of the most widely used visualization techniques. While a plethora of important contouring algorithms have been developed over the last few decades, many were created prior to the advent of ubiquitous parallel computing systems. With the emergence of large data and parallel architectures, a rethinking of isocontouring and other visualization algorithms is necessary to take full advantage of modern computing hardware. To this end we have developed a high-performance isocontouring algorithm for structured data that is designed to be inherently scalable. Processing is performed completely independently along edges over multiple passes. This novel algorithm also employs computational trimming based on geometric reasoning to eliminate unnecessary computation, and removes the parallel bottleneck due to coincident point merging. As a result the algorithm performs well in serial or parallel execution, and supports heterogeneous parallel computation combining data parallel and shared memory approaches. Further it is capable of processing data too large to fit entirely inside GPU memory, does not suffer additional costs due to preprocessing and search structures, and is the fastest non-preprocessed isocontouring algorithm of which we are aware on shared memory, multi-core systems. The software is currently available under a permissive, open source licence in the VTK visualization system.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schroeder, Maynard, Geveci - 2015 - Flying edges A high-performance scalable isocontouring algorithm.pdf:pdf},
  keywords  = {Algorithm design and analysis,Data visualization,Feature extraction,GPU memory,Graphics processing units,Hardware,I.3.1 [Computing Methodologies]: Computer Graphics,I.3.5 [Computing Methodologies]: Computational Geo,Memory management,Parallel processing,VTK visualization system,computational trimming,data parallel approach,data processing,geometric reasoning,heterogeneous parallel computation,high-performance scalable isocontouring algorithm,inference mechanisms,large data architectures,multiprocessing systems,nonpreprocessed isocontouring algorithm,open source licence,parallel architectures,parallel bottleneck,search problems,search structures,shared memory approach,shared memory multicore systems,shared memory systems,ubiquitous computing,ubiquitous parallel computing systems,visualization algorithms,visualization techniques},
}

@Article{Egger2013,
  author       = {Egger, Jan and Kapur, Tina and Fedorov, Andriy and Pieper, Steve and Miller, James V and Veeraraghavan, Harini and Freisleben, Bernd and Golby, Alexandra J and Nimsky, Christopher and Kikinis, Ron},
  year         = {2013-03},
  journal       = {Scientific reports},
  title        = {{GBM volumetry using the 3D Slicer medical image computing platform}},
  doi          = {10.1038/srep01364},
  issn         = {2045-2322},
  language     = {eng},
  pages        = {1364},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/23455483 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3586703/},
  volume       = {3},
  abstract     = {Volumetric change in glioblastoma multiforme (GBM) over time is a critical factor in treatment decisions. Typically, the tumor volume is computed on a slice-by-slice basis using MRI scans obtained at regular intervals. (3D)Slicer - a free platform for biomedical research - provides an alternative to this manual slice-by-slice segmentation process, which is significantly faster and requires less user interaction. In this study, 4 physicians segmented GBMs in 10 patients, once using the competitive region-growing based GrowCut segmentation module of Slicer, and once purely by drawing boundaries completely manually on a slice-by-slice basis. Furthermore, we provide a variability analysis for three physicians for 12 GBMs. The time required for GrowCut segmentation was on an average 61\% of the time required for a pure manual segmentation. A comparison of Slicer-based segmentation with manual slice-by-slice segmentation resulted in a Dice Similarity Coefficient of 88.43 ± 5.23\% and a Hausdorff Distance of 2.32 ± 5.23 mm.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Egger et al. - 2013 - GBM volumetry using the 3D Slicer medical image computing platform.pdf:pdf},
  keywords     = {*Imaging,*Magnetic Resonance Imaging,*Tumor Burden,Computer-Assisted,Glioblastoma/*diagnosis/pathology,Humans,Image Processing,Three-Dimensional},
  publisher    = {Nature Publishing Group},
}

@InProceedings{Bohak2014,
  author    = {Bohak, C and Sodja, A and Marolt, M and Mitrovi{\'{c}}, U and Pernu{\v{s}}, F},
  booktitle = {IWSSIP 2014 Proceedings},
  year      = {2014},
  title     = {{Fast segmentation, conversion and rendering of volumetric data using GPU}},
  isbn      = {2157-8672 VO -},
  pages     = {239--242},
  abstract  = {In this paper we present a proof-of-concept implementation of fast volumetric data segmentation, conversion to polygonal mesh geometry and rendering. All parts of the method are implemented on the graphical processing unit, which allows high degree of parallelization. Implementations of presented algorithms are done in the OpenCL framework and are integrated in blood vessel visualisation software Neck Veins. This paper presents where and to what degree parts of method can be parallelized. In results we also show to what degree we can speed-up the implementation by using parallel computing power of the graphical processing units.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bohak et al. - 2014 - Fast segmentation, conversion and rendering of volumetric data using GPU.pdf:pdf},
  keywords  = {3D visualisation,Biomedical imaging,Computed tomography,GPU,GPU computation,Graphics processing units,Neck Veins software,OpenCL framework,Three-dimensional displays,blood vessel visualisation software,data visualisation,fast volumetric data segmentation,graphics processing unit,graphics processing units,medical computing,medical visualisation,parallel computing power,parallelization degree,polygonal mesh geometry,rendering (computer graphics),volume data segmentation,volumetric data conversion,volumetric data rendering},
}

@Article{Kalavakonda2017,
  author     = {Kalavakonda, Niveditha},
  year       = {2017},
  title      = {{Isosurface Visualization Using Augmented Reality for Improving Tumor Resection Outcomes}},
  annotation = {This is a thesis},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalavakonda - 2017 - Isosurface Visualization Using Augmented Reality for Improving Tumor Resection Outcomes.pdf:pdf},
}

@Article{Liu2016,
  author       = {Liu, Baoquan and Clapworthy, Gordon J and Dong, Feng and Wu, Enhua},
  year         = {2016},
  journal       = {Computer Graphics Forum},
  title        = {{Parallel Marching Blocks: A Practical Isosurfacing Algorithm for Large Data on Many-Core Architectures}},
  doi          = {10.1111/cgf.12897},
  number       = {3},
  pages        = {211--220},
  url          = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12897},
  volume       = {35},
  abstract     = {Abstract Interactive isosurface visualisation has been made possible by mapping algorithms to GPU architectures. However, current state-of-the-art isosurfacing algorithms usually consume large amounts of GPU memory owing to the additional acceleration structures they require. As a result, the continued limitations on available GPU memory mean that they are unable to deal with the larger datasets that are now increasingly becoming prevalent. This paper proposes a new parallel isosurface-extraction algorithm that exploits the blocked organisation of the parallel threads found in modern many-core platforms to achieve fast isosurface extraction and reduce the associated memory requirements. This is achieved by optimising thread co-operation within thread-blocks and reducing redundant computation; ultimately, an indexed triangular mesh can be produced. Experiments have shown that the proposed algorithm is much faster (up to 10×) than state-of-the-art GPU algorithms and has a much smaller memory footprint, enabling it to handle much larger datasets (up to 64×) on the same GPU.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2016 - Parallel Marching Blocks A Practical Isosurfacing Algorithm for Large Data on Many-Core Architectures.pdf:pdf},
  keywords     = {Categories and Subject Descriptors (according to A,I.3.3 Computer Graphics: Picture/Image Generation—},
}

@Article{Alhazmi2018,
  author   = {Alhazmi, Anod},
  year     = {2018},
  title    = {{3D Volume Visualization in Medical Application}},
  pages    = {58--64},
  volume   = {Vi},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alhazmi - 2018 - 3D Volume Visualization in Medical Application.pdf:pdf},
  isbn     = {1601324863},
  keywords = {30,60,casting,frames per second,interaction,iso-surface,marching cube,not being fast enough,ray,rendering,the other challenge is,to permit for real-time,visualization},
  journal = {Virtual Reality \& Intelligent Hardware}
}

@Article{KamelBoulos2018,
  author       = {{Kamel Boulos}, Maged N and Wilson, James T and Clauson, Kevin A},
  year         = {2018-07},
  journal       = {International journal of health geographics},
  title        = {{Geospatial blockchain: promises, challenges, and scenarios in health and healthcare}},
  doi          = {10.1186/s12942-018-0144-x},
  issn         = {1476-072X},
  language     = {eng},
  number       = {1},
  pages        = {25},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/29973196 https://www.ncbi.nlm.nih.gov/pmc/PMC6033217/},
  volume       = {17},
  abstract     = {A PubMed query run in June 2018 using the keyword 'blockchain' retrieved 40 indexed papers, a reflection of the growing interest in blockchain among the medical and healthcare research and practice communities. Blockchain's foundations of decentralisation, cryptographic security and immutability make it a strong contender in reshaping the healthcare landscape worldwide. Blockchain solutions are currently being explored for: (1) securing patient and provider identities; (2) managing pharmaceutical and medical device supply chains; (3) clinical research and data monetisation; (4) medical fraud detection; (5) public health surveillance; (6) enabling truly public and open geo-tagged data; (7) powering many Internet of Things-connected autonomous devices, wearables, drones and vehicles, via the distributed peer-to-peer apps they run, to deliver the full vision of smart healthy cities and regions; and (8) blockchain-enabled augmented reality in crisis mapping and recovery scenarios, including mechanisms for validating, crediting and rewarding crowdsourced geo-tagged data, among other emerging use cases. Geospatially-enabled blockchain solutions exist today that use a crypto-spatial coordinate system to add an immutable spatial context that regular blockchains lack. These geospatial blockchains do not just record an entry's specific time, but also require and validate its associated proof of location, allowing accurate spatiotemporal mapping of physical world events. Blockchain and distributed ledger technology face similar challenges as any other technology threatening to disintermediate legacy processes and commercial interests, namely the challenges of blockchain interoperability, security and privacy, as well as the need to find suitable and sustainable business models of implementation. Nevertheless, we expect blockchain technologies to get increasingly powerful and robust, as they become coupled with artificial intelligence (AI) in various real-word healthcare solutions involving AI-mediated data exchange on blockchains.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kamel Boulos, Wilson, Clauson - 2018 - Geospatial blockchain promises, challenges, and scenarios in health and healthcare.pdf:pdf},
  publisher    = {BioMed Central},
}

@Article{Larobina2014,
  author       = {Larobina, Michele and Murino, Loredana},
  year         = {2014-04},
  journal       = {Journal of digital imaging},
  title        = {{Medical image file formats}},
  doi          = {10.1007/s10278-013-9657-9},
  issn         = {1618-727X},
  language     = {eng},
  number       = {2},
  pages        = {200--206},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/24338090 https://www.ncbi.nlm.nih.gov/pmc/PMC3948928/},
  volume       = {27},
  abstract     = {Image file format is often a confusing aspect for someone wishing to process medical images. This article presents a demystifying overview of the major file formats currently used in medical imaging: Analyze, Neuroimaging Informatics Technology Initiative (Nifti), Minc, and Digital Imaging and Communications in Medicine (Dicom). Concepts common to all file formats, such as pixel depth, photometric interpretation, metadata, and pixel data, are first presented. Then, the characteristics and strengths of the various formats are discussed. The review concludes with some predictive considerations about the future trends in medical image file formats.},
  annotation   = {This paper talks all about the different imaging standards, Mainly DICOMS it is a great resourse if I ever need some technical specs on what to use in certain practices},
  edition      = {2013/12/13},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Larobina, Murino - 2014 - Medical image file formats.pdf:pdf},
  publisher    = {Springer US},
}

@Article{Barbero2017,
  author       = {Barbero, Silvia and Pallaro, Agnese},
  year         = {2017},
  journal       = {The Design Journal},
  title        = {{Systemic Design for Sustainable Healthcare}},
  doi          = {10.1080/14606925.2017.1352762},
  number       = {sup1},
  pages        = {S2473--S2485},
  url          = {https://doi.org/10.1080/14606925.2017.1352762},
  volume       = {20},
  abstract     = {AbstractThe healthcare sector is a complex system that is facing new emerging trends without, however, having the proper resources and structures to address them. The necessity for a change in the approach to new challenges of the healthcare sector provides a stimulating field of application for design disciplines. One of the major issues to be tackled is the transition of the sector towards a more sustainable development, in which designers play a crucial role. The paper presents the approach and the methodology applied by the Systemic Innovation Design Network research team of the Department of Architecture and Design at Politecnico di Torino and discusses the potentialities offered by the application of a Systemic Design approach to healthcare, as an effective way of addressing the complexity of the issue.},
  annotation   = {Much of this paper isn't relivent However it dose reference a good servay and make some observations about the current state of the health industry going forward.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barbero, Pallaro - 2017 - Systemic Design for Sustainable Healthcare.pdf:pdf},
  publisher    = {Routledge},
}

@Article{Nordahl2016,
  author       = {Nordahl, Christine Wu and Mello, Melissa and Shen, Audrey M and Shen, Mark D and Vismara, Laurie A and Li, Deana and Harrington, Kayla and Tanase, Costin and Goodlin-Jones, Beth and Rogers, Sally and Abbeduto, Leonard and Amaral, David G},
  year         = {2016-05},
  journal       = {Journal of neurodevelopmental disorders},
  title        = {{Methods for acquiring MRI data in children with autism spectrum disorder and intellectual impairment without the use of sedation}},
  doi          = {10.1186/s11689-016-9154-9},
  issn         = {1866-1947},
  language     = {eng},
  pages        = {20},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/27158271 https://www.ncbi.nlm.nih.gov/pmc/PMC4858915/},
  volume       = {8},
  abstract     = {BACKGROUND: Magnetic resonance imaging (MRI) has been widely used in studies evaluating the neuropathology of autism spectrum disorder (ASD). Studies are often limited, however, to higher functioning individuals with ASD. MRI studies of individuals with ASD and comorbid intellectual disability (ID) are lacking, due in part to the challenges of acquiring images without the use of sedation. METHODS: Utilizing principles of applied behavior analysis (ABA), we developed a protocol for acquiring structural MRI scans in school-aged children with ASD and intellectual impairment. Board certified behavior analysts worked closely with each child and their parent(s), utilizing behavior change techniques such as pairing, shaping, desensitization, and positive reinforcement, through a series of mock scanner visits to prepare the child for the MRI scan. An objective, quantitative assessment of motion artifact in T1- and diffusion-weighted scans was implemented to ensure that high-quality images were acquired. RESULTS: The sample consisted of 17 children with ASD who are participants in the UC Davis Autism Phenome Project, a longitudinal MRI study aimed at evaluating brain developmental trajectories from early to middle childhood. At the time of their initial scan (2-3.5 years), all 17 children had a diagnosis of ASD and development quotient (DQ) <70. At the time of the current scan (9-13 years), 13 participants continued to have IQs in the range of ID (mean IQ = 54.1, sd = 12.1), and four participants had IQs in the normal range (mean = 102.2, sd = 7.5). The success rate in acquiring T1-weighted images that met quality assurance for acceptable motion artifact was 100 \%. The success rate for acquiring high-quality diffusion-weighted images was 94 \%. CONCLUSIONS: By using principles of ABA in a research MRI setting, it is feasible to acquire high-quality images in school-aged children with ASD and intellectual impairment without the use of sedation. This is especially critical to ensure that ongoing longitudinal studies of brain development can extend from infancy and early childhood into middle childhood in children with ASD at all levels of functioning, including those with comorbid ID.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nordahl et al. - 2016 - Methods for acquiring MRI data in children with autism spectrum disorder and intellectual impairment without the.pdf:pdf},
  publisher    = {BioMed Central},
}

@Article{Reiner2013,
  author       = {Reiner, Bruce I},
  year         = {2013},
  journal       = {Journal of Digital Imaging},
  title        = {{Strategies for Radiology Reporting and Communication. Part 1: Challenges and Heightened Expectations}},
  doi          = {10.1007/s10278-013-9615-6},
  issn         = {1618-727X},
  number       = {4},
  pages        = {610--613},
  url          = {https://doi.org/10.1007/s10278-013-9615-6},
  volume       = {26},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reiner - 2013 - Strategies for Radiology Reporting and Communication. Part 1 Challenges and Heightened Expectations.pdf:pdf},
}

@Article{Singh2008,
  author       = {Singh, Hardeep and Naik, Aanand Dinkar and Rao, Raghuram and Petersen, Laura Ann},
  year         = {2008},
  journal       = {Journal of General Internal Medicine},
  title        = {{Reducing Diagnostic Errors through Effective Communication: Harnessing the Power of Information Technology}},
  doi          = {10.1007/s11606-007-0393-z},
  issn         = {1525-1497},
  number       = {4},
  pages        = {489--494},
  url          = {https://doi.org/10.1007/s11606-007-0393-z},
  volume       = {23},
  abstract     = {Diagnostic errors are poorly understood despite being a frequent cause of medical errors. Recent efforts have aimed to advance the "basic science" of diagnostic error prevention by tracing errors to their most basic origins. Although a refined theory of diagnostic error prevention will take years to formulate, we focus on communication breakdown, a major contributor to diagnostic errors and an increasingly recognized preventable factor in medical mishaps. We describe a comprehensive framework that integrates the potential sources of communication breakdowns within the diagnostic process and identifies vulnerable steps in the diagnostic process where various types of communication breakdowns can precipitate error. We then discuss potential information technology-based interventions that may have efficacy in preventing one or more forms of these breakdowns. These possible intervention strategies include using new technologies to enhance communication between health providers and health systems, improve patient involvement, and facilitate management of information in the medical record.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh et al. - 2008 - Reducing Diagnostic Errors through Effective Communication Harnessing the Power of Information Technology.pdf:pdf},
}

@Article{Siewert2016,
  author       = {Siewert, B and Brook, O R and Hochman, M and Eisenberg, R L},
  year         = {2016},
  journal       = {American Journal of Roentgenology},
  title        = {{Impact of communication errors in radiology on patient care, customer satisfaction, and work-flow efficiency}},
  doi          = {10.2214/AJR.15.15117},
  number       = {3},
  pages        = {573--579},
  url          = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960172885&doi=10.2214\%2FAJR.15.15117&partnerID=40&md5=69b70ea9d34f3f0dfb0070285c3905cd},
  volume       = {206},
  annotation   = {Cited By :12 Export Date: 30 April 2019},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siewert et al. - 2016 - Impact of communication errors in radiology on patient care, customer satisfaction, and work-flow efficiency.pdf:pdf},
}

@Article{Daffner2013,
  author       = {Daffner, Richard H.},
  year         = {2013-11},
  journal       = {American Journal of Roentgenology},
  title        = {{Cervical Radiography for Trauma Patients}},
  doi          = {10.2214/ajr.175.5.1751309},
  issn         = {0361-803X},
  number       = {5},
  pages        = {1309--1311},
  url          = {https://doi.org/10.2214/ajr.175.5.1751309},
  volume       = {175},
  abstract     = {OBJECTIVE The purpose of this study was to determine the time necessary to perform a six-view radiographic examination of the cervical vertebral column of trauma victims. In addition we compared the added examination times for 30 patients who underwent an additional helical CT examination of the cervical region immediately after cranial CT. CONCLUSION Cervical radiography is a time-consuming procedure, which is a concern for trauma surgeons. A more efficient way for cervical evaluation of trauma patients needs to be adopted. Evidence now exists in the literature to suggest that helical CT can serve that purpose.},
  annotation   = {doi: 10.2214/ajr.175.5.1751309},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Daffner - 2013 - Cervical Radiography for Trauma Patients.pdf:pdf},
  publisher    = {American Roentgen Ray Society},
}

@Article{Gunderman2001,
  author       = {Gunderman, Richard B},
  year         = {2001-07},
  journal       = {American Journal of Roentgenology},
  title        = {{Patient Communication}},
  doi          = {10.2214/ajr.177.1.1770041},
  issn         = {0361-803X},
  number       = {1},
  pages        = {41--43},
  url          = {https://doi.org/10.2214/ajr.177.1.1770041},
  volume       = {177},
  annotation   = {doi: 10.2214/ajr.177.1.1770041},
  publisher    = {American Roentgen Ray Society},
}

@Article{Brenner2005,
  author       = {Brenner, R James and Bartholomew, Lori},
  year         = {2005},
  journal       = {Journal of the American College of Radiology},
  title        = {{Communication Errors in Radiology: A Liability Cost Analysis}},
  doi          = {https://doi.org/10.1016/j.jacr.2004.08.009},
  issn         = {1546-1440},
  number       = {5},
  pages        = {428--431},
  url          = {http://www.sciencedirect.com/science/article/pii/S1546144004003485},
  volume       = {2},
  abstract     = {Purpose We evaluated the economic effect on radiologists involved in litigation of failures in communication of results. Method We examined claims data from the Physicians Insurers Association of America 2002 report on breast cancer and identified malpractice cases in which miscommunication, rather than misdiagnosis, was the primary cause for litigation. Results The average indemnity payment for primary errors in communication by radiologists was between $228,000 and $236,000, or twice as high as when appropriate communication occurred. As a percentage of total indemnity payments to plaintiffs, such awards were 15 times higher than when communication was effective. Conclusions Notwithstanding diagnostic accuracy, errors attributable to ineffective communication of results account for high indemnity awards. These errors can be easily resolved in clinical practice.},
  annotation   = {I don't have access to this article How ever the abstract gives some clear numbers as a guide},
  keywords     = {Malpractice,communication,diagnosis,error},
}

@Article{Verrier2010,
  author       = {Verrier, William and Harvey, Jane},
  year         = {2010},
  journal       = {Radiography},
  title        = {{An investigation into work related stressors on diagnostic radiographers in a local district hospital}},
  doi          = {https://doi.org/10.1016/j.radi.2009.09.005},
  issn         = {1078-8174},
  number       = {2},
  pages        = {115--124},
  url          = {http://www.sciencedirect.com/science/article/pii/S107881740900090X},
  volume       = {16},
  abstract     = {Extensive research on the effects of work related stress amongst healthcare professions and the NHS has been undertaken. However, very little is known about the incidence of stress amongst UK radiographers although the few studies which have been conducted indicate that the prevalence and impact of stress on radiographers are considerable. The purpose of this study was to examine work related stressors which affect diagnostic radiographers in the imaging department of a local district hospital. The study utilised the HSE Indicator and Analysis Tools for Work Related Stress. These tools are based upon the HSE Management Standards for Work Related Stress which identifies six areas that represent potential stress hazards if managed inadequately. Two free response questions and a comments box were appended to the Indicator Tool to gain further insights into the radiographers' experiences of work related stress. The results of the study indicated that the hazards associated with work related stress risk were not being optimally managed in the department. Areas of Managers' Support, Relationships, Role and Change represented the greatest risks. In addition, the radiographers cited staff shortages, heavy workload and volume of patients as the greatest sources of pressure at work and their most common recommendations to reduce stress at work were increased staffing, improved communication and more effective feedback systems.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Verrier, Harvey - 2010 - An investigation into work related stressors on diagnostic radiographers in a local district hospital.pdf:pdf},
  keywords     = {HSE,Management standards,Occupational stress,Radiography,Work-place stress},
}

@Article{Goyal2009,
  author       = {Goyal, N and Jain, N and Rachapalli, V},
  year         = {2009},
  journal       = {Clinical Radiology},
  title        = {{Ergonomics in radiology}},
  doi          = {https://doi.org/10.1016/j.crad.2008.08.003},
  issn         = {0009-9260},
  number       = {2},
  pages        = {119--126},
  url          = {http://www.sciencedirect.com/science/article/pii/S000992600800336X},
  volume       = {64},
  abstract     = {The use of computers is increasing in every field of medicine, especially radiology. Filmless radiology departments, speech recognition software, electronic request forms and teleradiology are some of the recent developments that have substantially increased the amount of time a radiologist spends in front of a computer monitor. Computers are also needed for searching literature on the internet, communicating via e-mails, and preparing for lectures and presentations. It is well known that regular computer users can suffer musculoskeletal injuries due to repetitive stress. The role of ergonomics in radiology is to ensure that working conditions are optimized in order to avoid injury and fatigue. Adequate workplace ergonomics can go a long way in increasing productivity, efficiency, and job satisfaction. We review the current literature pertaining to the role of ergonomics in modern-day radiology especially with the development of picture archiving and communication systems (PACS) workstations.},
  annotation   = {This paper details the state of the working lab of many MRI technitions 10 years ago. I thought that it could be a good companion piece to the more recent one. This pretty much only exits to fill in the overarching narritive of my proposal. May not be used.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goyal, Jain, Rachapalli - 2009 - Ergonomics in radiology.pdf:pdf},
}

@Article{MohanSM2018,
  author       = {{Mohan SM}, Chander},
  year         = {2018},
  journal       = {The Indian journal of radiology {\&} imaging},
  title        = {{Ergonomics in radiology - Time to revisit}},
  doi          = {10.4103/ijri.IJRI_358_18},
  issn         = {0971-3026},
  language     = {eng},
  number       = {3},
  pages        = {271--272},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/30319201 https://www.ncbi.nlm.nih.gov/pmc/PMC6176669/},
  volume       = {28},
  annotation   = {Use URL the its going to be hard to get a pdf of this. Looks like the jornal uses a DRM},
  publisher    = {Medknow Publications {\&} Media Pvt Ltd},
}

@Article{Lohrke2016,
  author       = {Lohrke, Jessica and Frenzel, Thomas and Endrikat, Jan and Alves, Filipe Caseiro and Grist, Thomas M and Law, Meng and Lee, Jeong Min and Leiner, Tim and Li, Kun-Cheng and Nikolaou, Konstantin and Prince, Martin R and Schild, Hans H and Weinreb, Jeffrey C and Yoshikawa, Kohki and Pietsch, Hubertus},
  year         = {2016},
  journal       = {Advances in therapy},
  title        = {{25 Years of Contrast-Enhanced MRI: Developments, Current Challenges and Future Perspectives}},
  doi          = {10.1007/s12325-015-0275-4},
  issn         = {1865-8652},
  language     = {eng},
  number       = {1},
  pages        = {1--28},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/26809251 https://www.ncbi.nlm.nih.gov/pmc/PMC4735235/},
  volume       = {33},
  abstract     = {In 1988, the first contrast agent specifically designed for magnetic resonance imaging (MRI), gadopentetate dimeglumine (Magnevist({\textregistered})), became available for clinical use. Since then, a plethora of studies have investigated the potential of MRI contrast agents for diagnostic imaging across the body, including the central nervous system, heart and circulation, breast, lungs, the gastrointestinal, genitourinary, musculoskeletal and lymphatic systems, and even the skin. Today, after 25 years of contrast-enhanced (CE-) MRI in clinical practice, the utility of this diagnostic imaging modality has expanded beyond initial expectations to become an essential tool for disease diagnosis and management worldwide. CE-MRI continues to evolve, with new techniques, advanced technologies, and novel contrast agents bringing exciting opportunities for more sensitive, targeted imaging and improved patient management, along with associated clinical challenges. This review aims to provide an overview on the history of MRI and contrast media development, to highlight certain key advances in the clinical development of CE-MRI, to outline current technical trends and clinical challenges, and to suggest some important future perspectives. FUNDING: Bayer HealthCare.},
  edition      = {2016/01/25},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lohrke et al. - 2016 - 25 Years of Contrast-Enhanced MRI Developments, Current Challenges and Future Perspectives.pdf:pdf},
  publisher    = {Springer Healthcare},
}

@Article{Li2019a,
  author       = {Li, Jie and Li, Qiancheng and Dai, Xiuhong and Li, Jiong and Zhang, Xinxian},
  year         = {2019-02},
  journal       = {Medicine},
  title        = {{Does pre-scanning training improve the image quality of children receiving magnetic resonance imaging?: A meta-analysis of current studies}},
  doi          = {10.1097/MD.0000000000014323},
  issn         = {1536-5964},
  language     = {eng},
  number       = {5},
  pages        = {e14323--e14323},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/30702613 https://www.ncbi.nlm.nih.gov/pmc/PMC6380694/},
  volume       = {98},
  abstract     = {BACKGROUND: Magnetic resonance imaging (MRI) is often used in children for its clear display of body parts. But it is usually hard to acquire high-quality images, for the uncooperative ability of children. It is believed that pre-MRI training could ensure the high quality of images. The current meta-analysis was done to analyze the current evidences in this field. METHODS: PubMed, Cochrane Library, and Web of Science were systematically searched up to July 2018, for studies assessing the effects of training on pediatric MRI. Data, including image quality, failed scanning rate, and sedation use, were extracted and analyzed using Revman 5.2 software. RESULTS: There were 5 studies with 379 subjects in the meta-analysis. Training and control groups were quite comparable when accepted image quality was reviewed (P = .30), but a lower rate of excellent image quality was found in subjects with training (P = .02). The pooling results found no significance between training and control group in sedation use (P = .09) and successful MRI scanning (P = .63). CONCLUSIONS: It is cautious to conclude that pre-MRI training does not improve the image quality and reduce sedation use among children, for the limited number of studies and sample size. More trials should be encouraged to demonstrate this issue.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2019 - Does pre-scanning training improve the image quality of children receiving magnetic resonance imaging A meta-analysis.pdf:pdf},
  publisher    = {Wolters Kluwer Health},
}

@Article{Karsakov2018,
  author       = {Karsakov, Andrey},
  year         = {2018-01},
  journal       = {Procedia Computer Science},
  title        = {{Existing Teaching Practices in Augmented Reality}},
  doi          = {10.1016/J.PROCS.2018.08.232},
  issn         = {1877-0509},
  pages        = {5--15},
  url          = {https://www.sciencedirect.com/science/article/pii/S1877050918315370},
  volume       = {136},
  abstract     = {Rapid development of Augmented Reality technologies and expanding area of their applications has led to the demand for highly qualified specialists in this field. However, while the Augmented Reality research and development community is growing stronger, the teaching competence and exchange of good practices in this field are still very fragmented. The main purpose of article is to present a review of existing practices in teaching of Augmented Reality courses. This review aims to teachers, scientists and policy makers to inform them about learning methods, learning objectives, assessment criteria, and required knowledge, skills, and competences in the field of Augmented Reality.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karsakov - 2018 - Existing Teaching Practices in Augmented Reality.pdf:pdf},
  publisher    = {Elsevier},
}

@Article{Makowski2019,
  author       = {Makowski, Carolina and Lepage, Martin and Evans, Alan C},
  year         = {2019-01},
  journal       = {Journal of psychiatry {\&} neuroscience : JPN},
  title        = {{Head motion: the dirty little secret of neuroimaging in psychiatry}},
  doi          = {10.1503/jpn.180022},
  issn         = {1488-2434},
  language     = {eng},
  number       = {1},
  pages        = {62--68},
  url          = {https://www.ncbi.nlm.nih.gov/pubmed/30565907 https://www.ncbi.nlm.nih.gov/pmc/PMC6306289/},
  volume       = {44},
  abstract     = {Psychiatry is at a crossroads when choosing final samples for analysis of neuroimaging data. Many patient populations exhibit significantly increased motion in the scanner compared with healthy controls, suggesting that more patients would need to be excluded to obtain a clean sample. However, this need is often overshadowed by the extensive amount of time and effort required to recruit these valuable and uncommon samples. This commentary sheds light on the impact of motion on imaging studies, drawing examples from psychiatric patient samples to better understand how head motion can confound interpretation of clinically oriented questions. We discuss the impact of even subtle motion artifacts on the interpretation of results as well as how different levels of stringency in quality control can affect findings within nearly identical samples. We also summarize recent initiatives toward harmonization of quality-control procedures as well as tools to prospectively and retrospectively correct for motion artifacts.},
  annotation   = {This paper may not have too much 100\% relevent data but there might be a couple choice quotes in it that I can use. didn't seem relevent},
  edition      = {2018/08/14},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Makowski, Lepage, Evans - 2019 - Head motion the dirty little secret of neuroimaging in psychiatry.pdf:pdf},
  publisher    = {Joule Inc.},
}

@Article{BarneaGoraly2014,
  author       = {Barnea-Goraly, Naama and Weinzimer, Stuart A and Ruedy, Katrina J and Mauras, Nelly and Beck, Roy W and Marzelli, Matt J and Mazaika, Paul K and Aye, Tandy and White, Neil H and Tsalikian, Eva and Fox, Larry and Kollman, Craig and Cheng, Peiyao and Reiss, Allan L and {on behalf of the Diabetes Research in Children Network (DirecNet)}},
  year         = {2014-02},
  journal       = {Pediatric Radiology},
  title        = {{High success rates of sedation-free brain MRI scanning in young children using simple subject preparation protocols with and without a commercial mock scanner--the Diabetes Research in Children Network (DirecNet) experience}},
  doi          = {10.1007/s00247-013-2798-7},
  issn         = {1432-1998},
  number       = {2},
  pages        = {181--186},
  url          = {https://doi.org/10.1007/s00247-013-2798-7},
  volume       = {44},
  abstract     = {The ability to lie still in an MRI scanner is essential for obtaining usable image data. To reduce motion, young children are often sedated, adding significant cost and risk.},
  annotation   = {This paper seems to focus on some of the usablity aspects of MRI technology},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barnea-Goraly et al. - 2014 - High success rates of sedation-free brain MRI scanning in young children using simple subject preparation.pau:pau},
}

@Article{2013,
  year         = {2013-06},
  journal       = {International Journal of Computer Assisted Radiology and Surgery},
  title        = {{Interventional Radiology}},
  doi          = {10.1007/s11548-013-0844-4},
  issn         = {1861-6429},
  number       = {1},
  pages        = {5--11},
  url          = {https://doi.org/10.1007/s11548-013-0844-4},
  volume       = {8},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2013 - Interventional Radiology.pdf:pdf},
}

@InProceedings{Korn2014,
  author     = {Korn, Oliver and Funk, Markus and Abele, Stephan and H{\"{o}}rz, Thomas and Schmidt, Albrecht},
  year       = {2014},
  title      = {{Context-aware Assistive Systems at the Workplace: Analyzing the Effects of Projection and Gamification}},
  doi        = {10.1145/2674396.2674406},
  volume     = {2014},
  annotation = {Need to replace pdf with the full one on this article currently its just the first couple of pages},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Korn et al. - 2014 - Context-aware Assistive Systems at the Workplace Analyzing the Effects of Projection and Gamification.pdf:pdf},
}

@Article{Kraut2003,
  author       = {Kraut, Robert and {R. Fussell}, Susan and Siegel, Jane},
  year         = {2003},
  journal       = {Human-Computer Interaction},
  title        = {{Visual Information as a Conversational Resource in Collaborative Physical Tasks}},
  doi          = {10.1207/S15327051HCI1812_2},
  pages        = {13--49},
  volume       = {18},
  annotation   = {This is a really old paper but I am curious about its content.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kraut, R. Fussell, Siegel - 2003 - Visual Information as a Conversational Resource in Collaborative Physical Tasks.pdf:pdf},
}

@InProceedings{Blattgerste2018,
  author    = {Blattgerste, Jonas and Renner, Patrick and Strenge, Benjamin and Pfeiffer, Thies},
  booktitle = {Proceedings of the 11th PErvasive Technologies Related to Assistive Environments Conference},
  year      = {2018},
  title     = {{In-Situ Instructions Exceed Side-by-Side Instructions in Augmented Reality Assisted Assembly}},
  doi       = {10.1145/3197768.3197778},
  isbn      = {978-1-4503-6390-7},
  location  = {New York, NY, USA},
  pages     = {133--140},
  publisher = {ACM},
  series    = {PETRA '18},
  url       = {http://doi.acm.org/10.1145/3197768.3197778},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blattgerste et al. - 2018 - In-Situ Instructions Exceed Side-by-Side Instructions in Augmented Reality Assisted Assembly.pdf:pdf},
  keywords  = {Assistance Systems,Augmented Reality,Benchmarking,Head-Mounted Displays,Smart Glasses},
}

@InProceedings{Blattgerste2017,
  author     = {Blattgerste, Jonas and Strenge, Benjamin and Renner, Patrick and Pfeiffer, Thies and Essig, Kai},
  booktitle  = {Proceedings of the 10th International Conference on PErvasive Technologies Related to Assistive Environments},
  year       = {2017},
  title      = {{Comparing Conventional and Augmented Reality Instructions for Manual Assembly Tasks}},
  doi        = {10.1145/3056540.3056547},
  isbn       = {978-1-4503-5227-7},
  location   = {New York, NY, USA},
  pages      = {75--82},
  publisher  = {ACM},
  series     = {PETRA '17},
  url        = {http://doi.acm.org/10.1145/3056540.3056547},
  annotation = {this is a great paper for looking at the different levels of cognitive loads between different situations.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blattgerste et al. - 2017 - Comparing Conventional and Augmented Reality Instructions for Manual Assembly Tasks.pdf:pdf},
  keywords   = {Assistance Systems,Benchmarking,Head-Mounted Displays,Smartglasses},
}

@Article{Wang2009,
  author       = {Wang, Robert Y and Popovi{\'{c}}, Jovan},
  year         = {2009},
  journal       = {ACM Transactions on Graphics},
  title        = {{Real-time hand-tracking with a color glove}},
  number       = {3},
  url          = {http://people.csail.mit.edu/rywang/handtracking/},
  volume       = {28},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Popovi{\'{c}} - 2009 - Real-time hand-tracking with a color glove.pdf:pdf},
}

@InProceedings{Ishiyama2016,
  author    = {Ishiyama, H and Kurabayashi, S},
  booktitle = {2016 IEEE Virtual Reality (VR)},
  year      = {2016},
  title     = {{Monochrome glove: A robust real-time hand gesture recognition method by using a fabric glove with design of structured markers}},
  doi       = {10.1109/VR.2016.7504716},
  isbn      = {2375-5334 VO -},
  pages     = {187--188},
  abstract  = {This paper presents a method for recognizing human-hand postures in real time, even if environment light cannot be configured appropriately. The key technology is a monochrome glove that is patterned with augmented reality (AR) markers on its palm and is also designed with a structured marker on each finger. As the glove only uses white color for the design of the patterns, it can achieve robust gesture recognition in a natural lighting environment by using a single camera to track a hand wearing the glove. The structured marker facilitates the recognition of a fingertip. Our system can recognize at least 96 hand postures by using the features of the fingers. The AR pattern can be of various configurations. Thus, by utilizing different patterned gloves, we can represent different interactions from the same hand posture. The extensive experiments we conducted demonstrate the accuracy, efficiency, and robustness of our gesture recognition method.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ishiyama, Kurabayashi - 2016 - Monochrome glove A robust real-time hand gesture recognition method by using a fabric glove with design o.pdf:pdf},
  keywords  = {AR markers,AR pattern,Augmented reality,Gesture recognition,Image color analysis,Real-time systems,Robustness,Thumb,augmented reality,data gloves,fabric glove,fingertip,fingertips detection,gesture recognition,hand gesture recognition,hand wearing,human-computer interaction (HCI),human-hand postures,monochrome glove,natural lighting environment,patterned gloves,robust gesture recognition,robust real-time hand gesture recognition,single camera,structured markers,user interface},
}

@InProceedings{Doliotis2011,
  author    = {Doliotis, Paul and Stefan, Alexandra and McMurrough, Christopher and Eckhard, David and Athitsos, Vassilis},
  booktitle = {Proceedings of the 4th International Conference on PErvasive Technologies Related to Assistive Environments},
  year      = {2011},
  title     = {{Comparing Gesture Recognition Accuracy Using Color and Depth Information}},
  doi       = {10.1145/2141622.2141647},
  isbn      = {978-1-4503-0772-7},
  location  = {New York, NY, USA},
  pages     = {20:1----20:7},
  publisher = {ACM},
  series    = {PETRA '11},
  url       = {http://doi.acm.org/10.1145/2141622.2141647},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doliotis et al. - 2011 - Comparing Gesture Recognition Accuracy Using Color and Depth Information.pdf:pdf},
  keywords  = {DTW,Kinect,dynamic time warping,gesture recognition},
}

@Article{Nguyen2019,
  author       = {Nguyen, Hai Duong and Kim, Soo-Hyung},
  year         = {2019},
  journal       = {CoRR},
  title        = {{Hand Segmentation and Fingertip Tracking from Depth Camera Images Using Deep Convolutional Neural Network and Multi-task SegNet}},
  eprint       = {1901.03465},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/1901.03465},
  volume       = {abs/1901.0},
  arxivid      = {1901.03465},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, Kim - 2019 - Hand Segmentation and Fingertip Tracking from Depth Camera Images Using Deep Convolutional Neural Network and Multi.pdf:pdf},
}

@InProceedings{Lamberti2011,
  author = {Lamberti, Luigi and Camastra, Francesco},
  year   = {2011},
  title  = {{Real-Time Hand Gesture Recognition Using a Color Glove}},
  doi    = {10.1007/978-3-642-24085-0_38},
  pages  = {365--373},
}

@Article{Chen2019,
  author       = {Chen, Steven Szu-Chi and Duh, Henry},
  year         = {2019-01},
  journal       = {CCF Transactions on Pervasive Computing and Interaction},
  title        = {{Interface of mixed reality: from the past to the future}},
  doi          = {10.1007/s42486-018-0002-8},
  issn         = {2524-5228},
  url          = {https://doi.org/10.1007/s42486-018-0002-8},
  abstract     = {Mixed reality (MR) is an emerging technology which could potentially shape the future of our everyday lives by its unique approach to presenting information. Technology is changing rapidly and information can be presented on traditional computer screens following a WIMP (Windows, Icons, Menus, and Pointing) interface model, by using a head-mounted display to present virtual reality, or by MR which the process of presenting information through a combination of both virtual and physical elements. This paper classifies MR interfaces by applying a text mining method to a data base of 4296 relevant research papers published over the last two decades. The classification reveals the trends relating to each topic and the relations between them. This paper reviews the earlier studies and discusses the recent developments in each topic area and summarizes the advantages and disadvantages of the MR interface. Our objective is to assist researchers understand the trend for each topic and allows them to focus on the research challenges where technological advancements in the MR interface are most needed.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Duh - 2019 - Interface of mixed reality from the past to the future.pdf:pdf},
}

@Article{Song2018a,
  author     = {Song, Xiaolin and Lan, Cuiling and Zeng, Wenjun and Xing, Junliang and Yang, Jingyu and Sun, Xiaoyan},
  year       = {2018-09},
  title      = {{Temporal-Spatial Mapping for Action Recognition}},
  eprint     = {1809.03669},
  eprinttype = {arXiv},
  url        = {http://arxiv.org/abs/1809.03669},
  abstract   = {Deep learning models have enjoyed great success for image related computer vision tasks like image classification and object detection. For video related tasks like human action recognition, however, the advancements are not as significant yet. The main challenge is the lack of effective and efficient models in modeling the rich temporal spatial information in a video. We introduce a simple yet effective operation, termed Temporal-Spatial Mapping (TSM), for capturing the temporal evolution of the frames by jointly analyzing all the frames of a video. We propose a video level 2D feature representation by transforming the convolutional features of all frames to a 2D feature map, referred to as VideoMap. With each row being the vectorized feature representation of a frame, the temporal-spatial features are compactly represented, while the temporal dynamic evolution is also well embedded. Based on the VideoMap representation, we further propose a temporal attention model within a shallow convolutional neural network to efficiently exploit the temporal-spatial dynamics. The experiment results show that the proposed scheme achieves the state-of-the-art performance, with 4.2\% accuracy gain over Temporal Segment Network (TSN), a competing baseline method, on the challenging human action benchmark dataset HMDB51.},
  arxivid    = {1809.03669},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2018 - Temporal-Spatial Mapping for Action Recognition.pdf:pdf},
}

@Article{Xu2018,
  author       = {Xu, Weipeng and Chatterjee, Avishek and Zollh{\"{o}}fer, Michael and Rhodin, Helge and Fua, Pascal and Seidel, Hans-Peter and Theobalt, Christian},
  year         = {2018},
  journal       = {CoRR},
  title        = {{Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera}},
  eprint       = {1803.05959},
  eprinttype   = {arXiv},
  url          = {http://arxiv.org/abs/1803.05959},
  volume       = {abs/1803.0},
  arxivid      = {1803.05959},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2018 - Mo2Cap2 Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera.pdf:pdf},
}

@Article{Grubert2017a,
  author       = {Grubert, Jens and Langlotz, Tobias and Zollmann, Stefanie and Regenbrecht, Holger},
  year         = {2017},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Towards pervasive augmented reality: Context-awareness in augmented reality}},
  doi          = {10.1109/TVCG.2016.2543720},
  eprint       = {1512.00567},
  eprinttype   = {arXiv},
  issn         = {10772626},
  number       = {6},
  volume       = {23},
  abstract     = {Augmented Reality is a technique that enables users to interact with their physical environment through the overlay of digital information. While being researched for decades, more recently, Augmented Reality moved out of the research labs and into the field. While most of the applications are used sporadically and for one particular task only, current and future scenarios will provide a continuous and multi-purpose user experience. Therefore, in this paper, we present the concept of Pervasive Augmented Reality, aiming to provide such an experience by sensing the user's current context and adapting the AR system based on the changing requirements and constraints. We present a taxonomy for Pervasive Augmented Reality and context-aware Augmented Reality, which classifies context sources and context targets relevant for implementing such a context-aware, continuous Augmented Reality experience. We further summarize existing approaches that contribute towards Pervasive Augmented Reality. Based our taxonomy and survey, we identify challenges for future research directions in Pervasive Augmented Reality.},
  annotation   = {awsome paper, not 100\% relevant though},
  arxivid      = {1512.00567},
  isbn         = {1077-2626},
  pmid         = {27008668},
}

@Article{Petersen2015,
  author       = {Petersen, Nils and Stricker, Didier},
  year         = {2015},
  journal       = {Computers and Graphics (Pergamon)},
  title        = {{Cognitive Augmented Reality}},
  doi          = {10.1016/j.cag.2015.08.009},
  issn         = {00978493},
  volume       = {53},
  abstract     = {Although the concept of Augmented Reality (AR) has already been proposed more than 20 years ago, most AR-applications are still limited to simple visualization of virtual objects onto spatially limited scenes. Ideas such as the augmented reality manual, showing step-by-step instructions to a user wearing a Head-Mounted Display (HMD), have been implemented, but the developed systems did not pass the barrier of demonstration prototypes. One major reason, beside remaining ergonomic and hardware limitations, consists of the large effort required for creating the content of such virtual instructions and for building models allowing accurate tracking. In this paper, we introduce the concept of Cognitive Augmented Reality, which radically revises existing approaches on AR-based assistance systems and proposes a fundamentally new paradigm exploiting prior visual observation and learning of a complete manipulative workflow. More precisely, we present in this paper a complete approach for creating augmented reality content for procedural tasks from video examples, and give then details about the presentation of such content at runtime. We show that this new approach, in spite of its very challenging aspects, is scalable, and valuable from a practicable point of view.},
  isbn         = {0097-8493},
}

@Article{Sicat2019,
  author       = {Sicat, Ronell and Li, Jiabao and Choi, Junyoung and Cordeil, Maxime and Jeong, Won Ki and Bach, Benjamin and Pfister, Hanspeter},
  year         = {2019},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{DXR: A Toolkit for Building Immersive Data Visualizations}},
  doi          = {10.1109/TVCG.2018.2865152},
  issn         = {19410506},
  number       = {1},
  volume       = {25},
  abstract     = {This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate},
}

@Article{Volmer2018,
  author       = {Volmer, Benjamin and Baumeister, James and {Von Itzstein}, Stewart and Bornkessel-Schlesewsky, Ina and Schlesewsky, Matthias and Billinghurst, Mark and Thomas, Bruce H.},
  year         = {2018},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{A comparison of predictive spatial augmented reality cues for procedural tasks}},
  doi          = {10.1109/TVCG.2018.2868587},
  issn         = {19410506},
  number       = {11},
  volume       = {24},
  abstract     = {Previous research has demonstrated that Augmented Reality can reduce a user's task response time and mental effort when completing a procedural task. This paper investigates techniques to improve user performance and reduce mental effort by providing projector-based Spatial Augmented Reality predictive cues for future responses. The objective of the two experiments conducted in this study was to isolate the performance and mental effort differences from several different annotation cueing techniques for simple (Experiment 1) and complex (Experiment 2) button-pressing tasks. Comporting with existing cognitive neuroscience literature on prediction, attentional orienting, and interference, we hypothesized that for both simple procedural tasks and complex search-based tasks, having a visual cue guiding to the next task's location would positively impact performance relative to a baseline, no-cue condition. Additionally, we predicted that direction-based cues would provide a more significant positive impact than target-based cues. The results indicated that providing a line to the next task was the most effective technique for improving the users' task time and mental effort in both the simple and complex tasks.},
}

@InProceedings{Endsley2017,
  author    = {Endsley, Tristan C. and Sprehn, Kelly A. and Brill, Ryan M. and Ryan, Kimberly J. and Vincent, Emily C. and Martin, James M.},
  booktitle = {Proceedings of the Human Factors and Ergonomics Society},
  year      = {2017},
  title     = {{Augmented reality design heuristics: Designing for dynamic interactions}},
  doi       = {10.1177/1541931213602007},
  isbn      = {9780945289531},
  volume    = {2017-October},
  abstract  = {Augmented Reality (AR) has emerged as a rapidly developing technology, capable of a wide scope of applications across a variety of domains. AR technologies allow for a virtual experience to be overlaid on top of a physical environment, creating a hybrid experience in which virtual objects become a part of the user's perceptual and physical environment. Rapid progression of the AR field requires that effective and validated methods of design evaluation be developed. Failure to consider the usability of AR applications during the design process will result in an increase in user errors and accidents, limiting user trust of the technology and undermining user perceptions of the technology, for both AR and Virtual Reality (VR) technologies (Nordrum, 2016). Through a robust and iterative process, a set of Design Heuristics for AR were developed for multidimensional augmented environments with the aim of advancing AR design methods for human factors, ergonomics, and user experience practitioners within the expanding AR community.},
  issn      = {10711813},
}

@Article{Datcu2015,
  author       = {Datcu, Dragoş and Lukosch, Stephan and Brazier, Frances},
  year         = {2015},
  journal       = {International Journal of Human-Computer Interaction},
  title        = {{On the Usability and Effectiveness of Different Interaction Types in Augmented Reality}},
  doi          = {10.1080/10447318.2014.994193},
  issn         = {15327590},
  number       = {3},
  volume       = {31},
  abstract     = {One of the key challenges of augmented reality (AR) interfaces is to design effective hand-based interaction supported by computer vision. Hand-based interaction requires free-hands tracking to support user interaction in AR for which this article presents a novel approach. This approach makes it possible to compare different types of hand-based interaction in AR for navigating using a spatial user interface. Quantitative and qualitative analyses of a study with 25 subjects indicate that tangible interaction is the preferred type of interaction with which to determine the position of the user interface in AR and to physically point to a preferred option for navigation in augmented reality.},
  annotation   = {Good article must read},
}

@Misc{Palmarini2018,
  author    = {Palmarini, Riccardo and Erkoyuncu, John Ahmet and Roy, Rajkumar and Torabmostaedi, Hosein},
  year      = {2018},
  title     = {{A systematic review of augmented reality applications in maintenance}},
  doi       = {10.1016/j.rcim.2017.06.002},
  abstract  = {Augmented Reality (AR) technologies for supporting maintenance operations have been an academic research topic for around 50 years now. In the last decade, major progresses have been made and the AR technology is getting closer to being implemented in industry. In this paper, the advantages and disadvantages of AR have been explored and quantified in terms of Key Performance Indicators (KPI) for industrial maintenance. Unfortunately, some technical issues still prevent AR from being suitable for industrial applications. This paper aims to show, through the results of a systematic literature review, the current state of the art of AR in maintenance and the most relevant technical limitations. The analysis included filtering from a large number of publications to 30 primary studies published between 1997 and 2017. The results indicate a high fragmentation among hardware, software and AR solutions which lead to a high complexity for selecting and developing AR systems. The results of the study show the areas where AR technology still lacks maturity. Future research directions are also proposed encompassing hardware, tracking and user-AR interaction in industrial maintenance is proposed.},
  booktitle = {Robotics and Computer-Integrated Manufacturing},
  issn      = {07365845},
  volume    = {49},
}

@Article{Azuma1997,
  author       = {Azuma, Ronald T},
  year         = {1997-08},
  journal       = {Presence: Teleoper. Virtual Environ.},
  title        = {{A Survey of Augmented Reality}},
  doi          = {10.1162/pres.1997.6.4.355},
  issn         = {1054-7460},
  number       = {4},
  pages        = {355--385},
  url          = {http://dx.doi.org/10.1162/pres.1997.6.4.355},
  volume       = {6},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Azuma - 1997 - A Survey of Augmented Reality.pdf:pdf},
  location     = {Cambridge, MA, USA},
  publisher    = {MIT Press},
}

@Article{Karmonik2018,
  author       = {Karmonik, Christof and Boone, Timothy B and Khavari, Rose},
  year         = {2018-02},
  journal       = {Journal of Digital Imaging},
  title        = {{Workflow for Visualization of Neuroimaging Data with an Augmented Reality Device}},
  doi          = {10.1007/s10278-017-9991-4},
  issn         = {1618-727X},
  number       = {1},
  pages        = {26--31},
  url          = {https://doi.org/10.1007/s10278-017-9991-4},
  volume       = {31},
  abstract     = {Commercial availability of three-dimensional (3D) augmented reality (AR) devices has increased interest in using this novel technology for visualizing neuroimaging data. Here, a technical workflow and algorithm for importing 3D surface-based segmentations derived from magnetic resonance imaging data into a head-mounted AR device is presented and illustrated on selected examples: the pial cortical surface of the human brain, fMRI BOLD maps, reconstructed white matter tracts, and a brain network of functional connectivity.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karmonik, Boone, Khavari - 2018 - Workflow for Visualization of Neuroimaging Data with an Augmented Reality Device.pdf:pdf},
}

@Article{Wong2018,
  author       = {Wong, Kevin and Yee, Halina M and Xavier, Brian A and Grillone, Gregory A},
  year         = {2018},
  journal       = {Otolaryngology–Head and Neck Surgery},
  title        = {{Applications of Augmented Reality in Otolaryngology: A Systematic Review}},
  doi          = {10.1177/0194599818796476},
  number       = {6},
  pages        = {956--967},
  url          = {https://doi.org/10.1177/0194599818796476},
  volume       = {159},
  abstract     = {ObjectiveAugmented reality (AR) is a rapidly developing technology. The aim of this systematic review was to (1) identify and evaluate applications of AR in otolaryngology and (2) examine trends in publication over time.Data SourcesPubMed and EMBASE.Review MethodsA systematic review was performed according to PRISMA guidelines without temporal limits. Studies were included if they reported otolaryngology-related applications of AR. Exclusion criteria included non-English articles, abstracts, letters/commentaries, and reviews. A linear regression model was used to compare publication trends over time.ResultsTwenty-three articles representing 18 AR platforms were included. Publications increased between 1997 and 2018 (P < .05). Twelve studies were level 5 evidence; 9 studies, level 4; 1 study, level 2; and 1 study, level 1. There was no trend toward increased level of evidence over time. The most common subspecialties represented were rhinology (52.2\%), head and neck (30.4\%), and neurotology (26\%). The most common purpose of AR was intraoperative guidance (54.5\%), followed by surgical planning (24.2\%) and procedural simulations (9.1\%). The most common source of visual inputs was endoscopes (50\%), followed by eyewear (22.2\%) and microscopes (4.5\%). Computed tomography was the most common virtual input (83.3\%). Optical trackers and fiducial markers were the most common forms of tracking and registration, respectively (38.9\% and 44.4\%). Mean registration error was 2.48 mm.ConclusionAR holds promise in simulation, surgical planning, and perioperative navigation. Although level of evidence remains modest, the role of AR in otolaryngology has grown rapidly and continues to expand.},
  annotation   = {PMID: 30126323},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong et al. - 2018 - Applications of Augmented Reality in Otolaryngology A Systematic Review.pdf:pdf},
}

@Article{Rau2018,
  author       = {Rau, Pei-Luen Patrick and Zheng, Jian and Guo, Zhi and Li, Jiaqi},
  year         = {2018-10},
  journal       = {Computers {\&} Education},
  title        = {{Speed reading on virtual reality and augmented reality}},
  doi          = {10.1016/J.COMPEDU.2018.06.016},
  issn         = {0360-1315},
  pages        = {240--245},
  url          = {https://www.sciencedirect.com/science/article/pii/S0360131518301593},
  volume       = {125},
  abstract     = {Many virtual reality (VR) and augmented reality (AR) applications in education require speed reading. The current study aimed to explore whether the reading performance on VR and AR is different from that on traditional desktop display, and whether the difference is moderated by the reading speed. Sixty-three college students read Chinese passages at normal (650–750 characters per minute [cpm]) or fast speeds (1000–1400 cpm), and then answered multiple-choice questions. They spent approximately 10\% more time in making choice on VR and AR than they did on the desktop display. Teachers should be aware of this difference and allow 10\% more time when using VR and AR applications containing text components.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rau et al. - 2018 - Speed reading on virtual reality and augmented reality.pdf:pdf},
  publisher    = {Pergamon},
}

@Misc{Dixon2014,
  author     = {Dixon, Benjamin J and Daly, Michael J and Chan, Harley H L and Vescan, Allan and Witterick, Ian J and Irish, Jonathan C},
  year       = {2014},
  editor     = {Dixon, Benjamin J},
  title      = {{Inattentional blindness increased with augmented reality surgical navigation.}},
  doi        = {10.2500/ajra.2014.28.4067},
  url        = {https://journals-sagepub-com.access.library.unisa.edu.au/doi/abs/10.2500/ajra.2014.28.4067},
  abstract   = {BACKGROUNDAugmented reality (AR) surgical navigation systems, designed to increase accuracy and efficiency, have been shown to negatively impact on attention. We wished to assess the effect "head-up" AR displays have on attention, efficiency, and accuracy, while performing a surgical task, compared with the same information being presented on a submonitor (SM). METHODSFifty experienced otolaryngology surgeons (n = 42) and senior otolaryngology trainees (n = 8) performed an endoscopic surgical navigation exercise on a predissected cadaveric model. Computed tomography-generated anatomic contours were fused with the endoscopic image to provide an AR view. Subjects were randomized to perform the task with a standard endoscopic monitor with the AR navigation displayed on an SM or with AR as a single display. Accuracy, task completion time, and the recognition of unexpected findings (a foreign body and a critical complication) were recorded. RESULTSRecognition of the foreign body was significantly better in the SM group (15/25 [60\%]) compared with the AR alone group (8/25 [32\%]},
  annotation = {There is a drop in blindness when using AR in this feild according to this study. This study expected to find the oppiset.},
  booktitle  = {American journal of rhinology {\&} allergy},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dixon et al. - 2014 - Inattentional blindness increased with augmented reality surgical navigation.pdf:pdf},
  keywords   = {Computer-Assisted–Education,Endoscopy–Education,Humans–Education,Index Medicus,Otolaryngology–Education,Surgery,Task Performance and Analysis–Education,Tomography,X-Ray Computed–Education},
  number     = {5},
  pages      = {433--437},
  volume     = {28},
}

@Article{Wang2008,
  author       = {Wang, Xiangyu and Gu, Ning and Marchant, David},
  year         = {2008},
  journal       = {Journal of information technology in construction},
  title        = {{An empirical study on designers' perceptions of augmented reality within an architectural firm}},
  issn         = {1874-4753},
  pages        = {536--552},
  url          = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA11143331060001831},
  volume       = {13},
  abstract     = {Nowadays the design sector takes account of collaboration among design team members as seriously as the geometric and spatial features of the design itself. Effective collaboration should appreciate the difference in perceptions between design practitioners. Productivity in design heavily depends on how effectively design ideas can be communicated to other design team members. Augmented Reality (AR) technology, which can insert digital information into the designers' physical working environment, is envisaged to be a promising solution to the collaboration problem faced by design practitioners. Rather than focusing on Augmented Reality system development and application, the work presented in this paper is more focused on the strategic and organizational issues of Augmented Reality and it's role in the existing design sector. Specifically, this paper presents an analytical framework to identify and investigate potential issues of facilitating Augmented Reality technology transfer into the existing visual practices in design firms. Based on this analytical framework, this paper also implemented an empirical pilot study on the perceptions of using Augmented Reality technology for architectural design activity. The interviewees for this study were invited from a well-recognized Australian architecture company. The paper also presents the findings with a discussion of the application of AR technology in the design sector to achieve higher design efficiency.},
  annotation   = {I skim read this artical. It is a old paper but shows a novel idea for implementing AR for design. At the end there is a questionare that could be useful. I won't be able to use 100\% plus its old but its a start for a questioning standard.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Gu, Marchant - 2008 - An empirical study on designers' perceptions of augmented reality within an architectural firm.pdf:pdf},
  location     = {Netherlands},
  publisher    = {International Council for Research and Innovation in Building and Construction},
}

@Article{Plopski2015,
  author       = {Plopski, A and Itoh, Y and Nitschke, C and Kiyokawa, K and Klinker, G and Takemura, H},
  year         = {2015},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Corneal-Imaging Calibration for Optical See-Through Head-Mounted Displays}},
  doi          = {10.1109/TVCG.2015.2391857},
  issn         = {1077-2626 VO - 21},
  number       = {4},
  pages        = {481--490},
  url          = {https://ieeexplore-ieee-org.access.library.unisa.edu.au/stamp/stamp.jsp?tp=&arnumber=7012105},
  volume       = {21},
  abstract     = {In recent years optical see-through head-mounted displays (OST-HMDs) have moved from conceptual research to a market of mass-produced devices with new models and applications being released continuously. It remains challenging to deploy augmented reality (AR) applications that require consistent spatial visualization. Examples include maintenance, training and medical tasks, as the view of the attached scene camera is shifted from the user's view. A calibration step can compute the relationship between the HMD-screen and the user's eye to align the digital content. However, this alignment is only viable as long as the display does not move, an assumption that rarely holds for an extended period of time. As a consequence, continuous recalibration is necessary. Manual calibration methods are tedious and rarely support practical applications. Existing automated methods do not account for user-specific parameters and are error prone. We propose the combination of a pre-calibrated display with a per-frame estimation of the user's cornea position to estimate the individual eye center and continuously recalibrate the system. With this, we also obtain the gaze direction, which allows for instantaneous uncalibrated eye gaze tracking, without the need for additional hardware and complex illumination. Contrary to existing methods, we use simple image processing and do not rely on iris tracking, which is typically noisy and can be ambiguous. Evaluation with simulated and real data shows that our approach achieves a more accurate and stable eye pose estimation, which results in an improved and practical calibration with a largely improved distribution of projection error.},
  annotation   = {This was a interesting paper that is hevely sited providing a new way to track eyes. However the hardware it looked like was used was propiretary. This paper may be important as one that started this research or it just may just be refered to as one of the ways that tried a different method of eye tracking. I doubt this paper will be of much use to my research},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plopski et al. - 2015 - Corneal-Imaging Calibration for Optical See-Through Head-Mounted Displays.pdf:pdf},
  keywords     = {Calibration,Cameras,Computer Graphics,Cornea,Diagnostic Techniques,Estimation,Eye Movements,Head,Humans,Imaging,Iris,OST-HMD calibration,OST-HMD screen,Ophthalmological,Three-Dimensional,Three-dimensional displays,augmented reality,calibration,consistent spatial visualization,continuous recalibration,corneal imaging,corneal imaging calibration,data visualisation,eye,eye pose estimation,gaze direction,gaze tracking,helmet mounted displays,image processing,instantaneous uncalibrated eye gaze tracking,manual calibration method,mass produced device,natural scenes,optical see-through,optical see-through head mounted display,per-frame estimation,pose estimation,precalibrated display,projection error distribution,scene camera,user cornea position estimation,user specific parameters,user view},
}

@InProceedings{Lee2017,
  author    = {Lee, Y and Shin, C and Plopski, A and Itoh, Y and Piumsomboon, T and Dey, A and Lee, G and Kim, S and Billinghurst, M},
  booktitle = {2017 International Symposium on Ubiquitous Virtual Reality (ISUVR)},
  year      = {2017},
  title     = {{Estimating Gaze Depth Using Multi-Layer Perceptron}},
  doi       = {10.1109/ISUVR.2017.13},
  isbn      = {VO -},
  pages     = {26--29},
  url       = {https://ieeexplore.ieee.org/document/7988648},
  abstract  = {In this paper we describe a new method for determining gaze depth in a head mounted eye-tracker. Eye-trackers are being incorporated into head mounted displays (HMDs), and eye-gaze is being used for interaction in Virtual and Augmented Reality. For some interaction methods, it is important to accurately measure the x-and y-direction of the eye-gaze and especially the focal depth information. Generally, eye tracking technology has a high accuracy in x-and y-directions, but not in depth. We used a binocular gaze tracker with two eye cameras, and the gaze vector was input to an MLP neural network for training and estimation. For the performance evaluation, data was obtained from 13 people gazing at fixed points at distances from 1m to 5m. The gaze classification into fixed distances produced an average classification error of nearly 10\%, and an average error distance of 0.42m. This is sufficient for some Augmented Reality applications, but more research is needed to provide an estimate of a user's gaze moving in continuous space.},
  keywords  = {3D gaze,Augmented Reality,Calibration,Cameras,Error analysis,Eye-gaze,Head-mounted display,MLP neural network,Machine Learning,Meters,Resists,Three-dimensional displays,Training,augmented reality,average classification error,binocular gaze tracker,cameras,eye cameras,eye tracking technology,fixed distances,fixed points,focal depth information,gaze classification,gaze depth,gaze depth estimation,gaze tracking,gaze vector,head mounted displays,head mounted eye-tracker,helmet mounted displays,image classification,learning (artificial intelligence),multilayer perceptron,multilayer perceptrons,performance evaluation,training,virtual reality},
}

@Misc{Cutolo2014,
  author   = {Cutolo, Fabrizio and Parchi, Paolo Domenico and Ferrari, Vincenzo},
  year     = {2014},
  title    = {{Video see through AR head-mounted display for medical procedures}},
  doi      = {10.1109/ISMAR.2014.6948504},
  abstract = {<p>In the context of image-guided surgery (IGS), AR technology appears as a significant development in the field since it complements and integrates the concepts of surgical navigation based on virtual reality. The aim of the project is to optimize and validate an ergonomic, accurate and cheap video see-through AR system as an aid in various typologies of surgical procedures. The system will ideally have to be inexpensive and user-friendly to be successfully introduced in the clinical practice.</p>},
  keywords = {Image-Guided Surgery,Interest Point and Salient Region Detections,Medical Device Validation,Mixed / Augmented Reality,Object Detection,Tracking},
  pages    = {393--396},
}

@InProceedings{Newcombe2011,
  author    = {Newcombe, R A and Izadi, S and Hilliges, O and Molyneaux, D and Kim, D and Davison, A J and Kohi, P and Shotton, J and Hodges, S and Fitzgibbon, A},
  booktitle = {2011 10th IEEE International Symposium on Mixed and Augmented Reality},
  year      = {2011},
  title     = {{KinectFusion: Real-time dense surface mapping and tracking}},
  doi       = {10.1109/ISMAR.2011.6092378},
  isbn      = {VO -},
  pages     = {127--136},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Newcombe et al. - 2011 - KinectFusion Real-time dense surface mapping and tracking.pdf:pdf},
  keywords  = {AR,Cameras,Dense Reconstruction,Depth Cameras,GPU,Image reconstruction,Iterative closest point algorithm,Real time systems,Real-Time,SLAM,Simultaneous localization and mapping,Surface reconstruction,Three dimensional displays,Tracking,Volumetric Representation},
}

@Article{Liu2009,
  author       = {Liu, David and Jenkins, Simon Alan and Sanderson, Penelope M},
  year         = {2009},
  journal       = {Current opinion in anaesthesiology},
  title        = {{Patient monitoring with head-mounted displays}},
  doi          = {10.1097/ACO.0b013e32833269c1},
  issn         = {0952-7907},
  number       = {6},
  pages        = {796--803},
  url          = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA51110532800001831},
  volume       = {22},
  annotation   = {This paper shows were the state of the art was at 10 years ago. not much seems to have changed since then us for as Perceptions to AR. This paper has lots of useful references to old world in a AR on proof of concept stuff. notes here bust references but worth are read for my proposal.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu, Jenkins, Sanderson - 2009 - Patient monitoring with head-mounted displays.pdf:pdf},
  location     = {US},
  publisher    = {Lippincott Williams {\&} Wilkins},
}

@Article{Reuter2019,
  author       = {Reuter, Christian and Ludwig, Thomas and Mischur, Patrick},
  year         = {2019},
  journal       = {Computer Supported Cooperative Work (CSCW)},
  title        = {{RescueGlass: Collaborative Applications involving Head-Mounted Displays for Red Cross Rescue Dog Units}},
  doi          = {10.1007/s10606-018-9339-8},
  issn         = {1573-7551},
  number       = {1},
  pages        = {209--246},
  url          = {https://doi.org/10.1007/s10606-018-9339-8},
  volume       = {28},
  abstract     = {On-site work of emergency service teams consists of highly cooperative tasks. Especially during distributed search and rescue tasks there is a constant mix of routinized and non-routinized activities. Within this paper we focus on the work practices of the German Red Cross Rescue Dog Units who deal with several uncertainties regarding the involved dogs, the fragility of the respective situations as well as issues of using technologies under enormous time pressure. Smart glasses provide possibilities for enhanced and hands-free interaction in various contexts and a number of approaches have already been applied, aiming at efficient use of the respective technological innovation in private and professional contexts. However, the collaborative potential of smart glasses in time-critical and uncertain situations is still unexplored. Our design case study examines how the on-site work of emergency service teams can be supported by smart glasses: Based on examining the work practices of the German Red Cross Rescue Dogs, we introduce ‘RescueGlass' as a coordinative concept, encompassing hands-free head-mounted display (HMD) application as well as a corresponding smartphone application. Finally, we describe the evaluation of its use in the field of emergency response and management. We show how current features such as ‘fog of war' or various sensors support the cooperative practices of dog handlers, and outline current technical limitations offering future research questions. Our paper provides an initial design probe using smart glasses to engage in the field of collaborative professional mobile tasks.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reuter, Ludwig, Mischur - 2019 - RescueGlass Collaborative Applications involving Head-Mounted Displays for Red Cross Rescue Dog Units.pdf:pdf},
}

@Misc{Billinghurst2014,
  author     = {Billinghurst, Mark and {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) - Science and Technology Munich}, Germany 10-12 Sept 2014},
  year       = {2014},
  title      = {{The glass class: Designing wearable interfaces}},
  location   = {US},
  url        = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA11148759210001831},
  abstract   = {The course will teach how to create compelling user experiences for wearable computers focusing on design guidelines, prototyping tools, research directions, and a hands-on design experience. These topics will be presented using a number of platforms such as Google Glass, the Recon Jet and Vuzix M-100, although the material will be relevant to other wearable devices.The class will begin with an overview of almost 50 years of wearable computing, beginning with the casino computers of Ed Thorp, through the pioneering efforts of researchers at CMU and MIT, to the most recent commercial systems. The key technology components of a wearable system will be covered, as well as some of the theoretical underpinnings.Next, a set of design guidelines for developing wearable user interfaces will be presented. These include lessons learned from using wearables on a daily basis, design patterns from existing wearable interfaces, and relevant results from the research community. These will be presented in enough details that attendees will be able to use them in their own wearable designs.The third section of the course will introduce a number of tools that can be used for rapid prototyping of wearable interfaces. These include screen-building tools such as Glasssim, through to templating tools that support limited interactivity, and simple programming tools such as Processing.This will lead into a section that discusses the technology of wearable systems in more detail. For example, the different types of head mounted displays for wearables, tracking technology for wearable AR interfaces, input devices, etc.Finally, we will discuss active areas of research that will affect wearable interfaces over the next few years. This includes technologies such as new display hardware, input devices, body worn sensors, and connectivity.},
  annotation = {file contains an abstract that shows the class time table},
  booktitle  = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Billinghurst, 2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) - Science and Technology Munich - 2014 - The glas.pdf:pdf},
  pages      = {1--2},
  publisher  = {Curran Associates},
}

@Article{Abdelgawad2017,
  author       = {Abdelgawad, Kareem and Gausemeier, J{\"{u}}rgen and St{\"{o}}cklein, J{\"{o}}rg and Grafe, Michael and Berssenbr{\"{u}}gge, Jan and Dumitrescu, Roman},
  year         = {2017},
  journal       = {Designs},
  title        = {{A Platform with Multiple Head-Mounted Displays for Advanced Training in Modern Driving Schools}},
  doi          = {10.3390/designs1020008},
  number       = {2},
  pages        = {8},
  volume       = {1},
  abstract     = {Automotive manufacturers and suppliers develop new vehicle systems, such as Advanced Driver Assistance Systems (ADAS), to increase traffic safety and driving comfort. ADAS are technologies that provide drivers with essential information or take over demanding driving tasks. More complex and intelligent vehicle systems are being developed toward fully autonomous and cooperative driving. Apart from the technical development challenges, training of drivers with these complex vehicle systems represents an important concern for automotive manufacturers. This paper highlights the new evolving requirements concerning the training of drivers with future complex vehicle systems. In accordance with these requirements, a new training concept is introduced, and a prototype of a training platform is implemented for utilization in future driving schools. The developed training platform has a scalable and modular architecture so that more than one driving simulator can be networked to a common driving instructor unit. The participating driving simulators provide fully immersive visualization to the drivers by utilizing head-mounted displays instead of conventional display screens and projectors. The driving instructor unit consists of a computer with a developed software tool for training session control, monitoring, and evaluation. Moreover, the driving instructor can use a head-mounted display to participate interactively within the same virtual environment of any selected driver. A simulation model of an autonomous driving system was implemented and integrated in the participating driving simulators. Using this simulation model, training sessions were conducted with the help of a group of test drivers and professional driving instructors to prove the validity of the developed concept and show the usability of the implemented training platform.},
  annotation   = {not relive ant focused pencilice lyon driving sims not really about AR or techin cal stuff.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abdelgawad et al. - 2017 - A Platform with Multiple Head-Mounted Displays for Advanced Training in Modern Driving Schools.pdf:pdf},
  keywords     = {Autonomous and Cooperative Driving,Driver Assistance Systems,Driving Schools,Driving Simulators,Multiple Head-Mounted Displays,Shared Virtual Environments},
}

@Misc{Schmidt2018,
  author    = {Schmidt, S and Steinicke, F and Irlitti, A and Thomas, B and {13th ACM International Conference on Interactive Surfaces and Spaces}, I S S 2018 Tokyo and 2018, Japan 25-28 November},
  year      = {2018},
  title     = {{Floor-projected guidance cues for collaborative exploration of spatial augmented reality setups}},
  doi       = {10.1145/3279778.3279806},
  location  = {US},
  url       = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA11170688550001831 https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA51170847430},
  abstract  = {In this paper we present a floor-based user interface (UI) that allows multiple users to explore a spatial augmented reality (SAR) environment with both monoscopic and stereoscopic projections. Such environments are characterized by a low level of user instrumentation and the capability of providing a shared interaction space for multiple users. However,projector-based systems using stereoscopic display are usually single-user setups, since they can provide the correct perspective for only one tracked person. To address this problem, we developed a set of guidance cues, which are projected onto the floor in order to assist multiple users regarding (i) the interaction with the SAR system, (ii) the identification of regions of interest and ideal viewpoints, and (iii) the collaboration with each other. In a user study with 40 participants all cues were evaluated and a set of feedback elements, which are essential to guarantee an intuitive self-explaining interaction,was identified. The results of the study also indicate that the developed UI guides users to more favorable viewpoints and therefore is able to improve the experience in a multi-user SAR environment.},
  booktitle = {ISS 2018 - Proceedings of the ACM International Conference on Interactive Surfaces and Spaces},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt et al. - 2018 - Floor-projected guidance cues for collaborative exploration of spatial augmented reality setups.pdf:pdf},
  pages     = {279--289},
  publisher = {ACM},
}

@Misc{Irlitti2016,
  author    = {Irlitti, Andrew and Smith, Ross T and {Von Itzstein}, Stewart and Billinghurst, Mark and Thomas, Bruce H and {15th Adjunct IEEE International Symposium on Mixed and Augmented Reality Mexico 18-23 September 2016}, ISMAR-Adjunct 2016 Yucatan},
  year      = {2016},
  title     = {{Challenges for asynchronous collaboration in augmented reality}},
  doi       = {10.1109/ISMAR-Adjunct.2016.0032},
  location  = {US},
  url       = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA51143385560001831 https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA11143385530},
  abstract  = {Collaboration is a promising area of investigation for Augmented Reality (AR) applications. While there have been numerous examples of co-located and remote synchronous collaborative AR applications, there has not been the same interest in pursuing asynchronous interfaces. Asynchronous processes differ from their synchronous counterparts due to the collaboration occurring over a period of time, without the requirement of all parties being present simultaneously. For AR applications, asynchronous collaboration is typically considered to be the combination of drawn registered annotations, and their review at a later time. The potential on offer far exceeds this stance. This paper uncovers a unique opportunity for pursuing asynchronous collaboration support in AR, identifies how communications can be enhanced, and discusses the research challenges unique to asynchronous collaboration in AR.},
  booktitle = {Adjunct Proceedings of the 2016 IEEE International Symposium on Mixed and Augmented Reality},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Irlitti et al. - 2016 - Challenges for asynchronous collaboration in augmented reality.pdf:pdf},
  number    = {7836453},
  pages     = {31--35},
  publisher = {IEEE},
}

@InProceedings{ElSayed2016a,
  author     = {ElSayed, N A M and Smith, R T and Thomas, B H},
  booktitle  = {2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)},
  year       = {2016},
  title      = {{HORUS EYE: See the Invisible Bird and Snake Vision for Augmented Reality Information Visualization}},
  doi        = {10.1109/ISMAR-Adjunct.2016.0077},
  isbn       = {VO -},
  pages      = {203--208},
  url        = {https://ieeexplore-ieee-org.access.library.unisa.edu.au/document/7836498},
  annotation = {Just gave this a quick read, it has a novel way of using AR based images to detect fetures in images. I'm not sure if this method could reduce some of the object recionition needed without needing to use obvious symbols but it still relies on using bright and viberant colors. It is a good and usful paper but it does not have any relevent conent as of yet.},
  keywords   = {Augmented Reality,Birds,Blended Information,Brightness,Data visualization,Diminished Reality,Horus Eye,Horus eye,Image color analysis,Information Visualization,Scene Manipulation,Sensitivity,Sugar,Visualization,ancient Egyptian mythology,augmented reality,augmented reality information visualization,context-based interactive visualization,data visualisation,invisible bird vision,snake vision},
}

@Misc{Broecker2013,
  author     = {Broecker, Markus and Thomas, Bruce H and Smith, Ross T and {2013 IEEE International Symposium on Mixed and Augmented Reality Adelaide}, South Australia 1-4 October 2013},
  year       = {2013},
  title      = {{Adapting ray tracing to spatial augmented reality}},
  doi        = {10.1109/ISMAR.2013.6671826},
  location   = {US},
  url        = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA11144752700001831},
  abstract   = {Ray tracing is an elegant and intuitive image generation method. The introduction of GPU-accelerated ray tracing and corresponding software frameworks makes this rendering technique a viable option for Augmented Reality applications. Spatial Augmented Reality employs projectors to illuminate physical models and is used in fields that require photorealism, such as design and prototyping. Ray tracing can be used to great effect in this Augmented Reality environment to create scenes of high visual fidelity. However, the peculiarities of SAR systems require that core ray tracing algorithms be adapted to this new rendering environment. This paper highlights the problems involved in using ray tracing in a SAR environment and provides solutions to overcome them. In particular, the following issues are addressed: ray generation, hybrid rendering and view-dependent rendering.},
  annotation = {graphics and ray tracing notes nothing of real value.},
  booktitle  = {2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR) : science and technology proceedings},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Broecker et al. - 2013 - Adapting ray tracing to spatial augmented reality.pdf:pdf},
  number     = {6671826},
  pages      = {1--6},
  publisher  = {IEEE},
}

@Misc{Sarupuri2016,
  author     = {Sarupuri, Bhuvaneswari and Lee, Gun and Billinghurst, Mark and {IEEE International Symposium Mixed and Augmented Reality 2016 (ISMAR 2016) Merida}, Mexico 19-23 September 2016},
  year       = {2016},
  title      = {{An augmented reality guide for assisting forklift operation}},
  doi        = {10.1109/ISMAR-Adjunct.2016.0039},
  location   = {US},
  url        = {https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA51141107080001831 https://find.library.unisa.edu.au/primo-explore/fulldisplay?vid=UNISA&search_scope=All_Resources&docid=UNISA_ALMA11142958040},
  abstract   = {Operating forklifts in warehouses is becoming an increasingly difficult task due to higher shelves and narrower aisles. We investigate how Augmented Reality (AR) could aid forklift operators in performing their pallet racking and pick up tasks by superimposing virtual guidelines over a real world camera feed. To test this, we designed and developed a prototype system based on a toy forklift and conducted a user study with it. The results showed that AR cues helped the participants to perform tasks with a higher success rate and provided better usability.},
  annotation = {Not much to this paper how ever the concept could be concidered similar to what I am doing},
  booktitle  = {2016 IEEE International Symposium on Mixed and Augmented Reality Adjunct Proceedings},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarupuri et al. - 2016 - An augmented reality guide for assisting forklift operation.pdf:pdf},
  pages      = {59--60},
  publisher  = {IEEE},
}

@InProceedings{Eck2013,
  author    = {Eck, U and Sandor, C and Laga, H},
  booktitle = {2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2013},
  title     = {{Visuo-haptic augmented reality runtime environment for medical training}},
  doi       = {10.1109/ISMAR.2013.6671816},
  isbn      = {VO -},
  pages     = {1--4},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eck, Sandor, Laga - 2013 - Visuo-haptic augmented reality runtime environment for medical training.pdf:pdf},
  keywords  = {augmented reality,dataflow architectures,haptic interaction,medical training,mixed reality,physically-based simulation},
}

@inproceedings{Baumeister2015,
  author    = {Baumeister, James and Marner, Michael R and Smith, Ross T and Kohler, Mark and Thomas, Bruce H and 2015, 14th IEEE International Symposium on Mixed and Augmented Reality Workshops Japan 29 September - 3 October},
  year      = {2015},
  title     = {{Visual subliminal cues for spatial augmented reality}},
  doi       = {10.1109/ISMARW.2015.11},
  location  = {US},
  abstract  = {Augmented reality systems commonly employ overt cueing to direct a user's attention and guide their actions. Spatial augmented reality based supraliminal annotations have been shown to improve user performance compared to LCD screens. This paper explores subliminal cues: annotations that exist below the threshold of consciousness. We investigate whether subliminal cueing is technically possible with standard data projectors, and if subliminal cues can further improve users' reaction time in procedural tasks. This paper describes a new technique for temporal subliminal cues in spatial augmented reality, and presents the results of three user studies evaluating the effectiveness of this technique in a button-pressing task. The results show that the presentation of annotations is indeed subliminal, in that the visual stimulus was not perceivable to users. We found a statistically significant improvement in task performance when using subliminal cues and supraliminal annotations, compared to supraliminal annotations alone, with mean trial times improving from 633.49ms to 604.64ms.},
  booktitle = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality Workshops, ISMARW 2015},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baumeister et al. - 2015 - Visual subliminal cues for spatial augmented reality.pdf:pdf},
  number    = {7344747},
  pages     = {4--11},
  publisher = {IEEE},
}

@Article{Marchand2016,
  author       = {Marchand, Eric and Uchiyama, Hideaki and Spindler, Fabien},
  year         = {2016},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Pose Estimation for Augmented Reality: A Hands-On Survey}},
  doi          = {10.1109/TVCG.2015.2513408},
  issn         = {1077-2626},
  number       = {12},
  pages        = {2633--2651},
  url          = {https://ieeexplore-ieee-org.access.library.unisa.edu.au/document/7368948},
  volume       = {22},
  abstract     = {Augmented reality (AR) allows to seamlessly insert virtual objects in an image sequence. In order to accomplish this goal, it is important that synthetic elements are rendered and aligned in the scene in an accurate and visually acceptable way. The solution of this problem can be related to a pose estimation or, equivalently, a camera localization process. This paper aims at presenting a brief but almost self-contented introduction to the most important approaches dedicated to vision-based camera localization along with a survey of several extension proposed in the recent years. For most of the presented approaches, we also provide links to code of short examples. This should allow readers to easily bridge the gap between theoretical aspects and practical implementations.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marchand, Uchiyama, Spindler - 2016 - Pose Estimation for Augmented Reality A Hands-On Survey.pdf:pdf},
  keywords     = {Augmented Reality,Cameras,Code Examples,Engineering,Homography,Keypoint Matching,Motion Estimation,Pnp,Pose Estimation,Simultaneous Localization and Mapping,Slam,Solid Modeling,Survey,Three-Dimensional Displays,Vision-Based Camera Localization,Visual Analytics},
}

@Article{Doosti2019,
  author     = {Doosti, Bardia},
  year       = {2019},
  title      = {{Hand Pose Estimation: A Survey}},
  abstract   = {The success of Deep Convolutional Neural Networks (CNNs) in recent years in almost all the Computer Vision tasks on one hand, and the popularity of low-cost consumer depth cameras on the other, has made Hand Pose Estimation a hot topic in computer vision field. In this report, we will first explain the hand pose estimation problem and will review major approaches solving this problem, especially the two different problems of using depth maps or RGB images. We will survey the most important papers in each field and will discuss the strengths and weaknesses of each. Finally, we will explain the biggest datasets in this field in detail and list 21 datasets with all their properties. To the best of our knowledge this is the most complete list of all the datasets in the hand pose estimation field.},
  annotation = {Not peer reviewed may lead to some good findings though},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doosti - 2019 - Hand Pose Estimation A Survey.pdf:pdf},
  keywords   = {Computer Science - Computer Vision And Pattern Rec},
}

@InProceedings{Sun2015,
  author    = {Sun, Xiao and Wei, Yichen and Liang, Shuang and Tang, Xiaoou and Sun, Jian},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015-06},
  title     = {{Cascaded Hand Pose Regression}},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sun et al. - 2015 - Cascaded Hand Pose Regression.pdf:pdf},
}

@InProceedings{Hong2016,
  author    = {Hong, I and Lee, Y},
  booktitle = {2016 IEEE International Conference on Smart Computing (SMARTCOMP)},
  year      = {2016},
  title     = {{Key-Device Based Place Recognition Using Similarity Measure between IoT Spaces}},
  doi       = {10.1109/SMARTCOMP.2016.7501701},
  isbn      = {VO -},
  pages     = {1--5},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hong, Lee - 2016 - Key-Device Based Place Recognition Using Similarity Measure between IoT Spaces.pdf:pdf},
  keywords  = {Euclidean distance,Global Positioning System,Google Maps,Internet of Things,Internet of Things devices,Internet of things,IoT spaces,Smart buildings,Smart homes,Weight measurement,building structural configuration,context-aware service system,cosine similarity,geographical information,key-device based place recognition,mobile robotics community,similarity measure,smart environments,vision data processing,weighted cosine similarity},
}

@InProceedings{Bambach2015,
  author    = {Bambach, Sven and Lee, Stefan and Crandall, David J and Yu, Chen},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015-12},
  title     = {{Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions}},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bambach et al. - 2015 - Lending A Hand Detecting Hands and Recognizing Activities in Complex Egocentric Interactions.pdf:pdf},
}

@Article{Barsoum2016,
  author   = {Barsoum, Emad},
  year     = {2016},
  title    = {{Articulated Hand Pose Estimation Review}},
  abstract = {With the increase number of companies focusing on commercializing Augmented Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand based input mechanism is becoming essential in order to make the experience natural, seamless and immersive. Hand pose estimation has progressed drastically in recent years due to the introduction of commodity depth cameras. Hand pose estimation based on vision is still a challenging problem due to its complexity from self-occlusion (between fingers), close similarity between fingers, dexterity of the hands, speed of the pose and the high dimension of the hand kinematic parameters. Articulated hand pose estimation is still an open problem and under intensive research from both academia and industry. The 2 approaches used for hand pose estimation are: discriminative and generative. Generative approach is a model based that tries to fit a hand model to the observed data. Discriminative approach is appearance based, usually implemented with machine learning (ML) and require a large amount of training data. Recent hand pose estimation uses hybrid approach by combining both discriminative and generative methods into a single hand pipeline. In this paper, we focus on reviewing recent progress of hand pose estimation from depth sensor. We will survey discriminative methods, generative methods and hybrid methods. This paper is not a comprehensive review of all hand pose estimation techniques, it is a subset of some of the recent state-of-the-art techniques.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barsoum - 2016 - Articulated Hand Pose Estimation Review.pdf:pdf},
  keywords = {Computer Science - Computer Vision And Pattern Rec},
}

@Article{Taylor2016,
  author       = {Taylor, Jonathan and Bordeaux, Lucas and Cashman, Thomas and Corish, Bob and Keskin, Cem and Sharp, Toby and Soto, Eduardo and Sweeney, David and Valentin, Julien and Luff, Benjamin and Topalian, Arran and Wood, Erroll and Khamis, Sameh and Kohli, Pushmeet and Izadi, Shahram and Banks, Richard and Fitzgibbon, Andrew and Shotton, Jamie},
  year         = {2016-07},
  journal       = {ACM Trans. Graph.},
  title        = {{Efficient and Precise Interactive Hand Tracking Through Joint, Continuous Optimization of Pose and Correspondences}},
  doi          = {10.1145/2897824.2925965},
  issn         = {0730-0301},
  number       = {4},
  pages        = {143:1----143:12},
  url          = {http://doi.acm.org/10.1145/2897824.2925965},
  volume       = {35},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Taylor et al. - 2016 - Efficient and Precise Interactive Hand Tracking Through Joint, Continuous Optimization of Pose and Correspondence.pdf:pdf},
  keywords     = {articulated tracking,subdivision surfaces,virtual reality},
  location     = {New York, NY, USA},
  publisher    = {ACM},
}

@Article{MorelliAndres2014,
  author       = {{Morelli Andr{\'{e}}s}, A. and Padovani, S. and Tepper, M. and Jacobo-Berlles, J.},
  year         = {2014-01},
  journal       = {Pattern Recognition Letters},
  title        = {{Face recognition on partially occluded images using compressed sensing}},
  doi          = {10.1016/J.PATREC.2013.08.001},
  issn         = {0167-8655},
  pages        = {235--242},
  url          = {https://www.sciencedirect.com/science/article/pii/S0167865513002961},
  volume       = {36},
  abstract     = {In this work we have built a face recognition system using a new method based on recent advances in compressed sensing theory. The authors propose a method for recognizing faces that is robust to certain types and levels of occlusion. They also present tests that allow to assess the incidence of the proposed method.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morelli Andr{\'{e}}s et al. - 2014 - Face recognition on partially occluded images using compressed sensing.pdf:pdf},
  publisher    = {North-Holland},
}

@Article{Hinterstoisser2012,
  author       = {Hinterstoisser, S and Cagniart, C and Ilic, S and Sturm, P and Navab, N and Fua, P and Lepetit, V},
  year         = {2012},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title        = {{Gradient Response Maps for Real-Time Detection of Textureless Objects}},
  doi          = {10.1109/TPAMI.2011.206},
  issn         = {0162-8828 VO - 34},
  number       = {5},
  pages        = {876--888},
  url          = {https://ieeexplore.ieee.org/abstract/document/6042881},
  volume       = {34},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schwarz et al. - 2012 - Human skeleton tracking from depth data using geodesic distances and optical flow.pdf:pdf},
  keywords     = {3D object instance detection,3D surface normal orientation,Clutter,Computer vision,Computer-Assisted,Humans,Image Processing,Image edge detection,Imaging,Real time systems,Reproducibility of Results,Robustness,Surface Properties,Three dimensional displays,Three-Dimensional,Training,Transforms,computer vision,dense depth sensor,gradient methods,gradient response map,image matching,image representation,multimodality template matching.,object detection,real-time detection,real-time detection and object recognition,spread image gradient orientation,template matching,textureless object,tracking},
}

@InProceedings{Hinterstoisser2011,
  author    = {Hinterstoisser, S and Holzer, S and Cagniart, C and Ilic, S and Konolige, K and Navab, N and Lepetit, V},
  booktitle = {2011 International Conference on Computer Vision},
  year      = {2011},
  title     = {{Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes}},
  doi       = {10.1109/ICCV.2011.6126326},
  isbn      = {2380-7504 VO -},
  pages     = {858--865},
  url       = {https://ieeexplore.ieee.org/abstract/document/6126326},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinterstoisser et al. - 2011 - Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes.pdf:pdf},
  keywords  = {3D object detection,Robustness,cluttered scenes,complementary object information,computer graphics,dense depth map,multimodal templates,multimodalities,object detection,real-time detection,real-time systems,texture-less objects},
}

@InProceedings{Milletari2016,
  author    = {Milletari, F and Navab, N and Ahmadi, S},
  booktitle = {2016 Fourth International Conference on 3D Vision (3DV)},
  year      = {2016},
  title     = {{V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation}},
  doi       = {10.1109/3DV.2016.79},
  isbn      = {VO -},
  pages     = {565--571},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milletari, Navab, Ahmadi - 2016 - V-Net Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation.pdf:pdf},
  keywords  = {3D image segmentation,Biomedical imaging,CNN,Deep learning,Dice coefficient,Feature extraction,Image segmentation,MRI volumes,Magnetic resonance imaging,Neural networks,Three-dimensional displays,Two dimensional displays,V-Net,background voxel,biomedical MRI,clinical practice,computer vision,convolutional neural networks,foreground voxel,fully convolutional neural networks,histogram matching,image segmentation,machine learning,magnetic resonance imaging,medical image analysis,medical image processing,neural nets,prostate,random nonlinear transformations,segmentation,volumetric medical image segmentation},
}

@InProceedings{Regenbrecht2013,
  author     = {Regenbrecht, Holger and Collins, Jonny and Hoermann, Simon},
  booktitle  = {Proceedings of the 25th Australian Computer-Human Interaction Conference: Augmentation, Application, Innovation, Collaboration},
  year       = {2013},
  title      = {{A Leap-supported, Hybrid AR Interface Approach}},
  doi        = {10.1145/2541016.2541053},
  isbn       = {978-1-4503-2525-7},
  location   = {New York, NY, USA},
  pages      = {281--284},
  publisher  = {ACM},
  series     = {OzCHI '13},
  url        = {http://doi.acm.org/10.1145/2541016.2541053},
  annotation = {I think a lot of the information now in this paper is common practice. However it is good to keep this paper around just incase I'm wrong.},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Regenbrecht, Collins, Hoermann - 2013 - A Leap-supported, Hybrid AR Interface Approach.pdf:pdf},
  keywords   = {augmented reality,interaction,occlusion,physical and motor rehabilitation},
}

@article{Erat2013,
abstract = {Motivation: The existing visualization of the Camera augmented mobile C-arm (CamC) system does not have enough cues for depth information and presents the anatomical information in a confusing way to surgeons. Methods: We propose a method that segments anatomical information from X-ray and then augment it on the video images. To provide depth cues, pixels belonging to video images are classified as skin and object classes. The augmentation of anatomical information from X-ray is performed only when pixels have a larger probability of belonging to skin class. Results: We tested our algorithm by displaying the new visualization to 2 expert surgeons and 1 medical student during three surgical workflow sequences of the interlocking of intramedullary nail procedure, namely: skin incision, center punching, and drilling. Via a survey questionnaire, they were asked to assess the new visualization when compared to the current alphablending overlay image displayed by CamC. The participants all agreed (100%) that occlusion and instrument tip position detection were immediately improved with our technique. When asked if our visualization has potential to replace the existing alpha-blending overlay during interlocking procedures, all participants did not hesitate to suggest an immediate integration of the visualization for the correct navigation and guidance of the procedure. Conclusion: Current alpha blending visualizations lack proper depth cues and can be a source of confusion for the surgeons when performing surgery. Our visualization concept shows great potential in alleviating occlusion and facilitating clinician understanding during specific workflow steps of the intramedullary nailing procedure. {\textcopyright} 2013 SPIE.},
author = {Erat, Okan and Pauly, Olivier and Weidert, Simon and Thaller, Peter and Euler, Ekkehard and Mutschler, Wolf and Navab, Nassir and Fallavollita, Pascal},
doi = {10.1117/12.2006766},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Erat et al. - 2013 - How a surgeon becomes superman by visualization of intelligently fused multi-modalities.pdf:pdf},
isbn = {9780819494450},
issn = {0277786X},
journal = {Medical Imaging 2013: Image-Guided Procedures, Robotic Interventions, and Modeling},
mendeley-groups = {X-RayVision},
number = {March 2013},
pages = {86710L},
title = {{How a surgeon becomes superman by visualization of intelligently fused multi-modalities}},
volume = {8671},
year = {2013}
}


@Misc{Poupyrev2002,
  author     = {Poupyrev, I and Tan, D S and Billinghurst, M and Kato, H and Regenbrecht, H and Tetsutani, N},
  year       = {2002},
  title      = {{Developing a generic augmented-reality interface}},
  doi        = {10.1109/2.989929},
  location   = {[Long Beach, Calif., etc.] :},
  annotation = {Old paper hevly sighted but contains the first instance of UI desing in AR},
  booktitle  = {Computer},
  isbn       = {0018-9162},
  number     = {3},
  pages      = {44--50},
  volume     = {35},
}

@Article{Lin2014,
  author     = {Lin, Shu-Hwa and Johnson, Rayneld and Stricker, Didier and Cui, Yan},
  year       = {2014},
  title      = {{Exploratory Analysis of College Student's Satisfaction of Body Scanning with Kinect}},
  doi        = {10.15221/12.302},
  url        = {http://www.mendeley.com/research/exploratory-analysis-college-students-satisfaction-body-scanning-kinect},
  abstract   = {The purpose of this study is to explore college students&#8217; attitudes toward body scanning and the creation of an avatar using a Kinect operating system. A select sample of 86 female and male college students participated in the study. Using a Windows 7 operating system with Kinect to provide a stable platform for the NUI audio and motor devices, students&#8217; bodies were scanned and an avatar was created. Bodies were scanned from 360 degrees to obtain 360 pictures and 360 depth frames (i.e. about 10 degrees between each view). Outputs with PNG and PLY files were abstrated from the scan data and processed into a 3D model reconstruction or avatar by Wissenschaftlicher Mitarbeiter. The program, MeshLab, was used to view and measure the avatar. Following the scanning process, subjects responded to a 20 item questionnaire about the process and resulting avatar. Overall, participants expressed satisfaction with their avatar and body shape and provided information about the use of avatars. \n\n},
  annotation = {Link to full text is broken was never cited},
}

@Article{Jo2015,
  author       = {Jo, Dongsik and Kim, Ki Hong and Kim, Gerard J.},
  year         = {2015},
  journal       = {2014 International Workshop on Collaborative Virtual Environments, 3DCVE 2014},
  title        = {{Avatar motion adaptation for AR based 3D tele-conference}},
  doi          = {10.1109/3DCVE.2014.7160932},
  url          = {http://www.mendeley.com/research/avatar-motion-adaptation-ar-based-3d-teleconference},
  abstract     = {With the advent of inexpensive depth sensors and more viable methods for human tracking, traditional 2D tele-conference systems are evolving into one that is AR and 3D teleportation based. Compared to the traditional tele-conference systems which offer only flat 2D upper body imageries and mostly a fixed view point (and inconsistent gaze directions), an AR tele-conference with 3D teleported avatars would be more natural and realistic, and can give an enhanced and immersive communication experience. This paper presents an AR based 3D tele-conference prototype with a method to adapt the motion of the teleported avatar to the physical configuration of the other site. The adaptation is needed due to the differences in the physical environments between two sites where the human controller is interacting at one (e.g. sitting on a low chair) and the avatar is being displayed at the other (e.g. augmented on a high chair). The adaptation technique is based on preserving a particular spatial property among the avatar and its interaction objects between the two sites. The spatial relationship is pre-established between the important joint positions of the user/avatar and carefully selected points on the environment interaction objects. The motions of the user transmitted to the other site are then modified in real time considering the “changed” environment object and by preserving the spatial relationship as much as possible. We have developed a test prototype to demonstrate our approach using the Kinect-based human tracking and a video see-through head-mounted display.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jo, Kim, Kim - 2015 - Avatar motion adaptation for AR based 3D tele-conference.pdf:pdf},
}

@Article{Bholsithi2014,
  author       = {Bholsithi, Wisarut and Wongwaen, Nonlapas and Sinthanayothin, Chanjira},
  year         = {2014},
  journal       = {2014 International Computer Science and Engineering Conference, ICSEC 2014},
  title        = {{3D avatar developments in real time and accuracy assessments}},
  doi          = {10.1109/ICSEC.2014.6978174},
  url          = {http://www.mendeley.com/research/3d-avatar-developments-real-time-accuracy-assessments},
  abstract     = {This paper focuses on an application motion control of avatar and accuracy measurements using Kinect. The development of this application has used C++ Builder as a compiler and libraries to connect with Kinect and show 3D virtual world and avatars. OpenNI library is to detect a depth image while NITE Primesense library is to connect with Kinect, and Chai3D to display virtual worlds and avatar (human motion models). The tests to measure the application performance consist of 2D-3D angular measurements and time delay measurements on motions between avatar and a controller. The paired T-Test results on angular measurements showed that 2D and 3D angular measurements can be substitutable depended on the positions between Kinect and a controller. Furthermore, comparisons by ratio between the time difference showed varies due to the motion factors, the number of vertices and the distance between Kinect and the controller.},
  annotation   = {Paper released in 2014 and has no citations},
}

@Article{Cao2009,
  author       = {gang Cao, Guo and min Luo, Li},
  year         = {2009},
  journal       = {Zhongguo yi liao qi xie za zhi = Chinese journal of medical instrumentation},
  title        = {{[Non-rigid medical image registration based on mutual information and thin-plate spline].}},
  url          = {http://www.mendeley.com/research/nonrigid-medical-image-registration-based-mutual-information-thinplate-spline},
  abstract     = {To get precise and complete details, the contrast in different images is needed in medical diagnosis and computer assisted treatment. The image registration is the basis of contrast, but the regular rigid registration does not satisfy the clinic requirements. A non-rigid medical image registration method based on mutual information and thin-plate spline was present. Firstly, registering two images globally based on mutual information; secondly, dividing reference image and global-registered image into blocks and registering them; then getting the thin-plate spline transformation according to the shift of blocks' center; finally, applying the transformation to the global-registered image. The results show that the method is more precise than the global rigid registration based on mutual information and it reduces the complexity of getting control points and satisfy the clinic requirements better by getting control points of the thin-plate transformation automatically.},
}

@Article{Dey2018,
  author       = {Dey, Arindam and Billinghurst, Mark and Lindeman, Robert W and Swan, J Edward},
  year         = {2018},
  journal       = {Frontiers in Robotics and AI},
  title        = {{A Systematic Review of 10 Years of Augmented Reality Usability Studies: 2005 to 2014.(Report)}},
  doi          = {10.3389/frobt.2018.00037},
  issn         = {2296-9144},
  volume       = {5},
  annotation   = {There is a more modern version of this paper I just wanted this one close because it is referenced by its newer version a lot and I thought that it could be handy},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey et al. - 2018 - A Systematic Review of 10 Years of Augmented Reality Usability Studies 2005 to 2014.(Report).pdf:pdf},
  keywords     = {Augmented Reality – Methods,Interfaces (Computers) – Methods},
}

@InProceedings{Collins2013,
  author    = {Collins, Toby and Pizarro, Daniel and Bartoli, Adrien and Canis, Michel and Bourdel, Nicolas},
  booktitle = {Augmented Reality Environments for Medical Imaging and Computer-Assisted Interventions},
  year      = {2013},
  title     = {{Realtime Wide-Baseline Registration of the Uterus in Laparoscopic Videos Using Multiple Texture Maps}},
  editor    = {Liao, Hongen and Linte, Cristian A and Masamune, Ken and Peters, Terry M and Zheng, Guoyan},
  isbn      = {978-3-642-40843-4},
  location  = {Berlin, Heidelberg},
  pages     = {162--171},
  publisher = {Springer Berlin Heidelberg},
  url       = {https://link.springer.com/chapter/10.1007/978-3-642-40843-4_18},
  abstract  = {We present a way to register the uterus in monocular laparoscopy in realtime using a novel two-phase approach. This differs significantly to SLAM, which is currently the leading approach for registration in MIS when scenes are approximately rigid. In the first phase we construct a 3D model of the uterus using dense SfM. This involves a method for semi-automatically masking the uterus from background structures in a set of reference frames, which we call Mask Bootstrapping from Motion (MBM). In the second phase the 3D model is registered to the live laparoscopic video using a novel wide-baseline approach that uses many texture maps to capture the real changes in appearance of the uterus. Capturing these changes means that registration can be performed reliably without needing temporal priors, which are needed in SLAM. This simplifies registration and leads to far fewer tuning parameters. We show that our approach significantly outperforms SLAM on an in vivo dataset comprising three human uteri.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collins et al. - 2013 - Realtime Wide-Baseline Registration of the Uterus in Laparoscopic Videos Using Multiple Texture Maps.pdf:pdf},
}

@InProceedings{Haouchine2014,
  author    = {Haouchine, N and Dequidt, J and Berger, M and Cotin, S},
  booktitle = {2014 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)},
  year      = {2014},
  title     = {{Single view augmentation of 3D elastic objects}},
  doi       = {10.1109/ISMAR.2014.6948432},
  isbn      = {VO -},
  pages     = {229--236},
  url       = {https://ieeexplore.ieee.org/document/6948432},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haouchine et al. - 2014 - Single view augmentation of 3D elastic objects.pdf:pdf},
  keywords  = {3D elastic object,3D shape recovery,Artificial,Computational Geometry and Object Modeling \&#x2014,Computational modeling,Deformable models,H.5.1 [Information Interfaces and Presentation]: M,I.3.5 [Computer Graphics],Physically based modeling,Shape,Strain,Three-dimensional displays,Young modulus,Young's modulus,augmented,deformation property,elastic deformation,energy minimization problem,image reconstruction,image sequences,minimally invasive liver surgery,minimisation,monocular video sequence,shading constraint,single view augmentation,video signal processing,virtual realities},
}

@Article{Baumeister2017,
  author       = {Baumeister, James and Ssin, Seung Youb and ElSayed, Neven A M and Dorrian, Jillian and Webb, David P and Walsh, James A and Simon, Timothy M and Irlitti, Andrew and Smith, Ross T and Kohler, Mark and Thomas, Bruce H},
  year         = {2017},
  journal       = {IEEE transactions on visualization and computer graphics},
  title        = {{Cognitive cost of using augmented reality displays}},
  doi          = {10.1109/TVCG.2017.2735098},
  issn         = {1077-2626},
  pages        = {2378--2388},
  url          = {https://ieeexplore-ieee-org.access.library.unisa.edu.au/document/8007333},
  volume       = {23},
  abstract     = {This paper presents the results of two cognitive load studies comparing three augmented reality display technologies: spatial augmented reality, the optical see-through Microsoft HoloLens, and the video see-through Samsung Gear VR. In particular, the two experiments focused on isolating the cognitive load cost of receiving instructions for a button-pressing procedural task. The studies employed a self-assessment cognitive load methodology, as well as an additional dual-task cognitive load methodology. The results showed that spatial augmented reality led to increased performance and reduced cognitive load. Additionally, it was discovered that a limited field of view can introduce increased cognitive load requirements. The findings suggest that some of the inherent restrictions of head-mounted displays materialize as increased user cognitive load.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Baumeister et al. - 2017 - Cognitive cost of using augmented reality displays.pdf:pdf},
  location     = {US},
  publisher    = {IEEE},
}

@InProceedings{Kim2011,
  author     = {Kim, Hark-Joon and Jeong, Kyung-Ho and Kim, Seon-Kyo and Han, Tack-Don},
  booktitle  = {SIGGRAPH Asia 2011 Sketches on - SA '11},
  year       = {2011},
  title      = {{Ambient Wall}},
  doi        = {10.1145/2077378.2077380},
  isbn       = {9781450311380},
  location   = {New York, New York, USA},
  pages      = {1},
  publisher  = {ACM Press},
  url        = {http://dl.acm.org/citation.cfm?doid=2077378.2077380},
  abstract   = {The residential environment is getting smarter, and home appliances have become connected each other automatically. Accordingly, user interface also has become increasingly complicated. In this paper, we found the main causes of making the users uncomfortable at home by task analysis, and then suggest a new interactive system suitable to the smart home. Ambient Wall interface enables users to monitor what's happening in their house at a glance, and control their surroundings by simple gesture without any physical interface device.},
  annotation = {Related Persons: Pan, Zhigeng Talks about creating a projection based inferface for smart homes. this paper only goes in to a small amount of detail to how the conept will be done It is quickly noted that guestures will for this device will need to be unique. Some example suggestions are given All and all its a short paper I belive they have made a prototype, although its not that interestig atm},
  keywords   = {Ambient Intelligence,Engineering,Gesture Interface,Wall Display},
}

@Article{Boonbrahm2015,
  author       = {Boonbrahm, Poonpong and Sewata, Lanjkorn and Boonbrahm, Salin},
  year         = {2015},
  journal       = {Procedia Computer Science},
  title        = {{Transforming 2D Human Data into 3D Model for Augmented Reality Applications}},
  doi          = {10.1016/j.procs.2015.12.193},
  issn         = {1877-0509},
  number       = {C},
  pages        = {28--33},
  volume       = {75},
  abstract     = {Microsoft's Kinect has been used in many augmented reality applications and most of them for interactive purpose. But since the Kinect is a device with depth sensing technology, it has been used for capturing the figure both 2D and 3D as well. In this research, we would like to create a 3D model or avatar which has the shape just like the actual human. Dealing with 3D avatar requires less computer resource than actual 3D model getting from 3D scanner. The purpose of this work is to transform the 2D measured shape of the human body into the standard 3D avatar. The new Kinect2 for windows with Depth Frame and Skeleton Frame function was used for body measurement. Using T-pose and A-pose position, measurement of the width of parts of the body at each joint (Kinect joint: i.e. Shoulder Center Joint, Spine Joint, Hip Center Joint, Hip Left Joint, Knee Left Joint, Ankle Left Joint, Foot left Joint and etc.) can be done easily. Since both sides of the body were measured (T-pose and A-pose), the circumference at each node can be calculated. In order to transform these data into 3D Model that representing human body, a standard model of 3D avatar was created. The size of the human part (measured at the node) was compared with the standard 3D model. If the measured size is larger than the standard one, then we had to scale the size of the standard model up but if the measured size is smaller, then we have to scale down. To make a perfect 3D model, technique called “Soft Selection” was used. Soft Selection is the technique for gradually scaling each node smoothly to match the second node. If we scale without using this technique, then there will be a bump occurred from one node to other. The 3D avatar model created this way will have the same shape as the human body it represented and can be used in many applications such as virtual fitting room, augmented reality games and etc.},
  annotation   = {This paper is light on notes but contains a detailed way to create a humann 3D model using the Kinect 2. Some of this information may be useful for detecting human bodies.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boonbrahm, Sewata, Boonbrahm - 2015 - Transforming 2D Human Data into 3D Model for Augmented Reality Applications.pdf:pdf},
  keywords     = {2d Human Data,3d Model,Edge Detection,Kinect,Soft Selection},
}

@Article{ContrerasLopez2019,
  author       = {{Contreras L{\'{o}}pez}, William Omar and Navarro, Paula Alejandra and Crispin, Santiago},
  year         = {2019},
  journal       = {Clinical Neurology and Neurosurgery},
  title        = {{Intraoperative clinical application of augmented reality in neurosurgery: A systematic review}},
  doi          = {10.1016/j.clineuro.2018.11.018},
  issn         = {0303-8467},
  pages        = {6--11},
  url          = {https://www.sciencedirect.com/science/article/pii/S0303846718304517?via\%3Dihub},
  volume       = {177},
  abstract     = {•Augmented reality use in surgical fields is becoming more common.•Augmented reality is mostly used for tumors and aneurysms.•Augmented reality helps the neurosurgeon keep their eyes still on the site.•Augmented reality seems to improve safety and outcomes. The interest and potential use of augmented reality (AR) in several medical fields since the early 90′s has increased consistently. It provides intraoperative guidance for surgical procedures by rendering visible what cannot be seen directly, possibly affecting surgical outcomes. Our objective was to conduct a systematic review of the intraoperative clinical application of augmented reality in neurosurgery, in studies published during the last five years. We carried out an electronic search in the PUBMED database using the terms “Augmented Reality” and “Neurosurgery.” After exclusions, 12 published articles that evaluated the utility of intraoperative clinical applications in surgical settings were included in our review. The results evaluated involved AR technique and visualization, time, complications, projection error, and located structures. We can conclude that the neurovascular application is the most frequent type of use for AR in neurosurgery (47.3\%), followed by applications in neuro-oncological pathologies (46.7\%), and non-vascular and non-neoplasic lesions (5.9\%). The use of AR also allows a surgeon to maintain their view on the operative site permanently, and is useful for locating structures, guiding resections, and planning the craniotomy with more precision, decreasing the risk of injury. The intraoperative application of an augmented reality system helps to improve the quality and characteristics of the surgical field image. The injection of 3D images with AR allows for the successful integration of images in vascular, oncological and other lesions without the need of look away from the surgical field, improving safety, surgical experience, or clinical outcome. However, comparative studies are still required to determine its effectiveness.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Contreras L{\'{o}}pez, Navarro, Crispin - 2019 - Intraoperative clinical application of augmented reality in neurosurgery A systematic review.pdf:pdf},
  keywords     = {Augmented Reality,Head-Mounted Display,Intraoperative Application,Neurosurgery,Neurosurgical Procedures},
}

@Article{Negied2015,
  author       = {Negied, Nermin K and Hemayed, Elsayed E and Fayek, Magda B},
  year         = {2015},
  journal       = {Journal of Electrical Systems and Information Technology},
  title        = {{Pedestrians' detection in thermal bands – Critical survey}},
  doi          = {https://doi.org/10.1016/j.jesit.2015.06.002},
  issn         = {2314-7172},
  number       = {2},
  pages        = {141--148},
  url          = {http://www.sciencedirect.com/science/article/pii/S2314717215000343},
  volume       = {2},
  abstract     = {Thermal imaging is simply the technique of using the heat given off by an object to produce an image of it or locate it. New thermal imaging frameworks for detection, segmentation and unique feature extraction and similarity measurements for human physiological biometrics recognition have been introduced in literature. The research investigates specialized algorithms that would use the individual's heat signature for human detection, crowd counting and applications that take benefits of this new technology. The highly accurate results obtained by the algorithms presented clearly demonstrate the ability of the thermal infrared systems to extend in application to other thermal imaging based systems.},
  annotation   = {This artical is a good overveiw on AR technology at the given time.},
  keywords     = {Crowd counting,Infrared bands,Near IR imaging,Pedestrians detection,Thermal signature,Thermography},
}

@Article{Souzaki2013,
  author       = {Souzaki, Ryota and Ieiri, Satoshi and Uemura, Munenori and Ohuchida, Kenoki and Tomikawa, Morimasa and Kinoshita, Yoshiaki and Koga, Yuhki and Suminoe, Aiko and Kohashi, Kenichi and Oda, Yoshinao and Hara, Toshiro and Hashizume, Makoto and Taguchi, Tomoaki},
  year         = {2013},
  journal       = {Journal of Pediatric Surgery},
  title        = {{An augmented reality navigation system for pediatric oncologic surgery based on preoperative CT and MRI images}},
  doi          = {10.1016/j.jpedsurg.2013.08.025},
  issn         = {0022-3468},
  number       = {12},
  pages        = {2479--2483},
  volume       = {48},
  abstract     = {PurposeIn pediatric endoscopic surgery, a limited view and lack of tactile sensation restrict the surgeon's abilities. Moreover, in pediatric oncology, it is sometimes difficult to detect and resect tumors due to the adhesion and degeneration of tumors treated with multimodality therapies. We developed an augmented reality (AR) navigation system based on preoperative CT and MRI imaging for use in endoscopic surgery for pediatric tumors. MethodsThe patients preoperatively underwent either CT or MRI with body surface markers. We used an optical tracking system to register the reconstructed 3D images obtained from the CT and MRI data and body surface markers during surgery. AR visualization was superimposed with the 3D images projected onto captured live images. Six patients underwent surgery using this system. ResultsThe median age of the patients was 3.5years. Two of the six patients underwent laparoscopic surgery, two patients underwent thoracoscopic surgery, and two patients underwent laparotomy using this system. The indications for surgery were local recurrence of a Wilms tumor in one case, metastasis of rhabdomyosarcoma in one case, undifferentiated sarcoma in one case, bronchogenic cysts in two cases, and hepatoblastoma in one case. The average tumor size was 22.0±14.2mm. Four patients were treated with chemotherapy, three patients were treated with radiotherapy before surgery, and four patients underwent reoperation. All six tumors were detected using the AR navigation system and successfully resected without any complications. ConclusionsThe AR navigation system is very useful for detecting the tumor location during pediatric surgery, especially for endoscopic surgery.},
  annotation   = {Provides some use case data for AR in sergery and some human traking. This also points out some issues reagarding AR tech atm. It was noted in this that the childs dimentions needed to be added. Automaticly doing this is important. Also it should be noted that children wont have to move around during surgery.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Souzaki et al. - 2013 - An augmented reality navigation system for pediatric oncologic surgery based on preoperative CT and MRI images.pdf:pdf},
  keywords     = {Augmented Reality,Image-Guided Surgery,Laparoscopic Surgery},
}

@Article{Moro2017,
  author       = {Moro, Christian and {\v{S}}tromberga, Zane and Raikos, Athanasios and Stirling, Allan},
  year         = {2017},
  journal       = {Anatomical Sciences Education},
  title        = {{The Effectiveness of Virtual and Augmented Reality in Health Sciences and Medical Anatomy}},
  doi          = {10.1002/ase.1696},
  issn         = {1935-9772},
  number       = {6},
  pages        = {549--559},
  volume       = {10},
  abstract     = {Although cadavers constitute the gold standard for teaching anatomy to medical and health science students, there are substantial financial, ethical, and supervisory constraints on their use. In addition, although anatomy remains one of the fundamental areas of medical education, universities have decreased the hours allocated to teaching gross anatomy in favor of applied clinical work. The release of virtual (VR) and augmented reality (AR) devices allows learning to occur through hands-on immersive experiences. The aim of this research was to assess whether learning structural anatomy utilizing VR or AR is as effective as tablet-based (TB) applications, and whether these modes allowed enhanced student learning, engagement and performance. Participants (n = 59) were randomly allocated to one of the three learning modes: VR, AR, or TB and completed a lesson on skull anatomy, after which they completed an anatomical knowledge assessment. Student perceptions of each learning mode and any adverse effects experienced were recorded. No significant differences were found between mean assessment scores in VR, AR, or TB. During the lessons however, VR participants were more likely to exhibit adverse effects such as headaches (25\% in VR P < 0.05), dizziness (40\% in VR, P < 0.001), or blurred vision (35\% in VR, P < 0.01). Both VR and AR are as valuable for teaching anatomy as tablet devices, but also promote intrinsic benefits such as increased learner immersion and engagement. These outcomes show great promise for the effective use of virtual and augmented reality as means to supplement lesson content in anatomical education.},
  annotation   = {This paper in general convers information about using AR and VR to help teach anatomy and other related feilds. While the information in this paper is very high level it show cases a use case for these tools in AR and VR More reading should be done if the above notes look useful.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moro et al. - 2017 - The Effectiveness of Virtual and Augmented Reality in Health Sciences and Medical Anatomy.pdf:pdf},
  keywords     = {Anatomy,Computer Assisted Instruction,Computer Simulation,Handheld Devices,Health Sciences,Human Body,Learner Engagement,Medical Education,Outcomes Of Education,Science Instruction,Science Tests,Scores,Student Attitudes,Teaching Methods},
}

@Article{Henderson2010,
  author       = {Henderson, S and Feiner, S},
  year         = {2010},
  journal       = {IEEE Transactions on Visualization and Computer Graphics},
  title        = {{Opportunistic Tangible User Interfaces for Augmented Reality}},
  doi          = {10.1109/TVCG.2009.91},
  issn         = {1077-2626},
  number       = {1},
  pages        = {4--16},
  volume       = {16},
  abstract     = {Opportunistic Controls are a class of user interaction techniques that we have developed for augmented reality (AR) applications to support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. By leveraging characteristics of these affordances to provide passive haptics that ease gesture input, Opportunistic Controls simplify gesture recognition, and provide tangible feedback to the user. In this approach, 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons can be mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of two user studies. In the first, participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique. In the second, participants proposed and demonstrated user interfaces incorporating Opportunistic Controls for two domains, allowing us to gain additional insights into how user interfaces featuring Opportunistic Controls might be designed.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Henderson, Feiner - 2010 - Opportunistic Tangible User Interfaces for Augmented Reality.pdf:pdf},
  keywords     = {Augmented Reality,Character Recognition,Documentation,Engineering,Engines,Fasteners,Haptic I/O,Haptic Interfaces,Interaction Styles,Optical Control,Optical Design,Optical Feedback,User Interfaces,Virtual and Augmented Reality},
}

@Article{Barbieri2009,
  author       = {Barbieri, Filippo and Buonocore, Antimo and Volta, Riccardo Dalla and Gentilucci, Maurizio},
  year         = {2009},
  journal       = {Brain and Language},
  title        = {{How symbolic gestures and words interact with each other}},
  doi          = {10.1016/j.bandl.2009.01.002},
  issn         = {0093934X},
  number       = {1},
  pages        = {1--11},
  url          = {https://www.sciencedirect.com/science/article/pii/S0093934X09000091},
  volume       = {110},
  abstract     = {Previous repetitive Transcranial Magnetic Stimulation and neuroimaging studies showed that Broca's area is involved in the interaction between gestures and words. However, in these studies the nature of this interaction was not fully investigated; consequently, we addressed this issue in three behavioral experiments. When compared to the expression of one signal at a time, arm kinematics slowed down and voice parameters were amplified when congruent words plus gestures were simultaneously produced (experiment 1). When word and gesture were incongruent, arm kinematics did not change regardless of word category, whereas the gesture induced variation in vocal parameters of communicative and action words only (experiments 2 and 3). Data are discussed according to the hypothesis that integration between gesture and word occurs by transferring the social intention to interact directly with the interlocutor from the gesture to the word. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barbieri et al. - 2009 - How symbolic gestures and words interact with each other.pdf:pdf},
  keywords     = {Arm kinematics,Broca's area,Symbolic gestures,Voice spectra,Words},
}

@Article{Eren2015,
  author       = {Eren, A.L. and Burnett, Gary E. and Thompson, Simon and Harvey, Catherine and Skrypchuk, Lee},
  year         = {2015},
  journal       = {Contemporary Ergonomics and Human Factors 2015},
  title        = {{Identifying a set of gestures for in-car touch screens}},
  number       = {April},
  pages        = {454--461},
  abstract     = {{\textcopyright} 2015 Taylor \& Francis. Touch screens are commonplace in vehicles, but the distraction burden can be high. This paper presents a driving study aiming to identify and assess a set of task-independent gestures to be used as user-defined shortcuts on in-car touch screens. Ten common gestures were identified by users in a preliminary study and then assessed in a driving simulator study – where 12 participants drove two routes whilst undertaking each of the gestures on an in-vehicle touch screen. The effects of visual feedback from the screen whilst drawing gestures were observed. Measures were taken of eye movements, drawing accuracy and subjective ratings. A final set of four gestures (tick, roof, squiggle and triangle) was recommended for interaction with in-vehicle touch screens.},
  annotation   = {Not very relevent. The act of working out what gustures people want to do seems to be a theme. Also it might have some decent information about how I should go about conducting a study},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eren et al. - 2015 - Identifying a set of gestures for in-car touch screens.pdf:pdf},
  isbn         = {9781138028036},
}

@Article{Ramkumar2018,
  author       = {Ramkumar, V and Hoske, M T and Vavra, B},
  year         = {2018},
  journal       = {Control Engineering},
  title        = {{AR, VR help industry in 7 ways}},
  issn         = {00108049},
  number       = {8},
  pages        = {42--43},
  volume       = {65},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramkumar, Hoske, Vavra - 2018 - AR, VR help industry in 7 ways.pdf:pdf},
  keywords     = {Aerospace Industry,Aircraft,Augmented Reality,Automotive Components,Automotive Engineering,Automotive Glass,Automotive Industry,China,Costs,Design Engineering,Employees,Engineers,Factories,Internet of Things,Manufacturing,Newsletters,Product Design,Product Development,Product Quality,Software,Tablets,United States–Us,Virtual Prototyping,Virtual Reality,Workers},
}

@InProceedings{Vatavu2012,
  author     = {Vatavu, Radu-Daniel},
  booktitle  = {Proceedings of the 10th European conference on Interactive tv and video - EuroiTV '12},
  year       = {2012},
  title      = {{User-defined gestures for free-hand TV control}},
  doi        = {10.1145/2325616.2325626},
  isbn       = {9781450311076},
  abstract   = {As researchers and industry alike are proposing TV interfaces that use gestures in their designs, understanding users' preferences for gesture commands becomes an important problem. However, no rules or guidelines currently exist to assist designers and practitioners of such interfaces. The paper presents the results of the first study investigating users' preferences for free-hand gestures when controlling the TV set. By conducting an agreement analysis on user-elicited gestures, a set of gesture commands is proposed for basic TV control tasks. Also, guidelines and recommendations issued from observed user behavior are provided to assist practitioners interested in prototyping free-hand gestural designs for the interactive TV.},
  annotation = {Useful paper for detailing types of gestures that people would like to use. Looks into interacting with a TV what works what doesn't},
  file       = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vatavu - 2012 - User-defined gestures for free-hand TV control.pdf:pdf},
  issn       = {0067964X},
  pmid       = {732209},
}

@Article{MorrelSamuels1990,
  author       = {Morrel-Samuels, Palmer},
  year         = {1990},
  journal       = {International Journal of Man-Machine Studies},
  title        = {{Clarifying the distinction between lexical and gestural commands}},
  doi          = {10.1016/S0020-7373(05)80034-3},
  issn         = {00207373},
  abstract     = {A distinction is drawn between conventional lexical commands and gestural commands (e.g. circles, arrows, X'x, etc.). The distinction is discussed in the context of a central metaphor that likens computer use to communication between programmer and user. A number of limitations and benefits unique to gestural interfaces are described. It is suggested that gestural commands tend to be terse, common, unambiguous, iconic, and similar to the spontaneous hand gestures that accompany speech. The potential effects of these five qualities are outlined by summarizing selected research from cognitive and social psychology. Some potential applications are also described. {\textcopyright} 1990 Academic Press Limited.},
}

@InProceedings{Erazo2017,
  author     = {Erazo, Orlando and Rekik, Yosra and Grisoni, Laurent and Pino, Jos{\'{e}} A.},
  booktitle  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year       = {2017},
  title      = {{Understanding Gesture Articulations Variability}},
  doi        = {10.1007/978-3-319-67684-5_18},
  isbn       = {9783319676838},
  abstract   = {{\textcopyright} 2017, IFIP International Federation for Information Processing. Interfaces based on mid-air gestures often use a one-to-one mapping between gestures and commands, but most remain very basic. Actually, people exhibit inherent intrinsic variations for their gesture articulations because gestures carry dependency with both the person producing them and the specific context, social or cultural, in which they are being produced. We advocate that allowing applications to map many gestures to one command is a key step to give more flexibility, avoid penalizations, and lead to better user interaction experiences. Accordingly, this paper presents our results on mid-air gesture variability. We are mainly concerned with understanding variability in mid-air gesture articulations from a pure user-centric perspective. We describe a comprehensive investigation on how users vary the production of gestures under unconstrained articulation conditions. The conducted user study consisted in two tasks. The first one provides a model of user conception and production of gestures; from this study we also derive an embodied taxonomy of gestures. This taxonomy is used as a basis for the second experiment, in which we perform a fine grain quantitative analysis of gesture articulation variability. Based on these results, we discuss implications for gesture interface designs.},
  annotation = {Links to a book not the conference paper you can see the paper via the website though https://link.springer.com/chapter/10.1007/978-3-319-67684-5_18#enumeration},
  keywords   = {Gesture articulation,Gesture taxonomy,Gesture variability,Mid-air gestures,Whole body gestures},
}

@Article{Wolf1987,
  author       = {Wolf, Catherine G. and Morrel-Samuels, Palmer},
  year         = {1987},
  journal       = {International Journal of Man-Machine Studies},
  title        = {{The use of hand-drawn gestures for text editing}},
  doi          = {10.1016/S0020-7373(87)80045-7},
  issn         = {00207373},
  abstract     = {This paper reports results from a paper and pencil study of the use of hand-drawn gestures for simple editing tasks. The use of gesture is of particular interest in an interface which allows the user to write directly on the surface of a display with a stylus. The results of the study provided encouragement for the development of gesture-driven user interfaces. There was very good intra-subject consistency in the spatial form of gestures used for an editing operation, and also, good agreement across subjects in the form selected for a particular operation. Subjects' reactions to the use of gesture indicated that gesture commands were perceived as easy to use and remember. Specific implications for the design gestural interfaces are discussed. {\textcopyright} 1987, Academic Press Limited. All rights reserved.},
}

@InProceedings{Manitsaris2015,
  author    = {Manitsaris, Sotiris and Guettier, Christophe and Taralle, Florent and Paljic, Alexis and Grenier, Jordane},
  booktitle = {Extended Abstracts of the ACM CHI'15 Conference on Human Factors in Computing Systems},
  year      = {2015},
  title     = {{A Consensual and Non-ambiguous Set of Gestures to Interact with UAV in Infantrymen}},
  doi       = {10.1145/2702613.2702971},
  isbn      = {9781450331463},
  pages     = {797--803},
  abstract  = {In the context of using an Unmanned Aerial Vehicle (UAV) in hostile environments, gestures allow to free the operator of bulky control interfaces. Since a navigation plan is defined before the mission, only a few commands have to be activated during the mission. This allows a gestural symbolic interaction that maps commands to a set of gestures. Nevertheless, as gestures are not universal, this asks the question of choosing the proper gestures that are easy to learn memorize and perform. We propose a four step methodology for eliciting a gestural vocabulary, and apply it to this use case. The methodology consists of 4 steps: (1) collecting gestures through user creativity sessions, (2) extracting candidate gestures to build a catalogue, (3) electing the gesture vocabulary and (3) evaluating the non-ambiguity of it. We then discuss the relevance of the GV.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manitsaris et al. - 2015 - A Consensual and Non-ambiguous Set of Gestures to Interact with UAV in Infantrymen.pdf:pdf},
}

@Article{Jung2009,
  author       = {Jung, Younbo and Li, Wong and Gladys, Chieh and Lee, Kwan Min},
  year         = {2009},
  journal       = {Proceedings of the Sixth Australasian Conference on Interactive Entertainment},
  title        = {{Games for a Better Life : Effects of Playing Wii Games on the Well-Being of Seniors in a Long-Term Care Facility}},
  doi          = {10.1145/1746050.1746055},
  issn         = {00311766},
  pages        = {0--5},
  abstract     = {In the current study, we examined the impact of playing Nintendo Wii games on the psychological and physical well-being of seniors in a long-term care facility. A six week-long intervention was held in SASCO Senior Citizens' Home, a long-term care facility in Singapore. Forty five residents aged between 56 and 92 years old participated in the longitudinal field experiment. Results showed that playing Wii games had a positive impact on the overall well-being of the elderly, compared to a control group that played traditional board games. Implications for future applications of Wii in interventions for the elderly are discussed.},
  isbn         = {9781450300100},
  keywords     = {intervention,seniors,social games,video games,well-being},
}

@Article{Saposnik2010,
  author       = {Saposnik, G and Mamdani, M and Bayley, M and Thorpe, K E and Hall, J and Cohen, L G and Teasell, R},
  year         = {2010-02},
  journal       = {International Journal of Stroke},
  title        = {{Effectiveness of Virtual Reality Exercises in STroke Rehabilitation (EVREST): Rationale, Design, and Protocol of a Pilot Randomized Clinical Trial Assessing the Wii Gaming System}},
  doi          = {10.1111/j.1747-4949.2009.00404.x},
  issn         = {1747-4930},
  number       = {1},
  pages        = {47--51},
  url          = {https://doi.org/10.1111/j.1747-4949.2009.00404.x},
  volume       = {5},
  abstract     = {BackgroundEvidence suggests that increasing intensity of rehabilitation results in better motor recovery. Limited evidence is available on the effectiveness of an interactive virtual reality gaming system for stroke rehabilitation. EVREST was designed to evaluate feasibility, safety and efficacy of using the Nintendo Wii gaming virtual reality (VRWii) technology to improve arm recovery in stroke patients.MethodsPilot randomized study comparing, VRWii versus recreational therapy (RT) in patients receiving standard rehabilitation within six months of stroke with a motor deficit of ≥3 on the Chedoke-McMaster Scale (arm). In this study we expect to randomize 20 patients. All participants (age 18?85) will receive customary rehabilitative treatment consistent of a standardized protocol (eight sessions, 60 min each, over a two-week period).Outcome measuresThe primary feasibility outcome is the total time receiving the intervention. The primary safety outcome is the proportion of patients experiencing intervention-related adverse events during the study period. Efficacy, a secondary outcome measure, will be measured by the Wolf Motor Function Test, Box and Block Test, and Stroke Impact Scale at the four-week follow-up visit. From November, 2008 to September, 2009 21 patients were randomized to VRWii or RT. Mean age, 61 (range 41?83) years. Mean time from stroke onset 25 (range 10?56) days.ConclusionsEVREST is the first randomized parallel controlled trial assessing the feasibility, safety, and efficacy of virtual reality using Wii gaming technology in stroke rehabilitation. The results of this study will serve as the basis for a larger multicentre trial. ClinicalTrials.gov registration# NTC692523},
  annotation   = {doi: 10.1111/j.1747-4949.2009.00404.x Not hugely related to my work notes interestig things about stroke rehabitilion by using similar princibles},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saposnik et al. - 2010 - Effectiveness of Virtual Reality Exercises in STroke Rehabilitation (EVREST) Rationale, Design, and Protocol of.pdf:pdf},
  publisher    = {SAGE Publications},
}

@Article{Kahlbaugh2011,
  author       = {Kahlbaugh, Patricia E. and Sperandio, Amanda J. and Carlson, Ashley L. and Hauselt, Jerry},
  year         = {2011},
  journal       = {Activities, Adaptation and Aging},
  title        = {{Effects of Playing Wii on Well-Being in the Elderly: Physical Activity, Loneliness, and Mood}},
  doi          = {10.1080/01924788.2011.625218},
  issn         = {01924788},
  number       = {4},
  pages        = {331--344},
  volume       = {35},
  abstract     = {Effects of compensatory strategies offered by Wii technology on physical activity, loneliness, and mood are investigated. Thirty-five individuals (M = 82 years) were randomly assigned to either playing Wii or watching television with a partner for 10 weeks. Physical activity, loneliness, mood, life satisfaction, and health were assessed. The elderly playing Wii had lower loneliness and a pattern of greater positive mood compared to the television group. No differences in life satisfaction or physical activity were found, but loneliness predicted positive mood, and positive mood predicted physical activity. This investigation points to the benefits of using Wii for well-being, particularly social connection and enjoyment.},
  isbn         = {01924788},
  keywords     = {integrative game playing,loneliness,mood,older adult recreation,well-being},
  pmid         = {69733158},
}

@Article{Fernandes2014,
  author       = {Fernandes, H. and Filipe, Vitor and Costa, Paulo},
  year         = {2014},
  journal       = {Procedia Computer Science},
  title        = {{Location based Services for the Blind Supported by RFID Technology}},
  doi          = {10.1016/j.procs.2014.02.002},
  issn         = {18770509},
  pages        = {2--8},
  volume       = {27},
  abstract     = {Nowadays, navigation systems are widely used to find the correct path, or the quickest, between two places. These systems use the Global Positioning System (GPS) and only work well in outdoor environment since GPS signals cannot easily penetrate and/or are greatly degraded inside of buildings. Several technologies have been proposed to make navigation inside of buildings possible. One such technology is Radio-Frequency Identification (RFID). In the case of outside environments, some hybrid systems have been proposed that use GPS as main information source and RFID for corrections and location error minimization. In this article we propose a navigation system that uses RFID as the main technology to guide people with visual impairment in unfamiliar environments, both indoor and outdoor, complementing the traditional white cane and providing information about the user's geographical context.},
  annotation   = {Not relivent Used RFID chips in the ground to to workout were exactly blind people were and used thier phone to comunicate beter with them.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernandes, Filipe, Costa - 2014 - Location based Services for the Blind Supported by RFID Technology.pdf:pdf},
}

@Article{Carreira2017,
  author       = {Carreira, Micael and Ting, Karine Lan Hing and Csobanka, Petra and Gon{\c{c}}alves, Daniel},
  year         = {2017-08},
  journal       = {Universal Access in the Information Society},
  title        = {{Evaluation of in-air hand gestures interaction for older people}},
  doi          = {10.1007/s10209-016-0483-y},
  issn         = {1615-5289},
  number       = {3},
  pages        = {561--580},
  url          = {http://link.springer.com/10.1007/s10209-016-0483-y},
  volume       = {16},
}

@Article{Velleman2014,
  author       = {Velleman, Eric and van der Geest, Thea},
  year         = {2014},
  journal       = {Procedia Computer Science},
  title        = {{Online Test Tool to Determine the CEFR Reading Comprehension Level of Text}},
  doi          = {10.1016/j.procs.2014.02.039},
  issn         = {18770509},
  pages        = {350--358},
  volume       = {27},
  abstract     = {On the Common European Framework of Reference for Languages (CEFR) scale, the average reading comprehension level of the Dutch population is B1 and the average level of text provided by Dutch government organisations requires a considerably higher reading skills level (C1). This means that part of the population may have difficulty reading texts delivered to them by their own government. We built a simple and freely available online tool to give content editors an indication of the CEFR reading comprehension level of their texts. This paper describes the parameters of the tool and proposes a list of possible extensions to improve the quality and usability of the output.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Velleman, van der Geest - 2014 - Online Test Tool to Determine the CEFR Reading Comprehension Level of Text.pdf:pdf},
}

@Article{Punchoojit2017,
  author       = {Punchoojit, Lumpapun and Hongwarittorrn, Nuttanont},
  year         = {2017},
  journal       = {Advances in Human-Computer Interaction},
  title        = {{Usability Studies on Mobile User Interface Design Patterns: A Systematic Literature Review}},
  doi          = {10.1155/2017/6787504},
  issn         = {1687-5893},
  pages        = {1--22},
  url          = {https://www.hindawi.com/journals/ahci/2017/6787504/},
  volume       = {2017},
  abstract     = {Mobile platforms have called for attention from HCI practitioners, and, ever since 2007, touchscreens have completely changed mobile user interface and interaction design. Some notable differences between mobile devices and desktops include the lack of tactile feedback, ubiquity, limited screen size, small virtual keys, and high demand of visual attention. These differences have caused unprecedented challenges to users. Most of the mobile user interface designs are based on desktop paradigm, but the desktop designs do not fully fit the mobile context. Although mobile devices are becoming an indispensable part of daily lives, true standards for mobile UI design patterns do not exist. This article provides a systematic literature review of the existing studies on mobile UI design patterns. The first objective is to give an overview of recent studies on the mobile designs. The second objective is to provide an analysis on what topics or areas have insufficient information and what factors are concentrated upon. This article will benefit the HCI community in seeing an overview of present works, to shape the future research directions.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Punchoojit, Hongwarittorrn - 2017 - Usability Studies on Mobile User Interface Design Patterns A Systematic Literature Review.pdf:pdf},
}

@Article{Rodrigues2014,
  author       = {Rodrigues, {\'{E}}lvio and Carreira, Micael and Gon{\c{c}}alves, Daniel},
  year         = {2014-01},
  journal       = {Procedia Computer Science},
  title        = {{Developing a Multimodal Interface for the Elderly}},
  doi          = {10.1016/J.PROCS.2014.02.040},
  issn         = {1877-0509},
  pages        = {359--368},
  url          = {https://www.sciencedirect.com/science/article/pii/S1877050914000428?via\%3Dihub},
  volume       = {27},
  abstract     = {The elderly remain excluded from technology, since they regard traditional computer interfaces as overly technical and difficult to use. However, the older users consider other forms of interaction easier to use – like touch and gesture recognition interfaces. Regarding the touch interfaces, we focused on text-entry tasks and developed and tested 5 virtual QWERTY keyboard variants in order to improve text entry speed and accuracy on tablet devices. Preliminary user tests with young adults revealed that soft keyboards without visual changes remain the fastest method for text entry, and allowed us to rule out the least promising variants. Regarding gesture recognition, we developed regular gestures as well as alternative functionalities based on the motion sensing device: user and ambient sensing. These features allow to create a more intelligent system that reacts to the user and environment without explicit interaction. In the near future, we will perform tests for both interaction modalities with older adults.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodrigues, Carreira, Gon{\c{c}}alves - 2014 - Developing a Multimodal Interface for the Elderly.pdf:pdf},
  publisher    = {Elsevier},
}

@InProceedings{Matsumotot2001,
  author    = {Matsumotot, Y and Ino, T and Ogsawara, T},
  booktitle = {Proceedings 10th IEEE International Workshop on Robot and Human Interactive Communication. ROMAN 2001 (Cat. No.01TH8591)},
  year      = {2001},
  title     = {{Development of intelligent wheelchair system with face and gaze based interface}},
  doi       = {10.1109/ROMAN.2001.981912},
  isbn      = {VO -},
  pages     = {262--267},
  keywords  = {Aging,Humans,Information science,Intelligent robots,Intelligent sensors,Intelligent systems,Mobile robots,Navigation,Senior citizens,WATSON,Wheelchairs,aging society,computer vision,disabled people,electric wheelchairs,face based interface,face recognition,gaze based interface,gaze motion,handicapped aids,head motion,indoor environments,intelligent control,intelligent wheelchair system,intuitive interface,outdoor environments,senior people,speed control,steering control,stereo image processing,user interfaces},
}

@InProceedings{Kallam2017,
  author    = {Kallam, Rama Mohana Reddy and Sharma, Harish Kumar},
  booktitle = {Proceedings - 7th IEEE International Advanced Computing Conference, IACC 2017},
  year      = {2017},
  title     = {{Development of intelligent powerd wheelchair}},
  doi       = {10.1109/IACC.2017.0121},
  isbn      = {9781509015603},
  abstract  = {Independent mobility is an important aspect of self-esteem and plays a pivotal role in ”aging in place”. The smart wheelchair is an effort to provide an independent mobility to those persons who are unable to travel freely. A smart wheelchair is an amalgamation of a standard power driving modules and a collection of sensors with an interfacing section.Input control signal may be given by means of wireless joystick or touchpad that can be mounted on armrest also.This feature makes it useful for the disabled persons who are able to control the joystick and touchpad, and also it can be helpful for the caregiver like a nurse. Ultrasonic and infrared sensor systems are going to be integrated into this wheelchair for providing best possible and safe movement.},
  file      = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kallam, Sharma - 2017 - Development of intelligent powerd wheelchair.pdf:pdf},
  keywords  = {Disability,H-Bridge,Mobility,Motordriver,Smart},
}

@InProceedings{Matthews2019,
  author = {Matthews, Brandon and Thomas, Bruce and Itzstein, Stewart and Smith, Ross},
  year   = {2019},
  title  = {{Remapped Physical-Virtual Interfaces with Bimanual Haptic Retargeting}},
  doi    = {10.1109/VR.2019.8797974},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthews et al. - 2019 - Remapped Physical-Virtual Interfaces with Bimanual Haptic Retargeting.pdf:pdf},
}

@Article{Norouzi2019,
  author   = {Norouzi, Nahal and Bruder, Gerd and Belna, Brandon and Mutter, Stefanie and Turgut, Damla and Welch, Greg},
  year     = {2019},
  title    = {{A Systematic Review of the Convergence of Augmented Reality, Intelligent Virtual Agents, and the Internet of Things}},
  doi      = {10.1007/978-3-030-04110-6_1},
  pages    = {1--24},
  abstract = {In recent years we are beginning to see the convergence of three distinct research fields: augmented reality (AR), intelligent virtual agents (IVAs), and the Internet of things (IoT). Each of these has been classified as a disruptive technology for our society. Since their emergence, the advancement of knowledge and development of technologies and systems in these fields were traditionally performed with limited input from each other. However, over recent years, we have seen research prototypes and commercial products being developed that cross the boundaries between these distinct fields to leverage their collective strengths. In this paper, we review the body of literature published at the intersections between each two of these fields, and we discuss a vision for the nexus of all three technologies.},
  file     = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi et al. - 2019 - A Systematic Review of the Convergence of Augmented Reality, Intelligent Virtual Agents, and the Internet of Thi.pdf:pdf},
  isbn     = {9783030041106},
}

@Article{Phillips2021,
  author       = {Phillips, Nate and Khan, Farzana Alam and Kruse, Brady and Bethel, Cindy and Swan, J. Edward},
  year         = {2021},
  journal       = {Proceedings - 2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2021},
  title        = {{An X-ray vision system for situation awareness in action space}},
  doi          = {10.1109/VRW52623.2021.00179},
  pages        = {593--594},
  abstract     = {Usable x-ray vision has long been a goal in augmented reality research and development. X-ray vision, or the ability to view and understand information presented through an opaque barrier, would be imminently useful across a variety of domains. Unfortunately, however, the effect of x-ray vision on situation awareness, an operator's understanding of a task or environment, has not been significantly studied. This is an important question; if x-ray vision does not increase situation awareness, of what use is it? Thus, we have developed an x-ray vision system, in order to investigate situation awareness in the context of action space distances.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips et al. - 2021 - An X-ray vision system for situation awareness in action space.pdf:pdf},
  isbn         = {9780738113678},
  keywords     = {Augmented reality,Depth cues,Situation awareness,X-ray vision},
}

@Article{Schnabel2003,
  author       = {Schnabel, Marc A. and Kvan, Thomas},
  year         = {2003},
  journal       = {International Journal of Architectural Computing},
  title        = {{Spatial Understanding in Immersive Virtual Environments}},
  doi          = {10.1260/147807703773633455},
  issn         = {1478-0771},
  number       = {4},
  pages        = {435--448},
  volume       = {1},
  abstract     = {In this study, we examined the perception and understanding of spatial volumes within immersive and non-immersive virtual environments by comparison with representation using conventional media. We examined the relative effectiveness of these conditions in enabling the translation to a tangible representation, through a series of design experiments. Students experienced, assessed, and analysed spatial relationships of volumes and spaces and subsequently constructed real models of these spaces. The goal of our study is to identify how designers perceive space in Virtual Environments (VEs). We explore issues of quality, accuracy and understanding of reconstructing architectural space and forms. By comparison of the same spatial performance task undertaken within a Head Mounted Display, screen-based and real 2D environment, we are able to draw some conclusions about spatial understanding in immersive VE activity.},
  file         = {:D\:/Thomas/Downloads/Spatial_Understanding_in_Immersive_Virtual_Environ.pdf:pdf},
}

@Article{1965,
  year         = {1965},
  journal       = {Journal of the Mining Institute of Japan},
  title        = {サイクロン浮選に関する1, 2の予備実験: サイクロン浮選に関する研究 (第1報)},
  doi          = {10.2473/shigentosozai1953.81.922_235},
  issn         = {2185-6729},
  number       = {922},
  pages        = {235--236},
  volume       = {81},
  abstract     = {Прогенотипированы по гену ESR (PvuII-полиморфизм) свиноматки и хряки крупной белой, синтетического кросса alba и породы ландрас. Выявлена достоверно высокая частота гетерозигот во всех исследованных породных группах животных. Оценка влияния генотипа на продуктивность свидетельствует о том, что аллель В гена ESR дает преимущества его носителям по репродуктивным качествам (количество спермиев в эякуляте, масса гнезда при рождении, процент мертворожденных поросят), а аллель А улучшает откормочные показатели, что свидетельствует о хозяйственной ценности обоих генотипов. Таким образом, использование гетерозигот АВ может существенно повысить репродуктивные и откормочные качества свиней.},
  file         = {:D\:/Thomas/Downloads/BarthLesserTaggartSlusser2015_Preprint.pdf:pdf},
  keywords     = {bayesian,estimation,proportional reasoning,spatial cognition},
}

@Article{Matthews2018,
  author = {Matthews, Brandon},
  year   = {2018},
  title  = {{Bimanual Retargeting for Reconfigurable Haptic Interaction in Virtual Reality}},
  number = {July},
  file   = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthews - 2018 - Bimanual Retargeting for Reconfigurable Haptic Interaction in Virtual Reality.pdf:pdf},
}

@Article{Viola2004,
  author       = {Viola, Ivan and Armin, Kanistar and Groller, Eduard},
  year         = {2004},
  journal       = {IEEE Visualization},
  title        = {{Importance-Driven Volume Rendering}},
  file         = {:D\:/Thomas/Downloads/10.1.1.10.2971.pdf:pdf},
  keywords     = {context techniques,focus,ing,level-of-detail techniques,non-,photorealistic techniques,view-dependent visualization,volume render-},
}

@Misc{JunYoungChoi2018,
  author    = {{JunYoung Choi} and {Hae Jin Jeong} and {Jeong Wonki}},
  year      = {2018},
  title     = {{Improvement Depth Perception of Volume Rendering using Virtual Reality}},
  doi       = {10.15701/kcgs.2018.24.2.29},
  booktitle = {Journal of the Korea Computer Graphics Society},
  file      = {:D\:/Thomas/Downloads/Improvement Depth Perception of Volume Rendering using Virtual Reality.pdf:pdf},
  issn      = {1975-7883},
  number    = {2},
  pages     = {29--40},
  volume    = {24},
}

@Article{Drouin2018,
  author       = {Drouin, Simon and Collins, D. Louis},
  year         = {2018},
  journal       = {PLoS ONE},
  title        = {{PRISM: An open source framework for the interactive design of GPU volume rendering shaders}},
  doi          = {10.1371/journal.pone.0193636},
  issn         = {19326203},
  number       = {3},
  pages        = {1--22},
  volume       = {13},
  abstract     = {Direct volume rendering has become an essential tool to explore and analyse 3D medical images. Despite several advances in the field, it remains a challenge to produce an image that highlights the anatomy of interest, avoids occlusion of important structures, provides an intuitive perception of shape and depth while retaining sufficient contextual information. Although the computer graphics community has proposed several solutions to address specific visualization problems, the medical imaging community still lacks a general volume rendering implementation that can address a wide variety of visualization use cases while avoiding complexity. In this paper, we propose a new open source framework called the Programmable Ray Integration Shading Model, or PRISM, that implements a complete GPU ray-casting solution where critical parts of the ray integration algorithm can be replaced to produce new volume rendering effects. A graphical user interface allows clinical users to easily experiment with pre-existing rendering effect building blocks drawn from an open database. For programmers, the interface enables real-time editing of the code inside the blocks. We show that in its default mode, the PRISM framework produces images very similar to those produced by a widely-adopted direct volume rendering implementation in VTK at comparable frame rates. More importantly, we demonstrate the flexibility of the framework by showing how several volume rendering techniques can be implemented in PRISM with no more than a few lines of code. Finally, we demonstrate the simplicity of our system in a usability study with 5 medical imaging expert subjects who have none or little experience with volume rendering. The PRISM framework has the potential to greatly accelerate development of volume rendering for medical applications by promoting sharing and enabling faster development iterations and easier collaboration between engineers and clinical personnel.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Drouin, Collins - 2018 - PRISM An open source framework for the interactive design of GPU volume rendering shaders.pdf:pdf},
  isbn         = {1111111111},
  pmid         = {29534069},
}

@Article{Singh2002,
  author       = {Singh, Manish and Anderson, Barton L.},
  year         = {2002},
  journal       = {Psychological Review},
  title        = {{Toward a perceptual theory of transparency}},
  doi          = {10.1037/0033-295X.109.3.492},
  issn         = {0033295X},
  number       = {3},
  pages        = {492--519},
  volume       = {109},
  abstract     = {Theories of perceptual transparency have typically been developed within the context of a physical model that generates the percept of transparency (F. Metelli's episcotister model, 1974b). Here 2 fundamental questions are investigated: (a) When does the visual system initiate the percept of one surface seen through another? (b) How does it assign surface properties to a transparent layer? Results reveal systematic deviations from the predictions of Metelli's model, both for initiating image decomposition into multiple surfaces and for assigning surface attributes. Specifically, results demonstrate that the visual system uses Michelson contrast as a critical image variable to initiate percepts of transparency and to assign transmittance to transparent surfaces. Findings are discussed in relation to previous theories of transparency, lightness, brightness, and contrast-contrast.},
  file         = {:D\:/Thomas/Downloads/Toward a Perceptual Theory of Transparency.pdf:pdf},
  pmid         = {12088242},
}

@Article{Noguera2012,
  author       = {Noguera, Jos{\'{e}} M. and Jim{\'{e}}nez, Juan Roberto and Og{\'{a}}yar, Carlos J. and Segura, Rafael J.},
  year         = {2012},
  journal       = {GRAPP 2012 IVAPP 2012 - Proceedings of the International Conference on Computer Graphics Theory and Applications and International Conference on Information Visualization Theory and Applications},
  title        = {{Volume rendering strategies on mobile devices}},
  doi          = {10.5220/0003848604470452},
  number       = {August},
  pages        = {447--452},
  abstract     = {This paper proposes and compares several methods for interactive volume rendering in mobile devices. This kind of devices has several restrictions and limitations both in performance and in storage capacity. The paper reviews the suitability of some existing direct volume rendering methods, and proposes a novel approach that takes advantage of the graphics capabilities of modern OpenGL ES 2.0 enabled devices. Several experiments have been carried out to test the behaviour of the described method.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Noguera et al. - 2012 - Volume rendering strategies on mobile devices.pdf:pdf},
  isbn         = {9789898565020},
  keywords     = {GPU,Interactive frame rates,Mobile devices,Volume rendering},
}

@Article{Holub2017,
  author       = {Holub, Joseph and Winer, Eliot},
  year         = {2017},
  journal       = {Journal of Digital Imaging},
  title        = {{Enabling Real-Time Volume Rendering of Functional Magnetic Resonance Imaging on an iOS Device}},
  doi          = {10.1007/s10278-017-9986-1},
  issn         = {1618727X},
  number       = {6},
  pages        = {738--750},
  volume       = {30},
  abstract     = {Powerful non-invasive imaging technologies like computed tomography (CT), ultrasound, and magnetic resonance imaging (MRI) are used daily by medical professionals to diagnose and treat patients. While 2D slice viewers have long been the standard, many tools allowing 3D representations of digital medical data are now available. The newest imaging advancement, functional MRI (fMRI) technology, has changed medical imaging from viewing static to dynamic physiology (4D) over time, particularly to study brain activity. Add this to the rapid adoption of mobile devices for everyday work and the need to visualize fMRI data on tablets or smartphones arises. However, there are few mobile tools available to visualize 3D MRI data, let alone 4D fMRI data. Building volume rendering tools on mobile devices to visualize 3D and 4D medical data is challenging given the limited computational power of the devices. This paper describes research that explored the feasibility of performing real-time 3D and 4D volume raycasting on a tablet device. The prototype application was tested on a 9.7” iPad Pro using two different fMRI datasets of brain activity. The results show that mobile raycasting is able to achieve between 20 and 40 frames per second for traditional 3D datasets, depending on the sampling interval, and up to 9 frames per second for 4D data. While the prototype application did not always achieve true real-time interaction, these results clearly demonstrated that visualizing 3D and 4D digital medical data is feasible with a properly constructed software framework.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holub, Winer - 2017 - Enabling Real-Time Volume Rendering of Functional Magnetic Resonance Imaging on an iOS Device.pdf:pdf},
  keywords     = {Computer graphics,Functional imaging,Volume rendering},
  pmid         = {28585063},
  publisher    = {Journal of Digital Imaging},
}

@Article{Fong2017,
  author       = {Fong, Julian and Wrenninge, Magnus and Kulla, Christopher and Habel, Ralf},
  year         = {2017},
  journal       = {ACM SIGGRAPH 2017 Courses, SIGGRAPH 2017},
  title        = {{Production volume rendering SIGGRAPH 2017 course}},
  doi          = {10.1145/3084873.3084907},
  abstract     = {This document might be out of date, please check online for an updated version. With significant advances in techniques, along with increasing computational power, path tracing has now become the predominant rendering method used in movie production. Thanks to these advances, volume rendering can now take full advantage of the path tracing revolution, allowing the creation of photoreal images that would not have been feasible only a few years ago. However, volume rendering also provides its own set of unique challenges that can be daunting to path tracer developers and researchers accustomed to dealing only with surfaces. While recent texts and materials have covered some of these challenges, to the best of our knowledge none have comprehensively done so, especially when confronted with the complexity and scale demands required by production. For example, the last volume rendering course at SIGGRAPH in 2011 discussed ray marching and precomputed lighting and shadowing, none of which are techniques advisable for production purposes in 2017.},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fong et al. - 2017 - Production volume rendering SIGGRAPH 2017 course.pdf:pdf},
  isbn         = {9781450350143},
}

@Article{Dhamo2019,
  author       = {Dhamo, Helisa and Tateno, Keisuke and Laina, Iro and Navab, Nassir and Tombari, Federico},
  year         = {2019},
  journal       = {Pattern Recognition Letters},
  title        = {{Peeking behind objects: Layered depth prediction from a single image}},
  doi          = {10.1016/j.patrec.2019.05.007},
  eprint       = {1807.08776},
  eprinttype   = {arXiv},
  issn         = {01678655},
  pages        = {333--340},
  url          = {https://doi.org/10.1016/j.patrec.2019.05.007},
  volume       = {125},
  abstract     = {While conventional depth estimation can infer the geometry of a scene from a single RGB image, it fails to estimate scene regions that are occluded by foreground objects. This limits the use of depth prediction in augmented and virtual reality applications, that aim at scene exploration by synthesizing the scene from a different vantage point, or at diminished reality. To address this issue, we shift the focus from conventional depth map prediction to the regression of a specific data representation called Layered Depth Image (LDI), which contains information about the occluded regions in the reference frame and can fill in occlusion gaps in case of small view changes. We propose a novel approach based on Convolutional Neural Networks (CNNs) to jointly predict depth maps and foreground separation masks used to condition Generative Adversarial Networks (GANs) for hallucinating plausible color and depths in the initially occluded areas. We demonstrate the effectiveness of our approach for novel scene view synthesis from a single image.},
  annotation   = {This paper is looking at creating realistic depth illisions with just a single AR camera. It does this by using a CNN wich can detect the things like the corners of rooms and from that it can work out how far away they probably are from the camera with some very good presition.},
  arxivid      = {1807.08776},
  file         = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dhamo et al. - 2019 - Peeking behind objects Layered depth prediction from a single image.pdf:pdf},
  keywords     = {Generative adversarial networks,Layered depth image,Occlusion,RGB-D inpainting},
  publisher    = {Elsevier B.V.},
}


 
@Misc{Pretz2021,
  author       = {Pretz, Kathy},
  year         = {2021-06},
  title        = {Startup's thermal imaging and AR system for firefighters joins the COVID-19 fight},
  url          = {https://spectrum.ieee.org/startups-thermal-imaging-and-ar-system-for-firefighters-joins-the-covid19-fight},
  journal       = {IEEE Spectrum},
  publisher    = {IEEE Spectrum},
}

@InProceedings{Brooke1996,
  author = {J. B. Brooke},
  year   = {1996},
  title  = {SUS: A 'Quick and Dirty' Usability Scale},
}

@Article{Paas1992,
  author          = {Paas, Fred G.W.C.},
  year            = {1992},
  journal    = {Journal of Educational Psychology},
  title           = {{Training Strategies for Attaining Transfer of Problem-Solving Skill in Statistics: A Cognitive-Load Approach}},
  doi             = {10.1037/0022-0663.84.4.429},
  issn            = {00220663},
  number          = {4},
  pages           = {429--434},
  volume          = {84},
  abstract        = {In statistical problems, the differential effects on training performance, transfer performance, and cognitive load were studied for 3 computer-based training strategies. The conventional, worked, and completion conditions emphasized, respectively, the solving of conventional problems, the study of worked-out problems, and the completion of partly worked-out problems. The relation between practice-problem type and transfer was expected to be mediated by cognitive load. It was hypothesized that practice with conventional problems would require more time and more effort during training and result in lower and more effort-demanding transfer performance than practice with worked-out or partly worked-out problems. With the exception of time and effort during training, the results supported the hypotheses. The completion strategy and, in particular, the worked strategy proved to be superior to the conventional strategy for attaining transfer.},
  file            = {:D\:/Paas_1992-with-cover-page-v2.pdf:pdf},
  mendeley-groups = {Questionaires},
}

@Article{Aurand2017,
  author       = {Aurand, Alexander M. and Dufour, Jonathan S. and Marras, William S.},
  year         = {2017},
  journal       = {Journal of Biomechanics},
  title        = {{Accuracy map of an optical motion capture system with 42 or 21 cameras in a large measurement volume}},
  doi          = {10.1016/j.jbiomech.2017.05.006},
  issn         = {18732380},
  pages        = {237--240},
  url          = {http://dx.doi.org/10.1016/j.jbiomech.2017.05.006},
  volume       = {58},
  abstract     = {Optical motion capture is commonly used in biomechanics to measure human kinematics. However, no studies have yet examined the accuracy of optical motion capture in a large capture volume (>100 m3), or how accuracy varies from the center to the extreme edges of the capture volume. This study measured the dynamic 3D errors of an optical motion capture system composed of 42 OptiTrack Prime 41 cameras (capture volume of 135 m3) by comparing the motion of a single marker to the motion reported by a ThorLabs linear motion stage. After spline interpolating the data, it was found that 97\% of the capture area had error below 200 $\mu$m. When the same analysis was performed using only half (21) of the cameras, 91\% of the capture area was below 200 $\mu$m of error. The only locations that exceeded this threshold were at the extreme edges of the capture area, and no location had a mean error exceeding 1 mm. When measuring human kinematics with skin-mounted markers, uncertainty of marker placement relative to underlying skeletal features and soft tissue artifact produce errors that are orders of magnitude larger than the errors attributed to the camera system itself. Therefore, the accuracy of this OptiTrack optical motion capture system was found to be more than sufficient for measuring full-body human kinematics with skin-mounted markers in a large capture volume (>100 m3).},
  file         = {:D\:/1-s2.0-S0021929017302580-main.pdf:pdf},
  keywords     = {Accuracy,Gait,Marker error,Measurement error,Motion capture,Motion tracking,Optical motion capture},
  pmid         = {28549599},
  publisher    = {Elsevier Ltd},
}

@Article{MartinGomez2021,
  author          = {Martin-Gomez, Alejandro and Weiss, Jakob and Keller, Andreas and Eck, Ulrich and Roth, Daniel and Navab, Nassir},
  year            = {2021},
  journal    = {IEEE Transactions on Visualization and Computer Graphics},
  title           = {{The Impact of Focus and Context Visualization Techniques on Depth Perception in Optical See-Through Head-Mounted Displays}},
  doi             = {10.1109/TVCG.2021.3079849},
  issn            = {19410506},
  number          = {X},
  pages           = {1--16},
  volume          = {XX},
  abstract        = {Estimating the depth of virtual content has proven to be a challenging task in Augmented Reality (AR) applications. Existing studies have shown that the visual system uses multiple depth cues to infer the distance of objects, occlusion being one of the most important ones. Generating appropriate occlusions becomes particularly important for AR applications that require the visualization of augmented objects placed below a real surface. Examples of these applications are medical scenarios in which anatomical information needs to be observed within the patients body. In this regard, existing works have proposed several focus and context (F+C) approaches to aid users in visualizing this content using Video See-Through (VST) Head-Mounted Displays (HMDs). However, the implementation of these approaches in Optical See-Through (OST) HMDs remains an open question due to the additive characteristics of the display technology. In this paper, we, for the first time, design and conduct a user study that compares depth estimation between VST and OST HMDs using existing in-situ visualization methods. Our results show that these visualizations cannot be directly transferred to OST displays without increasing error in depth perception tasks. To tackle this gap, we perform a structured decomposition of the visual properties of AR F+C methods to find best-performing combinations. We propose the use of chromatic shadows and hatching approaches transferred from computer graphics. In a second study, we perform a factorized analysis of these combinations, showing that varying the shading type and using colored shadows can lead to better depth estimation when using OST HMDs.},
  file            = {:D\:/The_Impact_of_Focus_and_Context_Visualization_Techniques_on_Depth_Perception_in_Optical_See-Through_Head-Mounted_Displays.pdf:pdf},
  keywords        = {Augmented Reality,Augmented reality,Color,Depth Estimation,Design and Evaluation Methods,Estimation,Head-mounted displays,Human Computer Interaction,Perception,Rendering (computer graphics),Task analysis,User Studies,Visualization,Visualization Techniques},
  mendeley-groups = {X-RayVision},
}


@Article{Bichlmeier2007,
  author          = {Bichlmeier, Christoph and Sielhorst, Tobias and Heining, Sandro M. and Navab, Nassir},
  year            = {2007},
  journal         = {Informatik aktuell},
  title           = {{Improving depth perception in medical AR a virtual vision panel to the inside of the patient}},
  issn            = {1431472X},
  pages           = {217--221},
  abstract        = {We present the in-situ visualization of medical data taken from CT or MRI scans in real-time using a video see-through head mounted display (HMD). One of the challenges to improve acceptance of augmented reality (AR) for medical purpose is to overcome the misleading depth perception. This problem is caused by a restriction of such systems. Virtual entities of the AR scene can only be presented superimposed onto real imagery. Occlusion is the most effective depth cue [1] and let e.g. a correctly positioned visualization of the spinal column appear in front of the real skin. We present a technique to handle this problem and introduce a Virtual Window superimposed onto the real skin of the patient to create the feeling of getting a view on the inside of the patient. Due to motion of the observer the frame of the window covers and uncovers fragments of the visualized bones and tissue and enables the depth cues motion parallax and occlusion, which correct the perceptive misinformation. An earlier experiment has shown the perceptive advantage of the window. Therefore seven different visualization modes of the spinal column were evaluated regarding depth perception. This paper introduces the technical realization of the window.},
  file            = {:D\:/p217.pdf:pdf},
  isbn            = {3540710906},
  mendeley-groups = {DepthPerceptionMedical},
}

 
@Misc{Flick2017,
  author       = {Flick, Jasper},
  year         = {2017-11},
  title        = {Tessellation},
  howpublished = {https://catlikecoding.com/unity/tutorials/advanced-rendering/tessellation/},
  note         = {Accessed: 2020-10-06},
  journal       = {Catlike Coding},
  publisher    = {Catlike Coding},
}

 
 
@InProceedings{Lerotic2007,
  author          = {Lerotic, Mima and Chung, Adrian J and Mylonas, George and Yang, Guang Zhong},
  booktitle       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2007},
  title           = {{Pq-space based non-photorealistic rendering for augmented reality}},
  doi             = {10.1007/978-3-540-75759-7_13},
  isbn            = {9783540757580},
  number          = {PART 2},
  pages           = {102--109},
  volume          = {4792 LNCS},
  abstract        = {The increasing use of robotic assisted minimally invasive surgery (MIS) provides an ideal environment for using Augmented Reality (AR) for performing image guided surgery. Seamless synthesis of AR depends on a number of factors relating to the way in which virtual objects appear and visually interact with a real environment. Traditional overlaid AR approaches generally suffer from a loss of depth perception. This paper presents a new AR method for robotic assisted MIS, which uses a novel pq-space based nonphotorealistic rendering technique for providing see-through vision of the embedded virtual object whilst maintaining salient anatomical details of the exposed anatomical surface. Experimental results with both phantom and in vivo lung lobectomy data demonstrate the visual realism achieved for the proposed method and its accuracy in providing high fidelity AR depth perception. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
  file            = {:C\:/Users/adminuser/Downloads/pq-space_Based_Non-Photorealistic_Rendering_for_Au.pdf:pdf},
  issn            = {16113349},
  mendeley-groups = {X-RayVision},
  pmid            = {18044558},
}

@Article{Shi2020,
  author          = {Shi, Peiteng and Billeter, Markus and Eisemann, Elmar},
  year            = {2020},
  journaltitle    = {Computers and Graphics (Pergamon)},
  title           = {{SalientGaze: Saliency-based gaze correction in virtual reality}},
  doi             = {10.1016/j.cag.2020.06.007},
  issn            = {00978493},
  pages           = {83--94},
  url             = {https://doi.org/10.1016/j.cag.2020.06.007},
  volume          = {91},
  abstract        = {Eye-tracking with gaze estimation is a key element in many applications, ranging from foveated rendering and user interaction to behavioural analysis and usage metrics. For virtual reality, eye-tracking typically relies on near-eye cameras that are mounted in the VR headset. Such methods usually involve an initial calibration to create a mapping from eye features to a gaze position. However, the accuracy based on the initial calibration degrades when the position of the headset relative to the users' head changes; this is especially noticeable when users readjust the headset for comfort or even completely remove it for a short while. We show that a correction of such shifts can be achieved via 2D drift vectors in eye space. Our method estimates these drifts by extracting salient cues from the shown virtual environment to determine potential gaze directions. Our solution can compensate for HMD shifts, even those arising from taking off the headset, which enables us to eliminate reinitialization steps.},
  file            = {:D\:/Thomas/Downloads/1-s2.0-S0097849320300972-main.pdf:pdf},
  keywords        = {Drift estimation,Eye-tracking,Headsets shifts,Saliency,Stereo,Virtual reality},
  mendeley-groups = {GazeBasedOperations/Saliency},
  publisher       = {Elsevier Ltd},
}

@Article{Melillo2017,
  author          = {Melillo, Paolo and Riccio, Daniel and {Di Perna}, Luigi and {Sanniti Di Baja}, Gabriella and {De Nino}, Maurizio and Rossi, Settimio and Testa, Francesco and Simonelli, Francesca and Frucci, Maria},
  year            = {2017},
  journaltitle    = {IEEE Journal of Translational Engineering in Health and Medicine},
  title           = {{Wearable Improved Vision System for Color Vision Deficiency Correction}},
  doi             = {10.1109/JTEHM.2017.2679746},
  issn            = {21682372},
  number          = {February},
  volume          = {5},
  abstract        = {Color vision deficiency (CVD) is an extremely frequent vision impairment that compromises the ability to recognize colors. In order to improve color vision in a subject with CVD, we designed and developed a wearable improved vision system based on an augmented reality device. The system was validated in a clinical pilot study on 24 subjects with CVD (18 males and 6 females, aged 37.4 ± 14.2 years). The primary outcome was the improvement in the Ishihara Vision Test score with the correction proposed by our system. The Ishihara test score significantly improved (p = 0.03) from 5.8 ± 3.0 without correction to 14.8 ± 5.0 with correction. Almost all patients showed an improvement in color vision, as shown by the increased test scores. Moreover, with our system, 12 subjects (50\%) passed the vision color test as normal vision subjects. The development and preliminary validation of the proposed platform confirm that a wearable augmented-reality device could be an effective aid to improve color vision in subjects with CVD.},
  file            = {:D\:/Thomas/Downloads/jtehm-melillo-2679746.pdf:pdf},
  keywords        = {Augmented reality,color vision deficiency,medical device,wearable device},
  mendeley-groups = {ColorBlindness},
}

@Article{Gabbard2014,
  author          = {Gabbard, Joseph L. and Fitch, Gregory M. and Kim, Hyungil},
  year            = {2014},
  journal    = {Proceedings of the IEEE},
  title           = {{Behind the glass: Driver challenges and opportunities for AR automotive applications}},
  doi             = {10.1109/JPROC.2013.2294642},
  issn            = {00189219},
  number          = {2},
  pages           = {124--136},
  volume          = {102},
  abstract        = {As the automotive industry moves toward the car of the future, technology companies are developing cutting-edge systems, in vehicle and out, that aim to make driving safer, more pleasant, and more convenient. While we are already seeing some successful video-based augmented reality (AR) auxiliary displays (e.g., center-mounted backup aid systems), the application opportunities of optical see-through AR as presented on a drivers' windshield are yet to be fully tapped; nor are the visual perceptual and attention challenges fully understood. As we race to field AR applications in transportation, we should first consider the perceptual and distraction issues that are known in both the AR and transportation communities, with a focus on the unique and intersecting aspects for driving applications. This paper describes the some opportunities and driver challenges associated with AR applications in the automotive domain. We first present a basic research space to assist in these inquiries, which delineates head-mounted from heads-up and center-mounted displays; video from optical see-through displays; and world-fixed from screen-fixed AR graphics. We then address benefits of AR related to primary, secondary, and tertiary driver tasks as well as driver perception and cognition challenges inherent in automotive AR systems. {\textcopyright} 2014 IEEE.},
  file            = {:D\:/Thomas/Downloads/Behind_the_Glass_Driver_Challenges_and_Opportunities_for_AR_Automotive_Applications.pdf:pdf},
  keywords        = {Augmented reality (AR),displays,human factors,intelligent transportation systems},
  mendeley-groups = {ARDriving},
  publisher       = {IEEE},
}

@Article{Rolland2000,
  author          = {Rolland, Jannick P. and Fuchs, Henry},
  year            = {2000},
  journal    = {Presence: Teleoperators and Virtual Environments},
  title           = {{Optical Versus Video See-Through Head-Mounted Displays in Medical Visualization}},
  doi             = {10.1162/105474600566808},
  issn            = {10547460},
  number          = {3},
  pages           = {287--309},
  volume          = {9},
  abstract        = {We compare two technological approaches to augmented reality for 3-D medical visualization: optical and video see-through devices. We provide a context to discuss the technology by reviewing several medical applications of augmented-reality research efforts driven by real needs in the medical field, both in the United States and in Europe. We then discuss the issues for each approach, optical versus video, from both a technology and human-factor point of view. Finally, we point to potentially promising future developments of such devices including eye tracking and multifocus planes capabilities, as well as hybrid optical/video technology.},
  file            = {:D\:/Thomas/Downloads/Optical_Versus_Video_See-Through_Head-Mounted_Disp.pdf:pdf},
  isbn            = {1054746005668},
  mendeley-groups = {MedicalDataOverlay/Feasablity},
}

@Book{Phillips2020,
  author          = {Phillips, Nate and Kruse, Brady and Khan, Farzana Alam and Ii, J Edward Swan},
  year            = {2020},
  title           = {{Window for Law Enforcement Operations}},
  doi             = {10.1007/978-3-030-49695-1},
  isbn            = {9783030496951},
  pages           = {591--610},
  publisher       = {Springer International Publishing},
  url             = {http://dx.doi.org/10.1007/978-3-030-49695-1_40},
  file            = {:C\:/Users/adminuser/Downloads/Phillips2020_Chapter_ARoboticAugmentedRealityVirtua.pdf:pdf},
  keywords        = {Augmented reality,Situation awareness,X-Ray Vision,augmented reality},
  mendeley-groups = {X-RayVision},
}

@Article{Karlsson2005,
  author          = {Karlsson, Niklas and {Di Bernardo}, Enrico and Ostrowski, Jim and Goncalves, Luis and Pirjanian, Paolo and Munich, Mario E.},
  year            = {2005},
  journal    = {Proceedings - IEEE International Conference on Robotics and Automation},
  title           = {{The vSLAM algorithm for robust localization and mapping}},
  doi             = {10.1109/ROBOT.2005.1570091},
  issn            = {10504729},
  number          = {April},
  pages           = {24--29},
  volume          = {2005},
  abstract        = {This paper presents the Visual Simultaneous Localization and Mapping (vSLAM™) algorithm, a novel algorithm for simultaneous localization and mapping (SLAM). The algorithm is vision- and odometry-based, and enables low-cost navigation in cluttered and populated environments. No initial map is required, and it satisfactorily handles dynamic changes in the environment, for example, lighting changes, moving objects and/or people. Typically, vSLAM recovers quickly from dramatic disturbances, such as "kidnapping". {\textcopyright} 2005 IEEE.},
  file            = {:D\:/Thomas/Downloads/The_vSLAM_Algorithm_for_Robust_Localization_and_Mapping.pdf:pdf},
  isbn            = {078038914X},
  keywords        = {Kalman filter,Mixed proposal distribution,Particle filter,SLAM,Vision},
  mendeley-groups = {SLAM},
}

@article{Kalkofen2007,
abstract = {Figure 1 An example of an enhanced augmentation. Focus objects (in red) are not only overlaid on top of the video image, but they are partially occluded by key features from context objects. This provides object occlusion with key features of occluding objects. A second level context (yellow seats) further helps an understanding of the scene. Edges in this image are enhanced considering occlusions with other objects. This helps us to better control the depth complexity of the scene ABSTRACT In this article we present interactive Focus and Context (F+C) visualizations for Augmented Reality (AR) applications. We demonstrate how F+C visualizations are used to affect the user's perception of hidden objects by presenting contextual information in the area of augmentation. We carefully overlay synthetic data on top of the real world imagery by taking into account the information that is about to be occluded. Furthermore, we present operations to control the amount of augmented information. Additionally, we developed an interaction tool, based on the Magic Lens technique, which allows for interactive separation of focus from context. We integrated our work into a rendering framework developed on top of the Studierstube Augmented Reality system. We finally show examples to demonstrate how our work benefits AR. Categories},
author = {Kalkofen, Denis and Mendez, Eric and Schmalstieg, Dieter},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalkofen, Mendez, Schmalstieg - 2007 - Focus and Context Visualization for Medical Augmented Reality Focus and Context Visualization (4).pdf:pdf},
isbn = {9781424417506},
journal = {ISMAR '07: Proceedings of the 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality},
keywords = {Edge Based,Mixed Reality,X-Ray Vision,ar,interaction techniques for mr,layout techniques,mediated and,object overlay and spatial,real-time,rendering},
mendeley-groups = {X-RayVision},
mendeley-tags = {Edge Based,Mixed Reality,X-Ray Vision},
pages = {1--10},
title = {{Focus and Context Visualization for Medical Augmented Reality Focus and Context Visualization for Medical Augmented Reality}},
volume = {6},
year = {2007}
}

@article{Berning2014a,
abstract = {Displaying three-dimensional content on a flat display is bound to reduce the impression of depth, particularly for mobile video see-trough augmented reality. Several applications in this domain can benefit from accurate depth perception, especially if there are contradictory depth cues, like occlusion in a x-ray visualization. The use of stereoscopy for this effect is already prevalent in head-mounted displays, but there is little research on the applicability for hand-held augmented reality. We have implemented such a prototype using an off-the-shelf smartphone equipped with a stereo camera and an autostereoscopic display. We designed and conducted an extensive user study to explore the effects of stereoscopic hand-held augmented reality on depth perception. The results show that in this scenario depth judgment is mostly influenced by monoscopic depth cues, but our system can improve positioning accuracy in challenging scenes.},
author = {Berning, Matthias and Kleinert, Daniel and Riedel, Till and Beigl, Michael},
doi = {10.1109/ISMAR.2014.6948413},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berning et al. - 2014 - A study of depth perception in hand-held augmented reality using autostereoscopic displays(2).pdf:pdf},
isbn = {9781479961849},
journal = {ISMAR 2014 - IEEE International Symposium on Mixed and Augmented Reality - Science and Technology 2014, Proceedings},
keywords = {Autostereoscopy,augmented reality,depth perception,mobile devices,user study},
mendeley-groups = {DepthPerceptionPapers},
pages = {93--98},
title = {{A study of depth perception in hand-held augmented reality using autostereoscopic displays}},
year = {2014}
}

@article{Adelson1990,
author = {Adelson, Edward H and Anandan, P},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adelson, Anandan - 1990 - Ordinal characteristics of transparency .pdf:pdf},
journal = {Brain},
mendeley-groups = {Transperency},
pages = {77--81},
title = {{Ordinal characteristics of transparency .}},
year = {1990}
}

@article{Max1995,
abstract = {This tutorial survey paper reviews several different models for light interaction with volume densities of absorbing, glowing, reflecting, and/or scattering material. They are, in order of increasing realism, absorption only, emission only, emission and absorption combined, single scattering of external illumination without shadows, single scattering with shadows, and multiple scattering. For each model I give the physical assumptions, describe the applications for which it is appropriate, derive the differential or integral equations for light transport, present calculation methods for solving them, and show output images for a data set representing a cloud. Special attention is given to calculation methods for the multiple scattering model. {\textcopyright} 1995 IEEE},
author = {Max, Nelson},
doi = {10.1109/2945.468400},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Max - 1995 - Optical Models for Direct Volume Rendering.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Optical models,compositing,discrete ordinates method,emission,extinction,multiple scattering,participating media,volume rendering,volume shading,volume shadows},
mendeley-groups = {VoxelAlgorithms/lighting},
number = {2},
pages = {99--108},
title = {{Optical Models for Direct Volume Rendering}},
volume = {1},
year = {1995}
}

@article{Lee2021,
abstract = {In this paper, we propose a prediction algorithm, the combination of Long Short-Term Memory (LSTM) and attention model, based on machine learning models to predict the vision coordinates when watching 360-degree videos in a Virtual Reality (VR) or Augmented Reality (AR) system. Predicting the vision coordinates while video streaming is important when the network condition is degraded. However, the traditional prediction models such as Moving Average (MA) and Autoregression Moving Average (ARMA) are linear so they cannot consider the nonlinear relationship. Therefore, machine learning models based on deep learning are recently used for nonlinear predictions. We use the Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) neural network methods, originated in Recurrent Neural Networks (RNN), and predict the head position in the 360-degree videos. Therefore, we adopt the attention model to LSTM to make more accurate results. We also compare the performance of the proposed model with the other machine learning models such as Multi-Layer Perceptron (MLP) and RNN using the root mean squared error (RMSE) of predicted and real coordinates. We demonstrate that our model can predict the vision coordinates more accurately than the other models in various videos.},
author = {Lee, Dongwon and Choi, Minji and Lee, Joohyun},
doi = {10.3390/s21113678},
file = {:D\:/Thomas/Downloads/sensors-21-03678.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {Attention model,GRU,Head movement,LSTM,Machine learning,Time-series prediction},
mendeley-groups = {AI},
number = {11},
pages = {1--22},
pmid = {34070560},
title = {{Prediction of head movement in 360-degree videos using attention model}},
volume = {21},
year = {2021}
}

@book{Gamage2021,
abstract = {We contribute a novel user- and activity-independent kinematics-based regressive model for continuously predicting ballistic hand movements in virtual reality (VR). Compared to prior work on end-point prediction, continuous hand trajectory prediction in VR enables an early estimation of future events such as collisions between the user's hand and virtual objects such as UI widgets. We developed and validated our prediction model through a user study with 20 participants. The study collected hand motion data with a 3D pointing task and a gaming task with three popular VR games. Results show that our model can achieve a low Root Mean Square Error (RMSE) of 0.80 cm, 0.85 cm and 3.15 cm from future hand positions ahead of 100 ms, 200 ms and 300 ms respectively across all the users and activities. In pointing tasks, our predictive model achieves an average angular error of 4.0° and 1.5° from the true landing position when 50% and 70% of the way through the movement. A follow-up study showed that the model can be applied to new users and new activities without further training.},
author = {Gamage, Nisal Menuka and Ishtaweera, Deepana and Weigel, Martin and Withana, Anusha},
booktitle = {UIST 2021 - Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/3472749.3474753},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gamage et al. - 2021 - So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality.pdf:pdf},
isbn = {9781450386357},
keywords = {Activity-independent,Hand motion prediction,Kinematics-based Model,User-independent,Virtual Reality},
mendeley-groups = {AI},
number = {1},
pages = {332--343},
publisher = {Association for Computing Machinery},
title = {{So Predictable! Continuous 3D Hand Trajectory Prediction in Virtual Reality}},
volume = {1},
year = {2021}
}

@article{Heinrich2021,
abstract = {Depth perception is a common issue in augmented reality (AR). Projective AR, where the spatial relations between the projection surface and displayed virtual contents need to be represented properly, is particularly affected. This is crucial in the medical domain, e.g., for the distances between the patient's skin and projected inner anatomical structures, but not much research was conducted in this context before. To this end, this work investigates the applicability of surface visualization techniques to support the perception of spatial relations in projective AR. Four methods previously explored in different domains were combined with the projection of inner anatomical structures on a human torso phantom. They were evaluated in a comparative user study (n=21) with respect to a distance estimation and a sorting task. Measures included Task completion time, accuracy, total Head movement and Confidence of the participants. Consistent results across variables show advantages of more occluding surface visualizations for the distance estimation task. Opposite results were obtained for the sorting task. This suggests that the amount of needed surface preservation depends on the use case and individual occlusion compromises need to be explored in future work.},
author = {Heinrich, Florian and Schwenderling, Lovis and Streuber, Marcus and Bornemann, Kai and Lawonn, Kai and Hansen, Christian},
doi = {10.1109/ICHMS53169.2021.9582452},
file = {:C\:/Users/adminuser/Downloads/Effects_of_Surface_Visualizations_on_Depth_Perception_in_Projective_Augmented_Reality.pdf:pdf},
isbn = {9781665401708},
journal = {Proceedings of the 2021 IEEE International Conference on Human-Machine Systems, ICHMS 2021},
keywords = {Depth Perception,Medical Visualization,Occlusion Mask,Projective Augmented Reality},
mendeley-groups = {X-RayVision},
pages = {0--5},
publisher = {IEEE},
title = {{Effects of Surface Visualizations on Depth Perception in Projective Augmented Reality}},
year = {2021}
}

@article{Li2016,
abstract = {People often become disoriented and frustrated when navigating complex, multi-level buildings. We argue that the principle reason underlying these challenges is insufficient access to the requisite information needed for developing an accurate mental representation, called a multi-level cognitive map. We postulate that increasing access to global landmarks (i.e., those visible from multiple locations/floors of a building) will aid spatial integration between floors and the development of these representations. This prediction was investigated in three experiments, using either direct perception or Augmented Reality (AR) visualizations. Results of Experiment 1 demonstrated that increasing visual access to a global landmark promoted multi-level cognitive map development, supporting our hypothesis. Experiment 2 revealed no reliable performance benefits of using two minimalist (icon-based and wire-frame) visualization techniques. Experiment 3, using a third X-ray visualization, showed reliably better performance for not only a no-visualization control but also the gold standard of direct window access. These results demonstrate that improving information access through principled visualizations benefit multi-level cognitive map development.},
author = {Li, Hengshan and Corey, Richard R. and Giudice, Uro and Giudice, Nicholas A.},
doi = {10.1007/978-3-319-39952-2_30},
file = {:C\:/Users/adminuser/Downloads/Li2016_Chapter_AssessmentOfVisualizationInter.pdf:pdf},
isbn = {9783319399515},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Human factors,Multi-level cognitive maps,Multi-level indoor wayfinding,Visualization interface design,X-ray visualization},
mendeley-groups = {X-RayVision},
number = {1},
pages = {308--321},
title = {{Assessment of visualization interfaces for assisting the development of multi-level cognitive maps}},
volume = {9744},
year = {2016}
}

@article{Gagnon2020,
abstract = {Mixed reality is increasingly being used for training, especially for tasks that are difficult for humans to perform such as far distance perception. Although there has been much prior work on the perception of distance in mixed reality, the majority of the work has focused on distances ranging up to about 30 meters. The little work that has investigated distance perception beyond 30 meters has been mixed in terms of whether people tend to over-or underestimate distances in this range. In the current study, we investigated the perception of distances ranging from 25-500 meters in a fully mediated outdoor environment using the ZED Mini and HTC Vive Pro. To our knowledge, no work has examined distance perception to this range in mixed reality. Participants in the study verbally estimated the distances to a variety of objects. Distances were overestimated from 25-200 meters but underestimated from 300-500 meters. Overall, far distance perception in mixed reality may be biased in different directions depending on the range of distances, suggesting a need for future research in this area to support training of far distance perception.},
author = {Gagnon, Holly C. and Buck, Lauren and Smith, Taylor N. and Narasimham, Gayathri and Stefanucci, Jeanine and Creem-Regehr, Sarah H. and Bodenheimer, Bobby},
doi = {10.1145/3385955.3407933},
file = {:C\:/Users/tomis/OneDrive/Documents/3385955.3407933.pdf:pdf},
isbn = {9781450376181},
journal = {Proceedings - SAP 2020: ACM Symposium on Applied Perception},
keywords = {distance estimation,mixed reality,perception},
title = {{Far Distance Estimation in Mixed Reality}},
year = {2020}
}

@InProceedings{10.1007/978-3-540-75759-7_13,
author="Lerotic, Mirna
and Chung, Adrian J.
and Mylonas, George
and Yang, Guang-Zhong",
editor="Ayache, Nicholas
and Ourselin, S{\'e}bastien
and Maeder, Anthony",
title="pq-space Based Non-Photorealistic Rendering for Augmented Reality",
booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2007",
year="2007",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="102--109",
abstract="The increasing use of robotic assisted minimally invasive surgery (MIS) provides an ideal environment for using Augmented Reality (AR) for performing image guided surgery. Seamless synthesis of AR depends on a number of factors relating to the way in which virtual objects appear and visually interact with a real environment. Traditional overlaid AR approaches generally suffer from a loss of depth perception. This paper presents a new AR method for robotic assisted MIS, which uses a novel pq-space based non-photorealistic rendering technique for providing see-through vision of the embedded virtual object whilst maintaining salient anatomical details of the exposed anatomical surface. Experimental results with both phantom and in vivo lung lobectomy data demonstrate the visual realism achieved for the proposed method and its accuracy in providing high fidelity AR depth perception.",
isbn="978-3-540-75759-7"
}


@article{Martinez-Conde2013,
abstract = {When we attempt to fix our gaze, our eyes nevertheless produce so-called 'fixational eye movements', which include microsaccades, drift and tremor. Fixational eye movements thwart neural adaptation to unchanging stimuli and thus prevent and reverse perceptual fading during fixation. Over the past 10 years, microsaccade research has become one of the most active fields in visual, oculomotor and even cognitive neuroscience. The similarities and differences between microsaccades and saccades have been a most intriguing area of study, and the results of this research are leading us towards a unified theory of saccadic and microsaccadic function. {\textcopyright} 2013 Macmillan Publishers Limited. All rights reserved.},
author = {Martinez-Conde, Susana and Otero-Millan, Jorge and MacKnik, Stephen L.},
doi = {10.1038/nrn3405},
file = {:C\:/Users/adminuser/Downloads/nrn3405.pdf:pdf},
issn = {1471003X},
journal = {Nature Reviews Neuroscience},
number = {2},
pages = {83--96},
pmid = {23329159},
publisher = {Nature Publishing Group},
title = {{The impact of microsaccades on vision: Towards a unified theory of saccadic function}},
volume = {14},
year = {2013}
}

@article{DeVilliers2008,
abstract = {Inverse distortion is used to create an undistorted image from a distorted\nimage. For each pixel in the undistorted\n\nimage it is required to determine which pixel in the distorted image\nshould be used. However the process of\n\ncharacterizing a lens using a model such as that of Brown, yields\na non-invertible mapping from the distorted\n\ndomain to the undistorted domain. There are three current approaches\nto solving this: an approximation of the\n\ninverse distortion is derived from a low-order version of Brown�s\nmodel; an initial guess for the distorted position is\n\niteratively refined until it yields the desired undistorted pixel\nposition; or a look-up table is generated to store the\n\nmapping. Each approach requires one to sacrifice either accuracy,\nmemory usage or processing time. This paper\n\nshows that it is possible to have real-time, low memory, accurate\ninverse distortion correction. A novel method\n\nbased on the re-use of left-over distortion characterization data\nis combined with modern numerical optimization\n\ntechniques to fit a high-order version of Brown�s model to characterize\nthe inverse distortion. Experimental\n\nresults show that, for thirty-two 5mm lenses exhibiting extreme barrel\ndistortion, inverse distortion can be\n\nimproved 25 fold to 0.013 pixels RMS over the image.},
author = {de Villiers, Jason P. and Leuschner, F. Wilhelm and Geldenhuys, Ronelle},
doi = {10.1117/12.804771},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Villiers, Leuschner, Geldenhuys - 2008 - Centi-pixel accurate real-time inverse distortion correction.pdf:pdf},
issn = {0277786X},
journal = {Optomechatronic Technologies 2008},
keywords = {distortion characterization,inverse distortion,numerical optimization,real-time},
mendeley-groups = {RadialDistortion},
number = {1},
pages = {726611},
title = {{Centi-pixel accurate real-time inverse distortion correction}},
volume = {7266},
year = {2008}
}

@article{Zhang1998,
abstract = {We propose a flexible new technique to easily calibrate a camera. It is well suited for use without specialized knowledge of 3D geometry or computer vision. The technique only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique, and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthog- onal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one step from laboratory environments to real world use.},
author = {{Zhengyou Zhang}},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhengyou Zhang - 1998 - A Flexible New Technique for Camera Calibration Zhengyou.pdf:pdf},
journal = {Technical Report MSR TR-98-71 Microsoft},
mendeley-groups = {RadialDistortion},
number = {last updated on Aug. 13, 2008},
title = {{A Flexible New Technique for Camera Calibration Zhengyou}},
year = {1998}
}

@article{Saravanan2016,
abstract = {The color space conversion employs a significant function in preprocessing phase of digital image processing. Color conversion can improve the quality of images. The color space conversion is used in various applications such as commercial, multimedia, computer vision, visual tracking systems etc. The objective is to convert one color space to another and the inverse of same. Various color space conversions are used such as RGBHSV, RGBHSI and RGBHSL. The conversion process can be done using color space conversion algorithm. The hardware realization of color space conversion models can be implemented by using Xilinx System Generator (XSG) is implemented on Field Programmable Gate Array (FPGA) Spartan-6 XC6SLX16 Family. Finally, the above color space conversion models are implemented in real time and then tabulated in terms of percentage resources utilization and power report of various color space models. By means of color conversion the efficiency and speed are increased.},
author = {Saravanan, G. and Yamuna, G. and Nandhini, S.},
doi = {10.1109/ICCSP.2016.7754179},
file = {:C\:/Users/adminuser/Downloads/Real_time_implementation_of_RGB_to_HSV_HSI_HSL_and_its_reverse_color_space_models.pdf:pdf},
isbn = {9781509003969},
journal = {International Conference on Communication and Signal Processing, ICCSP 2016},
keywords = {Co-simulation,Color Space Conversions (CSCs),FPGA,Real Time Image Processing,Simulink,XSG},
mendeley-groups = {ColorBlending},
pages = {462--466},
publisher = {IEEE},
title = {{Real time implementation of RGB to HSV/HSI/HSL and its reverse color space models}},
year = {2016}
}

@article{Klein2007,
abstract = {This paper presents a method of estimating camera pose in an unknown scene. While this has previously been attempted by adapting SLAM algorithms developed for robotic exploration, we propose a system specifically designed to track a hand-held camera in a small AR workspace. We propose to split tracking and mapping into two separate tasks, processed in parallel threads on a dual-core computer: one thread deals with the task of robustly tracking erratic hand-held motion, while the other produces a 3D map of point features from previously observed video frames. This allows the use of computationally expensive batch optimisation techniques not usually associated with real-time operation: The result is a system that produces detailed maps with thousands of landmarks which can be tracked at frame-rate, with an accuracy and robustness rivalling that of state-of-the-art model-based systems. {\textcopyright}2007 IEEE.},
author = {Klein, Georg and Murray, David},
doi = {10.1109/ISMAR.2007.4538852},
file = {:D\:/Thomas/Downloads/Parallel_Tracking_and_Mapping_for_Small_AR_Workspaces.pdf:pdf},
isbn = {9781424417506},
journal = {2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality, ISMAR},
mendeley-groups = {SLAM},
pages = {225--234},
title = {{Parallel tracking and mapping for small AR workspaces}},
year = {2007}
}

@article{Turner2022,
abstract = {Augmented reality (AR) technologies function to ‘augment' normal perception by superimposing virtual objects onto an agent's visual field. The philosophy of augmented reality is a small but growing subfield within the philosophy of technology. Existing work in this subfield includes research on the phenomenology of augmented experiences, the metaphysics of virtual objects, and different ethical issues associated with AR systems, including (but not limited to) issues of privacy, property rights, ownership, trust, and informed consent. This paper addresses some epistemological issues posed by AR systems. I focus on a near-future version of AR technology called the Real-World Web, which promises to radically transform the nature of our relationship to digital information by mixing the virtual with the physical. I argue that the Real-World Web (RWW) threatens to exacerbate three existing epistemic problems in the digital age: the problem of digital distraction, the problem of digital deception, and the problem of digital divergence. The RWW is poised to present new versions of these problems in the form of what I call the augmented attention economy, augmented skepticism, and the problem of other augmented minds. The paper draws on a range of empirical research on AR and offers a phenomenological analysis of virtual objects as perceptual affordances to help ground and guide the speculative nature of the discussion. It also considers a few policy-based and designed-based proposals to mitigate the epistemic threats posed by AR technology.},
author = {Turner, Cody},
doi = {10.1007/s13347-022-00496-5},
file = {:C\:/Users/adminuser/Downloads/s13347-022-00496-5.pdf:pdf},
isbn = {0123456789},
issn = {22105441},
journal = {Philosophy and Technology},
keywords = {Affordance perception,Attention economy,Augmented epistemology,Augmented reality,Philosophy of technology,Real-World Web},
mendeley-groups = {PerceptionOfAugmentedRealityVisulizations},
number = {1},
pages = {1--28},
publisher = {Springer Netherlands},
title = {{Augmented Reality, Augmented Epistemology, and the Real-World Web}},
url = {https://doi.org/10.1007/s13347-022-00496-5},
volume = {35},
year = {2022}
}

@article{Fischer2020a,
abstract = {Many Augmented Reality (AR) applications require the alignment of virtual objects to the real world; this is particularly important in medical AR scenarios where medical imaging information may be displayed directly on a patient and is used to identify the exact locations of specific anatomical structures within the body. For optical see-Through AR, alignment accuracy depends both on the optical parameters of the AR display as well as the visualization parameters of the virtual model. In this paper, we explore how different static visualization techniques influence users' ability to perform perception-based alignment in AR for breast reconstruction surgery, where surgeons must accurately identify the locations of several perforator blood vessels while planning the procedure. We conducted a pilot study in which four subjects used four different visualization techniques with varying degrees of opaqueness and brightness as well as outline contrast to align virtual replicas of the relevant anatomy to their 3D-printed counterparts. We collected quantitative scores on spatial alignment accuracy using an external tracking system and qualitative scores on user preference and perceived performance. Results indicate that the highest source of alignment error was along the depth dimension, with users consistently overestimating depth when aligning the virtual renderings. The majority of subjects preferred visualization techniques rendered with lower levels of opaqueness and brightness as well as higher outline contrast, which were also found to support more accurate alignment.},
author = {Fischer, Marc and Leuze, Christoph and Perkins, Stephanie and Rosenberg, Jarrett and Daniel, Bruce and Martin-Gomez, Alejandro},
doi = {10.1109/ISMAR-Adjunct51615.2020.00027},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer et al. - 2020 - Evaluation of Different Visualization Techniques for Perception-Based Alignment in Medical AR.pdf:pdf},
isbn = {9781728176758},
journal = {Adjunct Proceedings of the 2020 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2020},
keywords = {AR,Alignment,Augmented Reality,FDA,HoloLens,Occlusion,Visualization techniques,XR,arms-length interaction,depth perception,egocentric depth,extended reality,mixed reality,near-field distance,optical see-Through HMD,perceptual accuracy},
mendeley-groups = {MedicalDataOverlay/Guidance},
pages = {45--50},
title = {{Evaluation of Different Visualization Techniques for Perception-Based Alignment in Medical AR}},
year = {2020}
}

@incollection{Kaptein2016,
abstract = {In HCI we often encounter dependent variables which are not (conditionally) normally distributed: we measure response-times, mouse-clicks, or the number of dialog steps it took a user to complete a task. Furthermore, we often encounter nested or grouped data; users are grouped within companies or institutes, or we obtain multiple observations within users. The standard linear regression models and ANOVAs used to analyze our experimental data are not always feasible in such cases since their assumptions are violated, or the predictions from the fitted models are outside the range of the observed data. In this chapter we introduce extensions to the standard linear model (LM) to enable the analysis of these data. The use of [R] to fit both Generalized Linear Models (GLMs) as well as Generalized Linear Mixed Models (GLMMs, also known as random effects models or hierarchical models) is explained. The chapter also briefly covers regularized regression models which are hardly used in the social sciences despite the fact that these models are extremely popular in Machine Learning, often for good reasons. We end with a number of recommendations for further reading on the topics that are introduced: the current text serves as a basic introduction.},
address = {Cham},
author = {Kaptein, Maurits},
booktitle = {Modern Statistical Methods for HCI},
doi = {10.1007/978-3-319-26633-6_11},
editor = {Robertson, Judy and Kaptein, Maurits},
file = {:C\:/Users/adminuser/Downloads/978-3-319-26633-6_11.pdf:pdf},
isbn = {978-3-319-26633-6},
mendeley-groups = {MixedEffectModels},
pages = {251--274},
publisher = {Springer International Publishing},
title = {{Using Generalized Linear (Mixed) Models in HCI}},
url = {https://doi.org/10.1007/978-3-319-26633-6_11},
year = {2016}
}

@article{Adams2022,
abstract = {Although it is commonly accepted that depth perception in augmented reality (AR) displays is distorted, we have yet to isolate which properties of AR affect people's ability to correctly perceive virtual objects in real spaces. From prior research on depth perception in commercial virtual reality, it is likely that ergonomic properties and graphical limitations impact visual perception in head-mounted displays (HMDs). However, an insufficient amount of research has been conducted in augmented reality HMDs for us to begin isolating pertinent factors in this family of displays. To this end, in the current research, we evaluate absolute measures of distance perception in the Microsoft HoloLens 2, an optical see-Through AR display, and the Varjo XR-3, a video see-Through AR display. The current work is the first to evaluate either device using absolute distance perception as a measure. For each display, we asked participants to verbally report distance judgments to both grounded and floating targets that were rendered either with or without a cast shadow along the ground. Our findings suggest that currently available video see-Through displays may induce more distance underestimation than their optical see-Through counterparts. We also find that the vertical position of an object and the presence of a cast shadow influence depth perception.},
author = {Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah and Bodenheimer, Bobby},
doi = {10.1109/VR51125.2022.00101},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adams et al. - 2022 - Depth Perception in Augmented Reality The Effects of Display, Shadow, and Position.pdf:pdf},
isbn = {9781665496179},
journal = {Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2022},
keywords = {OST AR,VST AR,depth,distance,perception,shadow,surface contact},
mendeley-groups = {DepthPerceptionPapers},
pages = {792--801},
publisher = {IEEE},
title = {{Depth Perception in Augmented Reality: The Effects of Display, Shadow, and Position}},
year = {2022}
}

@article{Guo2022,
abstract = {Mixed Reality (MR) is an evolving technology lying in the continuum spanned by related technologies such as Virtual Reality (VR) and Augmented Reality (AR), and creates an exciting way of interacting with people and the environment. This technology is fast becoming a tool used by many people, potentially improving living environments and work efficiency. Microsoft HoloLens has played an important role in the progress of MR, from the first generation to the second generation. In this paper, we systematically evaluate the functions of applicable functions in HoloLens 2. These evaluations can serve as a performance benchmark that can help people who need to use this instrument for research or applications in the future. The detailed tests and the performance evaluation of the different functionalities show the usability and possible limitations of each function. We mainly divide the experiment into the existing functions of the HoloLens 1, the new functions of the HoloLens 2, and the use of research mode. This research results will be useful for MR researchers who want to use HoloLens 2 as a research tool to design their own MR applications.},
archivePrefix = {arXiv},
arxivId = {2207.09554},
author = {Guo, Hung-Jui and Prabhakaran, Balakrishnan},
eprint = {2207.09554},
file = {:D\:/Thomas/2207.09554.pdf:pdf},
keywords = {hologram,hololens,hololens 2,imu,inertial measurement unit,mixed reality,spatial},
mendeley-groups = {HoloLensReview},
title = {{HoloLens 2 Technical Evaluation as Mixed Reality Guide}},
url = {http://arxiv.org/abs/2207.09554},
year = {2022},
journal = {Lecture Notes in Computer Science},
}

@article{Matyash2021,
abstract = {As augmented reality devices become more available, collaborative work in the augmented space is expected to increase. Knowing the position of participants allows for realistic interaction rather than passive participation. The HoloLens 2's camera and IMUs fuse data to locate the device. In this paper, accuracy and repeatability of the HoloLens2 position finding was analysed to provide a quantitative measure of pose repeatability and deviation from a path while in motion. Deviation from a circular path was found to be below 5 mm per 628 mm travelled.},
author = {Matyash, Ivan and Kutzner, Robin and Neumuth, Thomas and Rockstroh, Max},
doi = {10.1515/cdbme-2021-2161},
file = {:D\:/Thomas/10.1515_cdbme-2021-2161.pdf:pdf},
issn = {23645504},
journal = {Current Directions in Biomedical Engineering},
keywords = {AR,HoloLens2,Patient tracking,Pose repeatability,Position finding,Staff tracking,Telemedicine},
mendeley-groups = {HoloLensReview},
number = {2},
pages = {633--636},
title = {{Accuracy measurement of HoloLens2 IMUs in medical environments}},
volume = {7},
year = {2021}
}

@article{Park2021,
abstract = {Since Microsoft HoloLens first appeared in 2016, HoloLens has been used in various industries, over the past five years. This study aims to review academic papers on the applications of HoloLens in several industries. A review was performed to summarize the results of 44 papers (dated between January 2016 and December 2020) and to outline the research trends of applying HoloLens to different industries. This study determined that HoloLens is employed in medical and surgical aids and systems, medical education and simulation, industrial engineering, architecture, civil engineering and other engineering fields. The findings of this study contribute towards classifying the current uses of HoloLens in various industries and identifying the types of visualization techniques and functions.},
author = {Park, Sebeom and Bokijonov, Shokhrukh and Choi, Yosoon},
doi = {10.3390/app11167259},
file = {:D\:/Thomas/Downloads/applsci-11-07259.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Augmented reality,Head-mount display,HoloLens,Mixed reality,Virtual reality,Visualization},
mendeley-groups = {Reviews},
number = {16},
title = {{Review of microsoft hololens applications over the past five years}},
volume = {11},
year = {2021}
}

@article{Marto2022,
abstract = {The sense of presence in augmented reality (AR) has been studied by multiple researchers through diverse applications and strategies. In addition to the valuable information provided to the scientific community, new questions keep being raised. These approaches vary from following the standards from virtual reality to ascertaining the presence of users' experiences and new proposals for evaluating presence that specifically target AR environments. It is undeniable that the idea of evaluating presence across AR may be overwhelming due to the different scenarios that may be possible, whether this regards technological devices—from immersive AR headsets to the small screens of smartphones—or the amount of virtual information that is being added to the real scenario. Taking into account the recent literature that has addressed the sense of presence in AR as a true challenge given the diversity of ways that AR can be experienced, this study proposes a specific scope to address presence and other related forms of dimensions such as immersion, engagement, embodiment, or telepresence, when AR is used in games. This systematic review was conducted following the PRISMA methodology, carefully analysing all studies that reported visual games that include AR activities and somehow included presence data—or related dimensions that may be referred to as immersion-related feelings, analysis or results. This study clarifies what dimensions of presence are being considered and evaluated in AR games, how presence-related variables have been evaluated, and what the major research findings are. For a better understanding of these approaches, this study takes note of what devices are being used for the AR experience when immersion-related feelings are one of the behaviours that are considered in their evaluations, and discusses to what extent these feelings in AR games affect the player's other behaviours.},
author = {Marto, Anabela and Gon{\c{c}}alves, Alexandrino},
doi = {10.3390/jimaging8040091},
file = {:D\:/Thomas/jimaging-08-00091.pdf:pdf},
issn = {2313433X},
journal = {Journal of Imaging},
keywords = {augmented reality games,immersion,presence},
mendeley-groups = {Reviews},
number = {4},
title = {{Augmented Reality Games and Presence: A Systematic Review}},
volume = {8},
year = {2022}
}

@article{Parsons2021,
abstract = {This systematic review has been developed against a background of rapid developments in augmented reality (AR) technology and its application in medical education. The objectives are to provide a critical synthesis of current trends in the field and to highlight areas for further research. The data sources used for the study were the PubMed, Web of Science and Discover databases. Sources included in the study comprised peer reviewed journal articles published between 2015 and 2020. Inclusion criteria included empirical research findings related to learning outcomes and the populations for the selected studies were medical students. Studies were appraised in terms of to what extent the use of AR contributed to learning gains in knowledge and/or skill. Twenty-one studies were included in the analysis, and the dates of these suggested an increasing trend of publications in this area. The uses of AR in each selected study were analyzed through a lens of affordance, to identify which specific affordances of AR appear to be most effective in this domain. Results of the study indicated that AR seems to be more effective in supporting skill development rather than knowledge gain when compared to other techniques. Some key affordances of AR in medical education are identified as developing practical skills in a spatial context, device portability across locations and situated learning in context. It is suggested that a focus on relevant affordances when designing AR systems for medical education may lead to better learning outcomes. It is noted that the majority of AR systems reported in the selected studies are concentrated in the areas of anatomy and surgery, but that are also other areas of practice being explored, and these may provide opportunities for new types of AR learning systems to be developed for medical education.},
author = {Parsons, David and Maccallum, Kathryn},
doi = {10.2147/AMEP.S249891},
file = {:D\:/Thomas/Current Perspectives on Augmented Reality in Medical Education Applications Affordances and Limitations.pdf:pdf},
issn = {11797258},
journal = {Advances in Medical Education and Practice},
keywords = {Empirical study,Learning outcomes,Literature review,Medical students,Systematic review},
mendeley-groups = {MRinMedicine},
pages = {77--91},
title = {{Current perspectives on augmented reality in medical education: Applications, affordances and limitations}},
volume = {12},
year = {2021}
}

@article{Krevelen2010,
abstract = {We are on the verge of ubiquitously adopting Augmented Reality (AR) technologies to enhance our perception and help us see, hear, and feel our environments in new and enriched ways. AR will support us in fields such as education, maintenance, design and reconnaissance, to name but a few. This paper describes the field of AR, including a brief definition and development history, the enabling technologies and their characteristics. It surveys the state of the art by reviewing some recent applications of AR technology as well as some known limitations regarding human factors in the use of AR systems that developers will need to overcome. Index Terms-Augmented Reality, Technologies, Applications , Limitations. I INTRODUCTION Imagine a technology with which you could see more than others see, hear more than others hear, and perhaps even touch, smell and taste things that others can not. What if we had technology to perceive completely computational elements and objects within our real world experience, entire creatures and structures even that help us in our daily activities , while interacting almost unconsciously through mere gestures and speech? With such technology, mechanics could see instructions what to do next when repairing an unknown piece of equipment, surgeons could see ultrasound scans of organs while performing surgery on them, fire fighters could see building layouts to avoid otherwise invisible hazards, soldiers could see positions of enemy snipers spotted by un-manned reconnaissance aircraft, and we could read reviews for each restaurant in the street we"re walking in, or battle 10-foot tall aliens on the way to work [57]. Augmented reality (AR) is this technology to create a "next generation, reality-based interface" [77] and is moving from laboratories around the world into various industries and consumer markets. AR supplements the real world with virtual (computer-generated) objects that appear to coexist in the same space as the real world. AR was recognised as an emerging technology of 2007 [79], and with today"s smart phones and AR browsers we are starting to embrace this very new and exciting kind of human-computer interaction.},
author = {Krevelen, Ric, Van and {Poelman Ronald}},
file = {:D\:/Thomas/2767-Article Text-21335-1-2-20191013.pdf:pdf},
journal = {International journal of virtual reality},
number = {2},
pages = {10--20},
title = {{A Survey of Augmented Reality Technologies, Applications, and Limitations}},
url = {http://www.arvika.de/},
volume = {9},
year = {2010}
}


@article{Xiong2021,
abstract = {With rapid advances in high-speed communication and computation, augmented reality (AR) and virtual reality (VR) are emerging as next-generation display platforms for deeper human-digital interactions. Nonetheless, to simultaneously match the exceptional performance of human vision and keep the near-eye display module compact and lightweight imposes unprecedented challenges on optical engineering. Fortunately, recent progress in holographic optical elements (HOEs) and lithography-enabled devices provide innovative ways to tackle these obstacles in AR and VR that are otherwise difficult with traditional optics. In this review, we begin with introducing the basic structures of AR and VR headsets, and then describing the operation principles of various HOEs and lithography-enabled devices. Their properties are analyzed in detail, including strong selectivity on wavelength and incident angle, and multiplexing ability of volume HOEs, polarization dependency and active switching of liquid crystal HOEs, device fabrication, and properties of micro-LEDs (light-emitting diodes), and large design freedoms of metasurfaces. Afterwards, we discuss how these devices help enhance the AR and VR performance, with detailed description and analysis of some state-of-the-art architectures. Finally, we cast a perspective on potential developments and research directions of these photonic devices for future AR and VR displays.},
author = {Xiong, Jianghao and Hsiang, En Lin and He, Ziqian and Zhan, Tao and Wu, Shin Tson},
doi = {10.1038/s41377-021-00658-8},
file = {:D\:/Thomas/Downloads/s41377-021-00658-8.pdf:pdf},
issn = {20477538},
journal = {Light: Science and Applications},
mendeley-groups = {ARChallanges},
number = {1},
pages = {1--30},
publisher = {Springer US},
title = {{Augmented reality and virtual reality displays: emerging technologies and future perspectives}},
volume = {10},
year = {2021}
}

@article{Sousa2017,
abstract = {Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.},
author = {Sousa, Maur{\'{i}}cio and Mendes, Daniel and Paulo, Soraia and Matela, Nuno and Jorge, Joaquim and Lopes, Daniel Sim{\~{o}}es},
doi = {10.1145/3025453.3025566},
file = {:D\:/Thomas/3025453.3025566.pdf:pdf},
isbn = {9781450346559},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Interaction design,Medical visualization,Multitouch surfaces,Virtual reality},
mendeley-groups = {Medical Papers For Justication of Study 2},
pages = {4057--4062},
title = {{VRRRRoom: Virtual reality for radiologists in the reading room}},
volume = {2017-May},
year = {2017}
}

@article{Rosen2011,
abstract = {Surgical robotics is a rapidly evolving field. With roots in academic research, surgical robotic systems are now clinically used across a wide spectrum of surgical procedures. Surgical Robotics: Systems Applications and Visions provides a comprehensive view of the field both from the research and clinical perspectives. This volume takes a look at surgical robotics from four different perspectives, addressing vision, systems, engineering development and clinical applications of these technologies. The book also: -Discusses specific surgical applications of robotics that have already been deployed in operating rooms -Covers specific engineering breakthroughs that have occurred in surgical robotics -Details surgical robotic applications in specific disciplines of surgery including orthopedics, urology, cardiac surgery, neurosurgery, ophthalmology, pediatric surgery and general surgery Surgical Robotics: Systems Applications and Visions is an ideal volume for researchers and engineers working in biomedical engineering. {\textcopyright} Springer Science+Business Media, LLC 2011. All rights reserved.},
author = {Rosen, Jacob and Hannaford, Blake and Satava, Richard M.},
doi = {10.1007/978-1-4419-1126-1},
file = {:D\:/Thomas/978-1-4419-1126-1_9.pdf:pdf},
isbn = {9781441911254},
journal = {Surgical Robotics: Systems Applications and Visions},
keywords = {aesop,and drug administration,fda,history {\'{a}} computer motion,inc,patient-side manipulator {\'{a}} robotic,surgery history {\'{a}} safe,{\'{a}} clinical indication clearance,{\'{a}} da vinci,{\'{a}} device clearance history,{\'{a}} endoscope control manipulator,{\'{a}} food,{\'{a}} intuitive surgical,{\'{a}} laparoscopic surgery,{\'{a}} master manipulator {\'{a}}},
pages = {1--819},
title = {{Surgical robotics: Systems applications and visions}},
year = {2011}
}

@article{Booij2022,
abstract = {Objective: To assess the influence of breathing state on the accuracy of a 3D camera for body contour detection and patient positioning in thoracic CT. Materials and methods: Patients who underwent CT of the thorax with both an inspiratory and expiratory scan were prospectively included for analysis of differences in the ideal table height at different breathing states. For a subgroup, an ideal table height suggestion based on 3D camera images at both breathing states was available to assess their influence on patient positioning accuracy. Ideal patient positioning was defined as the table height at which the scanner isocenter coincides with the patient's isocenter. Results: The mean (SD) difference of the ideal table height between the inspiratory and the expiratory breathing state among the 64 included patients was 10.6 mm (4.5) (p < 0.05). The mean (SD) positioning accuracy, i.e., absolute deviation from the ideal table height, within the subgroup (n = 43) was 4.6 mm (7.0) for inspiratory scans and 7.1 mm (7.7) for expiratory scans (p < 0.05) when using corresponding 3D camera images. The mean (SD) accuracy was 14.7 mm (7.4) (p < 0.05) when using inspiratory camera images on expiratory scans; vice versa, the accuracy was 3.1 mm (9.5) (p < 0.05). Conclusion: A 3D camera allows for accurate and precise patient positioning if the camera image and the subsequent CT scan are acquired in the same breathing state. It is recommended to perform an expiratory planning image when acquiring a thoracic CT scan in both the inspiratory and expiratory breathing state. Key Points: • A 3D camera for body contour detection allows for accurate and precise patient positioning if the camera image and the subsequent CT scan are acquired in the same breathing state. • It is recommended to perform an expiratory planning image when acquiring a thoracic CT scan in both the inspiratory and expiratory breathing state.},
author = {Booij, Ronald and van Straten, Marcel and Wimmer, Andreas and Budde, Ricardo P.J.},
doi = {10.1007/s00330-021-08191-3},
file = {:D\:/Thomas/330_2021_Article_8191.pdf:pdf},
isbn = {0033002108191},
issn = {14321084},
journal = {European Radiology},
keywords = {Multidetector computed tomography,Thorax,Tomography scanners, X-ray computed},
number = {1},
pages = {442--447},
pmid = {34327574},
publisher = {European Radiology},
title = {{Influence of breathing state on the accuracy of automated patient positioning in thoracic CT using a 3D camera for body contour detection}},
volume = {32},
year = {2022}
}

@article{Zhou2022,
abstract = {Importance. Medical images are essential for modern medicine and an important research subject in visualization. However, medical experts are often not aware of the many advanced three-dimensional (3D) medical image visualization techniques that could increase their capabilities in data analysis and assist the decision-making process for specific medical problems. Our paper provides a review of 3D visualization techniques for medical images, intending to bridge the gap between medical experts and visualization researchers. Highlights. Fundamental visualization techniques are revisited for various medical imaging modalities, from computational tomography to diffusion tensor imaging, featuring techniques that enhance spatial perception, which is critical for medical practices. The state-of-the-art of medical visualization is reviewed based on a procedure-oriented classification of medical problems for studies of individuals and populations. This paper summarizes free software tools for different modalities of medical images designed for various purposes, including visualization, analysis, and segmentation, and it provides respective Internet links. Conclusions. Visualization techniques are a useful tool for medical experts to tackle specific medical problems in their daily work. Our review provides a quick reference to such techniques given the medical problem and modalities of associated medical images. We summarize fundamental techniques and readily available visualization tools to help medical experts to better understand and utilize medical imaging data. This paper could contribute to the joint effort of the medical and visualization communities to advance precision medicine.},
author = {Zhou, Liang and Fan, Mengjie and Hansen, Charles and Johnson, Chris R. and Weiskopf, Daniel},
doi = {10.34133/2022/9840519},
file = {:D\:/Thomas/2022_9840519.pdf:pdf},
issn = {27658783},
journal = {Health Data Science},
mendeley-groups = {Reviews},
title = {{A Review of Three-Dimensional Medical Image Visualization}},
volume = {2022},
year = {2022}
}



@article{Geng2013,
author = {Geng, Jason},
doi = {10.1364/AOP},
file = {:D\:/Thomas/Downloads/aop-5-4-456.pdf:pdf},
mendeley-groups = {Displays},
pages = {456--535},
title = {{Three-dimensional display technologies}},
year = {2013},
journal = {Advance Optical Photonics}
}

@article{Zhan2020,
abstract = {As one of the most promising candidates for next-generation mobile platform, augmented reality (AR) and virtual reality (VR) have potential to revolutionize the ways we perceive and interact with various digital information. In the meantime, recent advances in display and optical technologies, together with the rapidly developing digital processers, offer new development directions to advancing the near-eye display systems further. In this perspective paper, we start by analyzing the optical requirements in near-eye displays poised by the human visual system and then compare it against the specifications of state-of-the-art devices, which reasonably shows the main challenges in near-eye displays at the present stage. Afterward, potential solutions to address these challenges in both AR and VR displays are presented case by case, including the most recent optical research and development, which are already or have the potential to be industrialized for extended reality displays.},
author = {Zhan, Tao and Yin, Kun and Xiong, Jianghao and He, Ziqian and Wu, Shin Tson},
doi = {10.1016/j.isci.2020.101397},
file = {:D\:/Thomas/Downloads/1-s2.0-S258900422030585X-main.pdf:pdf},
issn = {25890042},
journal = {iScience},
keywords = {Laser,Optical Imaging,Optical Materials,Photonics},
mendeley-groups = {ARChallanges},
number = {8},
pages = {101397},
publisher = {Elsevier Inc.},
title = {{Augmented Reality and Virtual Reality Displays: Perspectives and Challenges}},
url = {https://doi.org/10.1016/j.isci.2020.101397},
volume = {23},
year = {2020}
}

@article{Sutherland1968,
address = {Washington D.C},
author = {Sutherland, Ivan E.},
file = {:D\:/Thomas/sutherland-headmount.pdf:pdf},
journal = {Proceedings of the AFIPS Fall Joint Computer Conference},
pages = {295--302},
title = {{A Head-Mounted Three Dimentional Display}},
year = {1968}
}

@article{Ding2022,
abstract = {In recent years, with the rapid development of information technology, the visualization and interaction of virtual reality technology has developed, making the application of VR technology in education more and more attractive to scholars. This paper adopts the literature analysis method, focusing on the application of VR technology in the field of higher education, selects 80 empirical studies in the Web of Science literature database, conducts in-depth reading and analysis of the papers, and summarizes the experience of applying VR technology in the field of higher education. In order to deepen the application of VR in higher education. The research results show that the research objects of VR application in higher education are mainly undergraduates, the main majors of application are science, engineering, and medical-related majors, and the application of humanities and social sciences is relatively rare. At present, the devices used for VR in higher education are mainly computers and headsets, which are not portable enough. In addition, students lack guidance and training in the use of VR equipment before class. Compared with traditional education, most of the studies show that the application of VR to higher education and teaching has positive effects, mainly by affecting students' behaviors to affect students' learning results. The researchers mainly use traditional evaluation methods to evaluate teaching effects, use questionnaires and tests to collect data, and use data analysis methods mainly difference analysis and descriptive analysis. Based on the research results, the researchers put forward some suggestions at the end of the paper.},
author = {Ding, Xiaoqin and Li, Zhe},
doi = {10.3389/feduc.2022.1048816},
file = {:D\:/Thomas/feduc-07-1048816.pdf:pdf},
issn = {2504284X},
journal = {Frontiers in Education},
keywords = {empirical research,higher education,immersion learning,virtual environment,virtual reality},
mendeley-groups = {Reviews},
title = {{A review of the application of virtual reality technology in higher education based on Web of Science literature data as an example}},
volume = {7},
year = {2022}
}

@article{Xie2021,
abstract = {This study aimed to discuss the research efforts in developing virtual reality (VR) technology for different training applications. To begin with, we describe how VR training experiences are typically created and delivered using the current software and hardware. We then discuss the challenges and solutions of applying VR training to different application domains, such as first responder training, medical training, military training, workforce training, and education. Furthermore, we discuss the common assessment tests and evaluation methods used to validate VR training effectiveness. We conclude the article by discussing possible future directions to leverage VR technology advances for developing novel training experiences.},
author = {Xie, Biao and Liu, Huimin and Alghofaili, Rawan and Zhang, Yongqi and Jiang, Yeling and Lobo, Flavio Destri and Li, Changyang and Li, Wanwan and Huang, Haikun and Akdere, Mesut and Mousas, Christos and Yu, Lap Fai},
doi = {10.3389/frvir.2021.645153},
file = {:D\:/Thomas/frvir-02-645153.pdf:pdf},
issn = {26734192},
journal = {Frontiers in Virtual Reality},
keywords = {content creation,personalization,simulation,training,virtual reality},
mendeley-groups = {Reviews},
number = {April},
pages = {1--19},
title = {{A Review on Virtual Reality Skill Training Applications}},
volume = {2},
year = {2021}
}

@article{Gsaxner2023,
abstract = {The HoloLens (Microsoft Corp., Redmond, WA), a head-worn, optically see-through augmented reality (AR) display, is the main player in the recent boost in medical AR research. In this systematic review, we provide a comprehensive overview of the usage of the first-generation HoloLens within the medical domain, from its release in March 2016, until the year of 2021. We identified 217 relevant publications through a systematic search of the PubMed, Scopus, IEEE Xplore and SpringerLink databases. We propose a new taxonomy including use case, technical methodology for registration and tracking, data sources, visualization as well as validation and evaluation, and analyze the retrieved publications accordingly. We find that the bulk of research focuses on supporting physicians during interventions, where the HoloLens is promising for procedures usually performed without image guidance. However, the consensus is that accuracy and reliability are still too low to replace conventional guidance systems. Medical students are the second most common target group, where AR-enhanced medical simulators emerge as a promising technology. While concerns about human–computer interactions, usability and perception are frequently mentioned, hardly any concepts to overcome these issues have been proposed. Instead, registration and tracking lie at the core of most reviewed publications, nevertheless only few of them propose innovative concepts in this direction. Finally, we find that the validation of HoloLens applications suffers from a lack of standardized and rigorous evaluation protocols. We hope that this review can advance medical AR research by identifying gaps in the current literature, to pave the way for novel, innovative directions and translation into the medical routine.},
archivePrefix = {arXiv},
arxivId = {2209.03245},
author = {Gsaxner, Christina and Li, Jianning and Pepe, Antonio and Jin, Yuan and Kleesiek, Jens and Schmalstieg, Dieter and Egger, Jan},
doi = {10.1016/j.media.2023.102757},
eprint = {2209.03245},
file = {:D\:/Thomas/2209.03245.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Augmented reality,Healthcare,HoloLens,Medicine,Review,Surgery,Taxonomy},
mendeley-groups = {HoloLensReview,Reviews},
pmid = {36706637},
title = {{The HoloLens in medicine: A systematic review and taxonomy}},
volume = {85},
year = {2023}
}


@misc{Valensi1945,
author = {Valensi, Georges},
file = {:C\:/Users/adminuser/Downloads/US2375966A_Original_document_20230602080617.pdf:pdf},
pages = {0--9},
title = {{System of television in colors}},
year = {1945}
}

@article{Piringer2004,
author = {Piringer, Harald and Kosara, Robert and Hauser, Helwig},
file = {:D\:/Thomas/AnDeModifiedLectureSlides/Week9/Interactive_focuscontext_visualization_with_linked_2D_3D_scatterplots.pdf:pdf},
isbn = {0769521797},
keywords = {brushing,linked views,scatterplots},
mendeley-groups = {GraphPapers},
title = {{Interactive Focus + Context Visualization with Linked 2D / 3D Scatterplots}},
year = {2004},
journal = {Proceedings. Second International Conference on Coordinated and Multiple Views in Exploratory Visualization},
publisher = {IEEE}
}

@article{Martin-Gomez2021,
abstract = {Estimating the depth of virtual content has proven to be a challenging task in Augmented Reality (AR) applications. Existing studies have shown that the visual system uses multiple depth cues to infer the distance of objects, occlusion being one of the most important ones. Generating appropriate occlusions becomes particularly important for AR applications that require the visualization of augmented objects placed below a real surface. Examples of these applications are medical scenarios in which anatomical information needs to be observed within the patients body. In this regard, existing works have proposed several focus and context (F+C) approaches to aid users in visualizing this content using Video See-Through (VST) Head-Mounted Displays (HMDs). However, the implementation of these approaches in Optical See-Through (OST) HMDs remains an open question due to the additive characteristics of the display technology. In this paper, we, for the first time, design and conduct a user study that compares depth estimation between VST and OST HMDs using existing in-situ visualization methods. Our results show that these visualizations cannot be directly transferred to OST displays without increasing error in depth perception tasks. To tackle this gap, we perform a structured decomposition of the visual properties of AR F+C methods to find best-performing combinations. We propose the use of chromatic shadows and hatching approaches transferred from computer graphics. In a second study, we perform a factorized analysis of these combinations, showing that varying the shading type and using colored shadows can lead to better depth estimation when using OST HMDs.},
author = {Martin-Gomez, Alejandro and Weiss, Jakob and Keller, Andreas and Eck, Ulrich and Roth, Daniel and Navab, Nassir},
doi = {10.1109/TVCG.2021.3079849},
file = {:D\:/The_Impact_of_Focus_and_Context_Visualization_Techniques_on_Depth_Perception_in_Optical_See-Through_Head-Mounted_Displays.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Augmented Reality,Augmented reality,Color,Depth Estimation,Design and Evaluation Methods,Estimation,Head-mounted displays,Human Computer Interaction,Perception,Rendering (computer graphics),Task analysis,User Studies,Visualization,Visualization Techniques},
mendeley-groups = {X-RayVision},
number = {X},
pages = {1--16},
title = {{The Impact of Focus and Context Visualization Techniques on Depth Perception in Optical See-Through Head-Mounted Displays}},
volume = {XX},
year = {2021}
}

@article{Martin-Gomez2019,
abstract = {Many studies explored the effectiveness of augmented, virtual, and mixed reality for object placement tasks. Two main approaches for assisting users during object alignment exist: Static visualization techniques and interactive guides. This paper presents a comparative evaluation of four static visualization techniques used to render virtual objects when precise alignment in 6 degrees of freedom (DoF) is required. The selection of these techniques is based on the amount of occlusion caused by the visual guides during the alignment task. To the best of our knowledge, no previous work exists that evaluates which visualization technique is most suitable to support users while precisely aligning objects in virtual environments. We designed a virtual reality scenario considering two conditions -with and without time constraints- in which users aligned pairs of objects. To evaluate the users performance, quantitative and qualitative scores were collected. Our results suggest that visualization techniques with low levels of occlusion can improve alignment performance and increase user acceptance.},
author = {Martin-Gomez, Alejandro and Eck, Ulrich and Navab, Nassir},
doi = {10.1109/VR.2019.8798135},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Martin-Gomez, Eck, Navab - 2019 - Visualization techniques for precise alignment in VR A comparative study.pdf:pdf},
isbn = {9781728113777},
journal = {26th IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2019 - Proceedings},
keywords = {Centered computing,Hci design and evaluation methods,Human,Human computer interaction (hci),Interaction paradigms,User studies,Virtual reality,Visualization,Visualization design and evaluation methods},
mendeley-groups = {NonPhotorealisticRendering},
pages = {735--741},
title = {{Visualization techniques for precise alignment in VR: A comparative study}},
year = {2019}
}

@article{AlJanabi2020,
abstract = {Background: The advent of Virtual Reality technologies presents new opportunities for enhancing current surgical practice. Studies suggest that current techniques in endoscopic surgery are prone to disturbance of a surgeon's visual-motor axis, influencing performance, ergonomics and iatrogenic injury rates. The Microsoft{\textregistered} HoloLens is a novel head-mounted display that has not been explored within surgical innovation research. This study aims to evaluate the HoloLens as a potential alternative to conventional monitors in endoscopic surgery. Materials and methods: This prospective, observational and comparative study recruited 72 participants consisting of novices (n = 28), intermediate-level (n = 24) and experts (n = 20). Participants performed ureteroscopy, within an inflatable operating environment, using a validated training model and the HoloLens mixed-reality device as a monitor. Novices also completed the assigned task using conventional monitors; whilst the experienced groups did not, due to their extensive familiarity. Outcome measures were procedural completion time and performance evaluation (OSATS) score. A final evaluation survey was distributed amongst all participants. Results: The HoloLens facilitated improved outcomes for procedural times (absolute difference, − 73 s; 95% CI − 115 to − 30; P = 0.0011) and OSAT scores (absolute difference, 4.1 points; 95% CI 2.9–5.3; P < 0.0001) compared to conventional monitors. Feedback evaluation demonstrated 97% of participants agreed or strongly agreed that the HoloLens will have a role in surgical education (mean rating, 4.6 of 5; 95% CI 4.5–4.8). Furthermore, 95% of participants agreed or strongly agreed that the HoloLens is feasible to introduce clinically and will have a role within surgery (mean rating, 4.4 of 5; 95% CI 4.2–4.5). Conclusion: This study demonstrates that the device facilitated improved outcomes of performance in novices and was widely accepted as a surgical visual aid by all groups. The HoloLens represents a feasible alternative to the conventional setup, possibly by aligning the surgeon's visual-motor axis.},
author = {{Al Janabi}, Hasaneen Fathy and Aydin, Abdullatif and Palaneer, Sharanya and Macchione, Nicola and Al-Jabir, Ahmed and Khan, Muhammad Shamim and Dasgupta, Prokar and Ahmed, Kamran},
doi = {10.1007/s00464-019-06862-3},
file = {:C\:/Users/adminuser/Downloads/AlJanabi2020_Article_EffectivenessOfTheHoloLensMixe.pdf:pdf},
isbn = {0046401906862},
issn = {14322218},
journal = {Surgical Endoscopy},
keywords = {Augmented reality,Endoscopy,Head-mounted displays,HoloLens,Surgery,Virtual reality},
mendeley-groups = {SpaitalPerceptionRegardingGeometricShapes},
number = {3},
pages = {1143--1149},
pmid = {31214807},
publisher = {Springer US},
title = {{Effectiveness of the HoloLens mixed-reality headset in minimally invasive surgery: a simulation-based feasibility study}},
url = {https://doi.org/10.1007/s00464-019-06862-3},
volume = {34},
year = {2020}
}

@article{Gilliam1996,
author = {Gilliam, Xiaoning and Manross, Kevin and Gamel, Matthew},
file = {:D\:/Thomas/OneFortyOne/47233.pdf:pdf},
journal = {Wind Engineering},
title = {{Visualization of Radar Data in Three-Dimensions}},
year = {1996}
}

@phdthesis{Macdonald1997,
author = {Macdonald, Steven D and Fields, Paul J and Stroup, Michael D},
file = {:D\:/Thomas/OneFortyOne/HollidayRecursiveRayAcousticsThesis.June1998.pdf:pdf},
number = {June},
title = {{REAL-TIME 3D SONAR MODELING AND VISUALIZATION}},
year = {1997},
school = "Navel Postgraduate School, Monterey, California"
}

@article{Adib2013,
abstract = {Wi-Fi signals are typically information carriers between a transmitter and a receiver. In this paper, we show that Wi-Fi can also extend our senses, enabling us to see moving objects through walls and behind closed doors. In particular, we can use such signals to identify the number of people in a closed room and their relative locations. We can also identify simple gestures made behind a wall, and combine a sequence of gestures to communicate messages to a wireless receiver without carrying any transmitting device. The paper introduces two main innovations. First, it shows how one can use MIMO interference nulling to eliminate reflections off static objects and focus the receiver on a moving target. Second, it shows how one can track a human by treating the motion of a human body as an antenna array and tracking the resulting RF beam. We demonstrate the validity of our design by building it into USRP software radios and testing it in office buildings. {\textcopyright} 2013 ACM.},
author = {Adib, Fadel and Katabi, Dina},
doi = {10.1145/2486001.2486039},
file = {:C\:/Users/adminuser/Downloads/2486001.2486039.pdf:pdf},
isbn = {9781450320566},
journal = {SIGCOMM 2013 - Proceedings of the ACM SIGCOMM 2013 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
keywords = {gesture-based user interface,mimo,seeing through walls,wireless},
pages = {75--86},
title = {{See through walls with WiFi!}},
year = {2013}
}

@Software{Snap2020,
  author  = {{Snap Inc.}},
  year    = {2020-10-19},
  title   = {Snaptchat},
  url     = {https://www.snapchat.com},
  version = {Android: 11.3.5.66},
}

@misc{Glass–Gl46:online,
author = {Google},
title = {Google - Glass},
howpublished = {\url{https://www.google.com.au/glass/start/}},
month = {},
year = {},
note = {(Accessed on 10/28/2022)}
}

@misc{MagicLeap:online,
author = {MagicLeap},
title = {MagicLeap},
howpublished = {\url{https://www.magicleap.com/en-us/}},
month = {},
year = {},
note = {(Accessed on 10/28/2022)}
}

@inproceedings{Brooke1996SUSA,
  title={Brooke1996SUSA},
  booktitle = {Usability Evaluation In Industry},
  author={J. B. Brooke},
  year={1996},
  publisher = {CRC Press},
}

@article{Lee2008,
abstract = {The major concern of educators is how to enhance the outcome of education. Better education media used to assist teaching has constantly been sought by researchers in the educational technology domain. Virtual Reality (VR) has been identified as one of them. Many have agreed that VR could help to improve performance and conceptual understanding on a specific range of task. However, there is limited understanding of how VR could enhance the learning outcomes. This paper reviews types of VR that have been used for learning, the theoretical framework for a VR learning environment, and instructional design for VR-based learning environment. Further research is suggested for VR-based learning environment. {\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
author = {Lee, Elinda Ai Lim and Wong, Kok Wai},
doi = {10.1007/978-3-540-69744-2_18},
file = {:D\:/Thomas/978-3-540-69744-2_18.pdf:pdf},
isbn = {3540697373},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {edutainment},
pages = {231--241},
title = {{A review of using virtual reality for learning}},
volume = {5080 LNCS},
year = {2008}
}


@article{Garcia-Vazquez2020,
abstract = {Endovascular aortic repair (EVAR) is a minimal-invasive technique that prevents life-threatening rupture in patients with aortic pathologies by implantation of an endoluminal stent graft. During the endovascular procedure, device navigation is currently performed by fluoroscopy in combination with digital subtraction angiography. This study presents the current iterative process of biomedical engineering within the disruptive interdisciplinary project Nav EVAR, which includes advanced navigation, image techniques and augmented reality with the aim of reducing side effects (namely radiation exposure and contrast agent administration) and optimising visualisation during EVAR procedures. This article describes the current prototype developed in this project and the experiments conducted to evaluate it. The current approach of the Nav EVAR project is guiding EVAR interventions in real-time with an electromagnetic tracking system after attaching a sensor on the catheter tip and displaying this information on Microsoft HoloLens glasses. This augmented reality technology enables the visualisation of virtual objects superimposed on the real environment. These virtual objects include three-dimensional (3D) objects (namely 3D models of the skin and vascular structures) and two-dimensional (2D) objects [namely orthogonal views of computed tomography (CT) angiograms, 2D images of 3D vascular models, and 2D images of a new virtual angioscopy whose appearance of the vessel wall follows that shown in ex vivo and in vivo angioscopies]. Specific external markers were designed to be used as landmarks in the registration process to map the tracking data and radiological data into a common space. In addition, the use of real-time 3D ultrasound (US) is also under evaluation in the Nav EVAR project for guiding endovascular tools and updating navigation with intraoperative imaging. US volumes are streamed from the US system to HoloLens and visualised at a certain distance from the probe by tracking augmented reality markers. A human model torso that includes a 3D printed patient-specific aortic model was built to provide a realistic test environment for evaluation of technical components in the Nav EVAR project. The solutions presented in this study were tested by using an US training model and the aortic-aneurysm phantom. During the navigation of the catheter tip in the US training model, the 3D models of the phantom surface and vessels were visualised on HoloLens. In addition, a virtual angioscopy was also built from a CT scan of the aortic-aneurysm phantom. The external markers designed for this study were visible in the CT scan and the electromagnetically tracked pointer fitted in each marker hole. US volumes of the US training model were sent from the US system to HoloLens in order to display them, showing a latency of 259±86 ms (mean±standard deviation). The Nav EVAR project tackles the problem of radiation exposure and contrast agent administration during EVAR interventions by using a multidisciplinary approach to guide the endovascular tools. Its current state presents several limitations such as the rigid alignment between preoperative data and the simulated patient. Nevertheless, the techniques shown in this study in combination with fibre Bragg gratings and optical coherence tomography are a promising approach to overcome the problems of EVAR interventions.},
author = {Garci{\'{a}}-V{\'{a}}zquez, Ver{\'{o}}nica and {Von Haxthausen}, Felix and J{\"{a}}ckle, Sonja and Schumann, Christian and Kuhlemann, Ivo and Bouchagiar, Juljan and H{\"{o}}fer, Anna Catharina and Matysiak, Florian and H{\"{u}}ttmann, Gereon and Goltz, Jan Peter and Kleemann, Markus and Ernst, Floris and Horn, Marco},
doi = {10.1515/iss-2018-2001},
file = {:D\:/Thomas/10.1515_iss-2018-2001.pdf:pdf},
issn = {23647485},
journal = {Innovative Surgical Sciences},
keywords = {3D rapid prototyping,EVAR,aortic aneurysm,augmented reality,image-guided therapy,real-time 3D ultrasound,tracking system},
mendeley-groups = {MedicalDataOverlay},
number = {3},
pages = {167--177},
title = {{Navigation and visualisation with HoloLens in endovascular aortic repair}},
volume = {3},
year = {2020}
}

@article{Soares2021,
abstract = {Augmented and virtual reality have been experiencing rapid growth in recent years, but there is still no deep knowledge regarding their capabilities and in what fields they could be explored. In that sense, this paper presents a study on the accuracy and repeatability of Microsoft's HoloLens 2 (augmented reality device) and HTC Vive (virtual reality device) using an OptiTrack system as ground truth. For the HoloLens 2, the method used was hand tracking, whereas, in HTC Vive, the object tracked was the system's hand controller. A series of tests in different scenarios and situations were performed to explore what could influence the measures. The HTC Vive obtained results in the millimeter range, while the HoloLens 2 revealed not very accurate measurements (around 2 cm). Although the difference can seem to be considerable, the fact that HoloLens 2 was tracking the user's hand and not the system's controller made a huge impact. The results are considered a significant step for the ongoing project of developing a human–robot interface by demonstrating an industrial robot using extended reality, which shows great potential to succeed based on our data.},
author = {Soares, In{\^{e}}s and Sousa, Ricardo B. and Petry, Marcelo and Moreira, Ant{\'{o}}nio Paulo},
doi = {10.3390/mti5080047},
file = {:D\:/Thomas/mti-05-00047.pdf:pdf},
issn = {24144088},
journal = {Multimodal Technologies and Interaction},
keywords = {Accuracy,Augmented reality,Programming by demonstration,Repeatability,Virtual reality},
mendeley-groups = {Calibration},
number = {8},
pages = {1--14},
title = {{Accuracy and repeatability tests on hololens 2 and htc vive}},
volume = {5},
year = {2021}
}

@article{Mewes2018,
abstract = {During MRI-guided interventions, navigation support is often separated from the operating field on displays, which impedes the interpretation of positions and orientations of instruments inside the patient's body as well as hand-eye coordination. To overcome these issues projector-based augmented reality can be used to support needle guidance inside the MRI bore directly in the operating field. The authors present two visualisation concepts for needle navigation aids which were compared in an accuracy and usability study with eight participants, four of whom were experienced radiologists. The results show that both concepts are equally accurate ( 2.0 ± 0.6 and 1.7 ± 0.5 mm ), useful and easy to use, with clear visual feedback about the state and success of the needle puncture. For easier clinical applicability, a dynamic projection on moving surfaces and organ movement tracking are needed. For now, tests with patients with respiratory arrest are feasible.},
author = {Mewes, Andr{\'{e}} and Heinrich, Florian and Hensen, Bennet and Wacker, Frank and Lawonn, Kai and Hansen, Christian},
doi = {10.1049/htl.2018.5076},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mewes et al. - 2018 - Concepts for augmented reality visualisation to support needle guidance inside the MRI.pdf:pdf},
issn = {2053-3713},
journal = {Healthcare technology letters},
keywords = {MRI-guided interventions,augmented reality,augmented reality visualisation,biomedical MRI,data visualisation,displays,hand–eye coordination,medical image processing,needle guidance,needle navigation aids,needle puncture,needles,operating field,patient,positions,projector-based augmented reality,visual feedback},
language = {eng},
month = {sep},
number = {5},
pages = {172--176},
publisher = {The Institution of Engineering and Technology},
title = {{Concepts for augmented reality visualisation to support needle guidance inside the MRI}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/30464849 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6222244/},
volume = {5},
year = {2018}
}

@article{Hanna2018,
abstract = {Context.—Augmented reality (AR) devices such as the Microsoft HoloLens have not been well used in the medical field. Objective.—To test the HoloLens for clinical and nonclinical applications in pathology. Design.—A Microsoft HoloLens was tested for virtual annotation during autopsy, viewing 3D gross and microscopic pathology specimens, navigating whole slide images, telepathology, as well as real-time pathology-radiology correlation. Results.—Pathology residents performing an autopsy wearing the HoloLens were remotely instructed with real-time diagrams, annotations, and voice instruction. 3D-scanned gross pathology specimens could be viewed as holograms and easily manipulated. Telepathology was supported during gross examination and at the time of intraoperative consultation, allowing users to remotely access a pathologist for guidance and to virtually annotate areas of interest on specimens in real-time. The HoloLens permitted radiographs to be coregistered on gross specimens and thereby enhanced locating important pathologic findings. The HoloLens also allowed easy viewing and navigation of whole slide images, using an AR workstation, including multiple coregistered tissue sections facilitating volumetric pathology evaluation. Conclusions.—The HoloLens is a novel AR tool with multiple clinical and nonclinical applications in pathology. The device was comfortable to wear, easy to use, provided sufficient computing power, and supported high-resolution imaging. It was useful for autopsy, gross and microscopic examination, and ideally suited for digital pathology. Unique applications include remote supervision and annotation, 3D image viewing and manipulation, telepathology in a mixed-reality environment, and real-time pathology-radiology correlation.},
author = {Hanna, Matthew G. and Ahmed, Ishtiaque and Nine, Jeffrey and Prajapati, Shyam and Pantanowitz, Liron},
doi = {10.5858/arpa.2017-0189-OA},
file = {:D\:/Thomas/arpa_2017-0189-oa.pdf:pdf},
issn = {15432165},
journal = {Archives of Pathology and Laboratory Medicine},
number = {5},
pages = {638--644},
pmid = {29384690},
title = {{Augmented reality technology using microsoft hololens in anatomic pathology}},
volume = {142},
year = {2018}
}

@article{Diaz-Barrancas2020,
abstract = {Virtual reality has reached a great maturity in recent years. However, the quality of its visual appearance still leaves room for improvement. One of the most difficult features to represent in real-time 3D rendered virtual scenes is color fidelity, since there are many factors influencing the faithful reproduction of color. In this paper we introduce a method for improving color fidelity in virtual reality systems based in real-time 3D rendering systems. We developed a color management system for 3D rendered scenes divided into two levels. At the first level, color management is applied only to light sources defined inside the virtual scene. At the second level, we applied spectral techniques over the hyperspectral textures of 3D objects to obtain a higher degree of color fidelity. To illustrate the application of this color management method, we simulated a virtual version of the Ishihara test for color blindness deficiency detection.},
author = {D{\'{i}}az-Barrancas, Francisco and Cwierz, Halina and Pardo, Pedro J. and P{\'{e}}rez, {\'{A}}ngel Luis and Suero, Mar{\'{i}}a Isabel},
doi = {10.3390/s20195658},
file = {:D\:/Thomas/Downloads/sensors-20-05658-v2.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Color fidelity,Hyperspectral textures,Ishihara test,Virtual reality},
mendeley-groups = {ARChallanges},
number = {19},
pages = {1--16},
pmid = {33022952},
title = {{Spectral color management in virtual reality scenes}},
volume = {20},
year = {2020}
}

@article{Beams2022,
abstract = {Augmented and virtual reality devices are being actively investigated and implemented for a wide range of medical uses. However, significant gaps in the evaluation of these medical devices and applications hinder their regulatory evaluation. Addressing these gaps is critical to demonstrating the devices' safety and effectiveness. We outline the key technical and clinical evaluation challenges discussed during the US Food and Drug Administration's public workshop, “Medical Extended Reality: Toward Best Evaluation Practices for Virtual and Augmented Reality in Medicine” and future directions for evaluation method development. Evaluation challenges were categorized into several key technical and clinical areas. Finally, we highlight current efforts in the standards communities and illustrate connections between the evaluation challenges and the intended uses of the medical extended reality (MXR) devices. Participants concluded that additional research is needed to assess the safety and effectiveness of MXR devices across the use cases.},
author = {Beams, Ryan and Brown, Ellenor and Cheng, Wei Chung and Joyner, Janell S. and Kim, Andrea S. and Kontson, Kimberly and Amiras, Dimitri and Baeuerle, Tassilo and Greenleaf, Walter and Grossmann, Rafael J. and Gupta, Atul and Hamilton, Christoffer and Hua, Hong and Huynh, Tran Tu and Leuze, Christoph and Murthi, Sarah B. and Penczek, John and Silva, Jennifer and Spiegel, Brennan and Varshney, Amitabh and Badano, Aldo},
doi = {10.1007/s10278-022-00622-x},
file = {:D\:/Thomas/s10278-022-00622-x.pdf:pdf},
isbn = {0123456789},
issn = {1618727X},
journal = {Journal of Digital Imaging},
keywords = {Augmented reality,Image quality,Medical imaging,Virtual reality},
mendeley-groups = {ARChallanges},
number = {5},
pages = {1409--1418},
pmid = {35469355},
publisher = {Springer International Publishing},
title = {{Evaluation Challenges for the Application of Extended Reality Devices in Medicine}},
url = {https://doi.org/10.1007/s10278-022-00622-x},
volume = {35},
year = {2022}
}

@book{Jha2021,
abstract = {In today's world, augmented reality (AR) is a highly challenging and immerging technology which presents some additional information to the existing real world. This is done by using special glasses like Google glasses or with help of advanced devices. This technology is an advance version of virtual reality. As, in VR, we have to work in a completely virtual environment, but in this technology, we do not have to work in virtual world, but being in the real world, we are getting some additional information. This paper provides a brief description about the architecture of AR, possible solutions provided by several researchers, and academicians, their challenging issues and real-time application in medical or emergency department.},
author = {Jha, Gouri and shm Sharma, Lavanya and Gupta, Shailja},
booktitle = {Lecture Notes in Networks and Systems},
doi = {10.1007/978-981-16-0733-2_47},
file = {:D\:/Thomas/978-981-16-0733-2_47.pdf:pdf},
isbn = {9789811607325},
issn = {23673389},
keywords = {Augmented reality,HOLO- BLSD,Medical field,Virtual reality},
mendeley-groups = {ARChallanges},
pages = {667--678},
publisher = {Springer Singapore},
title = {{Future of Augmented Reality in Healthcare Department}},
url = {http://dx.doi.org/10.1007/978-981-16-0733-2_47},
volume = {203 LNNS},
year = {2021}
}

@article{DaliliSaleh2022,
abstract = {Purpose: This study aimed to present a model for the use of augmented reality (AR) in the libraries of universities of medical sciences. The goal was to introduce the applications, advantages, opportunities and challenges of AR. Design/methodology/approach: This study adopted a qualitative approach, had an applied goal and was based on data theory. The statistical population comprised 20 experts in the field of AR, and the data were collected based on in-depth semi-structured interviews until achieving theoretical saturation. A model was proposed after open coding and the formation of the main categories, and the use of AR in the development of libraries of medical universities was discussed. Findings: The category of application consisted of strengthening education, promoting users' information literacy, finding resources, user guidance, gamification, educational justice, helping management, enriching resources, providing new services and economic savings. The advantages were library services, sociocultural excellence, educational level, software potential and helping the librarian. The challenges were technical, economic and cultural barriers. Libraries can attract many users by enacting effective policies, using technology and enriching the content of resources. AR can greatly assist library management and improve the librarians' and users' professional activities. Research limitations/implications: The limitation of this study was that some experts could not participate in the interviews. Originality/value: The results of this study are beneficial for managers, librarians, students and researchers. The use of AR in libraries is essential for achieving fourth-generation libraries. AR will be a necessity for the libraries of medical universities.},
author = {{Dalili Saleh}, Malihe and Salami, Maryam and Soheili, Faramarz and Ziaei, Soraya},
doi = {10.1108/LHT-01-2021-0033},
file = {:D\:/Thomas/10-1108_LHT-01-2021-0033.pdf:pdf},
issn = {07378831},
journal = {Library Hi Tech},
keywords = {Academic libraries,Augmented reality,Libraries of medical universities,Technology},
mendeley-groups = {ARChallanges},
number = {6},
pages = {1782--1795},
title = {{Augmented reality technology in the libraries of universities of medical sciences: identifying the application, advantages and challenges and presenting a model}},
volume = {40},
year = {2022}
}

@article{Birkfellner1998,
abstract = {The purpose of this paper was to assess to what extent an optical tracking system (OTS) used for position determination in computer-aided surgery (CAS) can be enhanced by combining it with a direct current (dc) driven electromagnetic tracking system (EMTS). The main advantage of the EMTS is the fact that it is not dependent on a free line-of-sight. Unfortunately, the accuracy of the EMTS is highly affected by nearby ferromagnetic materials. We have explored to what extent the influence of the metallic equipment in the operating room (OR) can be compensated by collecting precise information on the nonlinear local error in the EMTS by using the OTS for setting up a calibration look-up table. After calibration of the EMTS and registration of the sensor systems in the OR we have found the average euclidean deviation in position readings between the dc tracker and the OTS reduced from 2.9 ±1.0 mm to 2.1 ±0.8 mm within a half-sphere of 530-mm radius around the magnetic field emitter. Furthermore we have found the calibration to be stable after re-registration of the sensors under varying conditions such as different heights of the OR table and varying positions of the OR equipment over a longer time interval. These results encourage the further development of a hybrid magnetooptical tracker for computer-aided surgery where the electromagnetic tracker acts as an auxiliary source of position information for the optical system. Strategies for enhancing the reliability of the proposed hybrid magnetooptic tracker by detecting artifacts induced by mobile ferromagnetic objects such as surgical tools are discussed. {\textcopyright} 1998 IEEE.},
author = {Birkfellner, Wolfgang and Watzinger, Franz and Wanschitz, Felix and Ewers, Rolf and Bergmann, Helmar},
doi = {10.1109/42.736028},
file = {:C\:/Users/tomis/Downloads/Calibration_of_tracking_systems_in_a_surgical_environment.pdf:pdf},
issn = {02780062},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer-aided surgery (CAS),Electromagnetic tracking system,Frameless stereotaxy,Hybrid tracking system},
mendeley-groups = {Calibration},
number = {5},
pages = {737--742},
pmid = {9874297},
title = {{Calibration of tracking systems in a surgical environment}},
volume = {17},
year = {1998}
}

@article{Franz2014,
abstract = {Object tracking is a key enabling technology in the context of computer-assisted medical interventions. Allowing the continuous localization of medical instruments and patient anatomy, it is a prerequisite for providing instrument guidance to subsurface anatomical structures. The only widely used technique that enables real-time tracking of small objects without line-of-sight restrictions is electromagnetic (EM) tracking. While EM tracking has been the subject of many research efforts, clinical applications have been slow to emerge. The aim of this review paper is therefore to provide insight into the future potential and limitations of EM tracking for medical use. We describe the basic working principles of EM tracking systems, list the main sources of error, and summarize the published studies on tracking accuracy, precision and robustness along with the corresponding validation protocols proposed. State-of-the-art approaches to error compensation are also reviewed in depth. Finally, an overview of the clinical applications addressed with EM tracking is given. Throughout the paper, we report not only on scientific progress, but also provide a review on commercial systems. Given the continuous debate on the applicability of EM tracking in medicine, this paper provides a timely overview of the state-of-the-art in the field. {\textcopyright} 1982-2012 IEEE.},
author = {Franz, Alfred M. and Haidegger, Tamas and Birkfellner, Wolfgang and Cleary, Kevin and Peters, Terry M. and Maier-Hein, Lena},
doi = {10.1109/TMI.2014.2321777},
file = {:C\:/Users/tomis/Downloads/Electromagnetic_Tracking_in_MedicineA_Review_of_Technology_Validation_and_Applications.pdf:pdf},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Computer-assisted interventions (CAI),electromagnetic tracking (EMT),image-guided therapy (IGT),intraoperative surgical navigation,magnetic tracking},
mendeley-groups = {Calibration},
number = {8},
pages = {1702--1725},
pmid = {24816547},
title = {{Electromagnetic tracking in medicine -A review of technology, validation, and applications}},
volume = {33},
year = {2014}
}

@article{Clarke2022,
abstract = {When merging physical and virtual objects with optical see-through augmented reality there is little research that has focused on X-ray vision visualizations considering depth perception. We investigate partial occlusion visualizations when merging visual cues and real-world objects to explore the effect of the visualizations with a proce-dural placement task. We adapted existing X-ray visualization techniques designed for Video See-Through (VST) Augmented Reality to operate on Optical See-Through (OST) devices and investigated how these techniques affect accuracy in a placement task within arms reach. We evaluate the visualizations' impact on accuracy when user movement is unrestricted and report the perceived usability and mental load of each visualization. Our findings indicate that although the type of X-ray visualization is important, the presence of other virtual objects in the scene appears to have a stronger impact on the users' accuracy in placing objects and user experience.},
author = {Clarke, Thomas J. and Mayer, Wolfgang and Zucco, Joanne E. and Matthews, Brandon J. and Smith, Ross T.},
doi = {10.1109/ISMAR-Adjunct57072.2022.00104},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarke et al. - 2022 - Adapting VST AR X-Ray Vision Techniques to OST AR.pdf:pdf},
isbn = {9781665453653},
journal = {Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2022},
keywords = {Augmented Reality-Visualization techniques-X-ray V,Human Computer Interaction-Visualization-Visualiza},
pages = {495--500},
publisher = {IEEE},
title = {{Adapting VST AR X-Ray Vision Techniques to OST AR}},
year = {2022}
}

@inproceedings{9585812,
author = {Clarke, Thomas J},
booktitle = {2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)},
doi = {10.1109/ISMAR-Adjunct54149.2021.00114},
file = {:D\:/Thomas/Depth_Perception_using_X-Ray_Visualizations.pdf:pdf},
mendeley-groups = {My Stuff},
pages = {483--486},
title = {{Depth Perception using X-Ray Visualizations}},
year = {2021}
}

@article{Raab2019,
abstract = {In this conceptual analysis contribution to the special issue on radical embodied cognition, we discuss how embodied cognition can exist with and without representations. We explore this concept through the lens of judgment and decision-making in sports (JDMS). Embodied cognition has featured in many investigations of human behavior, but no single approach has emerged. Indeed, the very definitions of the concepts “embodiment” and “cognition” lack consensus, and consequently the degree of “radicalism” is not universally defined, either. In this paper, we address JDMS not from a rigid theoretical perspective but from two embodied cognition approaches: one that assumes there is mediation between the athlete and the environment through mental representation, and another that assumes direct contact between the athlete and the environment and thus no need for mental representation. Importantly, our aim was not to arrive at a theoretical consensus or set up a competition between approaches but rather to provide a legitimate scientific discussion about how to explain empirical results in JDMS from contrasting perspectives within embodied cognition. For this, we first outline the definitions and constructs of embodied cognition in JDMS. Second, we detail the theory underlying the mental representation and direct contact approaches. Third, we comment on two published research papers on JDMS, one selected by each of us: (1) Correia et al. (2012) and (2) Pizzera (2012). Fourth, following the interpretation of the empirical findings of these papers, we present a discussion on the commonalities and divergences of these two perspectives and the consequences of using one or the other approach in the study of JDMS.},
author = {Raab, Markus and Ara{\'{u}}jo, Duarte},
doi = {10.3389/fpsyg.2019.01825},
file = {:C\:/Users/tomis/Downloads/fpsyg-10-01825.pdf:pdf},
issn = {1664302X},
journal = {Frontiers in Microbiology},
keywords = {Affordance,Common coding,Ecological dynamics,Embodied cognition,Representation,Self-organization},
mendeley-groups = {EmbodiedCogntition},
number = {AUG},
title = {{Embodied cognition with and without mental representations: The case of embodied choices in sports}},
volume = {10},
year = {2019}
}

@article{Wilson2013,
abstract = {The most exciting hypothesis in cognitive science right now is the theory that cognition is embodied. Like all good ideas in cognitive science, however, embodiment immediately came to mean six different things. The most common definitions involve the straight-forward claim that "states of the body modify states of the mind." However, the implications of embodiment are actually much more radical than this. If cognition can span the brain, body, and the environment, then the "states of mind" of disembodied cognitive science won't exist to be modified. Cognition will instead be an extended system assembled from a broad array of resources. Taking embodiment seriously therefore requires both new methods and theory. Here we outline four key steps that research programs should follow in order to fully engage with the implications of embodiment. The first step is to conduct a task analysis, which characterizes from a first person perspective the specific task that a perceiving-acting cognitive agent is faced with. The second step is to identify the task-relevant resources the agent has access to in order to solve the task. These resources can span brain, body, and environment. The third step is to identify how the agent can assemble these resources into a system capable of solving the problem at hand. The last step is to test the agent's performance to confirm that agent is actually using the solution identified in step 3. We explore these steps in more detail with reference to two useful examples (the outfielder problem and the A-not-B error), and introduce how to apply this analysis to the thorny question of language use. Embodied cognition is more than we think it is, and we have the tools we need to realize its full potential.},
author = {Wilson, Andrew D. and Golonka, Sabrina},
doi = {10.3389/fpsyg.2013.00058},
file = {:C\:/Users/tomis/Downloads/fpsyg-04-00058.pdf:pdf},
journal = {Frontiers in Psychology},
keywords = {a-not-b,dynamical systems,embodied cognition,embodied cognition, dynamical systems, replacement hypothesis, robotics, outfielder problem, A-not-B error, language,outfielder problem,replacement hypothesis,robotics},
mendeley-groups = {EmbodiedCogntition},
number = {February},
pages = {1--13},
title = {{Embodied Cognition is Not What you Think it is}},
volume = {4},
year = {2013}
}

@article{Jang2017,
abstract = {With the advancement of virtual reality (VR) technologies, medical students may now study complex anatomical structures in three-dimensional (3-D) virtual environments, without relying solely upon high cost, unsustainable cadavers or animal models. When coupled with a haptic input device, these systems support direct manipulation and exploration of the anatomical structures. Yet, prior studies provide inconclusive support for direct manipulation beyond passive viewing in virtual environments. In some cases, exposure to an “optimal view” appears to be the main source of learning gains, regardless of participants' control of the system. In other cases, direct manipulation provides benefits beyond passive viewing. To address this issue, we compared medical students who either directly manipulated a virtual anatomical structure (inner ear) or passively viewed an interaction in a stereoscopic, 3-D environment. To ensure equal exposure to optimal views we utilized a yoked-pair design, such that for each participant who manipulated the structure a single matched participant viewed a recording of this interaction. Results indicate that participants in the manipulation group were more likely to successful generate (i.e., draw) the observed structures at posttest than the viewing group. Moreover, manipulation benefited students with low spatial ability more than students with high spatial ability. These results suggest that direct manipulation of the virtual environment facilitated embodiment of the anatomical structure and helped participants maintain a clear frame of reference while interacting, which particularly supported participants with low spatial ability.},
author = {Jang, Susan and Vitale, Jonathan M. and Jyung, Robert W. and Black, John B.},
doi = {10.1016/j.compedu.2016.12.009},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jang et al. - 2017 - Direct manipulation is better than passive viewing for learning anatomy in a three-dimensional virtual reality envi.pdf:pdf},
issn = {03601315},
journal = {Computers and Education},
keywords = {Direct manipulation,Embodied cognition,Medical education,Spatial ability,Virtual reality},
mendeley-groups = {VolumeRendering/Education,EmbodiedCogntition},
pages = {150--165},
publisher = {Elsevier Ltd},
title = {{Direct manipulation is better than passive viewing for learning anatomy in a three-dimensional virtual reality environment}},
url = {http://dx.doi.org/10.1016/j.compedu.2016.12.009},
volume = {106},
year = {2017}
}

@ARTICLE{7255417,
  author={Kaplan, Sam H.},
  journal={Journal of the Society of Motion Picture and Television Engineers}, 
  title={Theory of Parallax Barriers}, 
  year={1952},
  volume={59},
  number={1},
  pages={11-21},
  abstract={The parallax barrier, which is a type of selective masking device now being applied in color television and in stereoscopic imagery, is discussed. A brief history along with the principle and geometric relationship underlying its operation is given. Various systems employing two or more image elements per aperture and utilizing the maximum image area are described. It is also shown that nonplanar and nonparallel arrangements are possible, and that plane barrier surfaces may be coupled to nonplanar image surfaces. Furthermore, lenses may replace the mechanical-type barriers resulting in a more light-efficient system. Formulae are presented and specific applications to multiple-color television tubes are discussed.},
  keywords={},
  doi={10.5594/J01178},
  ISSN={0898-042X},
  month={July},
}

@INPROCEEDINGS{8885576,
  author={Danu, Manuela and Nita, Cosmin-Ioan and Vizitiu, Anamaria and Suciu, Constantin and Itu, Lucian Mihai},
  booktitle={2019 23rd International Conference on System Theory, Control and Computing (ICSTCC)}, 
  title={Deep learning based generation of synthetic blood vessel surfaces}, 
  year={2019},
  volume={},
  number={},
  pages={662-667},
  doi={10.1109/ICSTCC.2019.8885576}}


@INPROCEEDINGS{10049010,
  author={Nguyen, Loc X. and Sone Aung, Pyae and Le, Huy Q. and Park, Seong-Bae and Hong, Choong Seon},
  booktitle={2023 International Conference on Information Networking (ICOIN)}, 
  title={A New Chapter for Medical Image Generation: The Stable Diffusion Method}, 
  year={2023},
  volume={},
  number={},
  pages={483-486},
  doi={10.1109/ICOIN56518.2023.10049010}
}

@article{Ratcliff2010,
abstract = {The authors report 9 new experiments and reanalyze 3 published experiments that investigate factors affecting the time course of perceptual processing and its effects on subsequent decision making. Stimuli in letter-discrimination and brightness-discrimination tasks were degraded with static and dynamic noise. The onset and the time course of decision making were quantified by fitting the data with the diffusion model. Dynamic noise and, to a lesser extent, static noise, produced large shifts in the leading edge of the response-time distribution in letter discrimination but had little effect in brightness discrimination. The authors interpret these shifts as changes in the onset of decision making. The different pattern of shifts in letter discrimination and brightness discrimination implies that decision making in the 2 tasks was affected differently by noise. The changes in response-time distributions found with letter stimuli are inconsistent with the hypothesis that noise increases response times to letter stimuli simply by reducing the rate at which evidence accumulates in the decision process. Instead, they imply that noise also delays the time at which evidence accumulation begins. The delay is shown not to be the result of strategic processes or the result of using different stimuli in different tasks. The results imply, rather, that the onset of evidence accumulation in the decision process is time-locked to the perceptual encoding of the stimulus features needed to do the task. Two mechanisms that could produce this time-locking are described. {\textcopyright} 2010 American Psychological Association.},
author = {Ratcliff, Roger and Smith, Philip L.},
doi = {10.1037/a0018128},
file = {:C\:/Users/adminuser/Downloads/nihms185190.pdf:pdf},
issn = {00963445},
journal = {Journal of Experimental Psychology: General},
keywords = {diffusion model,dynamic noise,letter discrimination,perception,static noise},
mendeley-groups = {NoiseGeneration},
number = {1},
pages = {70--94},
pmid = {20121313},
title = {{Perceptual Discrimination in Static and Dynamic Noise: The Temporal Relation Between Perceptual Encoding and Decision Making}},
volume = {139},
year = {2010}
}

@article{Langner2008,
abstract = {Four dimensional (4D) computed tomography (CT) image sorting is currently a retrospective procedure. Mismatches in displacement and/or phase of a patient's respiratory signal, corresponding with two dimensional images taken at subsequent couch positions, become visible as artifacts in reconstructed 4D CT images. These artifacts appear as undefined or irregular boundaries in the 4D CT images and cause systematic errors in patient contouring and dose calculations. In addition, the substantially higher dose required for 4D CT, compared with 3D CT, is of concern. To minimize these problems, we developed a prospective respiratory displacement and velocity based cine 4D CT (PDV CT) method to trigger image acquisition if the displacement and velocity of the respiratory signal occurred within predetermined tolerances simultaneously. The use of velocity avoids real-time phase estimation. Real-time image acquisition ensures data sufficiency, while avoiding the need for redundant data. This may potentially result in a lower dose to the patient. PDV CT was compared with retrospective 4D CT acquisition methods, using respiratory signals of 24 lung cancer patients (103 sessions) under free breathing conditions. Image acquisition was simulated for each of these sessions from the respiratory signal. The root mean square (RMS) of differences between displacements and velocities of the respiratory signal corresponding to subsequent images was calculated in order to evaluate the image-sorting accuracy of each method. Patient dose reductions of 22 to 50\% were achieved during image acquisition depending on the model parameters chosen. The mean RMS differences over all sessions and image bins show that PDV CT produces similar results to retrospective displacement sorting overall, although improvements of the RMS difference up to 20\% were achieved depending on the model parameters chosen. Velocity RMS differences improved between 30 and 45\% when compared with retrospective phase sorting. The efficiency in acquisition compared with retrospective phase sorting varied from ∼10\% for displacement and velocity tolerances of 1 mm and 4 mms, respectively, to 80 to 93\% for 4 mm and 4 mms. The lower variation in the displacement and velocity of the respiratory signal in each image bin indicates that PDV CT could be a valuable tool for reducing artifacts in 4D CT images and lowering patient dose, although the cost may be increased acquisition time. {\textcopyright} 2008 American Association of Physicists in Medicine.},
author = {Langner, U. W. and Keall, P. J.},
doi = {10.1118/1.2977539},
file = {:C\:/Users/adminuser/Downloads/MPHYA6-000035-004501_1.pdf:pdf},
issn = {00942405},
journal = {Medical Physics},
keywords = {4D CT,Artifacts,Displacement sorting,Phase sorting},
mendeley-groups = {Medical Papers For Justication of Study 2/4D},
number = {10},
pages = {4501--4512},
pmid = {18975697},
title = {{Prospective displacement and velocity-based cine 4D CT}},
volume = {35},
year = {2008}
}

@article{Laha2016,
abstract = {Empirical findings from studies in one scientific domain have very limited applicability to other domains, unless we formally establish deeper insights on the generalizability of task types. We present a domain-independent classification of visual analysis tasks with volume visualizations. This taxonomy will help researchers design experiments, ensure coverage, and generate hypotheses in empirical studies with volume datasets. To develop our taxonomy, we first interviewed scientists working with spatial data in disparate domains. We then ran a survey to evaluate the design participants in which were scientists and professionals from around the world, working with volume data in various scientific domains. Respondents agreed substantially with our taxonomy design, but also suggested important refinements. We report the results in the form of a goal-based generic categorization of visual analysis tasks with volume visualizations. Our taxonomy covers tasks performed with a wide variety of volume datasets.},
author = {Laha, Bireswar and Bowman, Doug A. and Laidlaw, David H. and Socha, John J.},
doi = {10.1109/SciVis.2015.7429485},
file = {:D\:/Thomas/Downloads/laha2015.pdf:pdf},
isbn = {9781467397858},
journal = {2015 IEEE Scientific Visualization Conference, SciVis 2015 - Proceedings},
keywords = {3D Interaction,Empirical Evaluation,Scientific Visualization,Task Taxonomy,Virtual Reality,Volume Visualization},
mendeley-groups = {VolumeRendering},
pages = {1--8},
title = {{A classification of user tasks in visual analysis of volume data}},
volume = {D},
year = {2016}
}

@article{Coskun2022,
author = {Coşkun, {\"{O}}zge and {Nteli Chatzioglou}, Gkionoul and {\"{O}}zt{\"{u}}rk, Adnan},
doi = {10.1007/s00381-020-04748-7},
file = {:C\:/Users/adminuser/Downloads/s00381-020-04748-7.pdf:pdf},
issn = {14330350},
journal = {Child's Nervous System},
number = {8},
pages = {1421--1423},
pmid = {32583148},
title = {{Henry Gray (1827–1861): the great author of the most widely used resource in medical education}},
volume = {38},
year = {2022}
}

@book{gray1877anatomy,
  title={Anatomy},
  author={Gray, Henry},
  year={1877},
  publisher={Chrysalis Books plc}
}

@article{Wijayanto2023,
abstract = {Virtual Reality (VR) is well-known for its use in interdisciplinary applications and research. The visual representation of these applications could vary depending in their purpose and hardware limitation, and in those situations could require an accurate perception of size for task performance. However, the relationship between size perception and visual realism in VR has not yet been explored. In this contribution, we conducted an empirical evaluation using a between-subject design over four conditions of visual realism, namely Realistic, Local Lighting, Cartoon, and Sketch on size perception of target objects in the same virtual environment. Additionally, we gathered participants' size estimates in the real world via a within-subject session. We measured size perception using concurrent verbal reports and physical judgments. Our result showed that although participants' size perception was accurate in the realistic condition, surprisingly they could still tune into the invariant but meaningful information in the environment to accurately estimate the size of targets in the non-photorealistic conditions as well. We additionally found that size estimates in verbal and physical responses were generally different in real world and VR viewing and were moderated by trial presentation over time and target object widths.},
author = {Wijayanto, Ignatius Alex and Babu, Sabarish V. and Pagano, Christopher C. and Chuang, Jung Hong},
doi = {10.1109/TVCG.2023.3247109},
file = {:D\:/Thomas/Comparing_the_Effects_of_Visual_Realism_on_Size_Perception_in_VR_versus_Real_World_Viewing_through_Physical_and_Verbal_Judgments.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Empirical evaluation,Render Style,Size perception,User Studies,Virtual reality},
mendeley-groups = {NonPhotorealisticRendering},
number = {5},
pages = {2721--2731},
publisher = {IEEE},
title = {{Comparing the Effects of Visual Realism on Size Perception in VR versus Real World Viewing through Physical and Verbal Judgments}},
volume = {29},
year = {2023}
}

@article{Bui2015,
abstract = {In this study, we propose a novel and efficient method to generate 3D-look shade illustrations from simple sketches having outlines and hatching strokes. The basic idea is to use the contours and hatching strokes specified by a designer as a metaphor for constructing a normal field for 3D-look shading effects. The overall shape of a 3D object is depicted with contours, such as silhouette contours and feature contours related to geometric ridges and valleys. The shading effects are achieved by means of hatching strokes, which are usually along the principal directions of the object's local geometry. In our system, users first sketch an object's outline and can draw a set of hatching strokes over its interior regions. Next, the system estimates the 3D normals of the points on the contours and hatching strokes. A smooth normal field is then generated by interpolating the estimated normals with the diffusion processes. In addition, our system provides an intuitive method for adjusting normal transitions from edited to untouched regions using an alpha map. Finally, the texture coordinates of each pixel belonging to the drawn objects are computed based on the normal vector field so that the objects can be rendered in various 3D-look shadings, with changes for different materials and lighting. Experimental results show that our system can easily and effectively generate illustrations having 3D-look shading effects from various 2D sketches drawn by users.},
author = {Bui, Minh Tuan and Kim, Junho and Lee, Yunjin},
doi = {10.1016/j.cag.2015.05.026},
file = {:D\:/Thomas/1-s2.0-S0097849315000734-main.pdf:pdf},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {2D sketches,3D normal estimation,3D-look shading,Non-photorealistic rendering},
mendeley-groups = {NonPhotorealisticRendering},
pages = {167--176},
title = {{3D-look shading from contours and hatching strokes}},
volume = {51},
year = {2015}
}

@article{Yuan2005,
abstract = {We present a novel non-photorealistic rendering method that performs\nall operations in geometry-image domain. We first apply global conformal\nparameterization to the input geometry model and generate corresponding\ngeometry images. Strokes and silhouettes are then computed in the\ngeometry-image domain. The geometry-image space provides combined\nbenefits of the existing image space and object space approaches.\nIt allows us to take advantage of the regularity of 2D images and\nyet still have full access to the object geometry information. A\nwide range of image processing tools can be leveraged to assist various\noperations involved in achieving non-photorealistic rendering with\ncoherence.},
author = {Yuan, Xiaoru and Nguyen, Minh X and Zhang, Nan and Chen, Baoquan},
doi = {10.2312/EGWR/EGSR05/193-200},
file = {:D\:/Thomas/Stippling_and_Silhouettes_Rendering_in_Geometry-Im.pdf:pdf},
isbn = {3-905673-23-1},
journal = {Proceedings of Eurographics Symposium on Rendering 2005 (EGSR'05, June 29--July 1, 2005, Konstanz, Germany)},
keywords = {Non-Photorealistic Rendering, Stippling, Silhouett,age,geometry im-,non-photorealistic rendering,sampling,silhouette,stippling},
mendeley-groups = {NonPhotorealisticRendering},
number = {May},
pages = {193--200},
title = {{Stippling and Silhouettes Rendering in Geometry-Image Space}},
url = {http://www-users.cs.umn.edu/$\sim$xyuan/research/publication/egsr05.htm},
year = {2005}
}

@article{Busking2007,
abstract = {Non-photorealistic (NPR) techniques are usually applied to produce stylistic renderings. However, they can also be useful to visualize scientific datasets. NPR techniques are often able to simplify data, producing clearer images than traditional photorealistic methods. We propose a framework for visualizing volume datasets using non-photorealistic techniques. Our framework is based on particle systems, with user-selectable rules affecting properties of the particles such as position and appearance. The techniques presented do not require the generation of explicit intermediary surfaces. Furthermore, the framework is versatile enough to produce a variety of illustrative techniques within the same framework.},
author = {Busking, Stef and Vilanova, Anna and van Wijk, Jarke J},
file = {:D\:/Thomas/Downloads/VolumeFlies_Illustrative_Volume_Visualization_usin.pdf:pdf},
journal = {Thirteenth Annual Conference of the Advanced School for Computing and Imaging, Proceedings},
keywords = {non-photorealistic rendering,particle systems,visualization,volume rendering},
mendeley-groups = {NonPhotorealisticRendering},
number = {September},
pages = {51--58},
title = {{VolumeFlies: Illustrative Volume Visualization using Particles}},
url = {http://graphics.tudelft.nl/$\sim$stef/publications/Busking2007-ASCI.pdf},
volume = {Vi},
year = {2007}
}

@article{Pelt2008,
abstract = {Illustrative techniques are generally applied to produce stylized\nrenderings. Various illustrative styles have been applied to volumetric\ndata sets, producing clearer images and effectively conveying visual\ninformation. We adopt user-configurable particle systems to produce\nstylized renderings from the volume data, imitating traditional pen-and-ink\ndrawings. In the following, we present an interactive GPU-based illustrative\nframework, called VolFliesGPU, for rendering volume data, exploiting\nparallelism in both graphics hardware and particle systems. We achieve\nreal-time interaction and prompt parametrization of the illustrative\nstyles, using an intuitive GPGPU paradigm that delivers the computational\npower to drive our particle system and visualization algorithms.},
author = {Pelt, Roy Van and Vilanova, Anna and Wetering, Huub Van De},
file = {:D\:/Thomas/Downloads/GPU-basedParticleSystemsForIllustravtiveVolumeRendering.pdf:pdf},
journal = {Volume- and Point-based Graphics},
mendeley-groups = {NonPhotorealisticRendering},
pages = {2--9},
title = {{GPU-based Particle Systems for Illustrative Volume Rendering}},
volume = {vi},
year = {2008}
}

@article{Kim2008,
abstract = {This paper presents an automatic method for producing stipple renderings from photographs, following the style of professional hedcut illustrations. For effective depiction of image features, we introduce a novel dot placement algorithm which adapts stipple dots to the local shapes. The core idea is to guide the dot placement along 'feature flow' extracted from the feature lines, resulting in a dot distribution that conforms to feature shapes. The sizes of dots are adaptively determined from the input image for proper tone representation. Experimental results show that such feature-guided stippling leads to the production of stylistic and feature-emphasizing dot illustrations. {\textcopyright} 2008 The Eurographics Association and Blackwell Publishing Ltd.},
author = {Kim, Dongyeon and Son, Minjung and Lee, Yunjin and Kang, Henry and Lee, Seungyong},
doi = {10.1111/j.1467-8659.2008.01259.x},
file = {:D\:/Thomas/Downloads/kang_egsr08.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
mendeley-groups = {NonPhotorealisticRendering/stippling},
number = {4},
pages = {1209--1216},
title = {{Feature-guided image stippling}},
volume = {27},
year = {2008}
}

@inproceedings{Lum2002,
abstract = {Non-photorealistic rendering can be used to illustrate subtle spatial relationships that might not be visible with more realistic rendering techniques. We present a parallel hardware-accelerated rendering technique, making extensive use of multi-texturing and paletted textures, for the interactive non-photorealistic visualization of scalar volume data. With this technique, we can render a 512x512x512 volume using non-photorealistic techniques that include tone-shading, silhouettes, gradient-based enhancement, and color depth cueing, as shown in the images on the color plate, at multiple frames second. The interactivity we achieve with our method allows for the exploration of a large visualization parameter space for the creation of effective illustrations.},
author = {Lum, Eric B. and Ma, Kwan-Liu},
booktitle = {Association for Computing Machinery},
doi = {10.1145/508530.508542},
file = {:D\:/Thomas/Downloads/qt20j9c13z_noSplash_5e31d19134ad71e32284fd3d215d1327.pdf:pdf},
keywords = {function specification requires a,great deal of fine,interactive visualization,making,non-photorealistic rendering,parallel rendering,scientific visualization,silhouette,sual perception,texture graphics hardware,tuning,vi-,volume rendering},
mendeley-groups = {NonPhotorealisticRendering},
pages = {67--74},
title = {{Hardware-Accelerated Parallel Non-Photorealistic Volume Rendering}},
url = {https://doi.org/10.1145/508530.508542},
year = {2002}
}

@article{Lawonn2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.03605v1},
author = {Lawonn, Kai},
eprint = {arXiv:1501.03605v1},
file = {:C\:/Users/adminuser/Downloads/Feature_Lines_for_Illustrating_Medical_Surface_Mod.pdf:pdf},
mendeley-groups = {NonPhotorealisticRendering},
number = {January},
title = {{Feature Lines for Illustrating Medical Surface Models: Mathematical Background Feature Lines for Illustrating Medical Surface}},
year = {2015},
journal = {Visualization in Medicine and Life Sciences},
}

@article{Preim2005,
abstract = {Emphasis and focussing techniques are designed to consider the importance\nof objects within the rendering process. Such techniques are essential\nin medical education and therapy planning systems. Non-photorealistic\nrendering techniques, such as silhouette lines, are essential in\nthis context as they convey the shape of objects without occluding\nlarge areas. We discuss emphasis and focussing by means of NPR in\nmedical visualizations. We introduce techniques which emphasize a\nselected object in 2d as well as 3d visualizations.},
author = {Preim, Bernhard and Tietjen, Christian and D{\"{o}}rge, Christina},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preim, Tietjen, D{\"{o}}rge - 2005 - NPR, Focussing and Emphasis in Medical Visualizations.pdf:pdf},
journal = {Simulation und Visualisierung 2005},
keywords = {emphasis techniques,non-photorealistic rendering},
mendeley-groups = {NonPhotorealisticRendering},
number = {January},
pages = {139--152},
title = {{NPR, Focussing and Emphasis in Medical Visualizations}},
year = {2005}
}


@article{Joshi2008,
abstract = {The effective visualization of vascular structures is critical for diagnosis, surgical planning as well as treatment evaluation. In recent work, we have developed an algorithm for vessel detection that examines the intensity profile around each voxel in an angiographic image and determines the likelihood that any given voxel belongs to a vessel; we term this the "vesselness coefficient" of the voxel. Our results show that our algorithm works particularly well for visualizing branch points in vessels. Compared to standard Hessian based techniques, which are fine-tuned to identify long cylindrical structures, our technique identifies branches and connections with other vessels. Using our computed vesselness coefficient, we explore a set of techniques for visualizing vasculature. Visualizing vessels is particularly challenging because not only is their position in space important for clinicians but it is also important to be able to resolve their spatial relationship. We applied visualization techniques that provide shape cues as well as depth cues to allow the viewer to differentiate between vessels that are closer from those that are farther. We use our computed vesselness coefficient to effectively visualize vasculature in both clinical neurovascular x-ray computed tomography based angiography images, as well as images from three different animal studies. We conducted a formal user evaluation of our visualization techniques with the help of radiologists, surgeons, and other expert users. Results indicate that experts preferred distance color blending and tone shading for conveying depth over standard visualization techniques. {\textcopyright} 2008 IEEE.},
author = {Joshi, Alark and Qian, Xiaoning and Dione, Donald P. and Bulsara, Ketan R. and Breuer, Christopher K. and Sinusas, Albert J. and Papademetris, Xenophon},
doi = {10.1109/TVCG.2008.123},
file = {:D\:/Thomas/Effective_visualization_of_complex_vascular_structures_using_a_non-parametric_vessel_detection_method.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Evaluation of visualization techniques,Vessel identification,Vessel visualization},
mendeley-groups = {NonPhotorealisticRendering},
number = {6},
pages = {1603--1610},
pmid = {18989016},
title = {{Effective visualization of complex vascular structures using a non-parametric vessel detection method}},
volume = {14},
year = {2008}
}


@article{Svakhine2009,
abstract = {Volume illustration can be used to provide insight into source data from CT/MRI scanners in much the same way as medical illustration depicts the important details of anatomical structures. As such, proven techniques used in medical illustration should be transferable to volume illustration, providing scientists with new tools to visualize their data. In recent years, a number of techniques have been developed to enhance the rendering pipeline and create illustrative effects similar to the ones found in medical textbooks and surgery manuals. Such effects usually highlight important features of the subject while subjugating its context and providing depth cues for correct perception. Inspired by traditional visual and line-drawing techniques found in medical illustration, we have developed a collection of fast algorithms for more effective emphasis/de-emphasis of data as well as conveyance of spatial relationships. Our techniques utilize effective outlining techniques and selective depth enhancement to provide perceptual cues of object importance as well as spatial relationships in volumetric datasets. Moreover, we have used illustration principles to effectively combine and adapt basic techniques so that they work together to provide consistent visual information and a uniform style. {\textcopyright} 2006 IEEE.},
author = {Svakhine, N. A. and Ebert, D. S. and Andrews, W. M.},
doi = {10.1109/TVCG.2008.56},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Svakhine, Ebert, Andrews - 2009 - Illustration-inspired depth enhanced volumetric medical visualization.pdf:pdf},
isbn = {2007090139},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Illustrative techniques,Interactive volume illustration,Nonphotorealistic rendering,Volume rendering},
mendeley-groups = {VolumeRendering/Illustrations},
number = {1},
pages = {77--86},
pmid = {19008557},
publisher = {IEEE},
title = {{Illustration-inspired depth enhanced volumetric medical visualization}},
volume = {15},
year = {2009}
}

@article{Bruckner2006,
abstract = {In volume rendering, it is very difficult to simultaneously visualize interior and exterior structures while preserving clear shape cues. Highly transparent transfer functions produce cluttered images with many overlapping structures, while clipping techniques completely remove possibly important context information. In this paper, we present a new model for volume rendering, inspired by techniques from illustration. It provides a means of interactively inspecting the interior of a volumetric data set in a feature-driven way which retains context information. The context-preserving volume rendering model uses a function of shading intensity, gradient magnitude, distance to the eye point, and previously accumulated opacity to selectively reduce the opacity in less important data regions. It is controlled by two user-specified parameters. This new method represents an alternative to conventional clipping techniques, sharing their easy and intuitive user control, but does not suffer from the drawback of missing context information. {\textcopyright} 2006 IEEE.},
author = {Bruckner, Stefan and Grimm, S{\"{o}}ren and Kanitsar, Armin and Gr{\"{o}}ller, M. Eduard},
doi = {10.1109/TVCG.2006.96},
file = {:D\:/Thomas/Downloads/Illustrative_Context-Preserving_Exploration_of_Volume_Data.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Focus+context techniques,Illustrative visualization,Volume rendering},
mendeley-groups = {VolumeRendering},
number = {6},
pages = {1559--1569},
pmid = {17073377},
title = {{Illustrative context-preserving exploration of volume data}},
volume = {12},
year = {2006}
}


@article{Zheng2013,
abstract = {Visualizing complex volume data usually renders selected parts of the volume semitransparently to see inner structures of the volume or provide a context. This presents a challenge for volume rendering methods to produce images with unambiguous depth-ordering perception. Existing methods use visual cues such as halos and shadows to enhance depth perception. Along with other limitations, these methods introduce redundant information and require additional overhead. This paper presents a new approach to enhancing depth-ordering perception of volume rendered images without using additional visual cues. We set up an energy function based on quantitative perception models to measure the quality of the images in terms of the effectiveness of depth-ordering and transparency perception as well as the faithfulness of the information revealed. Guided by the function, we use a conjugate gradient method to iteratively and judiciously enhance the results. Our method can complement existing systems for enhancing volume rendering results. The experimental results demonstrate the usefulness and effectiveness of our approach. {\textcopyright} 1995-2012 IEEE.},
author = {Zheng, Lin and Wu, Yingcai and Ma, Kwan Liu},
doi = {10.1109/TVCG.2012.144},
file = {:D\:/Thomas/Downloads/Perceptually-Based_Depth-Ordering_Enhancement_for_Direct_Volume_Rendering.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Volume rendering,depth ordering,depth perception,transparency,visualization},
mendeley-groups = {DepthPerceptionPapers,VoxelAlgorithms},
number = {3},
pages = {446--459},
pmid = {22732679},
publisher = {IEEE},
title = {{Perceptually-based depth-ordering enhancement for direct volume rendering}},
volume = {19},
year = {2013}
}


@article{Laha2012,
abstract = {Volume visualization has been widely used for decades for analyzing datasets ranging from 3D medical images to seismic data to paleontological data. Many have proposed using immersive virtual reality (VR) systems to view volume visualizations, and there is anecdotal evidence of the benefits of VR for this purpose. However, there has been very little empirical research exploring the effects of higher levels of immersion for volume visualization, and it is not known how various components of immersion influence the effectiveness of visualization in VR. We conducted a controlled experiment in which we studied the independent and combined effects of three components of immersion (head tracking, field of regard, and stereoscopic rendering) on the effectiveness of visualization tasks with two xray microscopic computed tomography datasets. We report significant benefits of analyzing volume data in an environment involving those components of immersion. We find that the benefits do not necessarily require all three components simultaneously, and that the components have variable influence on different task categories. The results of our study improve our understanding of the effects of immersion on perceived and actual task performance, and provide guidance on the choice of display systems to designers seeking to maximize the effectiveness of volume visualization applications. {\textcopyright} 2012 IEEE.},
author = {Laha, Bireswar and Sensharma, Kriti and Schiffbauer, James D. and Bowman, Doug A.},
doi = {10.1109/TVCG.2012.42},
file = {:D\:/Thomas/Downloads/Effects_of_Immersion_on_Visual_Analysis_of_Volume_Data.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D visualization,CAVE,Immersion,data analysis,micro-CT,virtual environments,virtual reality,volume visualization},
mendeley-groups = {DataVis},
number = {4},
pages = {597--606},
pmid = {22402687},
publisher = {IEEE},
title = {{Effects of immersion on visual analysis of volume data}},
volume = {18},
year = {2012}
}


@article{Laha2014,
abstract = {Volume visualization is an important technique for analyzing datasets from a variety of different scientific domains. Volume data analysis is inherently difficult because volumes are three-dimensional, dense, and unfamiliar, requiring scientists to precisely control the viewpoint and to make precise spatial judgments. Researchers have proposed that more immersive (higher fidelity) VR systems might improve task performance with volume datasets, and significant results tied to different components of display fidelity have been reported. However, more information is needed to generalize these results to different task types, domains, and rendering styles. We visualized isosurfaces extracted from synchrotron microscopic computed tomography (SR-?CT) scans of beetles, in a CAVE-like display. We ran a controlled experiment evaluating the effects of three components of system fidelity (field of regard, stereoscopy, and head tracking) on a variety of abstract task categories that are applicable to various scientific domains, and also compared our results with those from our prior experiment using 3D texture-based rendering. We report many significant findings. For example, for search and spatial judgment tasks with isosurface visualization, a stereoscopic display provides better performance, but for tasks with 3D texture-based rendering, displays with higher field of regard were more effective, independent of the levels of the other display components. We also found that systems with high field of regard and head tracking improve performance in spatial judgment tasks. Our results extend existing knowledge and produce new guidelines for designing VR systems to improve the effectiveness of volume data analysis. {\textcopyright} 2014 IEEE.},
author = {Laha, Bireswar and Bowman, Doug A. and Socha, John J.},
doi = {10.1109/TVCG.2014.20},
file = {:D\:/Thomas/Downloads/2014_Laha_et_al.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {3D visualization,CAVE,Immersion,data analysis,micro-CT,virtual environments,virtual reality,volume visualization},
mendeley-groups = {DataVis},
number = {4},
pages = {513--522},
title = {{Effects of VR system fidelity on analyzing isosurface visualization of volume datasets}},
volume = {20},
year = {2014}
}

@article{Laha2012a,
abstract = {Researchers have traditionally used desktop display systems for visualizing and analyzing volume data. This is partially due to the lack of empirical results showing benefits of immersion for analysis of volume data, and also due to the cost of highly immersive virtual reality (VR) platforms. Researchers exploring the benefits of immersion tend to compare entire display systems rather than evaluating the benefits of individual components of immersion. The VR community needs controlled experimentation to gather empirical data on the benefits of individual components of immersion. In order to generalize the results to a variety of domains, a taxonomy that classifies tasks performed with volume data into general categories is also needed. In our work, we have developed a preliminary task taxonomy and are performing studies to identify the effects of various components of immersion on volume data analysis tasks. Keywords-Task taxonomy, benefits of immersion, volume visualization, controlled experiments, virtual reality, virtual environments, immersive visualization.},
author = {Laha, Bireswar and Bowman, Doug A},
file = {:D\:/Thomas/Downloads/10.1.1.452.3078.pdf:pdf},
journal = {Immersive Visualization Revisited Workshop of the IEEE VR conference},
keywords = {Task taxonomy benefits of immersion,controlled experiments,environments,immersion for analyzing visualizations,immersive visualization,of different levels of,task taxonomy,virtual,virtual environments,virtual reality,visualization,volume,volume visualization},
mendeley-groups = {DataVis},
number = {March},
pages = {1--2},
title = {{Identifying the Benefits of Immersion in Virtual Reality for Volume Data Visualization}},
volume = {D},
year = {2012}
}



@article{Laha2013,
abstract = {—In our research agenda to study the effects of immersion (level of fidelity) on various tasks in virtual reality (VR) systems, we have found that the most generalizable findings come not from direct comparisons of different technologies, but from controlled simulations of those technologies. We call this the mixed reality (MR) simulation approach. However, the validity of MR simulation, especially when different simulator platforms are used, can be questioned. In this paper, we report the results of an experiment examining the effects of field of regard (FOR) and head tracking on the analysis of volume visualized micro-CT datasets, and compare them with those from a previous study. The original study used a CAVE-like display as the MR simulator platform, while the present study used a high-end head-mounted display (HMD). Out of the 24 combinations of system characteristics and tasks tested on the two platforms, we found that the results produced by the two different MR simulators were similar in 20 cases. However, only one of the significant effects found in the original experiment for quantitative tasks was reproduced in the present study. Our observations provide evidence both for and against the validity of MR simulation, and give insight into the differences caused by different MR simulator platforms. The present experiment also examined new conditions not present in the original study, and produced new significant results, which confirm and extend previous existing knowledge on the effects of FOR and head tracking. We provide design guidelines for choosing display systems that can improve the effectiveness of volume visualization applications.},
author = {Laha, Bireswar and Bowman, Doug A. and Schiffbauer, James D.},
file = {:C\:/Users/adminuser/Downloads/Validation_of_the_MR_Simulation_Approach_for_Evaluating_the_Effects_of_Immersion_on_Visual_Analysis_of_Volume_Data.pdf:pdf},
journal = {IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS},
mendeley-groups = {DataVis},
number = {4},
pages = {529 -- 538},
title = {{Validation of the MR Simulation Approach for Evaluating the Effects of Immersion on Visual Analysis of Volume Data}},
volume = {9},
year = {2013}
}

@incollection{Munzner2014,
abstract = {The highest-level actions are to use vis to consume or produce information. The cases for consuming are to present, to discover, and to enjoy; discovery may involve generating or verifying a hypothesis. The high-level choices describe how the vis is being used to analyze, either to consume existing data or to also produce additional data. The mid-level choices cover what kind of search is involved, in terms of whether the target and location are known or not. At the highest level, the framework distinguishes between two possible goals of people who want to analyze data using a vis tool: users might want only to consume existing information or also to actively produce new information. A crucial aspect of presentation is that the knowledge communicated is already known to the presenter in advance. Sometimes the presenter knows it before using vis at all and uses the vis only for communication.},
address = {New York, NY, USA},
author = {Munzner, Tamara},
booktitle = {Visual Analysis and Design},
chapter = {3},
edition = {1},
file = {:C\:/Users/adminuser/Downloads/10.1201_b17511-3_chapterpdf.pdf:pdf;:C\:/Users/adminuser/Downloads/TaskAbstraction (3).pdf:pdf},
mendeley-groups = {GraphPapers},
pages = {43--62},
publisher = {CRC Press},
title = {{Why: Task Abstraction}},
url = {https://doi.org/10.1201/b17511},
year = {2014}
}

@article{Yoghourdjian2018,
abstract = {For decades, researchers in information visualisation and graph drawing have focused on developing techniques for the layout and display of very large and complex networks. Experiments involving human participants have also explored the readability of different styles of layout and representations for such networks. In both bodies of literature, networks are frequently referred to as being ‘large' or ‘complex', yet these terms are relative. From a human-centred, experiment point-of-view, what constitutes ‘large' (for example) depends on several factors, such as data complexity, visual complexity, and the technology used. In this paper, we survey the literature on human-centred experiments to understand how, in practice, different features and characteristics of node–link diagrams affect visual complexity.},
archivePrefix = {arXiv},
arxivId = {1809.00270},
author = {Yoghourdjian, Vahan and Archambault, Daniel and Diehl, Stephan and Dwyer, Tim and Klein, Karsten and Purchase, Helen C. and Wu, Hsiang Yun},
doi = {10.1016/j.visinf.2018.12.006},
eprint = {1809.00270},
file = {:D\:/Thomas/Downloads/1809.00270.pdf:pdf},
issn = {2468502X},
journal = {Visual Informatics},
keywords = {Cognitive scalability,Empirical studies,Evaluations,Graph visualisation,Network visualisation,node–link diagrams},
mendeley-groups = {DataVis},
number = {4},
pages = {264--282},
title = {{Exploring the limits of complexity: A survey of empirical studies on graph visualisation}},
volume = {2},
year = {2018}
}

@article{Oren2020,
abstract = {Artificial intelligence (AI) is a disruptive technology that involves the use of computerised algorithms to dissect complicated data. Among the most promising clinical applications of AI is diagnostic imaging, and mounting attention is being directed at establishing and fine-tuning its performance to facilitate detection and quantification of a wide array of clinical conditions. Investigations leveraging computer-aided diagnostics have shown excellent accuracy, sensitivity, and specificity for the detection of small radiographic abnormalities, with the potential to improve public health. However, outcome assessment in AI imaging studies is commonly defined by lesion detection while ignoring the type and biological aggressiveness of a lesion, which might create a skewed representation of AI's performance. Moreover, the use of non-patient-focused radiographic and pathological endpoints might enhance the estimated sensitivity at the expense of increasing false positives and possible overdiagnosis as a result of identifying minor changes that might reflect subclinical or indolent disease. We argue for refinement of AI imaging studies via consistent selection of clinically meaningful endpoints such as survival, symptoms, and need for treatment.},
author = {Oren, Ohad and Gersh, Bernard J. and Bhatt, Deepak L.},
doi = {10.1016/S2589-7500(20)30160-6},
file = {:D\:/Thomas/PIIS2589750020301606.pdf:pdf},
issn = {25897500},
journal = {The Lancet Digital Health},
mendeley-groups = {RadiologyAndMRI},
number = {9},
pages = {e486--e488},
pmid = {33328116},
publisher = {The Author(s). Published by Elsevier Ltd. This is an Open Access article under the CC BY 4.0 license},
title = {{Artificial intelligence in medical imaging: switching from radiographic pathological data to clinically meaningful endpoints}},
url = {http://dx.doi.org/10.1016/S2589-7500(20)30160-6},
volume = {2},
year = {2020}
}

@article{Hosny2018,
abstract = {Artificial intelligence (AI) algorithms, particularly deep learning, have demonstrated remarkable progress in image-recognition tasks. Methods ranging from convolutional neural networks to variational autoencoders have found myriad applications in the medical image analysis field, propelling it forward at a rapid pace. Historically, in radiology practice, trained physicians visually assessed medical images for the detection, characterization and monitoring of diseases. AI methods excel at automatically recognizing complex patterns in imaging data and providing quantitative, rather than qualitative, assessments of radiographic characteristics. In this Opinion article, we establish a general understanding of AI methods, particularly those pertaining to image-based tasks. We explore how these methods could impact multiple facets of radiology, with a general focus on applications in oncology, and demonstrate ways in which these methods are advancing the field. Finally, we discuss the challenges facing clinical implementation and provide our perspective on how the domain could be advanced.},
author = {Hosny, Ahmed and Parmar, Chintan and Quackenbush, John and Schwartz, Lawrence H and Aerts, Hugo J W L},
doi = {10.1038/s41568-018-0016-5},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosny et al. - 2018 - Artificial intelligence in radiology.pdf:pdf},
issn = {1474-1768},
journal = {Nature reviews. Cancer},
keywords = {*Algorithms,*Artificial Intelligence,*Radiology,Automated,Computer-Assisted,Deep Learning,Humans,Image Processing, Computer-Assisted,Machine Learning,Neoplasms/*diagnostic imaging,Pattern Recognition, Automated},
language = {eng},
mendeley-groups = {AIForMedicine},
month = {aug},
number = {8},
pages = {500--510},
title = {{Artificial intelligence in radiology}},
url = {https://www.ncbi.nlm.nih.gov/pubmed/29777175 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6268174/},
volume = {18},
year = {2018}
}

@article{Bongratz2022,
abstract = {The reconstruction of cortical surfaces from brain magnetic resonance imaging (MRI) scans is essential for quantitative analyses of cortical thickness and sulcal morphology. Although traditional and deep learning-based algorithmic pipelines exist for this purpose, they have two major drawbacks: lengthy runtimes of multiple hours (traditional) or intricate post-processing, such as mesh extraction and topology correction (deep learning-based). In this work, we address both of these issues and propose Vox2Cortex, a deep learning-based algorithm that directly yields topologically correct, three-dimensional meshes of the boundaries of the cortex. Vox2Cortex leverages convolutional and graph convolutional neural networks to deform an initial template to the densely folded geometry of the cortex represented by an input MRI scan. We show in extensive experiments on three brain MRI datasets that our meshes are as accurate as the ones reconstructed by state-of-the-art methods in the field, without the need for time- and resource-intensive post-processing. To accurately reconstruct the tightly folded cortex, we work with meshes containing about 168,000 vertices at test time, scaling deep explicit reconstruction methods to a new level.},
annote = {git hub: https://github.com/ai-med/Vox2Cortex},
archivePrefix = {arXiv},
arxivId = {2203.09446},
author = {Bongratz, Fabian and Rickmann, Anne-Marie and P{\"{o}}lsterl, Sebastian and Wachinger, Christian},
eprint = {2203.09446},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bongratz et al. - 2022 - Vox2Cortex Fast Explicit Reconstruction of Cortical Surfaces from 3D MRI Scans with Geometric Deep Neural Netwo.pdf:pdf},
mendeley-groups = {AIForMedicine},
pages = {1--16},
title = {{Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D MRI Scans with Geometric Deep Neural Networks}},
url = {http://arxiv.org/abs/2203.09446},
year = {2022}
}

@article{Cheung2021,
abstract = {The impact of the medical curricular reform on anatomy education has been inconclusive. A pervasive perception is that graduates do not possess a sufficient level of anatomical knowledge for safe medical practice; however, the reason is less well-studied. This qualitative study investigated the perceived challenges in learning anatomy, possible explanations, and ways to overcome these challenges. Unlike previous work, it explored the perceptions of multiple stakeholders in anatomy learning. Semi-structured interviews were conducted and the transcripts were analyzed by a grounded theory approach. Three main themes emerged from the data: (1) visualization of structures, (2) body of information, and (3) issues with curriculum design. The decreasing time spent in anatomy laboratories forced students to rely on alternative resources to learn anatomy but they lacked the opportunities to apply to human specimens, which impeded the “near” transfer of learning. The lack of clinical integration failed to facilitate the “far” transfer of learning. Learners also struggled to cope with the large amount of surface knowledge, which was pre-requisite to successful deep and transfer of learning. It was theorized that the perceived decline in anatomical knowledge was derived from this combination of insufficient surface knowledge and impeded “near” transfer resulting in impeded deep and “far” transfer of learning. Moving forward, anatomy learning should still be cadaveric-based coupled with complementary technological innovations that demonstrate “hidden” structures. A constant review of anatomical disciplinary knowledge with incremental integration of clinical contexts should also be adopted in medical curricula which could promote deep and far transfer of learning.},
author = {Cheung, Chun Chung and Bridges, Susan M. and Tipoe, George L.},
doi = {10.1002/ase.2071},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheung, Bridges, Tipoe - 2021 - Why is Anatomy Difficult to Learn The Implications for Undergraduate Medical Curricula.pdf:pdf},
issn = {19359780},
journal = {Anatomical Sciences Education},
keywords = {anatomy learning challenges,gross anatomy education,knowledge acquisition,medical education,transfer of learning,undergraduate medical curricula},
mendeley-groups = {3DVisulisationsVs2D},
number = {6},
pages = {752--763},
pmid = {33720515},
title = {{Why is Anatomy Difficult to Learn? The Implications for Undergraduate Medical Curricula}},
volume = {14},
year = {2021}
}

@article{Akpan2019,
abstract = {Research on the application of 3D visualization and virtual reality (VR) in discrete-event simulation (DES) has received increased attention in the past two decades. The increasing popularity of the 3D display in DES is mainly due to superior display capabilities and the associated benefits that it offers. However, the 2D display also continues to enjoy active use to date, thus provoking some fierce debates questioning the need for the 3D and VR if the 2D interface suffices. Several studies comparing the effectiveness of the different visualization methods also produce different conclusions. This paper undertakes a meta-analysis of the different positions and synthesizes the findings from 162 studies on the impacts of the 2D display versus 3D/VR on user performance on various DES tasks. The results highlight four key findings. First, the perception that the 2D display is more effective for model development is misleading as 3D/VR offers overall better performance and quality of models. Second, 3D/VR enables more effective performance than 2D display for model verification and validation. Third, 3D/VR decreases the time taken for verification, validation, experimentation, and analysis of results, but can increase model development time. Finally, the latent variables such as the application domains and nature of the problems tackled have no direct or indirect influence on the efficacy of the 3D display/VR versus 2D on DES task performance.},
author = {Akpan, Ikpe Justice and Shanker, Murali},
doi = {10.1177/0037549718757039},
file = {:D\:/Thomas/Downloads/0037549718757039.pdf:pdf},
issn = {17413133},
journal = {Simulation},
keywords = {3D visualization,discrete-event simulation,virtual reality,visual interactive simulation},
mendeley-groups = {Reviews,3DVisulisationsVs2D},
number = {2},
pages = {145--170},
title = {{A comparative evaluation of the effectiveness of virtual reality, 3D visualization and 2D visual interactive simulation: an exploratory meta-analysis}},
volume = {95},
year = {2019}
}

@article{Merino2018,
abstract = {Several usability issues (i.e., navigation, occlusion, selection, and text readability) affect the few 3D visualizations proposed to support developers on software engineering tasks. We observe that most 3D software visualizations are displayed on a standard computer screen, and hypothesize that displaying them in immersive augmented reality can help to (i) overcome usability issues of 3D visualizations, and (ii) increase their effectiveness to support software concerns. We investigate our hypothesis via a controlled experiment. In it, nine participants use 3D city visualizations displayed on a Microsoft HoloLens device to complete a set of software comprehension tasks. We further investigate our conjectures through an observational user study, in which the same participants of the experiment use a space-time cube visualization to analyze program executions. We collect data to (1) quantitatively analyze the effectiveness of visualizations in terms of user performance (i.e., completion time, correctness, and recollection), and user experience (i.e., difficulty, and emotions); and (2) qualitatively analyze how immersive augmented reality helps to overcome the limitations of 3D visualizations. We found that immersive augmented reality facilitates navigation and reduces occlusion, while performance is adequate, and developers obtain an outstanding experience. Selection and text readability still remain open issues.},
author = {Merino, Leonel and Bergel, Alexandre and Nierstrasz, Oscar},
doi = {10.1109/VISSOFT.2018.00014},
file = {:D\:/Thomas/Overcoming_Issues_of_3D_Software_Visualization_through_Immersive_Augmented_Reality.pdf:pdf},
isbn = {9781538682920},
journal = {Proceedings - 6th IEEE Working Conference on Software Visualization, VISSOFT 2018},
keywords = {case study,controlled experiment,hololens,immersive augmented reality,software comprehension,software visualization},
mendeley-groups = {3DVisulisationsVs2D},
pages = {54--64},
publisher = {IEEE},
title = {{Overcoming Issues of 3D Software Visualization through Immersive Augmented Reality}},
year = {2018}
}

@article{Cecotti2021,
abstract = {Fully immersive virtual reality systems allow the implementation of new immersive user interfaces and data visualization applications that offer effective ways to interact with multidimensional neuroimaging data. Learning to interpret 3D medical imaging scans can be an intimidating experience for students given the large range of pathologies to visualize and the relative placement(s) and location(s) of areas of interest, a process which is further complicated by interacting with, manipulating, and navigating through images across several possible views. Gamification and Serious Games (SG) have proved to have instructional benefits for teaching and learning. The adoption and use of virtual reality in education is growing, driven by the availability of more powerful and affordable virtual reality (VR) headsets, allowing novel ways to interact with and explore teaching material. This paper explores the use of VR to teach the fundamentals of medical imaging interpretation and the opportunities available in using immersive 3D visualizations to understand the spatial relationships between parts of the body, key organs, and their constituent parts. An integrated immersive learning environment is presented which orientates and guides the student through this process and assesses their subsequent level of understanding. The use of gamification in this context to increase student engagement and retention is discussed.},
author = {Cecotti, Hubert and Callaghan, Michael and Foucher, Benjamin and Joslain, Stevens},
doi = {10.1109/TALE52509.2021.9678721},
file = {:D\:/Thomas/Downloads/Serious_Game_for_Medical_Imaging_in_Fully_Immersive_Virtual_Reality.pdf:pdf},
isbn = {9781665436878},
journal = {TALE 2021 - IEEE International Conference on Engineering, Technology and Education, Proceedings},
keywords = {gamification,medical imaging,neuroimaging,virtual reality},
mendeley-groups = {VolumeRendering},
pages = {615--621},
publisher = {IEEE},
title = {{Serious Game for Medical Imaging in Fully Immersive Virtual Reality}},
year = {2021}
}

@article{Marconi2017,
abstract = {Background: In a preliminary experience, we claimed the potential value of 3D printing technology for pre-operative counseling and surgical planning. However, no objective analysis has ever assessed its additional benefit in transferring anatomical information from radiology to final users. We decided to validate the pre-operative use of 3D-printed anatomical models in patients with solid organs' diseases as a new tool to deliver morphological information. Methods: Fifteen patients scheduled for laparoscopic splenectomy, nephrectomy, or pancreatectomy were selected and, for each, a full-size 3D virtual anatomical object was reconstructed from a contrast-enhanced MDCT (Multiple Detector Computed Tomography) and then prototyped using a 3D printer. After having carefully evaluated—in a random sequence—conventional contrast MDCT scans, virtual 3D reconstructions on a flat monitor, and 3D-printed models of the same anatomy for each selected case, thirty subjects with different expertise in radiological imaging (10 medical students, 10 surgeons and 10 radiologists) were administered a multiple-item questionnaire. Crucial issues for the anatomical understanding and the pre-operative planning of the scheduled procedure were addressed. Results: The visual and tactile inspection of 3D models allowed the best anatomical understanding, with faster and clearer comprehension of the surgical anatomy. As expected, less experienced medical students perceived the highest benefit (53.9% ± 4.14 of correct answers with 3D-printed models, compared to 53.4 % ± 4.6 with virtual models and 45.5% ± 4.6 with MDCT), followed by surgeons and radiologists. The average time spent by participants in 3D model assessing was shorter (60.67 ± 25.5 s) than the one of the corresponding virtual 3D reconstruction (70.8 ± 28.18 s) or conventional MDCT scan (127.04 ± 35.91 s). Conclusions: 3D-printed models help to transfer complex anatomical information to clinicians, resulting useful in the pre-operative planning, for intra-operative navigation and for surgical training purposes.},
author = {Marconi, Stefania and Pugliese, Luigi and Botti, Marta and Peri, Andrea and Cavazzi, Emma and Latteri, Saverio and Auricchio, Ferdinando and Pietrabissa, Andrea},
doi = {10.1007/s00464-017-5457-5},
file = {:D\:/Thomas/Downloads/Marconi2017_Article_ValueOf3DPrintingForTheCompreh.pdf:pdf},
issn = {14322218},
journal = {Surgical Endoscopy},
keywords = {3D,Anatomy,Model,Printing,Surgery},
mendeley-groups = {DepthPerceptionMedical},
number = {10},
pages = {4102--4110},
pmid = {28281114},
publisher = {Springer US},
title = {{Value of 3D printing for the comprehension of surgical anatomy}},
volume = {31},
year = {2017}
}

@article{Martin2017,
abstract = {In this article we survey techniques for the digital simulation of hand-made stippling—one of the core techniques developed within non-photorealistic/expressive rendering. Over the years, a plethora of automatic or semi-automatic stippling algorithms have been proposed. As part of this expanding field of research, techniques have been developed that not only push the boundaries of traditional stippling but that also relate to other processes or techniques. Our general goal in this survey is thus to increase our understanding of both hand-made and computer-assisted stippling. For this purpose we not only provide an overview of the work on digital stippling but also examine its relationship to traditional stippling and to related fields such as halftoning. Finally, we propose several directions of future work in the field.},
author = {Mart{\'{i}}n, Domingo and Arroyo, Germ{\'{a}}n and Rodr{\'{i}}guez, Alejandro and Isenberg, Tobias},
doi = {10.1016/j.cag.2017.05.001},
file = {:C\:/Users/adminuser/Downloads/Martin_2017_SDS.pdf:pdf},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Halftoning,Hand-made stippling,NPR stippling,Stippling},
mendeley-groups = {NonPhotorealisticRendering/stippling},
number = {October},
pages = {24--44},
title = {{A survey of digital stippling}},
volume = {67},
year = {2017}
}

@article{Ritter2006,
abstract = {We present real-time vascular visualization methods, which extend on illustrative rendering techniques to particularly accentuate spatial depth and to improve the perceptive separation of important vascular properties such as branching level and supply area. The resulting visualization can and has already been used for direct projection on a patient's organ in the operation theater where the varying absorption and reflection characteristics of the surface limit the use of color. The important contributions of our work are a GPU-based hatching algorithm for complex tubular structures that emphasizes shape and depth as well as GPU-accelerated shadow-like depth indicators, which enable reliable comparisons of depth distances in a static monoscopic 3D visualization. In addition, we verify the expressiveness of our illustration methods in a large, quantitative study with 160 subjects. {\textcopyright} 2006 IEEE.},
author = {Ritter, Felix and Hansen, Christian and Dicken, Volker and Konrad, Olaf and Preim, Bernhard and Peitgen, Heinz Otto},
doi = {10.1109/TVCG.2006.172},
file = {:D\:/Thomas/Downloads/Real-Time_Illustration_of_Vascular_Structures.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Evaluation,Functional realism,Illustrative rendering,Spatial perception,Vessel visualization},
mendeley-groups = {NonPhotorealisticRendering},
number = {5},
pages = {877--884},
pmid = {17080812},
publisher = {IEEE},
title = {{Real-time illustration of vascular structures}},
volume = {12},
year = {2006}
}

@article{Salah2006,
abstract = {The art and profession of medical illustration depend not only on the talent and skills of an illustrator, but also on the complimentary knowledge of human anatomy. Therefore, anatomical illustrations were mostly based on comprehensive dissections and observations during surgery. Recently, illustrative visualization techniques have been utilized to illustrate features and shapes of anatomical objects. Thereby, illustrative visualization provides representations that highlight relevant features, while de-emphasizing irrelevant details. However, anatomical illustrations usually include multiple neighboring structures, which increases the complexity of the illustration problem, as the relationship between multiple organs still remains difficult to convey. In this paper, we present an approach for the combined illustrative visualization of multiple anatomical structures based on scanned patient data. To enhance the expressiveness, we incorporate multiple stylized rendering and shading techniques, and propose strategies for tuning object attributes (such as color, opacity, and silhouette), which allows to better perceive the spatial relationship between objects as well as to draw the focus to targeted structures.},
author = {Salah, Zein and Bartz, Dirk and Stra{\ss}er, Wolfgang and Tatagiba, Marcos},
file = {:D\:/Thomas/GraphsForThesis/curac000009.pdf:pdf},
keywords = {anatomical illustration,illustrative visualization,medical visualization},
mendeley-groups = {NonPhotorealisticRendering},
pages = {1--11},
title = {{Expressive anatomical illustrations based on scanned patient data}},
year = {2006},
journal = {ArXiv},
}

@book{alma9911190913502466,
address = {Leipzig},
author = {Fechner, Gustav Theodor and Wundt, Wilhelm Max},
booktitle = {Elemente der Psychophysik. Erster Theil},
edition = {2 unveränderte Auflage.},
keywords = {Psychophysiology},
mendeley-groups = {NonPhotorealisticRendering},
publisher = {Breitkopf \& Hartel},
title = {{Elemente der Psychophysik. Erster Theil / von Gustav Theodor Fechner ; [herausgeber: W. Wundt].}},
year = {1889}
}

@article{SUTHERLANDIE1968,
abstract = {Special spectacles containing two miniature cathode ray tubes are attached to user's head to surround him with displayed three- dimensional information; use is able to move his head three feet off axis in any direction to get better view of nearby objects; he can turn completely around and can tilt his head up or down 30 to 40 degrees; user has 40 degree field of view of synthetic information displayed on miniature cathode ray tubes; half-silvered mirrors in prisms through which user looks allow him to see both images from cathode ray tubes and objects in room simultaneously.},
author = {Sutherland, Ivan E},
doi = {10.1145/1476589.1476686},
file = {:D\:/Thomas/GraphsForThesis/1476589.1476686.pdf:pdf},
mendeley-groups = {AR hardware},
number = {pt 1},
pages = {757--764},
title = {{Head-Mounted Three Dimensional Display}},
volume = {33},
year = {1968}
}

@book{Boring1942,
abstract = {This book is a sequel to A history of experimental psychology (see IV: 456). It brings the history of the senses up to about 1930. The author does not trust his own perspective for events of the last decade. The first 2 chapters are introductory and consider general problems. The next 6 cover vision, including depth perception. 3 chapters are devoted to audition, and 1 each is given to smell and taste, touch, organic sensibility, and the perception of time and movement. A final chapter analyzes some of the factors that have inhibited and facilitated scientific progress. There is an index of names and one of subjects. References and comments follow each chapter and constitute a substantial portion of the book. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
address = {Oxford,  England},
author = {Boring, E G},
booktitle = {Sensation and perception in the history of experimental psychology.},
pages = {xv, 644--xv, 644},
publisher = {Appleton-Century},
title = {{Sensation and perception in the history of experimental psychology.}},
year = {1942}
}

@misc{Galper1971,
abstract = {Compared recognition memory in 52 male and 51 female undergraduates for faces seen in positive, negative, and 2 different expressions. Pure pattern characteristics plus those properties that differed in smile and resting state could not fully account for the accuracies obtained. But the characteristics that differ in smiling and nonsmiling expressions did play some role in recognition, which suggests that expressional variation offers a technique for investigating the properties by which faces are discriminated and remembered. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
address = {US},
author = {Galper, Ruth E and Hochberg, Julian},
booktitle = {The American Journal of Psychology},
doi = {10.2307/1420466},
isbn = {1939-8298(Electronic),0002-9556(Print)},
keywords = {*Memory,*Nonverbal Communication,Recognition (Learning)},
number = {3},
pages = {351--354},
publisher = {Univ of Illinois Press},
title = {{Recognition memory for photographs of faces.}},
volume = {84},
year = {1971}
}




@article{Lee1980,
author = {Lee, D. N.  and Kalmus, H.  and Longuet-Higgins, Hugh Christopher  and Sutherland, N. S. },
title = {The optic flow field: the foundation of vision},
journal = {Philosophical Transactions of the Royal Society of London. B, Biological Sciences},
volume = {290},
number = {1038},
pages = {169-179},
year = {1980},
doi = {10.1098/rstb.1980.0089},

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1980.0089},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.1980.0089},
    abstract = { As a basis for understanding the visual system, we need to consider the functions that vision has to perform, which are pre-eminently in the service of activity, and the circumstances in which it normally operates, namely when the head is moving. The fundamental ecological stimulus for vision is not a camera-like time-frozen image but a constantly changing optic array or flow field, the description of which must be in spatio-temporal terms. A mathematical analysis of the optic flow field is presented, revealing the information that it affords for controlling activity - information both about the topography of the environment and about the movement of the organism relative to the environment. Results of human behavioural experiments are also reported. It is suggested that the optic flow field should be the starting point in attempting to discover the physiological workings of the visual system. }
}

@inbook{Shojiro1991,
author = {Nagata, Shojiro},
year = {1991},
month = {01},
pages = {527-545},
title = {How to reinforce perception of depth in single two-dimensional pictures - A comparative study of various cues for depth perception},
publisher = {{National Aeronautics and Space Administration (NASA)}},
journal = {Spatial Displays and Spatial Instruments},
}

@incollection{Berkeley1948,
abstract = {Empiricism is applied here by George Berkeley to the perception of distance and magnitude, both in the context of vision. Berkeley employs 72 individual, enumerated paragraphs to support his theories. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
address = {East Norwalk,  CT,  US},
author = {Berkeley, George},
booktitle = {Readings in the history of psychology.},
doi = {10.1037/11304-009},
keywords = {*Distance Perception,*Magnitude Estimation,*Theories,*Vision,History of Psychology},
pages = {69--80},
publisher = {Appleton-Century-Crofts},
series = {Century psychology series.},
title = {{An essay toward a new theory of vision, 1709.}},
year = {1948}
}

@article{Wang1990,
author = {fang Wang, Jih and Chi, Vernon and Fuchs, Henry},
doi = {10.1145/91394.91447},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/91394.91447.pdf:pdf},
isbn = {0897913515},
issn = {00978930},
journal = {Computer Graphics (ACM)},
mendeley-groups = {Displays},
number = {2},
pages = {205--215},
title = {{Real-time optical 3D tracker for head-mounted display systems}},
volume = {24},
year = {1990}
}

@article{Pizer1986,
author = {Pizer, Stephen M.},
doi = {10.1007/978-3-642-82384-8_7},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/978-3-642-82384-8_7.pdf:pdf},
journal = {Pictorial Information Systems in Medicine},
mendeley-groups = {MedicalDataOverlay},
pages = {235--249},
title = {{Systems for 3D Display in Medical Imaging}},
year = {1986}
}

@article{Mann1997,
abstract = {Miniaturization of components has enabled systems that are wearable and nearly invisible, so that individuals can move about and interact freely, supported by their personal information domain.},
author = {Mann, Steve},
doi = {10.1109/2.566147},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/Wearable_computing_a_first_step_toward_personal_imaging.pdf:pdf},
issn = {00189162},
journal = {Computer},
mendeley-groups = {Displays},
number = {2},
pages = {25--32},
title = {{Wearable computing: A first step toward personal imaging}},
volume = {30},
year = {1997}
}

@article{Habert2015,
abstract = {This paper presents the first design of a mirror based RGBD X-ray imaging system and includes an evaluation study of the depth errors induced by the mirror when used in combination with an infrared pattern-emission RGBD camera. Our evaluation consisted of three experiments. The first demonstrated almost no difference in depth measurements of the camera with and without the use of the mirror. The final two experiments demonstrated that there were no relative and location-specific errors induced by the mirror showing the feasibility of the RGBDX-ray imaging system. Lastly, we showcase the potential of the RGBDX-ray system towards a visualization application in which an X-ray image is fused to the 3D reconstruction of the surgical scene via the RGBD camera, using automatic C-arm pose estimation.},
author = {Habert, Severine and Gardiazabal, Jose and Fallavollita, Pascal and Navab, Nassir},
doi = {10.1109/ISMAR.2015.17},
file = {:D\:/Thomas/RGBDX_First_Design_and_Experimental_Validation_of_a_Mirror-Based_RGBD_X-ray_Imaging_System.pdf:pdf},
isbn = {9781467376600},
journal = {Proceedings of the 2015 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2015},
keywords = {Medical Augmented Reality,Multi-modal Visualization,Range Imaging,X-ray imaging},
mendeley-groups = {X-RayVision},
pages = {13--18},
title = {{RGBDX: First design and experimental validation of a mirror-based RGBD X-ray imaging system}},
year = {2015}
}

@article{Becher2021,
abstract = {Augmented Reality is increasingly used for visualizing underground networks. However, standard visual cues for depth perception have never been thoroughly evaluated via user experiments in a context involving physical occlusions (e.g., ground) of virtual objects (e.g., elements of a buried network). We therefore evaluate the benefits and drawbacks of two techniques based on combinations of two well-known depth cues: grid and shadow anchors. More specifically, we explore how each combination contributes to positioning and depth perception. We demonstrate that when using shadow anchors alone or shadow anchors combined with a grid, users generate 2.7 times fewer errors and have a 2.5 times lower perceived workload than when only a grid or no visual cues are used. Our investigation shows that these two techniques are effective for visualizing underground objects. We also recommend the use of one technique or another depending on the situation.},
author = {Becher, Cindy and Bottecchia, S{\'{e}}bastien and Desbarats, Pascal},
doi = {10.1007/978-3-030-85623-6_35},
file = {:D\:/Thomas/LitReview/Scopus/Accepted/978-3-030-85623-6_35.pdf:pdf},
isbn = {9783030856229},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Augmented reality,Depth cues,Projection techniques,Underground objects,Visualization},
mendeley-groups = {X-RayVision},
pages = {611--630},
title = {{Projection Grid Cues: An Efficient Way to Perceive the Depths of Underground Objects in Augmented Reality}},
volume = {12932 LNCS},
year = {2021}
}

@article{Muthalif2022,
abstract = {Augmented reality (AR) is intensely explored due to its wide range of potential applications, from education and engineering to medicine and defense. Nevertheless, utilizing AR for visualizing underground utilities still faces a number of challenges, including the unavailability of reliable, readily accessible digital information about underground utilities, localization of AR devices in a GNSS-deprived environment, and visual perceptual challenges. This paper discusses the utilization of Mixed reality (MR) to address the visual perceptual challenges, which can provide a realistic visualization and convenient user experience compared to AR. Six MR visualization prototypes have been developed, namely, "General view", "General + Range view", "General + Elevator view", "X-Ray box view", "X-Ray box + Depthslider view", and "X-Ray + box + Clipping view", and they are deployed to Microsoft HoloLens 2 for testing purposes. These new MR visualization methods can potentially resolve AR's visual perceptual challenges for visualizing underground utilities.},
author = {Muthalif, M. Z.A. and Shojaei, D. and Khoshelham, K.},
doi = {10.5194/isprs-archives-XLVIII-4-W4-2022-101-2022},
file = {:D\:/Thomas/LitReview/Scopus/Accepted/isprs-archives-XLVIII-4-W4-2022-101-2022.pdf:pdf},
issn = {16821750},
journal = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
keywords = {Augmented Reality,Microsoft HoloLens,Mixed Reality,Underground Utility,Visual-Perceptual Challenges,Visualization},
mendeley-groups = {X-RayVision},
number = {4/W4-2022},
pages = {101--108},
title = {{Resolving Perceptual Challenges of Visualizing Underground Utilities in Mixed Reality}},
volume = {48},
year = {2022}
}

@article{Livingston2005,
author = {Livingston, Mark A. and Rosenblum, Larry and Macedonia, Michael},
doi = {10.1109/MCG.2005.130},
file = {:C\:/Users/adminuser/Downloads/Evaluating_human_factors_in_augmented_reality_systems.pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
mendeley-groups = {X-RayVision},
number = {6},
pages = {6--9},
pmid = {16315470},
title = {{Projects in VR: Evaluating human factors in augmented reality systems}},
volume = {25},
year = {2005}
}

@article{Yamamoto2014,
author = {Yamamoto, Goshiro and L, Arno Wolde},
file = {:C\:/Users/adminuser/Downloads/Yamamoto2014_Chapter_ASee-throughVisionWithHandheld.pdf:pdf},
keywords = {augmented reality,hand-,see-through vision,sightseeing},
mendeley-groups = {X-RayVision},
pages = {392--399},
title = {{A See-through Vision with Handheld}},
year = {2014},
journal = {Distributed, Ambient, and Pervasive Interactions},
}

@article{Eren2013,
abstract = {We propose a novel multi-view visualization technique, which allows effortless interaction with subterranean data and tries to maximize spatial perception whilst minimizing view clutter. The multi-view augmented reality technique introduces two correlating displays; i) a perspective egocentric view with focused edge overlay and focused geometry clipping and ii) an orthographic cut-away display that visualizes a thin slice of subterranean data intersecting a user controlled anchor. {\textcopyright} 2013 IEEE.},
author = {Eren, Mustafa Tolga and Cansoy, Murat and Balcisoy, Selim},
doi = {10.1109/VR.2013.6549390},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eren, Cansoy, Balcisoy - 2013 - Multi-view augmented reality for underground exploration(9).pdf:pdf},
isbn = {9781467347952},
journal = {Proceedings - IEEE Virtual Reality},
keywords = {Multi-View Augmented Reality,Outdoor Augmented Reality,Underground Visualization,X-Ray Visualization},
mendeley-groups = {X-RayVision},
pages = {117--118},
publisher = {IEEE},
title = {{Multi-view augmented reality for underground exploration}},
year = {2013}
}

@article{Heinrich2022,
abstract = {Surgical procedures requiring needle navigation assistance suffer from complicated hand-eye coordination and are mentally demanding. Augmented reality (AR) can help overcome these issues. How-ever, only an insufficient amount of fundamental research has focused on the design and hardware selection of such AR needle navigation systems. This work contributes to this research area by presenting a user study (n=24) comparing three state-of-the-art navigation concepts displayed by an optical see-through head-mounted display and a stereoscopic projection system. A two-dimensional glyph visualization resulted in higher targeting accuracy but required more needle insertion time. In contrast, punctures guided by a three-dimensional see-through vision concept were less accurate but faster and were favored in a qualitative interview. The third concept, a static representation of the correctly positioned needle, showed too high target errors for clinical accuracy needs. This concept per-formed worse when displayed by the projection system. Besides that, no meaningful differences between the evaluated AR display devices were detected. User preferences and use case restrictions, e.g., sterility requirements, seem to be more crucial selection criteria. Future work should focus on improving the accuracy of the see-through vision concept. Until then, the glyph visualization is recommended.},
author = {Heinrich, Florian and Schwenderling, Lovis and Joeres, Fabian and Hansen, Christian},
doi = {10.1109/VR51125.2022.00045},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heinrich et al. - 2022 - 2D versus 3D A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices.pdf:pdf},
isbn = {9781665496179},
journal = {Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces, VR 2022},
keywords = {Applied computing - Life and medical sciences - He,Computing methodologies - Computer graphics - Grap,Hardware - Communication hardware, interfaces and,Human-centered computing - Visualization - Empiric},
mendeley-groups = {X-RayVision},
pages = {260--269},
publisher = {IEEE},
title = {{2D versus 3D: A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices}},
year = {2022}
}

@article{Chen2010,
author = {Chen, Jiazhou and Granier, Xavier and Lin, Naiyang},
file = {:C\:/Users/tomis/OneDrive/Pictures/1889863.1889898.pdf:pdf},
isbn = {9781450304412},
journal = {Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST},
keywords = {and,augmented reality,context,create a visually correct,easily identified the objects,focus,ii,on-line video processing,provide some visual cues,rendering,spatial relationships,this problem is iden-,to,x-ray vision},
mendeley-groups = {X-RayVision},
pages = {167--170},
title = {{On-Line Visualization of Underground Structures using Context Features}},
year = {2010}
}


@article{Baker2007,
abstract = {Ground penetrating radar (also referred to as GPR, ground probing radar, or georadar) is a near-surface geophysical tool with a wide range of applications. Over the past 30 years, GPR has been used successfully to aid in constraining problems in diverse fields such as archaeology, environmental site characterization, glaciology, hydrology, land mine/unexploded ordinance detection, sedimentology, and structural geology. In many cases, however, GPR surveys have been planned or executed with little or no understanding of the physical basis by which GPR operates and is constrained. As a result, many unsuccessful GPR studies have also been presented or published over the past 30 years. The objectives of this primer are to (1) provide an introduction to the important variables pertinent to GPR and (2) to explain the relevant aspects of these variables in GPR acquisition, in an attempt to provide fundamental knowledge for improving GPR usage in the future. {\textcopyright} 2007 The Geological Society of America. All rights reserved.},
author = {Baker, Gregory S. and Jordan, Thomas E. and Pardy, Jennifer},
doi = {10.1130/2007.2432(01)},
file = {:C\:/Users/adminuser/Downloads/GSL.SP.2001.211.01.01.pdf:pdf},
issn = {00721077},
journal = {Special Paper of the Geological Society of America},
keywords = {GPR,Georadar introduction,Ground penetrating radar,Ground probing radar},
mendeley-groups = {VolumeRendering/Gologoly},
pages = {1--18},
title = {{An introduction to ground penetrating radar (GPR)}},
volume = {432},
year = {2007}
}

	
@inproceedings{Avila1994,
    title = {{VolVis}: A Diversified System for Volume Research and Development},
    author = {Avila, R. and He, Taosong and Hong, Lichan and Kaufman, A. and Pfister, H. and Silva, C. and Sobierajski, L. and Wang, S.},
    booktitle = {Proceedings Visualization '94},
    pages = {31--38},
    year = {1994},
    month = {oct},
    doi = {10.1109/VISUAL.1994.346340},
}

@article{Strasser1974,
author = {Stra{\ss}er, Wolfgang},
file = {:C\:/Users/adminuser/Pictures/Wolfgang_Stra{\ss}er_Schnelle_Kurven-_und_Flaechendarstellung_auf_grafischen_Sichtgeraeten.pdf:pdf},
mendeley-groups = {Books},
title = {{Schnelle Kurven-und Fl{\"{a}}chendarstellung auf grafischen Sichtger{\"{a}}ten}},
url = {http://isgwww.cs.uni-magdeburg.de/graphics/misc/Wolfgang_Stra{\ss}er_Schnelle_Kurven-_und_Flaechendarstellung_auf_grafischen_Sichtgeraeten.pdf},
year = {1974}
}

@article{Newell1972,
author = {Newell, M.E and Newell, R.G and T.L, Sancha},
file = {:C\:/Users/adminuser/Pictures/newell-newell-sancha.pdf:pdf},
journal = {Proceedings of the ACM annual conference on - ACM'72. ACM '72},
keywords = {1987,adenine specific dna chemical,brent l,d s research,dervan,iverson and peter b,nucleic a c i,sequencing reaction,ume 15 number 19},
number = {1},
pages = {7823--7830},
title = {{A Solution to the Hidden Surface Problem}},
volume = {1},
year = {1972}
}

@article{Baoquan2016,
abstract = {Abstract Interactive isosurface visualisation has been made possible by mapping algorithms to GPU architectures. However, current state-of-the-art isosurfacing algorithms usually consume large amounts of GPU memory owing to the additional acceleration structures they require. As a result, the continued limitations on available GPU memory mean that they are unable to deal with the larger datasets that are now increasingly becoming prevalent. This paper proposes a new parallel isosurface-extraction algorithm that exploits the blocked organisation of the parallel threads found in modern many-core platforms to achieve fast isosurface extraction and reduce the associated memory requirements. This is achieved by optimising thread co-operation within thread-blocks and reducing redundant computation; ultimately, an indexed triangular mesh can be produced. Experiments have shown that the proposed algorithm is much faster (up to 10×) than state-of-the-art GPU algorithms and has a much smaller memory footprint, enabling it to handle much larger datasets (up to 64×) on the same GPU.},
author = {Liu, Baoquan and Clapworthy, Gordon J and Dong, Feng and Wu, Enhua},
doi = {10.1111/cgf.12897},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2016 - Parallel Marching Blocks A Practical Isosurfacing Algorithm for Large Data on Many-Core Architectures.pdf:pdf},
journal = {Computer Graphics Forum},
keywords = {Categories and Subject Descriptors (according to A,I.3.3 Computer Graphics: Picture/Image Generation—},
number = {3},
pages = {211--220},
title = {{Parallel Marching Blocks: A Practical Isosurfacing Algorithm for Large Data on Many-Core Architectures}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.12897},
volume = {35},
year = {2016}
}

@article{Dai2021,
abstract = {Abstract: The high-resolution scanning devices developed in recent decades provide biomedical volume datasets that support the study of molecular structure and drug design. Isosurface analysis is an important tool in these studies, and the key is to construct suitable description vectors to support subsequent tasks, such as classification and retrieval. Traditional methods based on handcrafted features are insufficient for dealing with complex structures, while deep learning-based approaches have high memory and computation costs when dealing directly with volume data. To address these problems, we propose IsoExplorer, an isosurface-driven framework for 3D shape analysis of biomedical volume data. We first extract isosurfaces from volume data and split them into individual 3D shapes according to their connectivity. Then, we utilize octree-based convolution to design a variational autoencoder model that learns the latent representations of the shape. Finally, these latent representations are used for low-dimensional isosurface representation and shape retrieval. We demonstrate the effectiveness and usefulness of IsoExplorer via isosurface similarity analysis, shape retrieval of real-world data, and comparison with existing methods. Graphic abstract: [Figure not available: see fulltext.].},
author = {Dai, Haoran and Tao, Yubo and He, Xiangyang and Lin, Hai},
doi = {10.1007/s12650-021-00770-2},
file = {:C\:/Users/adminuser/Pictures/12650_2021_Article_770.pdf:pdf},
issn = {18758975},
journal = {Journal of Visualization},
keywords = {Isosurface,Shape analysis,Variational autoencoder},
mendeley-groups = {Medical Papers For Justication of Study 2},
number = {6},
pages = {1253--1266},
publisher = {Springer Berlin Heidelberg},
title = {{IsoExplorer: an isosurface-driven framework for 3D shape analysis of biomedical volume data}},
url = {https://doi.org/10.1007/s12650-021-00770-2},
volume = {24},
year = {2021}
}

@article{Asel2018,
author = {Asel, Christian and Malek, Michael and {A. Fellner}, Franz},
doi = {10.15761/gii.1000146},
file = {:C\:/Users/adminuser/Pictures/GII-3-146.pdf:pdf},
journal = {Global Imaging Insights},
number = {1},
pages = {1--2},
title = {{Preoperative cinematic rendering of a sinus frontalis frontal bone fracture}},
volume = {3},
year = {2018}
}


@article{Eid2017,
abstract = {OBJECTIVE. The purpose of this article is to present an overview of cinematic rendering, illustrating its potential advantages and applications. CONCLUSION. Volume-rendered reconstruction, obtaining 3D visualization from original CT datasets, is increasingly used by physicians and medical educators in various clinical and educational scenarios. Cinematic rendering is a novel 3D rendering algorithm that simulates the propagation and interaction of light rays as they pass through the volumetric data, showing a more photorealistic representation of 3D images than achieved with standard volume rendering.},
annote = {One of the first cinimatic rendering papers.},
author = {Eid, Marwen and {De Cecco}, Carlo N. and Nance, John W. and Caruso, Damiano and Albrecht, Moritz H. and Spandorfer, Adam J. and {De Santis}, Domenico and Varga-Szemes, Akos and {Joseph Schoepf}, U.},
doi = {10.2214/AJR.17.17850},
file = {:D\:/Thomas/Downloads/ajr.17.17850.pdf:pdf},
issn = {15463141},
journal = {American Journal of Roentgenology},
keywords = {3D visualization,CT,Cinematic rendering},
mendeley-groups = {VolumeRendering},
number = {2},
pages = {370--379},
pmid = {28504564},
title = {{Cinematic rendering in CT: A novel, lifelike 3D visualization technique}},
volume = {209},
year = {2017}
}

@article{Purcell2002,
abstract = {Recently a breakthrough has occurred in graphics hardware: fixed function pipelines have been replaced with programmable vertex and fragment processors. In the near future, the graphics pipeline is likely to evolve into a general programmable stream processor capable of more than simply feed-forward triangle rendering. In this paper, we evaluate these trends in programmability of the graphics pipeline and explain how ray tracing can be mapped to graphics hardware. Using our simulator, we analyze the performance of a ray casting implementation on next generation programmable graphics hardware. In addition, we compare the performance difference between non-branching programmable hardware using a multipass implementation and an architecture that supports branching. We also show how this approach is applicable to other ray tracing algorithms such as Whitted ray tracing, path tracing, and hybrid rendering algorithms. Finally, we demonstrate that ray tracing on graphics hardware could prove to be faster than CPU based implementations as well as competitive with traditional hardware accelerated feed-forward triangle rendering.},
author = {Purcell, Timothy J. and Buck, Ian and Mark, William R. and Hanrahan, Pat},
doi = {10.1145/566654.566640},
file = {:C\:/Users/adminuser/Downloads/rtongfx.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {Programmable graphics hardware,Ray tracing},
mendeley-groups = {Graphics},
number = {3},
pages = {703--712},
title = {{Ray tracing on programmable graphics hardware}},
volume = {21},
year = {2002}
}

@book{Kelly2021,
abstract = {In this chapter we describe implementation details of some of the Unreal Engine 4 ray tracing effects that shipped in Fortnite Season 15. In particular, we dive deeply into ray traced reflections and global illumination. This includes goals, practical considerations,...},
author = {Kelly, Patrick and O'Donnell, Yuriy and ter Elst, Kenzo and Ca{\~{n}}ada, Juan and Hart, Evan},
booktitle = {Ray Tracing Gems II},
doi = {10.1007/978-1-4842-7185-8_48},
file = {:C\:/Users/adminuser/Downloads/978-1-4842-7185-8_48.pdf:pdf},
isbn = {9781484271858},
mendeley-groups = {Graphics},
pages = {791--821},
title = {{Ray Tracing in Fortnite}},
year = {2021}
}

@article{Fischer2020,
abstract = {In clinical practice, medical imaging technologies, like computed tomography, have become an important and routinely used technique for diagnosis. Advanced 3D visualization techniques of this data, e.g. by using volume rendering, provide doctors a better spatial understanding for reviewing complex anatomy. There already exist sophisticated programs for the visualization of medical imaging data, however, they are usually limited to exactly this topic and can be hardly extended to new functionality; for instance, multi-user support, especially when considering immersive VR interfaces like tracked HMDs and natural user interfaces, can provide the doctors an easier, more immersive access to the information and support collaborative discussions with remote colleagues. We present an easy-to-use and expandable system for volumetric medical image visualization with support for multi-user VR interactions. The main idea is to combine a state-of-the-art open-source game engine, the Unreal Engine 4, with a new volume renderer. The underlying game engine basis guarantees the extensibility and allows for easy adaption of our system to new hardware and software developments. In our example application, remote users can meet in a shared virtual environment and view, manipulate and discuss the volume-rendered data in real-time. Our new volume renderer for the Unreal Engine is capable of real-time performance, as well as, high-quality visualization.},
author = {Fischer, Roland and Chang, Kai Ching and Weller, Ren{\'{e}} and Zachmann, Gabriel},
doi = {10.1007/978-3-030-62655-6_11},
file = {:D\:/volRenVR.pdf:pdf},
isbn = {9783030626549},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Collaborative VR,Computed tomography,Medical visualization,Unreal Engine,Virtual Reality,Volume rendering},
mendeley-groups = {VoxelAlgorithms},
number = {March 2021},
pages = {178--191},
title = {{Volumetric Medical Data Visualization for Collaborative VR Environments}},
volume = {12499 LNCS},
year = {2020}
}

@article{Kersten2006,
abstract = {We present empirical studies that consider the effects of stereopsis and simulated aerial perspective on depth perception in translucent volumes. We consider a purely absorptive lighting model, in which light is not scattered or reflected, but is simply absorbed as it passes through the volume. A purely absorptive lighting model is used, for example, when rendering digitally reconstructed radiographs (DRRs), which are synthetic X-ray images reconstructed from CT volumes. Surgeons make use of DRRs in planning and performing operations, so an improvement of depth perception in DRRs may help diagnosis and surgical planning. {\textcopyright} 2006 IEEE.},
author = {Kersten, Marta A. and Stewart, A. James and Troje, Niko and Ellis, Randy},
doi = {10.1109/TVCG.2006.139},
file = {:D\:/Thomas/StudyOneDataProcessing/OldData/Questionaire/04015472.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Radiograph,Stereo,Stereopsis,Volume rendering,X-ray},
mendeley-groups = {DepthPerceptionPapers},
number = {5},
pages = {1117--1123},
pmid = {17080842},
publisher = {IEEE},
title = {{Enhancing depth perception in translucent volumes}},
volume = {12},
year = {2006}
}


@article{Kersten-Oertel2014,
abstract = {Cerebral vascular images obtained through angiography are used by neurosurgeons for diagnosis, surgical planning, and intraoperative guidance. The intricate branching of the vessels and furcations, however, make the task of understanding the spatial three-dimensional layout of these images challenging. In this paper, we present empirical studies on the effect of different perceptual cues (fog, pseudo-chromadepth, kinetic depth, and depicting edges) both individually and in combination on the depth perception of cerebral vascular volumes and compare these to the cue of stereopsis. Two experiments with novices and one experiment with experts were performed. The results with novices showed that the pseudo-chromadepth and fog cues were stronger cues than that of stereopsis. Furthermore, the addition of the stereopsis cue to the other cues did not improve relative depth perception in cerebral vascular volumes. In contrast to novices, the experts also performed well with the edge cue. In terms of both novice and expert subjects, pseudo-chromadepth and fog allow for the best relative depth perception. By using such cues to improve depth perception of cerebral vasculature, we may improve diagnosis, surgical planning, and intraoperative guidance. {\textcopyright} 2014 IEEE.},
annote = {Later work by Kersten-Oertel et al. focused on cerebral vascular images obtained through angiography, where the volume looks like an intricate series of branching vessels and furcation, where they introduce different elements into the volume to increase the presence of depth. These included stereoscopic vision, Chroma depth, kinetic depth (or motion) and an Aerial Perspective. Users where asked to find differences between different separate vessels and find determine the relative depth between them. They found that pusdo chroma depth and adding fog to their depth better sense of depth and they found that the perpendicular placement of the objects was more accurate. Stereo vision and its combination with edges seems to aided people seeing though this data.},
author = {Kersten-Oertel, Marta and Chen, Sean Jy Shyang and Collins, D. Louis},
doi = {10.1109/TVCG.2013.240},
file = {:D\:/Thomas/Downloads/An_Evaluation_of_Depth_Enhancing_Perceptual_Cues_for_Vascular_Volume_Visualization_in_Neurosurgery.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Depth cues,angiography,chromadepth,fog,stereo,vascular data,vessels,volume rendering},
mendeley-groups = {DepthPerceptionMedical},
number = {3},
pages = {391--403},
pmid = {24434220},
publisher = {IEEE},
title = {{An evaluation of depth enhancing perceptual cues for vascular volume visualization in neurosurgery}},
volume = {20},
year = {2014}
}

@article{Erickson2020,
abstract = {Due to the additive light model employed by most optical see-through head-mounted displays (OST-HMDs), they provide the best augmented reality (AR) views in dark environments, where the added AR light does not have to compete against existing real-world lighting. AR imagery displayed on such devices loses a significant amount of contrast in well-lit environments such as outdoors in direct sunlight. To compensate for this, OST-HMDs often use a tinted visor to reduce the amount of environment light that reaches the user's eyes, which in turn results in a loss of contrast in the user's physical environment. While these effects are well known and grounded in existing literature, formal measurements of the illuminance and contrast of modern OST-HMDs are currently missing. In this paper, we provide illuminance measurements for both the Microsoft HoloLens 1 and its successor the HoloLens 2 under varying environment lighting conditions ranging from 0 to 20,000 lux. We evaluate how environment lighting impacts the user by calculating contrast ratios between rendered black (transparent) and white imagery displayed under these conditions, and evaluate how the intensity of environment lighting is impacted by donning and using the HMD. Our results indicate the further need for refinement in the design of future OST-HMDs to optimize contrast in environments with illuminance values greater than or equal to those found in indoor working environments.},
author = {Erickson, Austin and Kim, Kangsoo and Bruder, Gerd and Welch, Gregory F.},
doi = {10.1145/3385959.3418445},
file = {:C\:/Users/adminuser/Downloads/sui20a-sub1047-cam-i26-1.pdf:pdf},
isbn = {9781450379434},
journal = {Proceedings - SUI 2020: ACM Symposium on Spatial User Interaction},
keywords = {Augmented Reality,Contrast,Environment Lighting,Illuminance,Optical See-Through Head-Mounted Displays,Visual Perception},
number = {October},
title = {{Exploring the Limitations of Environment Lighting on Optical See-Through Head-Mounted Displays}},
year = {2020}
}

@article{Lee2020,
abstract = {An optical see-through (OST) display is affected more severely by ambient light than any other type of displays when placed in an outdoor environment with bright illuminance because of its transparency and thus, its inherent color distortion can worsen. It is hard to directly apply existing gamut mapping methods to an OST display because of its morphological gamut characteristic and the effect of ambient light. In this paper, we propose a new robust gamut mapping method which works against bright ambient light. The process is divided into two steps: lightness mapping (LM) and chroma reproduction. LM aligns the lightness level of sRGB gamut with OST gamut and partitions the region of OST gamut based on the relative size of the sRGB gamut and its lightness value. The second step (chroma reproduction) determines an appropriate chroma reproduction method (gamut compression or extension) and a proper direction for gamut mapping based on the characteristics of each region in order to minimize the effects of ambient light. The quality of color reproduction is qualitatively and quantitatively evaluated based on accurate measurements of the displayed colors. It has been experimentally confirmed that the proposed gamut mapping method can reduce color distortion more than the existing parametric gamut mapping algorithms.},
author = {Lee, Kang-Kyu and Kim, Jae-Woo and Ryu, Je-Ho and Kim, Jong-Ok},
doi = {10.1364/oe.391447},
file = {:C\:/Users/adminuser/Downloads/oe-28-10-15392.pdf:pdf},
issn = {10944087},
journal = {Optics Express},
mendeley-groups = {Displays},
number = {10},
pages = {15392},
pmid = {32403567},
title = {{Ambient light robust gamut mapping for optical see-through displays}},
volume = {28},
year = {2020}
}

@article{Ashtiani2023,
abstract = {Introduction: Augmented Reality (AR) systems are systems in which users view and interact with virtual objects overlaying the real world. AR systems are used across a variety of disciplines, i.e., games, medicine, and education to name a few. Optical See-Through (OST) AR displays allow users to perceive the real world directly by combining computer-generated imagery overlaying the real world. While perception of depth and visibility of objects is a widely studied field, we wanted to observe how color, luminance, and movement of an object interacted with each other as well as external luminance in OST AR devices. Little research has been done regarding the issues around the effect of virtual objects' parameters on depth perception, external lighting, and the effect of an object's mobility on this depth perception. Methods: We aim to perform an analysis of the effects of motion cues, color, and luminance on depth estimation of AR objects overlaying the real world with OST displays. We perform two experiments, differing in environmental lighting conditions (287 lux and 156 lux), and analyze the effects and differences on depth and speed perceptions. Results: We have found that while stationary objects follow previous research with regards to depth perception, motion and both object and environmental luminance play a factor in this perception. Discussion: These results will be significantly useful for developers to account for depth estimation issues that may arise in AR environments. Awareness of the different effects of speed and environmental illuminance on depth perception can be utilized when performing AR or MR applications where precision matters.},
author = {Ashtiani, Omeed and Guo, Hung Jui and Prabhakaran, Balakrishnan},
doi = {10.3389/frvir.2023.1243956},
file = {:C\:/Users/adminuser/Downloads/frvir-04-1243956.pdf:pdf},
issn = {26734192},
journal = {Frontiers in Virtual Reality},
keywords = {augmented reality,color perception,depth perception,motion cues,virtual reality},
mendeley-groups = {Displays},
number = {December},
pages = {1--11},
title = {{Impact of motion cues, color, and luminance on depth perception in optical see-through AR displays}},
volume = {4},
year = {2023}
}

@misc{Grinstein:2002:IVV,
  author = {Georges Grinstein and Daniel Keim and Matthew Ward},
  title = {Information Visualization, Visual Data Mining, and Its Application to Drug Design},
  howpublished = {IEEE Visualization Course \#1 Notes},
  month = {October},
  year = {2002},
	url = {http://vis.computer.org/vis2002/program/tutorials/tutorial_01_abstract.html}
}

@Article{Isenberg:2017:VMC,
  author = {Petra Isenberg and Florian Heimerl and Steffen Koch and Tobias Isenberg and Panpan Xu and Chad Stolper and Michael Sedlmair and Jian Chen and Torsten M{\"o}ller and John Stasko},
	title = {{vispubdata.org: A Metadata Collection about {IEEE} Visualization ({VIS}) Publications}},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	year = {2017},
	volume = {23},
	OPTnumber = {},
	OPTmonth = {},
	OPTpages = {},
	doi = {https://doi.org/10.1109/TVCG.2016.2615308},
  OPTgooglescholarid = {},
	note = {To appear},
}

@article{Pastor2004,
abstract = {Point hierarchies are suitable for creating frame-coherent animations of 3D models in non-photorealistic styles such as stippling, painterly and other artistic rendering. In this paper, we present a new approach to produce regular point distributions on the surface of a polygonal mesh. We propose a graph-based token distribution approach, where a graph that extends over the surface of the polygonal model is used as the playing field for the distribution of points. This graph-based approach eliminates the need of using geometrical operations to distribute points over the surface of a polygonal mesh. The graph exists at several levels of detail that are easily created using iterative patch fusion, and which are used to create a point hierarchy. In addition, we show how 3D stippling is used to render transparent surfaces and illustrate complex animations.},
author = {Pastor, Oscar Meruvia and Strotthote, Thomas},
doi = {10.1109/ENC.2004.1342599},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/enc04.pdf:pdf},
isbn = {0769521606},
journal = {Proceedings of the Fifth Mexican International Conference in Computer Science, ENC 2004},
mendeley-groups = {NonPhotorealisticRendering},
pages = {141--150},
title = {{Graph-based point relaxation for 3D stippling}},
year = {2004}
}


@mastersthesis{Kindlmann:1999:SAG,
  author = {Gordon Kindlmann},
  title = {Semi-Automatic Generation of Transfer Functions for Direct Volume Rendering},
  school = {Cornell University},
	address = {USA},
	url = {http://www.graphics.cornell.edu/pubs/1999/Kin99.html},
  year = 1999,
}

@manual{Kitware:2003,
  title = {The Visualization Toolkit User's Guide},
  organization = {Kitware, Inc.},
  year = {2003},
  month = {January},
	url = {http://www.kitware.com/publications/item/view/1269}
}

@phdthesis{Levoy:1989:DSV,
  author = {Marc Levoy},
  title = {Display of Surfaces from Volume Data},
  school = {University of North Carolina at Chapel Hill},
	address = {USA},
	url = {http://www.cs.unc.edu/techreports/89-022.pdf},
  year = {1989},
}

@article{Lorensen:1987:MCA,
 author = {Lorensen, William E. and Cline, Harvey E.},
 title = {Marching Cubes: A High Resolution {3D} Surface Construction Algorithm},
 journal = {SIGGRAPH Computer Graphics},
 volume = {21},
 number = {4},
 month = aug,
 year = {1987},
 pages = {163--169},
 doi = {10.1145/37402.37422},
}

@article{Max:1995:OMF,
 author = {Max, Nelson},
 title = {Optical Models for Direct Volume Rendering},
 journal = {IEEE Transactions on Visualization and Computer Graphics},
 volume = {1},
 number = {2},
 month = jun,
 year = {1995},
 pages = {99--108},
 doi = {10.1109/2945.468400},
} 

@inproceedings{Nielson:1991:TAD,
  author = {Gregory M. Nielson and Bernd Hamann},
  title = {The Asymptotic Decider: Removing the Ambiguity in Marching Cubes},
  year = {1991},
  booktitle = {Proc.\ Visualization},
  pages = {83--91},
  doi={10.1109/VISUAL.1991.175782},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos},
}

@book{Ware:2004:IVP,
  author = {Colin Ware},
  title = {Information Visualization: Perception for Design},
  edition = {2\textsuperscript{nd}},
  year = 2004,
  publisher = {Morgan Kaufmann Publishers Inc.},
  address = {San Francisco},
	doi = {https://doi.org/10.1016/B978-155860819-1/50001-7}
}

@ARTICLE{Wyvill:1986:DSS,
  author = {Wyvill, Geoff and McPheeters, Craig and Wyvill, Brian},
  title = {Data Structure for \emph{soft} Objects},
  journal = {The Visual Computer},
  year = {1986},
  volume = {2},
  pages = {227--234},
  number = {4},
  month = aug,
  doi = {10.1007/BF01900346}
}

@article{Dhariwal2021,
abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128×128, 4.59 on ImageNet 256×256, and 7.72 on ImageNet 512×512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256×256 and 3.85 on ImageNet 512×512.},
archivePrefix = {arXiv},
arxivId = {2105.05233},
author = {Dhariwal, Prafulla and Nichol, Alex},
eprint = {2105.05233},
file = {:C\:/Users/tomis/Downloads/2105.05233.pdf:pdf},
isbn = {9781713845393},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {NuralNetworkModels},
pages = {8780--8794},
title = {{Diffusion Models Beat GANs on Image Synthesis}},
volume = {11},
year = {2021}
}

@article{Goodfellow2020,
abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
archivePrefix = {arXiv},
arxivId = {1406.2661},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1145/3422622},
eprint = {1406.2661},
file = {:C\:/Users/tomis/Downloads/1406.2661.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
mendeley-groups = {NuralNetworkModels},
number = {11},
pages = {139--144},
title = {{Generative adversarial networks}},
volume = {63},
year = {2020}
}

@article{Fabian2015,
abstract = {In healthcare, inter-organizational sharing and collaborative use of big data become increasingly important. The cloud-computing paradigm is expected to provide an environment perfectly matching the needs of collaborating healthcare workers. However, there are still many security and privacy challenges impeding the wide adoption of cloud computing in this domain. In this paper, we present a novel architecture and its implementation for inter-organizational data sharing, which provides a high level of security and privacy for patient data in semi-trusted cloud computing environments. This architecture features attribute-based encryption for selective access authorization and cryptographic secret sharing in order to disperse data across multiple clouds, reducing the adversarial capabilities of curious cloud providers. An implementation and evaluation by several experiments demonstrate the practical feasibility and good performance of our approach.},
author = {Fabian, Benjamin and Ermakova, Tatiana and Junghanns, Philipp},
doi = {10.1016/j.is.2014.05.004},
file = {:C\:/Users/tomis/Downloads/Collaborativeandsecuresharingofhealthcaredatainmulti-clouds.pdf:pdf},
issn = {03064379},
journal = {Information Systems},
keywords = {Cloud computing,Healthcare,Privacy,Security},
mendeley-groups = {HealthcarePollocies},
number = {October 2022},
pages = {132--150},
publisher = {Elsevier},
title = {{Collaborative and secure sharing of healthcare data in multi-clouds}},
url = {http://dx.doi.org/10.1016/j.is.2014.05.004},
volume = {48},
year = {2015}
}

@article{Pinaya2022,
archivePrefix = {arXiv},
arxivId = {arXiv:2209.07162v1},
author = {Pinaya, Walter H L and Tudosiu, Petru-daniel and Dafflon, Jessica and Da, Pedro F and Fernandez, Virginia and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, Jorge M.},
eprint = {arXiv:2209.07162v1},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinaya et al. - 2022 - Brain Imaging Generation with Latent Diffusion Models.pdf:pdf},
journal = {arXiv},
mendeley-groups = {AutoMedicalImageGeneration},
pages = {1--10},
title = {{Brain Imaging Generation with Latent Diffusion Models}},
year = {2022}
}

@article{Ren2021,
abstract = {Radiologists and pathologists frequently make highly consequential perceptual decisions. For example, visually searching for a tumor and recognizing whether it is malignant can have a life-changing impact on a patient. Unfortunately, all human perceivers - even radiologists - have perceptual biases. Because human perceivers (medical doctors) will, for the foreseeable future, be the final judges of whether a tumor is malignant, understanding and mitigating human perceptual biases is important. While there has been research on perceptual biases in medical image perception tasks, the stimuli used for these studies were highly artificial and often critiqued. Realistic stimuli have not been used because it has not been possible to generate or control them for psychophysical experiments. Here, we propose to use Generative Adversarial Networks (GAN) to create vivid and realistic medical image stimuli that can be used in psychophysical and computer vision studies of medical image perception. Our model can generate tumor-like stimuli with specified shapes and realistic textures in a controlled manner. Various experiments showed the authenticity of our GAN-generated stimuli and the controllability of our model.},
author = {Ren, Zhihang and Yu, Stella X. and Whitney, David},
doi = {10.2352/ISSN.2470-1173.2021.11.HVEI-112},
file = {:C\:/Users/tomis/Downloads/Ren_MedImageGen.pdf:pdf},
issn = {24701173},
journal = {IS and T International Symposium on Electronic Imaging Science and Technology},
mendeley-groups = {AutoMedicalImageGeneration},
number = {11},
title = {{Controllable medical image generation via generative adversarial networks}},
volume = {2021},
year = {2021}
}

@article{Togo2019,
abstract = {In this paper, a novel synthetic gastritis image generation method based on a generative adversarial network (GAN) model is presented. Sharing medical image data is a crucial issue for realizing diagnostic supporting systems. However, it is still difficult for researchers to obtain medical image data since the data include individual information. Recently proposed GAN models can learn the distribution of training images without seeing real image data, and individual information can be completely anonymized by generated images. If generated images can be used as training images in medical image classification, promoting medical image analysis will become feasible. In this paper, we targeted gastritis, which is a risk factor for gastric cancer and can be diagnosed by gastric X-ray images. Instead of collecting a large amount of gastric X-ray image data, an image generation approach was adopted in our method. We newly propose loss function-based conditional progressive growing generative adversarial network (LC-PGGAN), a gastritis image generation method that can be used for a gastritis classification problem. The LC-PGGAN gradually learns the characteristics of gastritis in gastric X-ray images by adding new layers during the training step. Moreover, the LC-PGGAN employs loss function-based conditional adversarial learning to use generated images as the gastritis classification task. We show that images generated by the LC-PGGAN are effective for gastritis classification using gastric X-ray images and have clinical characteristics of the target symptom.},
author = {Togo, Ren and Ogawa, Takahiro and Haseyama, Miki},
doi = {10.1109/ACCESS.2019.2925863},
file = {:C\:/Users/tomis/Downloads/Synthetic_Gastritis_Image_Generation_via_Loss_Function-Based_Conditional_PGGAN.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Generative adversarial network,anonymization,data sharing,deep learning,medical image analysis},
mendeley-groups = {AutoMedicalImageGeneration},
pages = {87448--87457},
publisher = {IEEE},
title = {{Synthetic gastritis image generation via loss function-based conditional pggan}},
volume = {7},
year = {2019}
}

@article{Nguyen2023,
abstract = {Data collecting and sharing have been widely accepted and adopted to improve the performance of deep learning models in almost every field. Nevertheless, in the medical field, sharing the data of patients can raise several critical issues, such as privacy and security or even legal issues. Synthetic medical images have been proposed to overcome such challenges; these synthetic images are generated by learning the distribution of realistic medical images but completely different from them so that they can be shared and used across different medical institutions. The diffusion model (DM) has gained lots of attention due to its potential to generate realistic and high-resolution images, particularly outperforming generative adversarial networks (GANs) in many applications. The DM defines state of the art for various computer vision tasks such as image inpainting, class-conditional image synthesis, and others. However, due to its large size, the diffusion model is time and power consumption. Therefore, this paper proposes a lightweight DM to synthesize the medical image; we use computer tomography (CT) scans for SARS-CoV-2 (Covid-19) as the training dataset. Then we do extensive simulations to show the performance of the proposed diffusion model in medical image generation, and then we explain the key component of the model.},
author = {Nguyen, Loc X. and Aung, Pyae Sone and Le, Huy Q. and Park, Seong Bae and Hong, Choong Seon},
doi = {10.1109/ICOIN56518.2023.10049010},
file = {:C\:/Users/tomis/Downloads/A_New_Chapter_for_Medical_Image_Generation_The_Stable_Diffusion_Method.pdf:pdf},
isbn = {9781665462686},
issn = {19767684},
journal = {International Conference on Information Networking},
keywords = {CT scan of Covid-19,Diffusion Model,Medical Image Generation,UNet architecture},
mendeley-groups = {AutoMedicalImageGeneration},
pages = {483--486},
publisher = {IEEE},
title = {{A New Chapter for Medical Image Generation: The Stable Diffusion Method}},
volume = {2023-January},
year = {2023}
}

@article{Meyer_and_Nagler_and_Hogan_2021,
  doi = {10.5194/gmd-14-5205-2021},
  url = {https://doi.org/10.5194/gmd-14-5205-2021},
  year = {2021},
  publisher = {Copernicus {GmbH}},
  volume = {14},
  number = {8},
  pages = {5205--5215},
  author = {David Meyer and Thomas Nagler and Robin J. Hogan},
  title = {Copula-based synthetic data augmentation for machine-learning emulators},
  journal = {Geoscientific Model Development}
}

@article{Bossek2018,
author = {Bossek, Jakob},
doi = {10.21105/joss.00528},
file = {:C\:/Users/tomis/Downloads/10.21105.joss.00528.pdf:pdf},
journal = {The Journal of Open Source Software},
mendeley-groups = {DataGeneration},
number = {22},
pages = {528},
title = {{grapherator: A Modular Multi-Step Graph Generator}},
volume = {3},
year = {2018}
}

@article{Haghighi2017,
abstract = {Pyrgg is an easy-to-use synthetic random graph generator written in Python which supports various graph file formats including {DIMACS} .gr files. Pyrgg has the ability to generate graphs of different sizes and is designed to provide input files for broad range of graph-based research applications, including but not limited to testing, benchmarking and performance-analysis of graph processing frameworks. (Zhong and He 2012; Chakrabarti, Zhan, and Faloutsos 2004) Pyrgg target audiences are computer scientists who study graph algorithms and graph processing frameworks.},
author = {Haghighi, Sepand},
doi = {10.21105/joss.00331},
file = {:C\:/Users/tomis/Downloads/10.21105.joss.00331.pdf:pdf},
isbn = {9781611972740},
journal = {The Journal of Open Source Software},
mendeley-groups = {DataGeneration},
number = {17},
pages = {331},
title = {{Pyrgg: Python Random Graph Generator}},
volume = {2},
year = {2017}
}

@article{Patki2016,
abstract = {The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-The-Art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.},
author = {Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
doi = {10.1109/DSAA.2016.49},
file = {:C\:/Users/tomis/Downloads/The_Synthetic_Data_Vault.pdf:pdf},
isbn = {9781509052066},
journal = {Proceedings - 3rd IEEE International Conference on Data Science and Advanced Analytics, DSAA 2016},
keywords = {Crowd sourcing,Data science,Predictive modeling,Synthetic data generation},
mendeley-groups = {DataGeneration},
pages = {399--410},
publisher = {IEEE},
title = {{The synthetic data vault}},
year = {2016}
}

@article{Kim2022,
abstract = {In this paper, we propose a 3D color picker for virtual reality (VR) systems. Using the proposed color picker, the RGB and HSV color spaces are displayed in 3D shape, and a color can be selected through a one-step interaction by pointing at a location inside the color space with a 3D pointing device. This process is very simple compared with the multistep color selection interactions required for existing VR color pickers. Since the interior of the color space is not visible, we also propose three types of user interfaces that show the interior of the color space. These interfaces either cut out a portion of the color space, show cross-sections of the color space, or show discrete samples placed in the color space. We conducted user experiments on the color selection time, color selection accuracy, and usability and proved the usefulness and effectiveness of the proposed VR color picker.},
author = {Kim, Jieun and Hwang, Jae In and Lee, Jieun},
doi = {10.1109/ACCESS.2022.3184330},
file = {:C\:/Users/tomis/Downloads/VR_Color_Picker_Three-Dimensional_Color_Selection_Interfaces.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Color,humana-computer interaction,user interfaces,virtual reality,visualization},
pages = {65809--65824},
publisher = {IEEE},
title = {{VR Color Picker: Three-Dimensional Color Selection Interfaces}},
volume = {10},
year = {2022}
}

@article{Perlin1989,
author = {Perlin, Ken and Hoffert, Eric M.},
file = {:C\:/Users/tomis/Downloads/DataEngineeringBooks/perlin-hypertexture.pdf:pdf},
journal = {ACM SIGGRAPH Computer Graphics},
number = {3},
pages = {253--262},
title = {{Hypertexture}},
url = {htps://doi.org/10.1145/74334.74359},
volume = {23},
year = {1989}
}

@article{Grosset2013,
abstract = {In this paper we present a user study on the use of Depth of Field for depth perception in Direct Volume Rendering. Direct Volume Rendering with Phong shading and perspective projection is used as the baseline. Depth of Field is then added to see its impact on the correct perception of ordinal depth. Accuracy and response time are used as the metrics to evaluate the usefulness of Depth of Field. The onsite user study has two parts: static and dynamic. Eye tracking is used to monitor the gaze of the subjects. From our results we see that though Depth of Field does not act as a proper depth cue in all conditions, it can be used to reinforce the perception of which feature is in front of the other. The best results (high accuracy \& fast response time) for correct perception of ordinal depth occurs when the front feature (out of the two features users were to choose from) is in focus and perspective projection is used. {\textcopyright} 2013 IEEE.},
author = {Grosset, A. V.Pascal and Schott, Mathias and Bonneau, Georges Pierre and Hansen, Charles D.},
doi = {10.1109/PacificVis.2013.6596131},
file = {:D\:/Thomas/Downloads/Evaluation_of_Depth_of_Field_for_depth_perception_in_DVR.pdf:pdf},
isbn = {9781467347976},
issn = {21658765},
journal = {IEEE Pacific Visualization Symposium},
keywords = {I.3.7 [Computer Graphics]: Three-Dimensional Graph,I.3.m [Computer Graphics]: Miscellaneous,I.4.10 [Image Processing and Computer Vision]: Ima},
mendeley-groups = {DepthPerceptionPapers,VoxelAlgorithms},
pages = {81--88},
publisher = {IEEE},
title = {{Evaluation of depth of field for depth perception in DVR}},
year = {2013}
}

@article{Gillmann2021,
abstract = {The medical domain has been an inspiring application area in visualization research for many years already, but many open challenges remain. The driving forces of medical visualization research have been strengthened by novel developments, for example, in deep learning, the advent of affordable VR technology, and the need to provide medical visualizations for broader audiences. At IEEE VIS 2020, we hosted an Application Spotlight session to highlight recent medical visualization research topics. With this article, we provide the visualization community with ten such open challenges, primarily focused on challenges related to the visualization of medical imaging data. We first describe the unique nature of medical data in terms of data preparation, access, and standardization. Subsequently, we cover open visualization research challenges related to uncertainty, multimodal and multiscale approaches, and evaluation. Finally, we emphasize challenges related to users focusing on explainable AI, immersive visualization, P4 medicine, and narrative visualization.},
author = {Gillmann, Christina and Smit, Noeska N. and Groller, Eduard and Preim, Bernhard and Vilanova, Anna and Wischgoll, Thomas},
doi = {10.1109/MCG.2021.3094858},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gillmann et al. - 2021 - Ten Open Challenges in Medical Visualization.pdf:pdf},
issn = {15581756},
journal = {IEEE Computer Graphics and Applications},
mendeley-groups = {MedicalDataOverlay},
number = {5},
pages = {7--15},
pmid = {34506269},
title = {{Ten Open Challenges in Medical Visualization}},
volume = {41},
year = {2021}
}

@article{Englund2018,
abstract = {Direct Volume Rendering (DVR) provides the possibility to visualize volumetric data sets as they occur in many scientific disciplines. With DVR semi-transparency is facilitated to convey the complexity of the data. Unfortunately, semi-transparency introduces challenges in spatial comprehension of the data, as the ambiguities inherent to semi-transparent representations affect spatial comprehension. Accordingly, many techniques have been introduced to enhance the spatial comprehension of DVR images. In this paper, we present our findings obtained from two evaluations investigating the perception of semi-transparent structures from volume rendered images. We have conducted a user evaluation in which we have compared standard DVR with five techniques previously proposed to enhance the spatial comprehension of DVR images. In this study, we investigated the perceptual performance of these techniques and have compared them against each other in a large-scale quantitative user study with 300 participants. Each participant completed micro-tasks designed such that the aggregated feedback gives insight on how well these techniques aid the user to perceive depth and shape of objects. To further clarify the findings, we conducted a qualitative evaluation in which we interviewed three experienced visualization researchers, in order to find out if we can identify the benefits and shortcomings of the individual techniques.},
author = {Englund, R. and Ropinski, T.},
doi = {10.1111/cgf.13320},
file = {:D\:/Thomas/Downloads/Computer Graphics Forum - 2018 - Englund - Quantitative and Qualitative Analysis of the Perception of Semi‐Transparent.pdf:pdf},
journal = {Computer Graphics Forum},
number = {6},
pages = {174--187},
title = {{Quantitative and Qualitative Analysis of the Perception of Semi-Transparent Structures in Direct Volume Rendering}},
volume = {37},
year = {2018}
}

@article{McDade2022,
abstract = {The need to understand and communicate the nuances of complex situational information is an ever-present requirement in Command and Control (C2). It is often difficult for remote users of a system to clearly understand what a user is trying to relay. Mixed Reality (MR) technology presents a significant opportunity for exploring and communicating C2 data. In this paper, we present our system, CADET, as step towards enriching the collaborative C2 user ex-perience by allowing users to remotely and locally perform adhoc analysis through information displays created by hand interactions and speech in MR.},
author = {McDade, Jeremy and Drogemuller, Adam and Jing, Allison and Ireland, Nick and Walsh, James and Thomas, Bruce and Mayer, Wolfgang and Cunningham, Andrew},
doi = {10.1109/ISMAR-Adjunct57072.2022.00195},
file = {:D\:/Thomas/cadet_a_collaborative_agile_data_exploration_tool_for_mixed_reality.pdf:pdf},
isbn = {9781665453653},
journal = {Proceedings - 2022 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2022},
keywords = {Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality,Human-centered computing-Visualization-Visualization application domains-Information visualization},
pages = {899--900},
publisher = {IEEE},
title = {{CADET: A Collaborative Agile Data Exploration Tool for Mixed Reality}},
year = {2022}
}

@article{Nature2023,
author = {Nature},
doi = {10.1038/s41592-023-01865-4},
file = {:C\:/Users/tomis/Downloads/s41592-023-01865-4.pdf:pdf},
issn = {15487105},
journal = {Nature Methods},
number = {4},
pages = {471},
pmid = {37046014},
title = {{Data sharing is the future}},
url = {https://doi.org/10.1038/s41592-023-01865-4},
volume = {20},
year = {2023}
}

@article{Gonzalez2010,
abstract = {The area of a spherical region can be easily measured by considering which sampling points of a lattice are located inside or outside the region. This point-counting technique is frequently used for measuring the Earth coverage of satellite constellations, employing a latitude-longitude lattice. This paper analyzes the numerical errors of such measurements, and shows that they could be greatly reduced if the Fibonacci lattice were used instead. The latter is a mathematical idealization of natural patterns with optimal packing, where the area represented by each point is almost identical. Using the Fibonacci lattice would reduce the root mean squared error by at least 40\%. If, as is commonly the case, around a million lattice points are used, the maximum error would be an order of magnitude smaller. {\textcopyright} 2009 International Association for Mathematical Geosciences.},
archivePrefix = {arXiv},
arxivId = {0912.4540},
author = {Gonz{\'{a}}lez, {\'{A}}lvaro},
doi = {10.1007/s11004-009-9257-x},
eprint = {0912.4540},
file = {:C\:/Users/adminuser/Downloads/0912.4540.pdf:pdf},
issn = {18748961},
journal = {Mathematical Geosciences},
keywords = {Equal-angle grid,Fibonacci grid,Golden ratio,Non-standard grid,Phyllotaxis,Spherical grid},
mendeley-groups = {Math},
number = {1},
pages = {49--64},
title = {{Measurement of Areas on a Sphere Using Fibonacci and Latitude-Longitude Lattices}},
volume = {42},
year = {2010}
}

@article{Nguyen2022,
abstract = {We propose a new microscopy simulation system that can depict atomistic models in a micrograph visual style, similar to results of physical electron microscopy imaging. This system is scalable, able to represent simulation of electron microscopy of tens of viral particles and synthesizes the image faster than previous methods. On top of that, the simulator is differentiable, both its deterministic as well as stochastic stages that form signal and noise representations in the micrograph. This notable property has the capability for solving inverse problems by means of optimization and thus allows for generation of microscopy simulations using the parameter settings estimated from real data. We demonstrate this learning capability through two applications: (1) estimating the parameters of the modulation transfer function defining the detector properties of the simulated and real micrographs, and (2) denoising the real data based on parameters trained from the simulated examples. While current simulators do not support any parameter estimation due to their forward design, we show that the results obtained using estimated parameters are very similar to the results of real micrographs. Additionally, we evaluate the denoising capabilities of our approach and show that the results showed an improvement over state-of-the-art methods. Denoised micrographs exhibit less noise in the tilt-series tomography reconstructions, ultimately reducing the visual dominance of noise in direct volume rendering of microscopy tomograms.},
archivePrefix = {arXiv},
arxivId = {2205.04464},
author = {Nguyen, Ngan and Liang, Feng and Engel, Dominik and Bohak, Ciril and Wonka, Peter and Ropinski, Timo and Viola, Ivan},
eprint = {2205.04464},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - 2022 - Differentiable Electron Microscopy Simulation Methods and Applications for Visualization.pdf:pdf},
mendeley-groups = {VolumeRendering/Molecular},
number = {1},
pages = {1--22},
title = {{Differentiable Electron Microscopy Simulation: Methods and Applications for Visualization}},
url = {http://arxiv.org/abs/2205.04464},
year = {2022},
journal = {ArXiv},
}

@article{Goodsell1989,
abstract = {A raster-based computer graphics method of imaging volumetric data (data sampled on a three-dimensional grid) has been added to an existing molecular rendering program. Any molecular property expressed as a 3D grid of scalar data values can be displayed around and inside of the molecular van der Waals surface as a collection of colored clouds and transparent surfaces. Different antical properties, such as color and opacity, are assigned to ranges of the molecular property and are rendered by a ray-tracing technique simulating the shading and shadowing of real objects, making the final images highly eadable. We have found useful applications in the visualization of crystallographic electron density, electrostatic potentials and protein active sites. {\textcopyright} 1989.},
author = {Goodsell, David S. and Mian, I. Saira and Olson, Arthur J.},
doi = {10.1016/0263-7855(89)80055-4},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goodsell, Mian, Olson - 1989 - Rendering volumetric data in molecular systems.pdf:pdf},
issn = {02637855},
journal = {Journal of Molecular Graphics},
keywords = {cast shadows,raster graphics,transparent surfaces,volume rendering},
mendeley-groups = {VolumeRendering/Molecular},
number = {1},
pages = {41--47},
title = {{Rendering volumetric data in molecular systems}},
volume = {7},
year = {1989}
}

@article{HibbardL.1986,
abstract = {The Mexican transition zone is the area where the Neotropical and Nearctic regions overlap (Figure 4.1). In its broad sense, it comprises southwestern United States, Mexico, and most of Central America (Halffter, 1987; Zunino and Halffter, 1988; Guti{\'{e}}rrez-Vel{\'{a}}zquez et al., 2013). It is partially coincident with the areas named Megamexico 3 (Rzedowski, 1991) and biotic Mesoamerica (R{\'{i}}os-Mu{\~{n}}oz, 2013). The Mexican transition zone in the strict sense, which is followed in this book, corresponds to the moderate to high elevation highlands of Mexico, Guatemala, Honduras, El Salvador, and Nicaragua (Morrone, 2006, 2010b, 2014b, 2015c; Espinosa Organista et al., 2008).},
author = {{Hibbard L.}, William},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hibbard L. - 1986 - 4-D Display of Meterological Data.pdf:pdf},
journal = {Interactive 3D Graphics},
mendeley-groups = {VolumeRendering/Meterolgoy},
pages = {23--36},
title = {{4-D Display of Meterological Data}},
year = {1986}
}

@article{Lin2021,
abstract = {Volume rendering under complex, dynamic lighting is challenging, especially if targeting real-time. To address this challenge, we extend a recent direct illumination sampling technique, spatiotemporal reservoir resampling, to multi-dimensional path space for volumetric media.By fully evaluating just a single path sample per pixel, our volumetric path tracer shows unprecedented convergence. To achieve this, we properly estimate the chosen sample's probability via approximate perfect importance sampling with spatiotemporal resampling. A key observation is recognizing that applying cheaper, biased techniques to approximate scattering along candidate paths (during resampling) does not add bias when shading. This allows us to combine transmittance evaluation techniques: cheap approximations where evaluations must occur many times for reuse, and unbiased methods for final, per-pixel evaluation.With this reformulation, we achieve low-noise, interactive volumetric path tracing with arbitrary dynamic lighting, including volumetric emission, and maintain interactive performance even on high-resolution volumes. When paired with denoising, our low-noise sampling helps preserve smaller-scale volumetric details.},
author = {Lin, Daqi and Wyman, Chris and Yuksel, Cem},
doi = {10.1145/3478513.3480499},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Wyman, Yuksel - 2021 - Fast volume rendering with spatiotemporal reservoir resampling.pdf:pdf},
issn = {0730-0301},
journal = {ACM Transactions on Graphics},
mendeley-groups = {VolumeRendering/Meterolgoy},
number = {6},
pages = {1--18},
title = {{Fast volume rendering with spatiotemporal reservoir resampling}},
volume = {40},
year = {2021}
}

@article{Zehner2021,
abstract = {3D geological modelling is increasingly becoming a standard tool among geological survey organizations. The corresponding 3D models are made ever more widely available to general users, ranging from the interested public to politi-cians and decision makers who might be economists or lawyers, most of whom have received no specific professional train-ing in how to interpret these models. In order to allow these users to interpret this information meaningfully, clear and intuitive 3D visualization plays an important role. Further, the visualization should make the users aware of the uncertainty that is inherent in these 3D geological models, due to, e.g., scarce data and interpretation ambiguities. Currently, models are most commonly visualised as vector graphics, for example by using triangulated surfaces, which allows users to zoom in very closely without changing the quality of the representation. Thus, such visualizations give the impression that the geological structures in the subsurface are known with a precision of up to a centimetre. We will discuss different visualization options for certain geological models which are part of the standard geological modelling packages, such as Gocad or Petrel, outline the problems we see with the different visualizations, and show how the visualization could be improved and be made more intuitive using volume rendering. We will then provide a short review of which visualization methods are available in order to express the uncertainty in data and models, mostly from other scientific disciplines, and show how our sug-gested volume visualization could be augmented with the information on the uncertainty in the geological model. Using example data sets that we have generated and preprocessed using Skua-Gocad, we will explain the workflow and discuss the different options and problems to generate such visualizations.},
author = {Zehner, Bj{\"{o}}rn},
doi = {10.1127/zdgg/2020/0251},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zehner - 2021 - On the visualization of 3D geological models and their uncertainty.pdf:pdf},
issn = {18614094},
journal = {Zeitschrift der Deutschen Gesellschaft fur Geowissenschaften},
keywords = {3D visualization,Geological models,Uncertainty,Volume rendering},
mendeley-groups = {VolumeRendering/Gologoly},
number = {1},
pages = {83--98},
title = {{On the visualization of 3D geological models and their uncertainty}},
volume = {172},
year = {2021}
}

@article{Rheingans2001,
abstract = {Accurately and automatically conveying the structure of a volume model is a problem not fully solved by existing volume rendering approaches. Physics-based volume rendering approaches create images which may match the appearance of translucent materials in nature, but may not embody important structural details. Transfer function approaches allow flexible design of the volume appearance, but generally require substantial hand tuning for each new data set in order to be effective. We introduce the volume illustration approach, combining the familiarity of a physics-based illumination model with the ability to enhance important features using nonphotorealistic rendering techniques. Since features to be enhanced are defined on the basis of local volume characteristics rather than volume sample value, the application of volume illustration techniques requires less manual tuning than the design of a good transfer function. Volume illustration provides a flexible unified framework for enhancing structural perception of volume models d through the amplification of features and the addition of illumination effects.},
author = {Rheingans, Penny and Ebert, David},
doi = {10.1109/2945.942693},
file = {:C\:/Users/tomis/Downloads/Volume_illustration_nonphotorealistic_rendering_of_volume_models.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Illustration,Lighting models,Nonphotorealistic rendering,Shading,Transfer functions,Visualization,Volume rendering},
mendeley-groups = {VolumeRendering/Illustrations},
number = {3},
pages = {253--264},
title = {{Volume illustration: Nonphotorealistic rendering of volume models}},
volume = {7},
year = {2001}
}

@article{Held2019,
author = {Held, Benjamin and Aljuneidi, Saja and Pham, Tuan-Vu and Joseph, Arun},
doi = {10.13140/RG.2.2.28158.41287},
file = {:D\:/Thomas/ContentForX-rayDemo/Helon360_a_smart_firefighter_helmet2019.pdf:pdf},
mendeley-groups = {X-RayVision},
number = {May 2020},
pages = {0--12},
title = {{Helon 360: A smart firefighters' helmet Integrated Augmented Reality and 360° Thermal Image Data Streaming}},
url = {https://www.researchgate.net/publication/341764419},
year = {2019}
}

@article{Gerl2006,
abstract = {Hand-drawn illustrations are a traditional manner of conveying visual\ninformation in an abstract and aesthetic way. This work is dedicated\nto the automated generation of hand-drawn style hatching drawings\nfrom volumetric data. Based on a graphics-hardware accelerated determination\nof shape and lighting features, hatching strokes are generated in\nimage space. An approach depicting contours with multiple strokes\nis proposed for rendering object contours illustratively.},
author = {Gerl, Moritz},
file = {:D\:/Thomas/Downloads/pub-inf_3956.pdf:pdf},
mendeley-groups = {NonPhotorealisticRendering},
number = {November 2006},
title = {{Volume Hatching for Illustrative Visualization}},
url = {http://www.cg.tuwien.ac.at/research/publications/2006/gerl-2006-vhi/gerl-2006-vhi-pdf.pdf},
year = {2006},
journal = {TU Wien Library},
publisher = {TU Wien Library}
}

@article{Lu2002,
abstract = {Pen and ink rendering techniques provide artistic, illustrative, and\ninformative representations of objects. With recent advances in hardware\ngraphics technology, several researchers have developed interactive\nnon-photorealistic rendering techniques, including hatching, toon\nrendering, and silhouettes. However, the stippling method of drawing\nand shading using dots has not received as much focus. In this paper,\nwe present an interactive system for stipple drawing of surface-based\nobjects that provides illustrative stipple renderings of multiple\nobjects and includes adaptation of the stippling to provide a consistent\nrendering of objects at any scale. We also describe the use of the\nlatest graphics hardware capabilities for accelerating the rendering\nby performing the silhouette rendering on the GPU and the stipple\ndensity enhancement calculations as a vertex program.},
author = {Lu, Aidong and Taylor, Joe and Hartner, Mark and Ebert, David S and Hansen, Charles D},
file = {:C\:/Users/adminuser/Downloads/Hardware-Accelerated_Interactive_Illustrative_Stip.pdf:pdf},
journal = {Proceedings of Vision, Modeling, and Visualization 2002 (VMV 2002, November 20--22, 2002, Erlangen, Germany)},
mendeley-groups = {NonPhotorealisticRendering/stippling},
number = {June 2014},
pages = {61--68},
title = {{Hardware-Accelerated Interactive Illustrative Stipple Drawing of Polygonal Objects}},
url = {http://www.ecn.purdue.edu/purpl/level2/papers/stipple_VMV_2002.pdf},
year = {2002}
}

@article{Interrante1997,
abstract = {Transparency can be a useful device for depicting multiple overlapping surfaces in a single image. The challenge is to render the transparent surfaces in such a way that their three-dimensional shape can be readily understood and their depth distance from underlying structures clearly perceived. This paper describes our investigations into the use of sparsely-distributed discrete, opaque texture as an "artistic device" for more explicitly indicating the relative depth of a transparent surface and for communicating the essential features of its 3D shape in an intuitively meaningful and minimally occluding way. The driving application for this work is the visualization of layered surfaces in radiation therapy treatment planning data, and the technique is illustrated on transparent isointensity surfaces of radiation dose. We describe the perceptual motivation and artistic inspiration for defining a stroke texture that is locally oriented in the direction of greatest normal curvature (and in which individual strokes are of a length proportional to the magnitude of the curvature in the direction they indicate), and discuss two alternative methods for applying this texture to isointensity surfaces defined in a volume. We propose an experimental paradigm for objectively measuring observers' ability to judge the shape and depth of a layered transparent surface, in the course of a task relevant to the needs of radiotherapy treatment planning, and use this paradigm to evaluate the practical effectiveness of our approach through a controlled observer experiment based on images generated from actual clinical data. {\textcopyright} 1997 IEEE.},
author = {Interrante, Victoria and Fuchs, Henry and Pizer, Stephen M.},
doi = {10.1109/2945.597794},
file = {:C\:/Users/adminuser/Downloads/Conveying_the_3D_shape_of_smoothly_curving_transparent_surfaces_via_texture.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Principal direction texture,Shape and depth perception,Shape representation,Transparent surfaces},
mendeley-groups = {NonPhotorealisticRendering},
number = {2},
pages = {98--117},
title = {{Conveying the 3D shape of smoothly curving transparent surfaces via texture}},
volume = {3},
year = {1997}
}

@article{Interrante1997a,
abstract = {This paper describes how the set of principal directions and principal curvatures can be understood to define a natural flow over the surface of an object and, as such, can be used to guide the placement of the lines of a stroke texture that seeks to represent 3D shape in a perceptually intuitive way. The driving application for this work is the visualization of layered isovalue surfaces in volume data, where the particular identity of an individual surface is not generally known a priori and observers will typically wish to view a variety of different level surfaces from the same distribution, superimposed over underlying opaque structures.},
author = {Interrante, Victoria},
doi = {10.1145/258734.258796},
file = {:C\:/Users/adminuser/Downloads/258734.258796.pdf:pdf},
isbn = {0897918967},
journal = {Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1997},
keywords = {isosurfaces,line integral convolution,principal directions,shape representation,solid texture,stroke textures,transparent surfaces,visualization,volume rendering},
mendeley-groups = {NonPhotorealisticRendering/stippling,NonPhotorealisticRendering},
pages = {109--116},
title = {{Illustrating surface shape in volume data via principal direction-driven 3D line integral convolution}},
volume = {D},
year = {1997}
}

@article{Lu20,
abstract = {Simulating hand-drawn illustration techniques can succinctly express information in a manner that is communicative and informative. We present a framework for an interactive direct volume illustration system that simulates traditional stipple drawing. By combining the principles of artistic and scientific illustration, we explore several feature enhancement techniques to create effective, interactive visualizations of scientific and medical datasets. We also introduce a rendering mechanism that generates appropriate point lists at all resolutions during an automatic preprocess, and modifies rendering styles through different combinations of these feature enhancements. The new system is an effective way to interactively preview large, complex volume datasets in a concise, meaningful, and illustrative manner. Volume stippling is effective for many applications and provides a quick and efficient method to investigate volume models.},
author = {Lu, Aidong and Morris, Christopher J. and Ebert, David S. and Rheingans, Penny and Hansen, Charles},
doi = {10.1109/visual.2002.1183777},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu et al. - 2002 - Non-photorealistic volume rendering using stippling techniques.pdf:pdf},
journal = {Proceedings of the IEEE Visualization Conference},
keywords = {Medical imaging,Non-photorealistic rendering,Scientific visualization,Volume rendering},
number = {May 2013},
pages = {211--218},
title = {{Non-photorealistic volume rendering using stippling techniques}},
year = {2002}
}

@phdthesis{Salvetti2020,
abstract = {Non-photorealistic rendering can be used to create abstracted images of real-life objects. Cross-hatching is a common style under the non-photorealistic rendering umbrella, where characteristics such as lighting and shape of objects are represented by layered patches of parallel lines. The work in this master thesis explores the creation of cross-hatched renders where lighting and shape can be implied by density and direction of the hatching lines. We choose to place the hatching lines following the curvature characteristics of the object. For this purpose, we describe a way to compute and interpolate principal curvature directions and an image-space algorithm to draw lines following such directions. 1},
annote = {Can allow for more immersion into scenes},
author = {Salvetti, Isadora Albrecht and Gran, Carlos And{\'{u}}jar},
file = {:C\:/Users/adminuser/Downloads/152575.pdf:pdf},
mendeley-groups = {ArtisticShadingEffects},
number = {June},
title = {{Non-Photorealistic Rendering: Cross Hatching}},
year = {2020},
school = {Universitat Politecnica De Catalunya}
}

@article{Praun2001,
abstract = {Drawing surfaces using hatching strokes simultaneously conveys material, tone, and form. We present a real-time system for non-photorealistic rendering of hatching strokes over arbitrary surfaces. During an automatic preprocess, we construct a sequence of mipmapped hatch images corresponding to different tones, collectively called a tonal art map. Strokes within the hatch images are scaled to attain appropriate stroke size and density at all resolutions, and are organized to maintain coherence across scales and tones. At runtime, hardware multitexturing blends the hatch images over the rendered faces to locally vary tone while maintaining both spatial and temporal coherence. To render strokes over arbitrary surfaces, we build a lapped texture parametrization where the overlapping patches align to a curvature-based direction field. We demonstrate hatching strokes over complex surfaces in a variety of styles.},
author = {Praun, Emil and Hoppe, Hugues and Webb, Matthew and Finkelstein, Adam},
doi = {10.1145/383259.383328},
file = {:C\:/Users/adminuser/Downloads/hatching.pdf:pdf},
journal = {SIGGRAPH 2001},
keywords = {chicken-and-egg problem,line art,multitexturing,non-photorealistic rendering},
pages = {581},
title = {{Real-Time Hatching}},
url = {https://blendernpr.org/cross-hatch-shader/ https://doi.org/10.1145/383259.383328},
volume = {1},
year = {2001}
}

@article{Rocha2011,
abstract = {Scientific visualization techniques create images attempting to reveal complex structures and phenomena. Illustrative techniques have been incorporated to scientific visualization systems in order to improve the expressiveness of such images. The rendering of feature lines is an important technique for better conveying surface shapes. In this paper, we propose to combine volume visualization of unstructured meshes with direct rendering of illustrated isosurfaces. This is accomplished by extending a GPU-based ray-casting algorithm to incorporate illustration with photic extremum lines, a type of feature lines that captures sudden change of luminance, conveying shapes in a perceptually correct way. {\textcopyright} 2011 IEEE.},
annote = {Used Illustative effects to empasise minro objects in volume rendering.},
author = {Rocha, Allan and Miranda, F{\'{a}}bio Markus and Celes, Waldemar},
doi = {10.1109/SIBGRAPI.2011.20},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rocha, Miranda, Celes - 2011 - Illustrative volume visualization for unstructured meshes based on photic extremum lines.pdf:pdf},
isbn = {9780769545486},
journal = {Proceedings - 24th SIBGRAPI Conference on Graphics, Patterns and Images},
keywords = {Illustrative volume visualization,Photic extremum lines},
mendeley-groups = {VolumeRendering/Illustrations},
pages = {101--108},
title = {{Illustrative volume visualization for unstructured meshes based on photic extremum lines}},
volume = {vi},
year = {2011}
}

@article{Ma2018,
abstract = {In this paper, we present a novel real-time approach to generate high-quality stippling on 3D scenes. The proposed method is built on a precomputed 2D sample sequence called incremental Voronoi set with blue-noise properties. A rejection sampling scheme is then applied to achieve tone reproduction, by thresholding the sample indices proportional to the inverse target tonal value to produce a suitable stipple density. Our approach is suitable for stippling large-scale or even dynamic scenes because the thresholding of individual stipples is trivially parallelizable. In addition, the static nature of the underlying sequence benefits the frame-to-frame coherence of the stippling. Finally, we propose an extension that supports stipples of varying sizes and tonal values, leading to smoother spatial and temporal transitions. Experimental results reveal that the temporal coherence and real-time performance of our approach are superior to those of previous approaches.},
author = {Ma, Lei and Guo, Jianwei and Yan, Dong Ming and Sun, Hanqiu and Chen, Yanyun},
doi = {10.1111/cgf.13565},
file = {:C\:/Users/adminuser/Downloads/Computer Graphics Forum - 2018 - Ma - Instant Stippling on 3D Scenes.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {CCS Concepts,Image processing; Texturing; Appearance and textur,Non-photorealistic rendering,•Computing methodologies → Computer graphics},
mendeley-groups = {ArtisticShadingEffects},
number = {7},
pages = {255--266},
title = {{Instant Stippling on 3D Scenes}},
volume = {37},
year = {2018}
}

@article{Lloyd1982,
abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 2b quanta, b = 1,2, {\textperiodcentered}{\textperiodcentered}{\textperiodcentered}, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes. {\textcopyright}1982 IEEE},
author = {Lloyd, Stuart P.},
doi = {10.1109/TIT.1982.1056489},
file = {:C\:/Users/adminuser/Downloads/Least_squares_quantization_in_PCM.pdf:pdf},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
mendeley-groups = {NonPhotorealisticRendering/stippling},
number = {2},
pages = {129--137},
title = {{Least Squares Quantization in PCM}},
volume = {28},
year = {1982}
}

@article{Ulichney1988,
abstract = {Digital halftoning, also referred to as spatial dithering, is the method of rendering the illusion of continuous-tone pictures on displays that are capable of only producing binary picture elements. The concept of blue noise-high frequency white noiseis introduced and found to have desirable properties for haltoning. €fficient algorithms for dithering with blue noise are developed, based on perturbed error diffusion. The nature of dither patterns produced is extensively examined in the frequency domain. New metrics for analyzing the frequency content of aperiodic patterns for both rectangular and hexagonal grids are developed; blue-noise dithering is found to be ideally suited for rectangular grids. Several carefully selected digitally produced examples are included.},
author = {Ulichney, Robert A},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulichney - 1988 - Dithering with Blue Noise.pdf:pdf},
journal = {Proceedings of the IEEE},
mendeley-groups = {NoiseGeneration},
number = {1},
pages = {56--79},
title = {{Dithering with Blue Noise}},
volume = {76},
year = {1988}
}

@article{Heitz2018,
abstract = {We propose a new by-example noise algorithm that takes as input a small example of a stochastic texture and synthesizes an infinite output with the same appearance. It works on any kind of random-phase inputs as well as on many non-random-phase inputs that are stochastic and non-periodic, typically natural textures such as moss, granite, sand, bark, etc. Our algorithm achieves high-quality results comparable to state-of-the-art procedural-noise techniques but is more than 20 times faster.Our approach is conceptually simple: we partition the output texture space on a triangle grid and associate each vertex with a random patch from the input such that the evaluation inside a triangle is done by blending 3 patches. The key to this approach is the blending operation that usually produces visual artifacts such as ghosting, softened discontinuities and reduced contrast, or introduces new colors not present in the input. We analyze these problems by showing how linear blending impacts the histogram and show that a blending operator that preserves the histogram prevents these problems.The main requirement for a rendering application is to implement such an operator in a fragment shader without further post-processing, i.e. we need a histogram-preserving blending operator that operates only at the pixel level. Our insight for the design of this operator is that, with Gaussian inputs, histogram-preserving blending boils down to mean and variance preservation, which is simple to obtain analytically. We extend this idea to non-Gaussian inputs by "Gaussianizing" them with a histogram transformation and "de-Gaussianizing" them with the inverse transformation after the blending operation. We show how to precompute and store these histogram transformations such that our algorithm can be implemented in a fragment shader.},
author = {Heitz, Eric and Neyret, Fabrice},
doi = {10.1145/3233304},
file = {:C\:/Users/adminuser/Downloads/3233304.pdf:pdf},
journal = {Proceedings of the ACM on Computer Graphics and Interactive Techniques},
mendeley-groups = {NoiseGeneration},
number = {2},
pages = {1--25},
title = {{High-Performance By-Example Noise using a Histogram-Preserving Blending Operator}},
volume = {1},
year = {2018}
}

@article{Chen2018,
abstract = {When occlusion and binocular disparity cues conflict, what visual features determine how they combine? Sensory cues, such as T-junctions, have been suggested to be necessary for occlusion to influence stereoscopic depth perception. Here we show that illusory occlusion, with no retinal sensory cues, interacts with binocular disparity when perceiving depth. We generated illusory occlusion using stimuli filled in across the retinal blind spot. Observers viewed two bars forming a cross with the intersection positioned within the blind spot. One of the bars was presented binocularly with a disparity signal; the other was presented monocularly, extending through the blind spot, with no defined disparity. When the monocular bar was perceived as filled in through the blind spot, it was perceived as occluding the binocular bar, generating illusory occlusion. We found that this illusory occlusion influenced perceived stereoscopic depth: depth estimates were biased to be closer or farther, depending on whether a bar was perceived as in front of or behind the other bar, respectively. Therefore, the perceived relative depth position, based on filling-in cues, set boundaries for interpreting metric stereoscopic depth cues. This suggests that filling-in can produce opaque surface representations that can trump other depth cues such as disparity.},
author = {Chen, Zhimin and Denison, Rachel N. and Whitney, David and Maus, Gerrit W.},
doi = {10.1038/s41598-018-23548-3},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2018 - Illusory occlusion affects stereoscopic depth perception.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {DepthPerceptionPapers},
number = {1},
pages = {1--9},
pmid = {29593236},
publisher = {Springer US},
title = {{Illusory occlusion affects stereoscopic depth perception}},
url = {http://dx.doi.org/10.1038/s41598-018-23548-3},
volume = {8},
year = {2018}
}

@phdthesis{Alberto2017,
    author = {Alberto, Carlos and Galarza, Aviles},
    title = {Image Stylization Using Depth Information for Hatching and Engraving Effects},
    school = {Carleton University},
    year = {2017},
    doi = {https://doi.org/10.22215/etd/2017-11956}
}

@article{Hansen2010,
abstract = {Purpose: Augmented reality (AR) obtains increasing acceptance in the operating room. However, a meaningful augmentation of the surgical view with a 3D visualization of planning data which allows reliable comparisons of distances and spatial relations is still an open request. Methods: We introduce methods for intraoperative visualization of 3D planning models which extend illustrative rendering and AR techniques. We aim to reduce visual complexity of 3D planning models and accentuate spatial relations between relevant objects. The main contribution of our work is an advanced silhouette algorithm for 3D planning models (distance-encoding silhouettes) combined with procedural textures (distance-encoding surfaces). In addition, we present a method for illustrative visualization of resection surfaces. Results: The developed algorithms have been embedded into a clinical prototype that has been evaluated in the operating room. To verify the expressiveness of our illustration methods, we performed a user study under controlled conditions. The study revealed a clear advantage in distance assessment with the proposed illustrative approach in comparison to classical rendering techniques. Conclusion: The presented illustration methods are beneficial for distance assessment in surgical AR. To increase the safety of interventions with the proposed approach, the reduction of inaccuracies in tracking and registration is a subject of our current research. {\textcopyright} 2009 CARS.},
author = {Hansen, Christian and Wieferich, Jan and Ritter, Felix and Rieder, Christian and Peitgen, Heinz Otto},
doi = {10.1007/s11548-009-0365-3},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/s11548-009-0365-3.pdf:pdf},
isbn = {1154800903},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {Augmented reality,Illustrative rendering,Image-guided surgery,Intraoperative visualization},
mendeley-groups = {NonPhotorealisticRendering},
number = {2},
pages = {133--141},
title = {{Illustrative visualization of 3D planning models for augmented reality in liver surgery}},
volume = {5},
year = {2010}
}

@article{Lawonn2017,
abstract = {Incorrect spatial interpretation of 3D vascular models is a main perceptional problem in medical visualization. For improved depth perception, we propose supporting anchors between vascular trees and a cylindrical cutaway that serves as an insight for a virtual resection surface or a path for a tumor ablation. The supporting anchors are optimally arranged in a circular manner such that the depth can be perceived without time-consuming interaction. For improved shape perception and distance-encoding, we additionally employ a novel and fast hatching approach that produces results comparable to state-of-the-art techniques. The advantages of our new visualization approach are demonstrated using the example of laparoscopic liver surgery and confirmed in a quantitative user study with 81 participants. The results show that participants were able to assess relative distances more precisely and were most confident using our illustrative visualization approach.},
author = {Lawonn, Kai and Luz, Maria and Hansen, Christian},
doi = {10.1016/j.cag.2017.02.002},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/1-s2.0-S0097849317300171-main.pdf:pdf},
issn = {00978493},
journal = {Computers and Graphics (Pergamon)},
keywords = {Evaluation,Image guidance of interventions,Line and curve generation,Planning,Visualization},
mendeley-groups = {NonPhotorealisticRendering},
pages = {37--49},
publisher = {Elsevier Ltd},
title = {{Improving spatial perception of vascular models using supporting anchors and illustrative visualization}},
url = {http://dx.doi.org/10.1016/j.cag.2017.02.002},
volume = {63},
year = {2017}
}

@incollection{Lowonn2013,
abstract = {This paper deals with the application of feature lines on patient-specific anatomical surfaces used for treatment planning. We introduce the most commonly used feature line methods and evaluate them qualitatively. The evaluation is conducted by physicians and medical researchers to assess shape interpretation and visual impression of these methods compared to surface shading. We utilize several anatomical models, which were derived from clinical image data. Furthermore, we identify the limitations of this kind of illustrative visualization and discuss requirements for their application.},
author = {Lowonn, Kai and Gasteiger, Rocco and Preim, Bernhard},
booktitle = {Bildverarbeitung f{\"{u}}r die Medizin},
doi = {10.1007/978-3-642-36480-8_34},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/lawonn_2013_bvm.pdf:pdf},
isbn = {9783642364792},
issn = {1431472X},
mendeley-groups = {NonPhotorealisticRendering},
number = {May 2014},
pages = {187--193},
title = {{Qualitative Evaluation of Feature Lines on Anatomical Surfaces}},
year = {2013}
}

@article{Lawonn2013,
abstract = {Line drawing techniques are important methods to illustrate shapes. Existing feature line methods, e.g., suggestivecontours, apparent ridges, or photic extremum lines, solely determine salient regions and illustrate them with sepa-rate lines. Hatching methods convey the shape by drawing a wealth of lines on the whole surface. Both approachesare often not sufficient for a faithful visualization of organic surface models, e.g., in biology or medicine. In thispaper, we present a novel object-space line drawing algorithm that conveys the shape of such surface modelsin real-time. Our approach employscontour- andfeature-basedillustrativestreamlines to convey surface shape(ConFIS). For every triangle, precise streamlines are calculated on the surface with a given curvature vector field.Salient regions are detected by determining maxima and minima of a scalar field. Compared with existing featurelines and hatching methods, ConFIS uses the advantages of both categories in an effective and flexible manner. Wedemonstrate this with different anatomical and artificial surface models. In addition, we conducted a qualitativeevaluation of our technique to compare our results with exemplary feature line and hatching methods.},
author = {Lawonn, K. and Moench, T. and Preim, B.},
doi = {https://doi.org/10.1111/cgf.12119},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/Computer Graphics Forum - 2013 - Lawonn - Streamlines for Illustrative Real‐Time Rendering.pdf:pdf},
journal = {Computer Graphics Forum},
mendeley-groups = {NonPhotorealisticRendering},
number = {3},
pages = {312--330},
title = {{Computer Graphics Forum - 2013 - Lawonn - Streamlines for Illustrative Real‐Time Rendering}},
volume = {32},
year = {2013}
}


@article{Philbrick2019,
abstract = {We define hatching—a drawing technique—as rigorously as possible. A pure mathematical formulation or even a binary this-or-that definition is unreachable, but useful insights come from driving as close as we can. First we explain hatching's purposes. Then we define hatching as the use of patches: groups of roughly parallel curves that form flexible, simple patterns. After elaborating on this definition's parts, we briefly treat considerations for research in expressive rendering.},
author = {Philbrick, G. and Kaplan, C. S.},
doi = {10.2312/exp.20191082},
file = {:C\:/Users/adminuser/Downloads/111-121.pdf:pdf},
isbn = {9783038680789},
journal = {EXPRESSIVE 2019 - ACM/EG Expressive Symposium},
keywords = {Computer Graphics Forum, EUROGRAPHICS},
mendeley-groups = {NonPhotorealisticRendering},
pages = {111--121},
title = {{Defining Hatching in Art}},
year = {2019}
}

@article{Chen2012,
abstract = {Many augmented reality (AR) applications require a seamless blending of real and virtual content as key to increased immersion and improved user experiences. Photorealistic and non-photorealistic rendering (NPR) are two ways to achieve this goal. Compared with photorealistic rendering, NPR stylizes both the real and virtual content and makes them indistinguishable. Maintaining temporal coherence is a key challenge in NPR. We propose a NPR framework with support for temporal coherence by leveraging model-space information. Our systems targets painterly rendering styles of NPR. There are three major steps in this rendering framework for creating coherent results: tensor field creation, brush anchor placement, and brush stroke reshaping. To achieve temporal coherence for the final rendered results, we propose a new projection-based surface sampling algorithm which generates anchor points on model surfaces. The 2D projections of these samples are uniformly distributed in image space for optimal brush stroke placement. We also propose a general method for averaging various properties of brush stroke textures, such as their skeletons and colors, to further improve the temporal coherence. We apply these methods to both static and animated models to create a painterly rendering style for AR. Compared with existing image space algorithms our method renders AR with NPR effects with a high degree of coherence. {\textcopyright} 2012 IEEE.},
author = {Chen, Jiajian and Turk, Greg and MacIntyre, Blair},
doi = {10.1109/ISMAR.2012.6402552},
file = {:C\:/Users/adminuser/Downloads/A_non-photorealistic_rendering_framework_with_temporal_coherence_for_augmented_reality.pdf:pdf},
isbn = {9781467346603},
journal = {ISMAR 2012 - 11th IEEE International Symposium on Mixed and Augmented Reality 2012, Science and Technology Papers},
keywords = {H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial, Augmented, and Virtual Realities,I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality},
mendeley-groups = {NonPhotorealisticRendering},
pages = {151--160},
publisher = {IEEE},
title = {{A non-photorealistic rendering framework with temporal coherence for augmented reality}},
year = {2012}
}

@article{Chen2011,
abstract = {A seamless blending of the real and virtual worlds is key to increased immersion and improved user experiences for augmented reality (AR). Photorealistic and non-photorealistic rendering (NPR) are two ways to achieve this goal. Non-photorealistic rendering creates an abstract version of both the real and virtual world by stylization to make them indistinguishable. We present a painterly rendering algorithm for AR applications. This algorithm paints composed AR video frames with bump-mapping curly brushstrokes. Tensor fields are created for each frame to define the direction for the brushstrokes. We use tensor field pyramids to interpolate sparse tensor field values over the frame to avoid the numeric problems caused by global radial basis interpolation in existing algorithms. Due to the characteristics of AR applications we use only information from the current frame and previous frame to provide temporal coherence in two ways for the painted video. First, brushstroke anchors are warped from the previous frame to the current frame based on their neighbor feature points. Second, brushstroke appearances are reshaped by blending two parameterized brushstrokes to achieve better temporal coherence. Themajor difference between our algorithmand existing NPR work in general graphics and AR/VR areas is that we use feature points across AR frames to maintain coherence in the rendering. The use of tensor field pyramids and extra properties of brushstrokes, such as cut-off angles, are also novel features that extend exiting NPR algorithms. {\textcopyright} 2011 IEEE.},
author = {Chen, Jiajian and Turk, Greg and MacIntyre, Blair},
doi = {10.1109/ISVRI.2011.5759610},
file = {:C\:/Users/adminuser/Downloads/Painterly_rendering_with_coherence_for_augmented_reality.pdf:pdf},
isbn = {9781457700538},
journal = {ISVRI 2011 - IEEE International Symposium on Virtual Reality Innovations 2011, Proceedings},
keywords = {H.5.1 [Information Interfaces and Presentation]: Multimedia Information SystemsArtificial Augmented and Virtual Realities,I.3.7 [Computer Graphics]: Three-Dimensional Graphics and RealismVirtual Reality},
mendeley-groups = {NonPhotorealisticRendering},
pages = {103--110},
publisher = {IEEE},
title = {{Painterly rendering with coherence for augmented reality}},
year = {2011}
}

@article{Nagata1983,
abstract = {The physical conditions of the display of single two-dimensional pictures, which produce images realistically, were studied by using the characteristics of the intake of the information for visual depth perception. 'Depth sensitivity' which is defined as the ratio of viewing distance to depth discrimination threshold has been introduced in order to evaluate the availability of various cues for depth perception; binocular parallax, motion parallax, accommodation, convergence, size, texture, brightness, and air-perspective contrast. The effects of binocular parallax in different conditions, the depth sensitivity of which is greatest at a distance of up to about 10 m, were studied with the new versatile stereoscopic display. From these results, four conditions to reinforce the perception of depth in single pictures were proposed, and these conditions are met by the old viewing devices and the new high definition and wide television displays.},
author = {Nagata, Shojiro},
doi = {10.1201/9781482295177-48},
file = {:D\:/Thomas/AnDeModifiedLectureSlides/Week9/19900013621.pdf:pdf},
issn = {00361496},
journal = {Proceedings of the SID},
mendeley-groups = {DepthPerceptionPapers/2FAC},
number = {3},
pages = {239--246},
title = {{How To Reinforce Perception of Depth in Single Two-Dimensional Pictures.}},
volume = {25},
year = {1983}
}

@article{Maciejewski2008,
abstract = {When people compare a computer-generated illustration to a hand-drawn illustration of the same object, they usually perceive differences. This seems to indicate that the two kinds of images follow different aesthetic principles. To explore and explain these differences, the authors compare texture stippling in hand-drawn and computer-generated illustrations, using image-processing analysis techniques. {\textcopyright} 2008 IEEE.},
author = {Maciejewski, Ross and Isenberg, Tobias and Andrews, William M. and Ebert, David S. and Sousa, Mario Costa and Chen, Wei},
doi = {10.1109/MCG.2008.35},
file = {:C\:/Users/adminuser/Downloads/Measuring_Stipple_Aesthetics_in_Hand-Drawn_and_Com (1).pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
keywords = {Computer-generated stippling,Hand-drawn stippling,Non-photorealistic rendering},
mendeley-groups = {NonPhotorealisticRendering},
number = {2},
pages = {62--74},
pmid = {18350934},
title = {{Measuring stipple aesthetics in hand-drawn and computer-generated images}},
volume = {28},
year = {2008}
}

@article{Gerl2012,
author = {Gerl, Moritz and Isenberg, Tobias},
file = {:C\:/Users/adminuser/Downloads/Gerl_2013_IEH.pdf:pdf},
keywords = {example-based,hatching,hatching by example,illustrations by example,illustrative rendering,interactive example-based,interactive illustrative rendering,learning,non-photorealistic rendering,pen-and-ink,style transfer},
mendeley-groups = {NonPhotorealisticRendering},
title = {{Interactive Example-based Hatching}},
year = {2012},
journal = {Computers \& Graphics}
}

@article{Chen2022,
abstract = {The autostereoscopic display is a promising way towards three-dimensional-display technology since it allows humans to perceive stereoscopic images with naked eyes. However, it faces great challenges from low resolution, narrow viewing angle, ghost images, eye strain, and fatigue. Nowadays, the prevalent liquid crystal display (LCD), the organic light-emitting diode (OLED), and the emerging micro light-emitting diode (Micro-LED) offer more powerful tools to tackle these challenges. First, we comprehensively review various implementations of autostereoscopic displays. Second, based on LCD, OLED, and Micro-LED, their pros and cons for the implementation of autostereoscopic displays are compared. Lastly, several novel implementations of autostereoscopic displays with Micro-LED are proposed: a Micro-LED light-stripe backlight with an LCD, a high-resolution Micro-LED display with a micro-lens array or a high-speed scanning barrier/deflector, and a transparent floating display. This work could be a guidance for Micro-LED applications on autostereoscopic displays.},
author = {Chen, Fu Hao and Qiu, Chengfeng and Liu, Zhaojun},
doi = {10.3390/nano12030429},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen, Qiu, Liu - 2022 - Investigation of Autostereoscopic Displays Based on Various Display Technologies.pdf:pdf},
issn = {20794991},
journal = {Nanomaterials},
keywords = {Autostereoscopic display,Light-emitting diode,Micro-LED},
mendeley-groups = {Displays},
number = {3},
pages = {1--16},
title = {{Investigation of Autostereoscopic Displays Based on Various Display Technologies}},
volume = {12},
year = {2022}
}

@article{Philbrick2022,
abstract = {In art, hatching means drawing patterns of roughly parallel lines. Even with skill and time, an artist can find these patterns difficult to create and edit. Our new artistic primitive - the hatching shape - facilitates hatching for an artist drawing from imagination. A hatching shape comprises a mask and three fields: width, spacing, and direction. Streamline advection uses these fields to create hatching marks. A hatching shape also contains barrier curves: deliberate discontinuities useful for drawing complex forms. We explain several operations on hatching shapes, such as the multi-dir operation, an easy way to depict 3D form using a hatching shape's direction field. We also explain the modifications to streamline advection necessary to produce hatching marks from a hatching shape.},
author = {Philbrick, Greg and Kaplan, Craig S.},
doi = {10.1145/3503460},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/3503460.pdf:pdf},
issn = {15577368},
journal = {ACM Transactions on Graphics},
keywords = {Drawing,direction field,hatching,illustration,texture},
mendeley-groups = {NonPhotorealisticRendering},
number = {2},
title = {{A Primitive for Manual Hatching}},
volume = {41},
year = {2022}
}




@book{Vinson1967,
address = {Atlanta},
author = {Vinson, J. Chal},
edition = {1},
mendeley-groups = {NonPhotorealisticRendering},
pages = {0--178},
publisher = {Athens: University of Georgia Press},
title = {{Thomas Nast. Political Cartoonist}},
year = {1967}
}


@article{McCanless2000,
author = {McCanless, Jeffrey W. and Ellis, Stephen R. and Adelstein, Bernard D.},
doi = {10.1162/105474600566583},
file = {:D\:/Thomas/Downloads/mccandless2000.pdf:pdf},
journal = {Presence: Teleoperators and Virtual Environments},
mendeley-groups = {DepthPerceptionPapers},
number = {1},
pages = {15--24},
title = {{Localization of a Time-Delayed, Monocular Virtual Object Superimposed on a Real Environment}},
volume = {9},
year = {2000}
}

@article{McIntire2012,
abstract = {This work reviews the human factors-related literature on the task performance implications of stereoscopic 3D displays, in order to point out the specific performance benefits (or lack thereof) one might reasonably expect to observe when utilizing these displays. What exactly is 3D good for? Relative to traditional 2D displays, stereoscopic displays have been shown to enhance performance on a variety of depth-related tasks. These tasks include judging absolute and relative distances, finding and identifying objects (by breaking camouflage and eliciting perceptual "pop-out"), performing spatial manipulations of objects (object positioning, orienting, and tracking), and navigating. More cognitively, stereoscopic displays can improve the spatial understanding of 3D scenes or objects, improve memory/recall of scenes or objects, and improve learning of spatial relationships and environments. However, for tasks that are relatively simple, that do not strictly require depth information for good performance, where other strong cues to depth can be utilized, or for depth tasks that lie outside the effective viewing volume of the display, the purported performance benefits of 3D may be small or altogether absent. Stereoscopic 3D displays come with a host of unique human factors problems including the simulator-sickness-type symptoms of eyestrain, headache, fatigue, disorientation, nausea, and malaise, which appear to effect large numbers of viewers (perhaps as many as 25% to 50% of the general population). Thus, 3D technology should be wielded delicately and applied carefully; and perhaps used only as is necessary to ensure good performance. {\textcopyright} 2012 Copyright Society of Photo-Optical Instrumentation Engineers (SPIE).},
author = {McIntire, John P. and Havig, Paul R. and Geiselman, Eric E.},
doi = {10.1117/12.920017},
file = {:C\:/Users/adminuser/Downloads/McIntireHavigGeiselman2012-AReviewofTaskPerformanceonStereoscopic3DDisplays.pdf:pdf},
isbn = {9780819490612},
issn = {0277786X},
journal = {Head- and Helmet-Mounted Displays XVII; and Display Technologies and Applications for Defense, Security, and Avionics VI},
mendeley-groups = {3DVisulisationsVs2D},
number = {February},
pages = {83830X},
title = {{What is 3D good for? A review of human performance on stereoscopic 3D displays}},
volume = {8383},
year = {2012}
}

@article{Thomas2015,
abstract = {This paper presents a proposal for the use of Spatial Augmented Reality as a tool for 3D data visualizations. The use of traditional display technologies such as LCD monitors provides a fish tank metaphor for the user, i.e. the information is behind a plane of glass. The use of VR technologies allows the user to be immersed in the 3D volume and remove this fish tank problem, but can limit the set of techniques that allow users to interact with spatial data. Spatial Augmented Reality employed in conjunction with a highresolution monitor provides an elegant blend of spatial reasoning, tangible interaction, and detailed information viewing. This paper purposes a range of usages for SAR in 3D visualizations.},
author = {Thomas, Bruce H. and Marner, Michael and Smith, Ross T. and Elsayed, Neven Abdelaziz Mohamed and {Von Itzstein}, Stewart and Klein, Karsten and Adcock, Matt and Eades, Peter and Irlitti, Andrew and Zucco, Joanne and Simon, Timothy and Baumeister, James and Suthers, Timothy},
doi = {10.1109/3DVis.2014.7160099},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Thomas et al. - 2015 - Spatial augmented reality - A tool for 3D data visualization.pdf:pdf},
isbn = {9781479968268},
journal = {2014 IEEE VIS International Workshop on 3DVis, 3DVis 2014},
keywords = {3D Visualization,Spatial Augmented Reality,Spatial User Interaction},
mendeley-groups = {3DVisulisationsVs2D},
number = {September 2016},
pages = {45--50},
title = {{Spatial augmented reality - A tool for 3D data visualization}},
year = {2015}
}

@article{Brath2015,
abstract = {3D information visualization has existed for more than 100 years. 3D offers intrinsic attributes such as an extra dimension for encoding position and length, meshes and surfaces; lighting and separation. Further 3D can aid mental models for configuration of data within a 3D spatial framework. Perceived issues with 3D are solvable and successful, specialized information visualizations can be built.},
author = {Brath, Richard},
doi = {10.1109/3DVis.2014.7160096},
file = {:D\:/Thomas/Downloads/3D_InfoVis_is_here_to_stay_Deal_with_it.pdf:pdf},
isbn = {9781479968268},
journal = {2014 IEEE VIS International Workshop on 3DVis, 3DVis 2014},
keywords = {3D Information Visualization},
mendeley-groups = {DataVis/3D,3DVisulisationsVs2D,GraphPapers},
pages = {25--31},
publisher = {IEEE},
title = {{3D InfoVis is here to stay: Deal with it}},
year = {2015}
}

@article{Tang2019,
abstract = {Introduction
 The field of augmented reality (AR) is rapidly growing with many new potential applications in medical education. This systematic review aims to investigate the current state of augmented reality applications (ARAs) as teaching tools in the healthcare field.
 Methods
 A literature search was conducted using PubMed, Embase, Web of Science, Cochrane Library, and Google Scholar. This review followed PRISMA guidelines and included publications from January 1, 2000 to June 18, 2018. Inclusion criteria were experimental studies evaluating ARAs implemented in healthcare education published in English. The quality of each study was assessed using GRADE criteria. Five stages of validity were also evaluated for each ARA.
 Results
 We identified 100,807 articles in the initial literature search; 36 met inclusion criteria for final review and were categorized into three categories: Surgery (23), Anatomy (9), and Classroom (4). The overall quality of the studies was poor. No ARAs were validated at all five levels.
 Conclusion
 While AR technology is growing at a rapid rate, the current quality and breadth of AR research in medical training is insufficient to recommend the adoption into educational curricula. More coordinated and comprehensive research is needed to define the role of AR technology in medical education. },
author = {Tang, Kevin S and Cheng, Derrick L and Mi, Eric and Greenberg, Paul B},
doi = {10.36834/cmej.61705},
file = {:D\:/Thomas/Downloads/CMEJ-11-e081.pdf:pdf},
journal = {Canadian Medical Education Journal},
mendeley-groups = {GreatPapersThatICouldn'tGetAroundToReading},
number = {1},
pages = {81--96},
title = {{Augmented reality in medical education: a systematic review}},
volume = {11},
year = {2019}
}

@article{Okuyan2014,
abstract = {Visualization of the materials is an indispensable part of their structural analysis. We developed a visualization tool for amorphous as well as crystalline structures, called MaterialVis. Unlike the existing tools, MaterialVis represents material structures as a volume and a surface manifold, in addition to plain atomic coordinates. Both amorphous and crystalline structures exhibit topological features as well as various defects. MaterialVis provides a wide range of functionality to visualize such topological structures and crystal defects interactively. Direct volume rendering techniques are used to visualize the volumetric features of materials, such as crystal defects, which are responsible for the distinct fingerprints of a specific sample. In addition, the tool provides surface visualization to extract hidden topological features within the material. Together with the rich set of parameters and options to control the visualization, MaterialVis allows users to visualize various aspects of materials very efficiently as generated by modern analytical techniques such as the Atom Probe Tomography. {\textcopyright} 2014 Elsevier Inc.},
author = {Okuyan, Erhan and G{\"{u}}d{\"{u}}kbay, Uǧur and Bulutay, Ceyhun and Heinig, Karl Heinz},
doi = {10.1016/j.jmgm.2014.03.007},
file = {:C\:/Users/adminuser/Downloads/1-s2.0-S1093326314000436-main.pdf:pdf},
issn = {18734243},
journal = {Journal of Molecular Graphics and Modelling},
keywords = {Crystal defects,Direct volume rendering,Embedded nano-structure visualization,Material visualization,Unstructured tetrahedral meshes},
mendeley-groups = {Material Stucture Anaysis},
pages = {50--60},
pmid = {24739396},
title = {{MaterialVis: Material visualization tool using direct volume and surface rendering techniques}},
volume = {50},
year = {2014}
}

@article{Groger2022,
abstract = {Recent developments in automotive and aircraft industry towards a multi-material design pose challenges for modern joining technologies due to different mechanical properties and material compositions of various materials such as composites and metals. Therefore, mechanical joining technologies like clinching are in the focus of current research activities. For multi-material joints of metals and thermoplastic composites thermally assisted clinching processes with advanced tool concepts are well developed. The material-specific properties of fibre-reinforced thermoplastics have a significant influence on the joining process and the resulting material structure in the joining zone. For this reason, it is important to investigate these influences in detail and to understand the phenomena occurring during the joining process. Additionally, this provides the basis for a validation of a numerical simulation of such joining processes. In this paper, the material structure in a joint resulting from a thermally assisted clinching process is investigated. The joining partners are an aluminium sheet and a thermoplastic composite (organo sheet). Using computed tomography enables a three-dimensional investigation that allows a detailed analysis of the phenomena in different joining stages and in the material structure of the finished joint. Consequently, this study provides a more detailed understanding of the material behavior of thermoplastic composites during thermally assisted clinching.},
author = {Gr{\"{o}}ger, Benjamin and K{\"{o}}hler, Daniel and Vorderbr{\"{u}}ggen, Julian and Troschitz, Juliane and Kupfer, Robert and Meschut, Gerson and Gude, Maik},
doi = {10.1007/s11740-021-01091-x},
file = {:C\:/Users/adminuser/Downloads/s11740-021-01091-x (1).pdf:pdf},
isbn = {0123456789},
issn = {18637353},
journal = {Production Engineering},
keywords = {Clinching,Composite,Computed tomography (CT),Multi-material-design},
mendeley-groups = {Material Stucture Anaysis},
number = {2-3},
pages = {203--212},
publisher = {Springer Berlin Heidelberg},
title = {{Computed tomography investigation of the material structure in clinch joints in aluminium fibre-reinforced thermoplastic sheets}},
url = {https://doi.org/10.1007/s11740-021-01091-x},
volume = {16},
year = {2022}
}

@article{Xu2021,
abstract = {Volume rendering techniques have been developed for decades and have been widely applied in many research fields, such as medical image visualization, geological exploration, scientific computing. etc. With the maturity of volume visualization techniques, one may have many choices to analyze volume data. However, facing different application requirements in specific cases, one may need pertinent methods to visualize volume data and highlight specific volume features. In this paper, we review and classify the existing literature on feature enhancement volume rendering. The classification is conducted based on the enhancement of four types of features (the external feature, the internal feature, the structure feature, and the ideographic feature) in volume data. Finally, we conclude this survey with future challenges in feature enhancement volume visualization.},
author = {Xu, Chaoqing and Sun, Guodao and Liang, Ronghua},
doi = {10.1016/j.visinf.2021.08.001},
file = {:D\:/Thomas/Downloads/1-s2.0-S2468502X21000358-main.pdf:pdf},
issn = {2468502X},
journal = {Visual Informatics},
keywords = {Feature enhancement,Visualization,Volume rendering},
mendeley-groups = {VolumeRendering},
number = {3},
pages = {70--81},
publisher = {Elsevier B.V.},
title = {{A survey of volume visualization techniques for feature enhancement}},
url = {https://doi.org/10.1016/j.visinf.2021.08.001},
volume = {5},
year = {2021}
}

@article{Khlebnikov2013,
abstract = {Analysis of multivariate data is of great importance in many scientific disciplines. However, visualization of 3D spatially-fixed multivariate volumetric data is a very challenging task. In this paper we present a method that allows simultaneous real-time visualization of multivariate data. We redistribute the opacity within a voxel to improve the readability of the color defined by a regular transfer function, and to maintain the see-through capabilities of volume rendering. We use predictable procedural noise-random-phase Gabor noise-to generate a high-frequency redistribution pattern and construct an opacity mapping function, which allows to partition the available space among the displayed data attributes. This mapping function is appropriately filtered to avoid aliasing, while maintaining transparent regions. We show the usefulness of our approach on various data sets and with different example applications. Furthermore, we evaluate our method by comparing it to other visualization techniques in a controlled user study. Overall, the results of our study indicate that users are much more accurate in determining exact data values with our novel 3D volume visualization method. Significantly lower error rates for reading data values and high subjective ranking of our method imply that it has a high chance of being adopted for the purpose of visualization of multivariate 3D data. {\textcopyright} 2013 IEEE.},
author = {Khlebnikov, Rostislav and Kainz, Bernhard and Steinberger, Markus and Schmalstieg, Dieter},
doi = {10.1109/TVCG.2013.180},
file = {:C\:/Users/adminuser/Downloads/KainzTVCG2013.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Volume rendering,multi-variate data visualization,multi-volume rendering,scientific visualization},
mendeley-groups = {VoxelAlgorithms/noise},
number = {12},
pages = {2926--2935},
pmid = {24051860},
title = {{Noise-based volume rendering for the visualization of multivariate volumetric data}},
volume = {19},
year = {2013}
}

@article{Maloca2018,
abstract = {Purpose: Feasibility testing of a novel volume renders technology to display optical coherence tomography data (OCT) in a virtual reality (VR) environment. Methods: A VR program was written in C++/OpenGL to import and display volumetric OCT data in real time with 180 frames per second using a high-end computer and a tethered head-mounted display. Following exposure, participants completed a Simulator Sickness Questionnaire (SSQ) to assess for nausea, disorientation, and oculomotor disturbances. A user evaluation study of this software was conducted to explore the potential utility of this application. Results: Fifty-seven subjects completed the user testing (34 males and 23 females). Mean age was 48.5 years (range, 21–77 years). Mean acquired work experience of the 35 ophthalmologists (61.40%) included in the group was 15.46 years (range, 1–37 years). Twenty-nine participants were VR-na{\"{i}}ve. The SSQ showed a mean total score of 5.8 (SD = 9.44) indicating that the system was well tolerated and produced minimal side effects. No difference was reported between VR-na{\"{i}}ve participants and experienced users. Overall, immersed subjects reported an enjoyable VR-OCT presence effect. Conclusions: A usable and satisfying VR imaging technique was developed to display and interact with original OCT data. Translational Relevance: An advanced high-end VR image display method was successfully developed to provide new views and interactions in an ultra high-speed projected digital scenery using point-cloud OCT data. This represents the next generation of OCT image display technology and a new tool for patient engagement, medical education, professional training, and telecommunications.},
author = {Maloca, Peter M. and de Carvalho, J. Emanuel Ramos and Heeren, Tjebo and Hasler, Pascal W. and Mushtaq, Faisal and Mon-Williams, Mark and Scholl, Hendrik P.N. and Balaskas, Konstantinos and Egan, Catherine and Tufail, Adnan and Witthauer, Lilian and Cattin, Philippe C.},
doi = {10.1167/tvst.7.4.2},
file = {:D\:/i1552-5783-7-4-2.pdf:pdf},
issn = {21642591},
journal = {Translational Vision Science and Technology},
keywords = {Mesh,Ophthalmology,Optical coherence tomography,Point cloud data,Polygon,Ray casting,Virtual reality,Volume rendering},
mendeley-groups = {VoxelAlgorithms},
number = {4},
title = {{High-performance virtual reality volume rendering of original optical coherence tomography point-cloud data enhanced with real-time ray casting}},
volume = {7},
year = {2018}
}

@article{Chandrasekhar1950,
author = {Chandrasekhar, S},
doi = {https://doi.org/10.1002/qj.49707633016},
journal = {Quarterly Journal of the Royal Meteorological Society},
number = {330},
pages = {498},
title = {{Radiative transfer. By S. Chandrasekhar. London (Oxford University Press) 1950. 8vo. Pp. 393, 35 figures. 35s}},
url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49707633016},
volume = {76},
year = {1950}
}

@article{Nakagawa2023,
abstract = {The growing need for high-frame-rate projectors in the fields of dynamic projection mapping (DPM) and three-dimensional (3D) displays has increased. Conventional methods allow for an increase in the frame rate to as much as 2,841 frames per second (fps) for 8-bit image projection, using digital light processing (DLP) technology when the minimum digital mirror device (DMD) control time is $44 \mu \mathrm{s}$. However, this rate needs to be further augmented to suit specific applications. In this study, we developed a novel high-frame-rate projection method, which divides the bit depth of an image among multiple projectors and simultaneously projects them in synchronization. The simultaneously projected bit images are superimposed such that a high-bit-depth image is generated within a reduced single-frame duration. Additionally, we devised an optimization process to determine the system parameters necessary for attaining maximum brightness. We constructed a prototype system utilizing two high-frame-rate projectors and validated the feasibility of using our system to project 8-bit images at a rate of 5,600 fps. Furthermore, the quality assessment of our projected image exhibited superior performance in comparison to a dithered image.},
author = {Nakagawa, Soran and Watanabe, Yoshihiro},
doi = {10.1109/ISMAR59233.2023.00089},
file = {:C\:/Users/tomis/OneDrive/Pictures/FiguresForBookChapterHandoff/High-Frame-Rate_Projection_with_Thousands_of_Frames_Per_Second_Based_on_the_Multi-Bit_Superimposition_Method.pdf:pdf},
isbn = {9798350328387},
journal = {Proceedings - 2023 IEEE International Symposium on Mixed and Augmented Reality, ISMAR 2023},
keywords = {Communication hardware,Displays and imagers,Hardware,Human computer interaction (HCI),Human-centered computing,Interaction paradigms,Mixed / augmented reality,interfaces,storage},
pages = {741--750},
publisher = {IEEE},
title = {{High-Frame-Rate Projection with Thousands of Frames Per Second Based on the Multi-Bit Superimposition Method}},
year = {2023}
}


@article{Joshi2009,
abstract = {The devastating power of hurricanes was evident during the 2005 hurricane season, the most active season on record. This has prompted increased efforts by researchers to understand the physical processes that underlie the genesis, intensification, and tracks of hurricanes. This research aims at facilitating an improved understanding into the structure of hurricanes with the aid of visualization techniques. Our approach was developed by a mixed team of visualization and domain experts. To better understand these systems, and to explore their representation in NWP models, we use a variety of illustration-inspired techniques to visualize their structure and time evolution. Illustration-inspired techniques aid in the identification of the amount of vertical wind shear in a hurricane, which can help meteorologists predict dissipation. Illustration-style visualization, in combination with standard visualization techniques, helped explore the vortex rollup phenomena and the mesovortices contained within. We evaluated the effectiveness of our visualization with the help of six hurricane experts. The expert evaluation showed that the illustration-inspired techniques were preferred over existing tools. Visualization of the evolution of structural features is a prelude to a deeper visual analysis of the underlying dynamics. {\textcopyright} 2006 IEEE.},
author = {Joshi, Alark and Caban, Jesus and Rheingans, Penny and Sparling, Lynn},
doi = {10.1109/TVCG.2008.105},
file = {:C\:/Users/adminuser/Downloads/Case_Study_on_Visualizing_Hurricanes_Using_Illustration-Inspired_Techniques.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Expert evaluation,Hurricane visualization,Illustration-inspired visualization,Visualization in physics},
number = {5},
pages = {709--718},
publisher = {IEEE},
title = {{Case Study on Visualizing Hurricanes Using Illustration-Inspired Techniques}},
volume = {15},
year = {2009}
}

@book{Rinck2024,
author = {Rinck, Peter A.},
edition = {14},
publisher = {The Basic Textbook of the European Magnetic Resonance Forum},
title = {{Magnetic Resonance in Medicine}},
url = {https://www.magnetic-resonance.org/},
year = {2024}
}

@incollection{DenOtter2024,
abstract = {The Hounsfield unit (HU) is a relative quantitative measurement of radio density  used by radiologists in the interpretation of computed tomography (CT) images. The absorption/attenuation coefficient of radiation within a tissue is used during CT reconstruction to produce a grayscale image. The physical density of tissue is proportional to the absorption/attenuation of the X-ray beam. The Hounsfield unit, also referred to as the CT unit, is then calculated based on a linear transformation of the baseline linear attenuation coefficient of the X-ray beam, where distilled water (at standard temperature and pressure) is arbitrarily defined to be zero Hounsfield Units and air defined as -1000 HU. The upper limits can reach up to 1000 for bones, 2000 for dense bones like the cochlea, and more than 3000 for metals like steel or silver. The linear transformation produces a Hounsfield scale that displays as gray tones. More dense tissue, with greater X-ray beam absorption, has positive values and appears bright; less dense tissue, with less X-ray beam absorption, has negative values and appears dark. The Hounsfield unit was named after Sir Godfrey Hounsfield, recipient of the Nobel Prize in Physiology or Medicine in 1979, for his part in the invention of CT, as it had immediate recognition as a revolutionary diagnostic instrument.},
author = {DenOtter, Tami D and Schubert, Johanna},
language = {eng},
month = {jan},
pmid = {31613501},
title = {{Hounsfield Unit.}},
year = {2024},
booktitle = {StatPearls},
publisher = {StatPearls Publishing},
}

@article{Jung2022,
abstract = {Direct volume rendering (DVR) is a standard technique for visualizing scientific volumetric data in three-dimension (3D). Utilizing current mixed reality head-mounted displays (MR-HMDs), the DVR can be displayed as a 3D hologram that can be superimposed on the original 'physical' object, offering supplementary X-ray visions showing its interior features. These MR-MHDs are stimulating innovations in a range of scientific application fields, yet their capabilities on DVR have yet to be thoroughly investigated. In this study, we explore a key requirement of rendering latency capability for MR*HMDs by proposing a benchmark application with 5 volumes and 30 rendering parameter variations.},
author = {Jung, Hoijoon and Jung, Younhyun and Kim, Jinman},
doi = {10.1109/VRW55335.2022.00200},
file = {:D\:/Thomas/Downloads/Understanding_the_Capabilities_of_the_HoloLens_1_and_2_in_a_Mixed_Reality_Environment_for_Direct_Volume_Rendering_with_a_Ray-casting_Algorithm.pdf:pdf},
isbn = {9781665484022},
journal = {Proceedings - 2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2022},
keywords = {Computer graphics,Computing methodologies,Cross-computing tools and techniques,Evaluation,General and reference,Graphics systems and interfaces,Mixed/augmented reality},
mendeley-groups = {VoxelAlgorithms},
pages = {698--699},
publisher = {IEEE},
title = {{Understanding the Capabilities of the HoloLens 1 and 2 in a Mixed Reality Environment for Direct Volume Rendering with a Ray-casting Algorithm}},
year = {2022}
}

@article{Cetinsaya2020,
abstract = {The volumetric rendering technique generates 3D images of computed tomography data with unprecedented image quality. Volumetric rendering is a general technique with applications in many different fields. This article describes in detail the methods and algorithms used for volumetric rendering of medical computed tomography data and includes a step-by-step description of the process used to generate two types of images (unshaded and shaded surfaces). Images generated with this technique are in routine clinical use. {\textcopyright} 1990 IEEE},
author = {Cetinsaya, Berk and Neumann, Carsten and Reiners, Dirk},
doi = {10.1109/VRW55335.2022.00235},
file = {:D\:/Thomas/AnDeModifiedLectureSlides/Week9/Using_Direct_Volume_Rendering_for_Augmented_Reality_in_Resource-constrained_Platforms.pdf:pdf},
journal = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)},
mendeley-groups = {VolumeRendering/InAR},
pages = {768--769},
title = {{Using Direct Volume Rendering for Augmented Reality in Resource-constrained Platforms}},
url = {https://ieeexplore.ieee.org/document/9757418},
year = {2022}
}

@article{Morrical2019,
abstract = {Sample based ray marching is an effective method for direct volume rendering of unstructured meshes. However, sampling such meshes remains expensive, and strategies to reduce the number of samples taken have received relatively little attention. In this paper, we introduce a method for rendering unstructured meshes using a combination of a coarse spatial acceleration structure and hardware-accelerated ray tracing. Our approach enables efficient empty space skipping and adaptive sampling of unstructured meshes, and outperforms a reference ray marcher by up to 7×.},
archivePrefix = {arXiv},
arxivId = {1908.01906},
author = {Morrical, Nate and Usher, Will and Wald, Ingo and Pascucci, Valerio},
doi = {10.1109/VISUAL.2019.8933539},
eprint = {1908.01906},
file = {:C\:/Users/adminuser/Pictures/PromoPhotosForStudy1/vis19-space-skipping-short.pdf:pdf},
isbn = {9781728149417},
journal = {2019 IEEE Visualization Conference, VIS 2019},
keywords = {Volume rendering,adaptive sampling,space skipping},
mendeley-groups = {VoxelAlgorithms/SpaceSkipping},
pages = {256--260},
title = {{Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes Using Hardware Accelerated Ray Tracing}},
volume = {1},
year = {2019}
}

@article{Hadwiger2018,
abstract = {Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.},
author = {Hadwiger, Markus and Al-Awami, Ali K. and Beyer, Johanna and Agus, Marco and Pfister, Hanspeter},
doi = {10.1109/TVCG.2017.2744238},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hadwiger et al. - 2018 - SparseLeap Efficient Empty Space Skipping for Large-Scale Volume Rendering.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Empty Space Skipping,Hybrid Image/Object-Order Approaches,Segmented Volume Data,Volume Rendering},
mendeley-groups = {VoxelAlgorithms/SpaceSkipping (1)},
number = {1},
pages = {974--983},
pmid = {28866532},
title = {{SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering}},
volume = {24},
year = {2018}
}


@article{Deakin2019,
abstract = {Volume rendering has useful applications with emerging technologies such as virtual and augmented reality. The high frame rate targets of these technologies poses a problem for volume rendering because of its very high computational complexity compared with conventional surface rendering. We developed an efficient empty space skipping algorithm for accelerating volume rendering. A distance map is generated which indicates the Chebyshev distance to the nearest occupied region (with non-transparent voxels) within a volume. The distance map is used to efficiently skip empty regions while volume ray casting. We show improved performance over state-of-the-art empty space skipping techniques.},
author = {Deakin, Lachlan and Knackstedt, Mark},
doi = {10.1145/3355088.3365164},
file = {:C\:/Users/adminuser/Downloads/2019_Deakin_Knackstedt_Accelerated_Volume_Rendering_with_Chebyshev_Distance_Maps.pdf:pdf},
isbn = {9781450369459},
journal = {SIGGRAPH Asia 2019 Technical Briefs, SA 2019},
keywords = {Distance map,Empty space skipping,Ray casting,Volume rendering},
mendeley-groups = {VoxelAlgorithms/SpaceSkipping},
pages = {25--28},
title = {{Accelerated volume rendering with Chebyshev distance maps}},
year = {2019}
}

@article{VanDamme2021,
abstract = {We develop a method based on tensor networks to create localized single-particle excitations on top of strongly correlated quantum spin chains. In analogy to the problem of creating localized Wannier modes, this is achieved by optimizing the gauge freedom of momentum excitations on top of matrix product states. The corresponding wave packets propagate almost dispersionlessly. The time-dependent variational principle is used to scatter two such wave packets, and we extract the phase shift from the collision data. We also study reflection and transmission coefficients of a wave packet impinging on an impurity.},
archivePrefix = {arXiv},
arxivId = {1907.02474},
author = {{Van Damme}, Maarten and Vanderstraeten, Laurens and {De Nardis}, Jacopo and Haegeman, Jutho and Verstraete, Frank},
doi = {10.1103/PhysRevResearch.3.013078},
eprint = {1907.02474},
file = {:C\:/Users/adminuser/Downloads/PhysRevResearch.3.013078 (1).pdf:pdf},
issn = {26431564},
journal = {Physical Review Research},
keywords = {doi:10.1103/PhysRevResearch.3.013078 url:https://d},
number = {1},
publisher = {American Physical Society},
title = {{Real-time scattering of interacting quasiparticles in quantum spin chains}},
volume = {3},
year = {2021}
}

@article{Corcoran2012,
abstract = {We propose improvements to real-time two-level rendering incorporating ray cast shadows and ambient occlusion of 3D volumetric datasets. Our ambient occlusion calculation utilises the same sampling scheme as standard per-voxel Phong shading thus allowing for an extremely computationally efficient rendering in comparison to other ambient occlusion algorithms. Our ray cast shadows technique requires no pre-processing, does not significantly increase memory requirements and is compatible with ray cast volume renderers. We validate these techniques through a number of user experiments. The results indicate that our ambient occlusion method increases the visual information in an image. Meanwhile, ray cast shadows appear to not provide the hypothesised improvement to image understanding, however this raises some interesting implications regarding the practical relevance of shadows in such two-level volume renderings. {\textcopyright} 2012 Springer-Verlag.},
author = {Corcoran, Andrew and Dingliana, John},
doi = {10.1007/978-3-642-33179-4_52},
file = {:C\:/Users/adminuser/Downloads/978-3-642-33179-4_52.pdf:pdf},
isbn = {9783642331787},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {VolumeRendering},
number = {PART 1},
pages = {544--555},
title = {{Real-time illumination for two-level volume rendering}},
volume = {7431 LNCS},
year = {2012}
}

@incollection{Vicente17,
address = {Rijeka},
author = {Vicente, Miguel A and M{\'{i}}nguez, Jes{\'{u}}s and Gonz{\'{a}}lez, Dorys C},
booktitle = {Computed Tomography},
chapter = {10},
doi = {10.5772/intechopen.69245},
editor = {Halefoglu, Ahmet Mesrur},
mendeley-groups = {Material Stucture Anaysis},
publisher = {IntechOpen},
title = {{The Use of Computed Tomography to Explore the Microstructure of Materials in Civil Engineering: From Rocks to Concrete}},
url = {https://doi.org/10.5772/intechopen.69245},
year = {2017}
}


@InCollection{sep-embodied-cognition,
	author       =	{Shapiro, Lawrence and Spaulding, Shannon},
	title        =	{{Embodied Cognition}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2021/entries/embodied-cognition/}},
	year         =	{2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@book{GPU_Gems2,
author = {Pharr, Matt and Fernando, Randima},
title = {GPU Gems 2: Programming Techniques for High-Performance Graphics and General-Purpose Computation (Gpu Gems)},
year = {2005},
isbn = {0321335597},
publisher = {Addison-Wesley Professional}
}

@inproceedings{Kraft2020,
abstract = {Direct volume rendering is used to visualize data from sources such as tomographic imaging devices. The perception of certain structures depends very much on visual cues such as lighting and shadowing. According illumination techniques have been proposed for both surface rendering and volume rendering. However, in the case of direct volume rendering, some form of precomputation is typically required for real-time rendering. This however limits the application of the visualization. In this work we present adaptive volumetric illumination sampling, a ray-casting-based direct volume rendering method that strongly reduces the amount of necessary illumination computations without introducing any noise. By combining it with voxel cone tracing, realistic lighting including ambient occlusion and image-based lighting is facilitated in real-time. The method only requires minimal precomputation and allows for interactive transfer function updates and clipping of the visualized data.},
author = {Kraft, Valentin and Link, Florian and Schenk, Andrea and Schumann, Christian},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-61864-3_10},
file = {:C\:/Users/adminuser/Downloads/978-3-030-61864-3_10.pdf:pdf},
isbn = {9783030618636},
issn = {16113349},
keywords = {Ambient occlusion,Augmented reality,Direct volume rendering,Ray casting,Realistic lighting,Voxel cone tracing},
mendeley-groups = {VoxelAlgorithms},
pages = {107--118},
publisher = {Springer International Publishing},
title = {{Adaptive Illumination Sampling for Direct Volume Rendering}},
url = {http://dx.doi.org/10.1007/978-3-030-61864-3_10},
volume = {12221 LNCS},
year = {2020}
}

@article{Li2003,
abstract = {We propose methods to accelerate texture-based volume rendering by skipping invisible voxels. We partition the volume into sub-volumes, each containing voxels with similar properties. Sub-volumes composed of only voxels mapped to empty by the transfer function are skipped. To render the adaptively partitioned sub-volumes in visibility order, we reorganize them into an orthogonal BSP tree. We also present an algorithm that computes incrementally the intersection of the volume with the slicing planes, which avoids the overhead of the intersection and texture coordinates computation introduced by the partitioning. Rendering with empty space skipping is 2 to 5 times faster than without it. To skip occluded voxels, we introduce the concept of orthogonal opacity map, that simplifies the transformation between the volume coordinates and the opacity map coordinates, which is intensively used for occlusion detection. The map is updated efficiently by the GPU. The sub-volumes are then culled and clipped against the opacity map. We also present a method that adaptively adjusts the optimal number of the opacity map updates. With occlusion clipping, about 60% of non-empty voxels can be skipped and an additional 80% speedup on average is gained for iso-surface-like rendering.},
author = {Li, Wei and Mueller, Klaus and Kaufman, Arie},
doi = {10.1109/visual.2003.1250388},
file = {:C\:/Users/adminuser/Pictures/PromoPhotosForStudy1/Empty_space_skipping_and_occlusion_clipping_for_texture-based_volume_rendering.pdf:pdf},
isbn = {0780381203},
journal = {Proceedings of the IEEE Visualization Conference},
keywords = {Empty space skipping,Graphics hardware,Occlusion clipping,Orthogonal opacity map,Texture-based volume rendering},
mendeley-groups = {VoxelAlgorithms/SpaceSkipping},
number = {Cvc},
pages = {317--324},
title = {{Empty Space Skipping and Occlusion Clipping for Texture-based Volume Rendering}},
volume = {4400},
year = {2003}
}

@article{Zellmann2019,
abstract = {Empty space skipping can be efficiently implemented with hierarchical data structures such as k-d trees and bounding volume hierarchies. This paper compares several recently published hierarchical data structures with regard to construction and rendering performance. The papers that form our prior work have primarily focused on interactively building the data structures and only showed that rendering performance is superior to using simple acceleration data structures such as uniform grids with macro cells. In the area of surface ray tracing, there exists a trade-off between construction and rendering performance of hierarchical data structures. In this paper we present performance comparisons for several empty space skipping data structures in order to determine if such a trade-off also exists for volume rendering with uniform data topologies.},
archivePrefix = {arXiv},
arxivId = {1912.09596},
author = {Zellmann, Stefan},
doi = {10.36227/techrxiv.11416422},
eprint = {1912.09596},
file = {:C\:/Users/adminuser/Pictures/PromoPhotosForStudy1/1912.09596.pdf:pdf},
mendeley-groups = {VoxelAlgorithms/SpaceSkipping},
title = {{Comparing Hierarchical Data Structures for Sparse Volume Rendering with Empty Space Skipping}},
url = {http://arxiv.org/abs/1912.09596},
year = {2019},
journal = {ArXiv},
publisher = {ArXiv},
}

@article{Wu2009,
abstract = {Digital breast tomosynthesis uses a limited number (typically 10-20) of low-dose x-ray projections to produce a pseudo-three-dimensional volume tomographic reconstruction of the breast. The purpose of this investigation was to characterize and evaluate the effect of scattered radiation on the image quality for breast tomosynthesis. In a simulation, scatter point spread functions generated by a Monte Carlo simulation method were convolved over the breast projection to estimate the distribution of scatter for each angle of tomosynthesis projection. The results demonstrate that in the absence of scatter reduction techniques, images will be affected by cupping artifacts, and there will be reduced accuracy of attenuation values inferred from the reconstructed images. The effect of x-ray scatter on the contrast, noise, and lesion signal-difference-to-noise ratio (SDNR) in tomosynthesis reconstruction was measured as a function of the tumor size. When a with-scatter reconstruction was compared to one without scatter for a 5 cm compressed breast, the following results were observed. The contrast in the reconstructed central slice image of a tumorlike mass (14 mm in diameter) was reduced by 30%, the voxel value (inferred attenuation coefficient) was reduced by 28%, and the SDNR fell by 60%. The authors have quantified the degree to which scatter degrades the image quality over a wide range of parameters relevant to breast tomosynthesis, including x-ray beam energy, breast thickness, breast diameter, and breast composition. They also demonstrate, though, that even without a scatter rejection device, the contrast and SDNR in the reconstructed tomosynthesis slice are higher than those of conventional mammographic projection images acquired with a grid at an equivalent total exposure. {\textcopyright} 2009 American Association of Physicists in Medicine.},
author = {Wu, Gang and Mainprize, James G. and Boone, John M. and Yaffe, Martin J.},
doi = {10.1118/1.3215926},
file = {:C\:/Users/adminuser/Downloads/MPHYA6-000036-004425_1.pdf:pdf},
issn = {00942405},
journal = {Medical Physics},
keywords = {Breast imaging,Contrast,Image quality,SDNR,Tomosynthesis,X-ray scatter},
mendeley-groups = {VoxelAlgorithms},
number = {10},
pages = {4425--4432},
pmid = {19928073},
title = {{Evaluation of scatter effects on image quality for breast tomosynthesis}},
volume = {36},
year = {2009}
}

@article{Siewerdsen2006,
abstract = {X-ray scatter poses a significant limitation to image quality in cone-beam CT (CBCT), resulting in contrast reduction, image artifacts, and lack of CT number accuracy. We report the performance of a simple scatter correction method in which scatter fluence is estimated directly in each projection from pixel values near the edge of the detector behind the collimator leaves. The algorithm operates on the simple assumption that signal in the collimator shadow is attributable to x-ray scatter, and the 2D scatter fluence is estimated by interpolating between pixel values measured along the top and bottom edges of the detector behind the collimator leaves. The resulting scatter fluence estimate is subtracted from each projection to yield an estimate of the primary-only images for CBCT reconstruction. Performance was investigated in phantom experiments on an experimental CBCT bench-top, and the effect on image quality was demonstrated in patient images (head, abdomen, and pelvis sites) obtained on a preclinical system for CBCT-guided radiation therapy. The algorithm provides significant reduction in scatter artifacts without compromise in contrast-to-noise ratio (CNR). For example, in a head phantom, cupping artifact was essentially eliminated, CT number accuracy was restored to within 3%, and CNR (breast-to-water) was improved by up to 50%. Similarly in a body phantom, cupping artifact was reduced by at least a factor of 2 without loss in CNR. Patient images demonstrate significantly increased uniformity, accuracy, and contrast, with an overall improvement in image quality in all sites investigated. Qualitative evaluation illustrates that soft-tissue structures that are otherwise undetectable are clearly delineated in scatter-corrected reconstructions. Since scatter is estimated directly in each projection, the algorithm is robust with respect to system geometry, patient size and heterogeneity, patient motion, etc. Operating without prior information, analytical modeling, or Monte Carlo, the technique is easily incorporated as a preprocessing step in CBCT reconstruction to provide significant scatter reduction. {\textcopyright} 2006 American Association of Physicists in Medicine.},
author = {Siewerdsen, J. H. and Daly, M. J. and Bakhtiar, B. and Moseley, D. J. and Richard, S. and Keller, H. and Jaffray, D. A.},
doi = {10.1118/1.2148916},
file = {:C\:/Users/adminuser/Downloads/Medical Physics - 2005 - Siewerdsen - A simple  direct method for x‐ray scatter estimation and correction in digital.pdf:pdf},
issn = {00942405},
journal = {Medical Physics},
keywords = {Artifacts,Computed tomography,Cone-beam CT,Flat-panel imager,Imaging performance,Scatter correction,Scatter-to-primary ratio,X-ray scatter},
mendeley-groups = {VoxelAlgorithms},
number = {1},
pages = {187--197},
pmid = {16485425},
title = {{A simple, direct method for x-ray scatter estimation and correction in digital radiography and cone-beam CT}},
volume = {33},
year = {2006}
}


@article{Li2010c,
abstract = {In order to efficiently and effectively reconstruct 3D medical images and clearly display the detailed information of inner structures and the inner hidden interfaces between different media, an Improved Volume Rendering Optical Model (IVROM) for medical translucent volume rendering and its implementation using the preintegrated Shear-Warp Volume Rendering algorithm are proposed in this paper, which can be readily applied on a commodity PC. Based on the classical absorption and emission model, effects of volumetric shadows and direct and indirect scattering are also considered in the proposed model IVROM. Moreover, the implementation of the Improved Translucent Volume Rendering Method (ITVRM) integrating the IVROM model, Shear-Warp and preintegrated volume rendering algorithm is described, in which the aliasing and staircase effects resulting from under-sampling in Shear-Warp, are avoided by the preintegrated volume rendering technique. This study demonstrates the superiority of the proposed method. Copyright {\textcopyright} 2010 Bin Li et al.},
author = {Li, Bin and Tian, Lianfang and Ou, Shanxing},
doi = {10.1155/2010/429051},
file = {:C\:/Users/adminuser/Downloads/IJBI2010-429051.pdf:pdf},
issn = {16874188},
journal = {International Journal of Biomedical Imaging},
mendeley-groups = {VoxelAlgorithms},
title = {{An optical model for translucent volume rendering and its implementation using the preintegrated shear-warp algorithm}},
volume = {2010},
year = {2010}
}

@article{Kniss2003,
abstract = {Direct volume rendering is a commonly used technique in visualization applications. Many of these applications require sophisticated shading models to capture subtle lighting effects and characteristics of volumetric data and materials. For many volumes, homogeneous regions pose problems for typical gradient-based surface shading. Many common objects and natural phenomena exhibit visual quality that cannot be captured using simple lighting models or cannot be solved at interactive rates using more sophisticated methods. We present a simple yet effective interactive shading model which captures volumetric light attenuation effects that incorporates volumetric shadows, an approximation to phase functions, an approximation to forward scattering, and chromatic attenuation that provides the subtle appearance of translucency. We also present a technique for volume displacement or perturbation that allows realistic interactive modeling of high frequency detail for both real and synthetic volumetric data.},
author = {Kniss, Joe and Premoze, Simon and Hansen, Charles and Shirley, Peter and McPherson, Allen},
doi = {10.1109/TVCG.2003.1196003},
file = {:C\:/Users/adminuser/Downloads/volumeModeling.pdf:pdf},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Clouds,Fur,Procedural modeling,Shading model,Volume modeling,Volume perturbation,Volume rendering},
mendeley-groups = {VoxelAlgorithms},
number = {2},
pages = {150--162},
title = {{A model for volume lighting and modeling}},
volume = {9},
year = {2003}
}

@article{Li2010d,
abstract = {In order to efficiently and effectively reconstruct 3D medical images and clearly display the detailed information of inner structures and the inner hidden interfaces between different media, an Improved Volume Rendering Optical Model (IVROM) for medical translucent volume rendering and its implementation using the preintegrated Shear-Warp Volume Rendering algorithm are proposed in this paper, which can be readily applied on a commodity PC. Based on the classical absorption and emission model, effects of volumetric shadows and direct and indirect scattering are also considered in the proposed model IVROM. Moreover, the implementation of the Improved Translucent Volume Rendering Method (ITVRM) integrating the IVROM model, Shear-Warp and preintegrated volume rendering algorithm is described, in which the aliasing and staircase effects resulting from under-sampling in Shear-Warp, are avoided by the preintegrated volume rendering technique. This study demonstrates the superiority of the proposed method. Copyright {\textcopyright} 2010 Bin Li et al.},
author = {Li, Bin and Tian, Lianfang and Ou, Shanxing},
doi = {10.1155/2010/429051},
file = {:C\:/Users/adminuser/Downloads/IJBI2010-429051.pdf:pdf},
issn = {16874188},
journal = {International Journal of Biomedical Imaging},
mendeley-groups = {VoxelAlgorithms},
title = {{An optical model for translucent volume rendering and its implementation using the preintegrated shear-warp algorithm}},
volume = {2010},
year = {2010}
}

@article{Matsui2004,
abstract = {This paper presents an efficient parallel algorithm for volume rendering of large-scale datasets. Our algorithm focuses on an optimization technique, namely early ray termination (ERT), which aims to reduce the amount of computation by avoiding enumeration of invisible voxels in the visualizing volume. The novelty of the algorithm is that it incorporates this technique into a distributed volume rendering system with global reduction of the computational amount. The algorithm also is capable of statically balancing the processor workloads. The experimental results show that our algorithm with global ERT further achieves the maximum reduction of 33% compared to an earlier algorithm with local ERT. As a result, our load-balanced algorithm reduces the execution time to at least 66%, not only for dense objects but also for transparent objects. {\textcopyright} Springer-Verlag Berlin Heidelberg 2004.},
author = {Matsui, Manabu and Ino, Fumihiko and Hagihara, Kenichi},
doi = {10.1007/978-3-540-30566-8_30},
file = {:C\:/Users/adminuser/Downloads/978-3-540-30566-8_30.pdf:pdf},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {VoxelAlgorithms},
pages = {245--256},
title = {{Parallel volume rendering with early ray termination for visualizing large-scale datasets}},
volume = {3358},
year = {2004}
}

@article{Jabonski2016,
abstract = {In this paper, we present a novel approach to efficient real-time rendering of numerous high-resolution voxelized objects. We present a voxel rendering algorithm based on triangle rasterization pipeline with screen space rendering computational complexity. In order to limit the number of vertex shader invocations, voxel filtering algorithm with fixed size voxel data buffer was developed. Voxelized objects are represented by sparse voxel octree (SVO) structure. Using sparse texture available in modern graphics APIs, we create a 3D lookup table for voxel ids. Voxel filtering algorithm is based on 3D sparse texture ray marching approach. Screen Space Billboard Voxel Buffer is filled by voxels from visible voxels point cloud. Thanks to using 3D sparse textures, we are able to store high-resolution objects in VRAM memory. Moreover, sparse texture mipmaps can be used to control object level of detail (LOD). The geometry of a voxelized object is represented by a collection of points extracted from object SVO. Each point is defined by position, normal vector and texture coordinates. We also showhowto take advantage of programmable geometry shaders in order to store voxel objects with extremely lowmemory requirements and to perform real-time visualization. Moreover, geometry shaders are used to generate billboard quads from the point cloud and to perform fast face culling. As a result, we obtained comparable or even better performance results in comparison to SVO ray tracing approach. The number of rendered voxels is limited to defined Screen Space Billboard Voxel Buffer resolution. Last but not least, thanks to graphics card adapter support, developed algorithm can be easily integrated with any graphics engine using triangle rasterization pipeline.},
author = {Jab{\l}o{\'{n}}ski, Szymon and Martyn, Tomasz},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jab{\l}o{\'{n}}ski, Martyn - 2016 - Real-time voxel rendering algorithm based on Screen Space Billboard Voxel Buffer with Sparse Lookup Textures.pdf:pdf},
journal = {24th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2016 - Full Papers Proceedings},
keywords = {Billboarding,Computer graphics,Geometry shader,Point cloud,Sparse texture,Sparse voxel octree,Voxel rendering},
pages = {27--36},
title = {{Real-time voxel rendering algorithm based on Screen Space Billboard Voxel Buffer with Sparse Lookup Textures}},
year = {2016}
}

@article{Wang2023,
abstract = {The refresh rate of virtual reality (VR) head-mounted displays (HMDs) has been growing rapidly in recent years because of the demand to provide higher frame rate content as it is often linked with a better experience. Today's HMDs come with different refresh rates ranging from 20Hz to 180Hz, which determines the actual maximum frame rate perceived by users' naked eyes. VR users and content developers often face a choice because having high frame rate content and the hardware that supports it comes with higher costs and other trade-offs (such as heavier and bulkier HMDs). Both VR users and developers can choose a suitable frame rate if they are aware of the benefits of different frame rates in user experience, performance, and simulator sickness (SS). To our knowledge, limited research on frame rate in VR HMDs is available. In this paper, we aim to fill this gap and report a study with two VR application scenarios that compared four of the most common and highest frame rates currently available (60, 90, 120, and 180 frames per second (fps)) to explore their effect on users' experience, performance, and SS symptoms. Our results show that 120fps is an important threshold for VR. After 120fps, users tend to feel lower SS symptoms without a significant negative effect on their experience. Higher frame rates (e.g., 120 and 180fps) can ensure better user performance than lower rates. Interestingly, we also found that at 60fps and when users are faced with fast-moving objects, they tend to adopt a strategy to compensate for the lack of visual details by predicting or filling the gaps to try to meet the performance needs. At higher fps, users do not need to follow this compensatory strategy to meet the fast response performance requirements.},
author = {Wang, Jialin and Shi, Rongkai and Zheng, Wenxuan and Xie, Weijie and Kao, Dominic and Liang, Hai Ning},
doi = {10.1109/TVCG.2023.3247057},
file = {:C\:/Users/adminuser/Downloads/Effect_of_Frame_Rate_on_User_Experience_Performance_and_Simulator_Sickness_in_Virtual_Reality.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
number = {5},
pages = {2478--2488},
publisher = {IEEE},
title = {{Effect of Frame Rate on User Experience, Performance, and Simulator Sickness in Virtual Reality}},
volume = {29},
year = {2023}
}

@article{Zhao2022,
abstract = {Virtual reality (VR) head mounted displays (HMDs) require both high spatial resolution and fast temporal response. However, methods to quantify the VR image quality in the spatiotemporal domain when motion exists are not yet standardized. In this study, we characterize the spatiotemporal capabilities of three VR devices: the HTC VIVE, VIVE Pro, and VIVE Pro 2 during smooth pursuit. A spatiotemporal model for VR HMDs is presented using measured spatial and temporal characteristics. Among the three evaluated headsets, the VIVE Pro 2 improves the display temporal performance using a fast 120 Hz refresh rate and pulsed emission with a small duty cycle of 5%. In combination with a high pixel resolution beyond 2 k × 2 k per eye, the VIVE Pro 2 achieves an improved spatiotemporal performance compared to the VIVE and VIVE Pro in the high spatial frequency range above 8 cycles per degree during smooth pursuit. The result demonstrates that reducing the display emission duty cycle to less than 20% is beneficial to mitigate motion blur in VR HMDs. Frame rate reduction (e.g., to below 60 Hz) of the input signal compared to the display refresh rate of 120 Hz yields replicated shadow images that can affect the image quality under motion. This work supports the regulatory science research efforts in development of testing methods to characterize the spatiotemporal performance of VR devices for medical use.},
author = {Zhao, Chumin and Kim, Andrea S. and Beams, Ryan and Badano, Aldo},
doi = {10.1038/s41598-022-24345-9},
file = {:C\:/Users/adminuser/Downloads/s41598-022-24345-9.pdf:pdf},
isbn = {0123456789},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--13},
pmid = {36424434},
publisher = {Nature Publishing Group UK},
title = {{Spatiotemporal image quality of virtual reality head mounted displays}},
url = {https://doi.org/10.1038/s41598-022-24345-9},
volume = {12},
year = {2022}
}

@misc{flick_2017, 
title={Tessellation}, 
howpublished ={https://catlikecoding.com/unity/tutorials/advanced-rendering/tessellation/}, 
journal={Catlike Coding}, 
publisher={Catlike Coding}, 
author={Flick, Jasper}, 
year={2017}, 
month={Nov}, 
note  = {Accessed: 2020-10-06}
} 

@article{Zari2023,
abstract = {The adoption of extended reality solutions is growing rapidly in the healthcare world. Augmented reality (AR) and virtual reality (VR) interfaces can bring advantages in various medical-health sectors; it is thus not surprising that the medical MR market is among the fastest-growing ones. The present study reports on a comparison between two of the most popular MR head-mounted displays, Magic Leap 1 and Microsoft HoloLens 2, for the visualization of 3D medical imaging data. We evaluate the functionalities and performance of both devices through a user-study in which surgeons and residents assessed the visualization of 3D computer-generated anatomical models. The digital content is obtained through a dedicated medical imaging suite (Verima imaging suite) developed by the Italian start-up company (Witapp s.r.l.). According to our performance analysis in terms of frame rate, there are no significant differences between the two devices. The surgical staff expressed a clear preference for Magic Leap 1, particularly for the better visualization quality and the ease of interaction with the 3D virtual content. Nonetheless, even though the results of the questionnaire were slightly more positive for Magic Leap 1, the spatial understanding of the 3D anatomical model in terms of depth relations and spatial arrangement was positively evaluated for both devices.},
author = {Zari, Giulia and Condino, Sara and Cutolo, Fabrizio and Ferrari, Vincenzo},
doi = {10.3390/s23063040},
file = {:C\:/Users/adminuser/Downloads/sensors-23-03040.pdf:pdf},
issn = {14248220},
journal = {Sensors},
keywords = {head-mounted displays,magic leap,medical imaging,microsoft hololens,mixed reality},
mendeley-groups = {Augmented Reality Hardware},
number = {6},
pages = {3040},
pmid = {36991751},
title = {{Magic Leap 1 versus Microsoft HoloLens 2 for the Visualization of 3D Content Obtained from Radiological Images}},
volume = {23},
year = {2023}
}

@article{Bruce2009,
abstract = {A proposal for saliency computation within the visual cortex is put forth based on the premise that localized saliency computation serves to maximize information sampled from one's environment. The model is built entirely on computational constraints but nevertheless results in an architecture with cells and connectivity reminiscent of that appearing in the visual cortex. It is demonstrated that a variety of visual search behaviors appear as emergent properties of the model and therefore basic principles of coding and information transmission. Experimental results demonstrate greater efficacy in predicting fixation patterns across two different data sets as compared with competing models. {\textcopyright} ARVO.},
author = {Bruce, Neil D.B. and Tsotsos, John K.},
doi = {10.1167/9.3.5},
file = {:D\:/Thomas/jov-9-3-5.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
keywords = {Efficient coding,Eye movements,Independent components,Information theory,Pop-out,Redundancy,Saliency,Search asymmetry,Statistical inference,Visual attention,Visual search},
mendeley-groups = {Saliency/ComputerVsHuman},
number = {3},
pages = {1--24},
pmid = {19757944},
title = {{Saliency, attention and visual search: An information theoretic approach}},
volume = {9},
year = {2009}
}

@article{VanDyck2021,
abstract = {Deep convolutional neural networks (DCNNs) and the ventral visual pathway share vast architectural and functional similarities in visual challenges such as object recognition. Recent insights have demonstrated that both hierarchical cascades can be compared in terms of both exerted behavior and underlying activation. However, these approaches ignore key differences in spatial priorities of information processing. In this proof-of-concept study, we demonstrate a comparison of human observers (N = 45) and three feedforward DCNNs through eye tracking and saliency maps. The results reveal fundamentally different resolutions in both visualization methods that need to be considered for an insightful comparison. Moreover, we provide evidence that a DCNN with biologically plausible receptive field sizes called vNet reveals higher agreement with human viewing behavior as contrasted with a standard ResNet architecture. We find that image-specific factors such as category, animacy, arousal, and valence have a direct link to the agreement of spatial object recognition priorities in humans and DCNNs, while other measures such as difficulty and general image properties do not. With this approach, we try to open up new perspectives at the intersection of biological and computer vision research.},
author = {van Dyck, Leonard Elia and Kwitt, Roland and Denzler, Sebastian Jochen and Gruber, Walter Roland},
doi = {10.3389/fnins.2021.750639},
file = {:D\:/Thomas/fnins-15-750639.pdf:pdf},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {brain,deep neural network,eye tracking,object recognition,saliency map,seeing,vision},
mendeley-groups = {Saliency/ComputerVsHuman},
number = {October},
pages = {1--15},
title = {{Comparing Object Recognition in Humans and Deep Convolutional Neural Networks—An Eye Tracking Study}},
volume = {15},
year = {2021}
}

@article{Models2006,
abstract = {Many common statistical models can be expressed as linear models that incorporate both fixed effects, which are parameters associated with an entire population or with certain repeatable levels of experimental factors, and random effects, which are associated with individual experimental units drawn at random from a population. A model with both fixed effects and random effects is called a mixed-effects model. Mixed-effects models are primarily used to describe relationships between a response variable and some covariates in data that are grouped according to one or more classification factors. Examples of such grouped data include longitudinal data, repeated measures data, multilevel data, and block designs. By associating common random effects to observations sharing the same level of a classification factor, mixed-effects models flexibly represent the covariance structure induced by the grouping of the data. In this chapter we present an overview of linear mixed-effects (LME) models, introducing their basic concepts through the analysis of several real-data examples, starting from simple models and gradually moving to more complex models. Although the S code to fit these models is shown, the purpose here is to present the motivation for using LME models to analyze grouped data and not to concentrate on the software for fitting and displaying the models. This chapter serves as an appetizer for the material covered in later chapters: the theoretical and computational methods for LME models described in Chapter 2 and the linear mixed-effects modeling facilities available in the nlme library, covered in detail in Chapter 4. The examples described in this chapter also serve to illustrate the breadth of applications of linear mixed-effects models.},
author = {Models, Linear Mixed-effects and Concepts, Basic},
doi = {10.1007/0-387-22747-4_6},
file = {:D\:/Thomas/0-387-22747-4_1.pdf:pdf},
journal = {Mixed-Effects Models in S and S-PLUS},
mendeley-groups = {Statistics},
pages = {273--304},
title = {{Nonlinear Mixed-effects Models: Basic Concepts and Motivating Examples}},
year = {2006}
}

@book{Singmann2019,
abstract = {We have set up a set of many-body kinetic Bloch equations with spacial inhomogeneity. We reexamined the widely adopted quasi-independent electron model and showed the inadequacy of this model in studying the spin transport. We further pointed out a new decoherence effect based on interference effect along the diffusion in spin transport problem due to the so called inhomogeneous broadening effect in the Bloch equations. We have shown that this inhomogeneous broadening effect can cause the spin decoherence alone even without the scattering and that the resulting decoherence is more important than the dephasing effect due to the D'yakonov-Perel' (DP) term together with the scattering.},
author = {Singmann, Henrik and Kellen, David},
booktitle = {New Methods in Cognitive Psychology},
doi = {10.4324/9780429318405-2},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singmann, Kellen - 2019 - An Introduction to Mixed Models for Experimental Psychology.pdf:pdf},
isbn = {9780429318405},
mendeley-groups = {Statistics,MixedEffectModels},
number = {October},
pages = {4--31},
title = {{An Introduction to Mixed Models for Experimental Psychology}},
year = {2019},
publisher = {Tailor and Frances}
}

@book{Cameron2005,
author = {Cameron, Adrian and Trivedi, Pravin},
year = {2005},
month = {05},
pages = {},
title = {Microeconometrics: Methods and Applications},
isbn = {0521848059},
doi = {10.1017/CBO9780511811241},
publisher = {Cambridge University Press}
}


@article{Harvey1986,
author = {Harvey, Lewis},
year = {1986},
month = {11},
pages = {623-632},
title = {Efficient estimation of sensory thresholds},
volume = {18},
journal = {Behavior Research Methods, Instruments, \& Computers},
doi = {10.3758/BF03201438}
}

@article{Ryu2018,
abstract = {The field of enacted/embodied cognition has emerged as a contemporary attempt to connect the mind and body in the study of cognition. However, there has been a paucity of methods that enable a multi-layered approach tapping into different levels of functionality within the nervous systems (e.g., continuously capturing in tandem multi-modal biophysical signals in naturalistic settings). The present study introduces a new theoretical and statistical framework to characterize the influences of cognitive demands on biophysical rhythmic signals harnessed from deliberate, spontaneous and autonomic activities. In this study, nine participants performed a basic pointing task to communicate a decision while they were exposed to different levels of cognitive load. Within these decision-making contexts, we examined the moment-by-moment fluctuations in the peak amplitude and timing of the biophysical time series data (e.g., continuous waveforms extracted from hand kinematics and heart signals). These spike-trains data offered high statistical power for personalized empirical statistical estimation and were well-characterized by a Gamma process. Our approach enabled the identification of different empirically estimated families of probability distributions to facilitate inference regarding the continuous physiological phenomena underlying cognitively driven decision-making. We found that the same pointing task revealed shifts in the probability distribution functions (PDFs) of the hand kinematic signals under study and were accompanied by shifts in the signatures of the heart inter-beat-interval timings. Within the time scale of an experimental session, marked changes in skewness and dispersion of the distributions were tracked on the Gamma parameter plane with 95% confidence. The results suggest that traditional theoretical assumptions of stationarity and normality in biophysical data from the nervous systems are incongruent with the true statistical nature of empirical data. This work offers a unifying platform for personalized statistical inference that goes far beyond those used in conventional studies, often assuming a “one size fits all model” on data drawn from discrete events such as mouse clicks, and observations that leave out continuously co-occurring spontaneous activity taking place largely beneath awareness.},
author = {Ryu, Jihye and Torres, Elizabeth B.},
doi = {10.3389/fnhum.2018.00116},
file = {:C\:/Users/tomis/Downloads/fnhum-12-00116.pdf:pdf},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {Cognitive load,Embodied cognition,Heart rate variability,Pointing movements,Sensory-motor integration,Stochastic processes},
mendeley-groups = {EmbodiedCogntition},
number = {April},
pages = {1--19},
title = {{Characterization of sensory-motor behavior under cognitive load using a new statistical platform for studies of embodied cognition}},
volume = {12},
year = {2018}
}

@article{Bell2023,
abstract = {(1) Background: The Automated Test of Embodied Cognition (ATEC) uses video administration of cognitively demanding physical tasks and motion capture technology to assess cognition in action. Embodied cognition is a radical departure from conventional approaches to cognitive assessment and is in keeping with contemporary neuroscience. (2) Methods: ATEC was administered to a convenience sample of 20 patients with substance use disorder and 25 age-matched community controls. Patients were administered concurrent cognitive assessments. (3) Results: Psychometric analysis revealed excellent internal consistency, test–retest reliability and small practice effects. Groups were significantly different on ATEC scores and ATEC scores significantly related to concurrent measures of cognition. (4) Conclusions: The preliminary results support the reliability and validity of ATEC for older adults.},
author = {Bell, Morris David and Hauser, Alexander J. and Weinstein, Andrea J.},
doi = {10.3390/brainsci13060856},
file = {:C\:/Users/tomis/Downloads/brainsci-13-00856.pdf:pdf},
issn = {20763425},
journal = {Brain Sciences},
keywords = {aging,alcohol use disorder,cognitive assessment,embodied cognition,neuropsychology,substance use disorder},
mendeley-groups = {EmbodiedCogntition},
number = {6},
title = {{The Automated Test of Embodied Cognition: Concept, Development, and Preliminary Findings}},
volume = {13},
year = {2023}
}

@article{Guo2023,
abstract = {X-Ray vision is a technique providing a Superman-like ability to help users see through physical obstacles. In this paper, we utilized the HoloLens 2 [3] to build a virtual environment and a dynamic X-Ray vision window based on the created environment. Unlike previous works, our system allows users to physically move and still have the X-Ray vision ability in real-time, providing depth cues and additional information in the surrounding environment. Additionally, we designed an experiment and recruited multiple participants as a proof of concept for our system's ability.},
author = {Guo, Hung Jui and Bakdash, Jonathan Z. and Marusich, Laura R. and Ashtiani, Omeed Eshaghi and Prabhakaran, Balakrishnan},
doi = {10.1109/VRW58643.2023.00268},
file = {:D\:/Thomas/bulk-download/User Evaluation of Dynamic X-Ray Vision in Mixed Reality.pdf:pdf},
isbn = {9798350348392},
journal = {Proceedings - 2023 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops, VRW 2023},
keywords = {Collaborative interaction,Human,Human-centered computing,Mixed / augmented reality,centered computing},
mendeley-groups = {X-RayVision},
pages = {851--852},
publisher = {IEEE},
title = {{User Evaluation of Dynamic X-Ray Vision in Mixed Reality}},
year = {2023}
}

@article{Shen2014,
abstract = {Abstract: Interactive visualization has been an active subfield of scientific visualization for a longtime, in which user interfaces have traditionally followed the WIMP (Window, Icon, Menu, Pointer) paradigm. Though functional and powerful, they can also be cumbersome and daunting to a novice user, and exploring complex data requires considerable expertise and effort. A recent trend is toward more easy-to-use and natural user interfaces, which has led to interfaces like sketch-based one for interactive visualization (SIV). The goal is to allow intuitive interactive visualization by sketching in the visualization process, from data filtering to rendering. In this state-of-the-art report, we give an overview of relevant research works related to sketch-based interface in visualization. We discuss how sketch-based interaction takes effect at specific stages of the visualization pipeline. We present a categorization based on the aim of interaction, of which there are seven primary categories: selection, cutting, segmentation, matching, coloring, augmentation, and illustration. What is more, we present important items related to SIV interface design, including SIV system modes, necessary tools and some fundamental principles. This survey also provides an overview of some specific applications of SIV and a discussion of important challenges and open problems for researchers to tackle in the coming years.
Graphical Abstract: [Figure not available: see fulltext.].},
author = {Shen, Enya and Li, Sikun and Cai, Xun and Zeng, Liang and Wang, Wenke},
doi = {10.1007/s12650-014-0225-2},
file = {:D\:/Thomas/Downloads/s12650-014-0225-2.pdf:pdf},
issn = {18758975},
journal = {Journal of Visualization},
keywords = {Interactive visualization,Natural user interfaces,Sketch-based interface,Sketch-based visualization},
mendeley-groups = {VolumeRendering},
number = {4},
pages = {275--294},
title = {{Sketch-based interactive visualization: a survey}},
volume = {17},
year = {2014}
}

@article{Mathiesen2012,
abstract = {Geological visualisation while working in the field often requires expensive specialised equipment that is conventionally hard to master. Knowledge and prior experience of the specific techniques and formats used by the different devices is required to create data. This paper presents a new method that applies Augmented Reality (AR) with generic smart phones and tablets to view existing geological data sets. AR is an emerging technology that is a synthesised hybrid between the virtual world and the real world. Here, this method negates the need to understand mapping techniques when referencing three-dimensional (3D) models to the above ground terrain. Geologists can explore subterranean phenomenon with datasets visually laid accurately over the environment so the need to reference diagrams and maps to the physical world while in the field is no longer necessary. Geologists can see data as though it were part of the environment, analogous to giving them x-ray vision in the field. We present a prototype that can be applied in fields of education or as device to assist the mining industry to enhance understanding of subterranean geological structures. For example, volcanic structures, faults and fractures can be seen as they would appear from the surface or mine data such as tunnels, ventilation, ore bodies and rock types. AR techniques used for geological visualisation in the field is a new application area with potential for wider commercial applications. {\textcopyright} 2012 IEEE.},
author = {Mathiesen, Dylan and Myers, Trina and Atkinson, Ian and Trevathan, Jarrod},
doi = {10.1109/NBiS.2012.199},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mathiesen et al. - 2012 - Geological visualisation with augmented reality(2).pdf:pdf},
isbn = {9780769547794},
journal = {Proceedings of the 2012 15th International Conference on Network-Based Information Systems, NBIS 2012},
keywords = {Augmented Reality,Geological Visualisation,Geology,Smart Phones/Tablets},
mendeley-groups = {VolumeRendering/Gologoly},
number = {September},
pages = {172--179},
title = {{Geological visualisation with augmented reality}},
year = {2012}
}

@article{Rolland2002,
author = {Rolland, J. P and Meyer, C and Arthur, K and Rinalducci, E},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rolland et al. - 2002 - Method of Adjustments versus Method of Constant St imuli in the Quantification of Accuracy and Precision of Rend.pdf:pdf},
journal = {Technology},
mendeley-groups = {DepthPerceptionPapers},
number = {6},
title = {{Method of Adjustments versus Method of Constant St imuli in the Quantification of Accuracy and Precision of Rendered Depth in Head-Mounted Displays}},
volume = {11},
year = {2002}
}

@article{Mather2004,
abstract = {Two experiments investigated how the number of available depth cues affected the speed and accuracy of depth-ordering judgements. A series of textured tiles was presented on a computer monitor, with relative depths defined by combinations of contrast, blur and interposition. Subjects were required to move a mouse pointer inside each tile in turn, starting with the tile that appeared nearest, clicking on each. Accuracy of depth-ordering was much higher than chance in all conditions, though performance using the interposition cue alone was worse than in all other conditions. The only difference in reaction time in different cue conditions was in the time elapsed before the first-click. Subjects responded substantially faster when three depth cues were present (0.84 s) than when only one depth cue was present (1.41 s). The improvement in reaction time with cue numerosity is consistent with probability summation between cues extracted by independent processes. {\textcopyright} 2003 Elsevier Ltd. All rights reserved.},
author = {Mather, George and Smith, David R.R.},
doi = {10.1016/j.visres.2003.09.036},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mather, Smith - 2004 - Combining depth cues Effects upon accuracy and speed of performance in a depth-ordering task.pdf:pdf},
issn = {00426989},
journal = {Vision Research},
mendeley-groups = {DepthPerceptionPapers,VoxelAlgorithms},
number = {6},
pages = {557--562},
pmid = {14693183},
title = {{Combining depth cues: Effects upon accuracy and speed of performance in a depth-ordering task}},
volume = {44},
year = {2004}
}

@article{Wither2005,
abstract = {This paper presents and evaluates a set of pictorial depth cues for far-field outdoor mobile augmented reality (AR). We examine the problem of accurately placing virtual annotations at physical target points from a static point of view. While it is easy to line up annotations with a target point's projection in the view plane, finding the correct distance for the annotation is difficult if the target point is not represented in an environment model. We have found that AR depth cues, such as vertical and horizontal shadow planes, a small top-down map, or color encodings of relative depth, have a positive impact on a user's ability to align a 3D cursor with physical objects at various distances. These cues aid the user's depth perception and estimation by providing information about the 3D cursor's distance and its relationship in 3-space to any features that may already have been annotated. We conducted a user study that measures the effects of different depth cues for both absolute 3D cursor placement as well as placement relative to a small number of marked reference points, whose distances are known. Our study provides insight about mobile AR users' ability to judge distances both absolutely and relatively, and we identify techniques that successfully enhance their performance. {\textcopyright} 2005 IEEE.},
author = {Wither, Jason and H{\"{o}}llerer, Tobias},
doi = {10.1109/ISWC.2005.41},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wither, H{\"{o}}llerer - 2005 - Pictorial depth cues for outdoor augmented reality.pdf:pdf},
isbn = {0769524192},
issn = {15504816},
journal = {Proceedings - International Symposium on Wearable Computers, ISWC},
mendeley-groups = {DepthPerceptionPapers},
pages = {92--99},
title = {{Pictorial depth cues for outdoor augmented reality}},
volume = {2005},
year = {2005}
}


@article{Fuchs1977,
author = {Fuchs, H. and Kedem, Z. M. and Uselton, S. P.},
title = {Optimal surface reconstruction from planar contours},
year = {1977},
issue_date = {Oct. 1977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/359842.359846},
doi = {10.1145/359842.359846},
abstract = {In many scientific and technical endeavors, a three-dimensional solid must be reconstructed from serial sections, either to aid in the comprehension of the object's structure or to facilitate its automatic manipulation and analysis. This paper presents a general solution to the problem of constructing a surface over a set of cross-sectional contours. This surface, to be composed of triangular tiles, is constructed by separately determining an optimal surface between each pair of consecutive contours. Determining such a surface is reduced to the problem of finding certain minimum cost cycles in a directed toroidal graph. A new fast algorithm for finding such cycles is utilized. Also developed is a closed-form expression, in terms of the number of contour points, for an upper bound on the number of operations required to execute the algorithm. An illustrated example which involves the construction of a minimum area surface describing a human head is included.},
journal = {Commun. ACM},
month = {oct},
pages = {693–702},
numpages = {10},
keywords = {three-dimensional computer graphics, surface reconstruction, serial sections, minimum cost paths, contour data, continuous tone displays}
}

@ARTICLE{Keppel1975,
  author={Keppel, E.},
  journal={IBM Journal of Research and Development}, 
  title={Approximating Complex Surfaces by Triangulation of Contour Lines}, 
  year={1975},
  volume={19},
  number={1},
  pages={2-11},
  keywords={},
  doi={10.1147/rd.191.0002}}

@article{Herman1981,
author = {Herman, G.T. and Udupa, J.K.},
journal = {Proceedings of the Society of Photo-Optical Instrumentation Engineers},
pages = {90--97},
title = {{Display of three-dimensional discrete surfaces}},
volume = {283},
year = {1981}
}

@article{MEAGHER1982129,
abstract = {A geometric modeling technique called Octree Encoding is presented. Arbitrary 3-D objects can be represented to any specified resolution in a hierarchical 8-ary tree structure or “octree” Objects may be concave or convex, have holes (including interior holes), consist of disjoint parts, and possess sculptured (i.e., “free-form”) surfaces. The memory required for representation and manipulation is on the order of the surface area of the object. A complexity metric is proposed based on the number of nodes in an object's tree representation. Efficient (linear time) algorithms have been developed for the Boolean operations (union, intersection and difference), geometric operations (translation, scaling and rotation), N-dimensional interference detection, and display from any point in space with hidden surfaces removed. The algorithms require neither floating-point operations, integer multiplications, nor integer divisions. In addition, many independent sets of very simple calculations are typically generated, allowing implementation over many inexpensive high-bandwidth processors operating in parallel. Real time analysis and manipulation of highly complex situations thus becomes possible.},
author = {Meagher, Donald},
doi = {https://doi.org/10.1016/0146-664X(82)90104-6},
issn = {0146-664X},
journal = {Computer Graphics and Image Processing},
number = {2},
pages = {129--147},
title = {{Geometric modeling using octree encoding}},
url = {https://www.sciencedirect.com/science/article/pii/0146664X82901046},
volume = {19},
year = {1982}
}


@article{Christiansen1978,
abstract = {A simple algorithm is presented for processing complex contour arrangements to produce polygonal element mosaics which are suitable for line drawing and continuous tone display. The program proceeds by mapping adjacent contours onto the same unit square and, subject to ordering limitations, connecting nodes of one contour to their nearest neighbors in the other contour. While the mapping procedure provides a basis for branching decisions, highly ambiguous situations are resolved by user interaction. The program was designed to interface a contour definition of the components of a human brain. These brain data are a most complex definition and, as such, serve to illustrate both the capabilities and limitations of the procedures.},
author = {Christiansen, H. N. and Sederberg, T. W.},
doi = {10.1145/800248.807388},
file = {:D\:/Thomas/800248.807388.pdf:pdf},
journal = {Proceedings of the 5th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1978},
keywords = {Continuous tone displays,Contour data,Mapping.,Serial sections,Surface reconstruction,Three dimensional computer graphics},
pages = {187--192},
title = {{Conversion of complex contour line definitions into polygonal element mosaics}},
year = {1978}
}


@ARTICLE{Farrell1983,
  author={Farrell, Edward J.},
  journal={IBM Journal of Research and Development}, 
  title={Color Display and Interactive Interpretation of Three-Dimensional Data}, 
  year={1983},
  volume={27},
  number={4},
  pages={356-366},
  keywords={},
  doi={10.1147/rd.274.0356}}

@article{Riley2003,
abstract = {Weather visualization is a difficult problem because it comprises volumetric multi-field data and traditional surface-based approaches obscure details of the complex three-dimensional structure of cloud dynamics. Therefore, visually accurate volumetric multi-field visualization of storm scale and cloud scale data is needed to effectively and efficiently communicate vital information to weather forecasters, improving storm forecasting, atmospheric dynamics models, and weather spotter training. We have developed a new approach to multi-field visualization that uses field specific, physically-based opacity, transmission, and lighting calculations per-field for the accurate visualization of storm and cloud scale weather data. Our approach extends traditional transfer function approaches to multi-field data and to volumetric illumination and scattering.},
author = {Riley, Kirk and Ebert, David and Hansen, Charles and Levit, Jason},
doi = {10.1109/VISUAL.2003.1250383},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Riley et al. - 2003 - Visually Accurate Multi-Field Weather Visualization.pdf:pdf},
isbn = {0780381203},
journal = {Proceedings of the IEEE Visualization Conference},
keywords = {Multi-Field Visualization,Visually Accurate Visualization,Weather Visualization},
mendeley-groups = {VolumeRendering/Meterolgoy},
pages = {279--286},
title = {{Visually Accurate Multi-Field Weather Visualization}},
year = {2003}
}

@article{Engel2001,
abstract = {We introduce a novel texture-based volume rendering approach that achieves the image quality of the best post-shading approaches with far less slices. It is suitable for new flexible consumer graphics hardware and provides high image quality even for low-resolution volume data and non-linear transfer functions with high frequencies, without the performance overhead caused by rendering additional interpolated slices. This is especially useful for volumetric effects in computer games and professional scientific volume visualization, which heavily depend on memory bandwidth and rasterization power. We present an implementation of the algorithm on current programmable consumer graphics hardware using multi-textures with advanced texture fetch and pixel shading operations. We implemented direct volume rendering, volume shading, arbitrary number of isosurfaces, and mixed mode rendering. The performance does neither depend on the number of isosurfaces nor the definition of the transfer functions, and is therefore suited for interactive high-quality volume graphics.},
author = {Engel, K. and Kraus, M. and Ertl, T.},
doi = {10.1145/383507.383515},
file = {:C\:/Users/tomis/Downloads/383507.383515.pdf:pdf},
isbn = {158113407X},
journal = {Proceedings of the ACM SIGGRAPH Conference on Computer Graphics},
keywords = {Direct volume rendering,Flexible graphics hardware,Multi-textures,PC graphics hardware,Rasterization,Volume graphics,Volume shading,Volume visualization},
number = {WORKSHOP},
pages = {9--16},
title = {{High-quality pre-integrated volume rendering using hardware-accelerated pixel shading}},
year = {2001}
}

@article{Ney1990,
abstract = {The volumetric rendering technique generates 3D images of computed tomography data with unprecedented image quality. Volumetric rendering is a general technique with applications in many different fields. This article describes in detail the methods and algorithms used for volumetric rendering of medical computed tomography data and includes a step-by-step description of the process used to generate two types of images (unshaded and shaded surfaces). Images generated with this technique are in routine clinical use. {\textcopyright} 1990 IEEE},
author = {Ney, Derek R. and Drebin, Robert A. and Magid, Donna},
doi = {10.1109/38.50670},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ney, Drebin, Magid - 1990 - Volumetric Rendering of Computed Tomography Data Principles and Techniques.pdf:pdf},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
mendeley-groups = {Books},
number = {2},
pages = {24--32},
title = {{Volumetric Rendering of Computed Tomography Data: Principles and Techniques}},
volume = {10},
year = {1990}
}

@book{Kaufman2000,
abstract = {Summary Volume data are three-dimensional (3D) entities that may have information inside them, may not consist of surfaces and edges, or may be too voluminous to be represented geometrically. Volume visualization is a method of extracting meaningful information from volumetric data using interactive graphics and imaging. It is concerned with volume data representation, modeling, manipulation, and rendering [30.], [32.] and [33.]. Volume data are obtained by sampling, simulation, or modeling techniques. For example, a sequence of 2D slices obtained from magnetic resonance imaging (MRI) or computed tomography (CT) is 3D reconstructed into a volume model and visualized for diagnostic purposes or for planning of treatment or surgery. In many computational fields, as in fluid dynamics, the results of simulation typically running on a supercomputer are often visualized as volume data for analysis and verification. In addition, many traditional geometric computer graphics applications, such as CAD and simulation, as well as applications mixing geometric objects with medical data (see Fig. 1), have exploited the advantages of volume techniques called volume graphics for modeling, manipulation, and visualization [31].},
annote = {has a good explaination of the definition of volume rendering},
author = {Kaufman, Arie E.},
booktitle = {Handbook of Medical Imaging},
doi = {10.1016/b978-012077790-7/50050-3},
file = {:C\:/Users/tomis/OneDrive/Documents/3-s2.0-B9780120777907500503-main.pdf:pdf},
mendeley-groups = {VolumeRendering},
pages = {713--730},
publisher = {Academic Press},
title = {{Volume Visualization in Medicine}},
url = {http://dx.doi.org/10.1016/B978-012077790-7/50050-3},
volume = {Vi},
year = {2000}
}

@article{Meissner2000,
abstract = {There is a wide range of devices and scientific simulation generating volumetric data. Visualizing such data, ranging from regular data sets to scattered data, is a challenging task. This course will give an introduction to the volume rendering transport theory and the involved issues such as interpolation, illumination, classification and others. Different volume rendering techniques will be presented illustrating their fundamental features and differences as well as their limitations. Furthermore, acceleration techniques will be presented including pure software optimizations as well as utilizing special purpose hardware as VolumePro but also dedicated hardware such as polygon graphics subsystems.},
author = {Mei{\ss}ner, M and Pfister, H and Westermann, R and Wittenbrink, C M},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mei{\ss}ner et al. - 2000 - Volume Visualization and Volume Rendering Techniques.pdf:pdf},
journal = {Eurographics},
mendeley-groups = {VolumeRendering},
number = {June},
title = {{Volume Visualization and Volume Rendering Techniques}},
url = {http://ismusicmake.googlecode.com/svn-hi/trunk/rc_Paper/10.1.1.93.2014.pdf},
volume = {Vi},
year = {2000}
}

@article{Nguyen2016,
abstract = {Recent advances in serial block-face imaging using scanning electron microscopy (SEM) have enabled the rapid and efficient acquisition of 3-dimensional (3D) ultrastructural information from a large volume of biological specimens including brain tissues. However, volume imaging under SEM is often hampered by sample charging, and typically requires specific sample preparation to reduce charging and increase image contrast. In the present study, we introduced carbon-based conductive resins for 3D analyses of subcellular ultrastructures, using serial block-face SEM (SBF-SEM) to image samples. Conductive resins were produced by adding the carbon black filler, Ketjen black, to resins commonly used for electron microscopic observations of biological specimens. Carbon black mostly localized around tissues and did not penetrate cells, whereas the conductive resins significantly reduced the charging of samples during SBF-SEM imaging. When serial images were acquired, embedding into the conductive resins improved the resolution of images by facilitating the successful cutting of samples in SBF-SEM. These results suggest that improving the conductivities of resins with a carbon black filler is a simple and useful option for reducing charging and enhancing the resolution of images obtained for volume imaging with SEM.},
author = {Nguyen, Huy Bang and Thai, Truc Quynh and Saitoh, Sei and Wu, Bao and Saitoh, Yurika and Shimo, Satoshi and Fujitani, Hiroshi and Otobe, Hirohide and Ohno, Nobuhiko},
doi = {10.1038/srep23721},
file = {:C\:/Users/tomis/Unity/srep23721.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
mendeley-groups = {VolumeRendering/MaterialSciences},
number = {March},
pages = {1--10},
pmid = {27020327},
publisher = {Nature Publishing Group},
title = {{Conductive resins improve charging and resolution of acquired images in electron microscopic volume imaging}},
volume = {6},
year = {2016}
}

@article{Grottel2012,
abstract = {In many different application fields particle-based simulation, like molecular dynamics, are used to study material properties and behavior. Nowadays, simulation data sets consist of millions of particles and thousands of time steps challenging interactive visualization. Direct glyph-based representations of the particle data are important for the visual analysis process and these rendering methods can be optimized to be able to work sufficiently fast with huge data sets. However, the perception of the implicit spatial structures formed by such data is often hindered by aliasing and visual clutter. Especially the depth of these structures can be grasped better if visual cues are applied, even in interactive representations. We hence present a method to apply object-space ambient occlusion, based on local neighborhood information, to large timedependent particle-based data sets without the need for any precomputations. Based on density information collected in real-time, glyph-based representations of the data sets can be visually enhanced without significant impact on the rendering performance allowing to visualize multi-million particle data sets interactively on commodity workstations. {\textcopyright} 2012 IEEE.},
author = {Grottel, Sebastian and Krone, Michael and Scharnowski, Katrin and Ertl, Thomas},
doi = {10.1109/PacificVis.2012.6183593},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grottel et al. - 2012 - Object-space ambient occlusion for molecular dynamics.pdf:pdf},
isbn = {9781467308649},
journal = {IEEE Pacific Visualization Symposium 2012, PacificVis 2012 - Proceedings},
mendeley-groups = {VolumeRendering/Molecular},
pages = {209--216},
publisher = {IEEE},
title = {{Object-space ambient occlusion for molecular dynamics}},
year = {2012}
}

@article{Hui1993,
abstract = {An interface metaphor is proposed for developing a 3-D cursor for volume rendering applications. The metaphor, luminous gel, is a material for molding a 3-D cursor. It has been demonstrated that such a cursor's location and spatial relationships with volume-rendered 3-D objects are readily perceivable. Editing tools made in luminous gel have also been developed, which are useful as effective positioning tools for functional operations and inspecting devices for feature exploration and scene understanding by enabling immediate depth perception and object highlighting. The editing tools have also be further elaborated for performing planar and curved sectioning directly and naturally},
author = {Hui, Y.W. and Ng, W.Y.},
doi = {10.1109/nssmic.1992.301492},
file = {:D\:/Thomas/Downloads/3D_cursors_for_volume_rendering_applications.pdf:pdf},
isbn = {0780308832},
journal = {IEEE Access},
mendeley-groups = {VolumeRendering/Interaction},
pages = {1243--1245},
title = {{3D cursors for volume rendering applications}},
year = {1993}
}

@article{Cordeil2017,
abstract = {We introduce ImAxes, an immersive system for exploring multivariate data using fluid, modeless interaction. The basic interface element is an embodied data axis. The user can manipulate these axes like physical objects in the immersive environment and combine them into sophisticated visualisations. The type of visualisation that appears depends on the proximity and relative orientation of the axes with respect to one another, which we describe with a formal grammar. This straight-forward composability leads to a number of emergent visualisations and interactions, which we review, and then demonstrate with a detailed multivariate data analysis use case.},
author = {Cordeil, Maxime and Cunningham, Andrew and Dwyer, Tim and Thomas, Bruce H. and Marriott, Kim},
doi = {10.1145/3126594.3126613},
file = {:C\:/Users/tomis/Downloads/3126594.3126613.pdf:pdf},
isbn = {9781450349819},
journal = {UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
mendeley-groups = {3DVisulisationsVs2D},
pages = {71--83},
title = {{ImAxes: Immersive axes as embodied affordances for interactive multivariate data visualisation}},
year = {2017}
}

@article{Mujber2004,
abstract = {Virtual reality (VR) is a rapidly developing computer interface that strives to immerse the user completely within an experimental simulation, thereby greatly enhancing the overall impact and providing a much more intuitive link between the computer and the human participants. Virtual reality has been applied successfully to hundreds if not thousands of scenarios in diverse areas including rapid prototyping, manufacturing, scientific visualisation, engineering, and education. This paper gives an overview on the virtual reality applications in manufacturing processes. {\textcopyright} 2004 Elsevier B. V. All rights reserved.},
author = {Mujber, T. S. and Szecsi, T. and Hashmi, M. S.J.},
doi = {10.1016/j.jmatprotec.2004.04.401},
file = {:C\:/Users/tomis/Downloads/1-s2.0-S0924013604005618-main.pdf:pdf},
issn = {09240136},
journal = {Journal of Materials Processing Technology},
keywords = {Virtual environment,Virtual manufacturing,Virtual reality},
mendeley-groups = {3DVisulisationsVs2D},
number = {1-3},
pages = {1834--1838},
title = {{Virtual reality applications in manufacturing process simulation}},
volume = {155-156},
year = {2004}
}

@article{Qu2010,
abstract = {This research presented a teleonomic-based simulation approach to virtual plants integrating the technology of intelligent agent as well as the knowledge of plant physiology and morphology. Plant is represented as the individual metamers and root agents with both functional and geometrical structure. The development of plant is achieved by the flush growth of metamer and root agents controlled by their internal physiological status and external environment. The eggplant based simulation results show that simple rules and actions (internal carbon allocation among organs, dynamic carbon reserve/mobilization, carbon transport in parallel using a discrete pressure-flow paradigm and child agent position choosing for maximum light interception, etc.) executed by agents can cause the complex adaptive behaviors on the whole plant level: carbon partitioning among metamers and roots, carbon reserve dynamics, architecture and biomass adaptation to environmental heterogeneity and the phototropism, etc. This phenomenon manifest that the virtual plant simulated in presented approach can be viewed as a complex adaptive system. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Qu, Hongchun and Zhu, Qingsheng and Guo, Mingwei and Lu, Zhonghua},
doi = {10.1016/j.simpat.2010.01.004},
file = {:C\:/Users/tomis/Downloads/1-s2.0-S1569190X10000055-main.pdf:pdf},
issn = {1569190X},
journal = {Simulation Modelling Practice and Theory},
keywords = {Complex adaptive system,Intelligent agents,Teleonomic modeling,Virtual plants},
mendeley-groups = {3DVisulisationsVs2D},
number = {6},
pages = {677--695},
publisher = {Elsevier B.V.},
title = {{Simulation of carbon-based model for virtual plants as complex adaptive system}},
url = {http://dx.doi.org/10.1016/j.simpat.2010.01.004},
volume = {18},
year = {2010}
}

@article{Andrews2021,
abstract = {Many clinical procedures would benefit from direct and intuitive real-time visualization of anatomy, surgical plans, or other information crucial to the procedure. Three-dimensional augmented reality (3D-AR) is an emerging technology that has the potential to assist physicians with spatial reasoning during clinical interventions. The most intriguing applications of 3D-AR involve visualizations of anatomy or surgical plans that appear directly on the patient. However, commercially available 3D-AR devices have spatial localization errors that are too large for many clinical procedures. For this reason, a variety of approaches for improving 3D-AR registration accuracy have been explored. The focus of this review is on the methods, accuracy, and clinical applications of registering 3D-AR devices with the clinical environment. The works cited represent a variety of approaches for registering holograms to patients, including manual registration, computer vision-based registration, and registrations that incorporate external tracking systems. Evaluations of user accuracy when performing clinically relevant tasks suggest that accuracies of approximately 2 mm are feasible. 3D-AR device limitations due to the vergence-accommodation conflict or other factors attributable to the headset hardware add on the order of 1.5 mm of error compared to conventional guidance. Continued improvements to 3D-AR hardware will decrease these sources of error.},
author = {Andrews, Christopher M. and Henry, Alexander B. and Soriano, Ignacio M. and Southworth, Michael K. and Silva, Jonathan R.},
doi = {10.1109/JTEHM.2020.3045642},
file = {:C\:/Users/tomis/Downloads/jtehm_andrews_3045642_pdf.pdf:pdf},
issn = {21682372},
journal = {IEEE Journal of Translational Engineering in Health and Medicine},
keywords = {Augmented reality (AR),HoloLens,image registration,medical imaging,surgery},
mendeley-groups = {Study2,X-RayVision,Calibration},
number = {November 2020},
title = {{Registration Techniques for Clinical Applications of Three-Dimensional Augmented Reality Devices}},
volume = {9},
year = {2021}
}

@article{Rodrigues2017,
abstract = {Recent technology advances in both Virtual Reality and Augmented Reality are creating an opportunity for a paradigm shift in the design of human-computer interaction systems. Delving into the Reality-Virtuality Continuum, we find Mixed Reality - systems designed to augment the physical world with virtual entities that embody characteristics of real world objects. In the medical field, Mixed Reality systems can overlay real-time and spatially accurate results onto a patient's body without the need for external screens. The complexity of these systems previously required specialized prototypes, but newly available commercial products like the Microsoft HoloLens make the technology more available. Through a combination of literature review, expert analysis, and prototyping we explore the use of Mixed Reality in healthcare. From the experience of prototyping Patiently and HoloSim, two applications for augmenting medical training, we outline considerations for the future design and development of virtual interfaces grounded in reality.},
author = {Rodrigues, Danilo Gasques and Jain, Ankur and Rick, Steven R. and Liu, Shangley and Suresh, Preetham and Weibel, Nadir},
doi = {10.1145/3027063.3053273},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodrigues et al. - 2017 - Exploring Mixed Reality in specialized surgical environments.pdf:pdf},
isbn = {9781450346566},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Augmented Reality,HoloLens,Mixed Reality,Surgery},
mendeley-groups = {MedicalDataOverlay},
pages = {2591--2598},
title = {{Exploring Mixed Reality in specialized surgical environments}},
volume = {Part F1276},
year = {2017}
}

@article{Guo2019,
abstract = {An optical see-through (OST) head-mounted display (HMD) is an appropriate option for an image-guided surgical navigation system (IGS) based on augmented reality (AR). However, the calibration of OST-HMDs has been challenging, since the augmented results can only be observed by the users wearing it. In this paper, we proposed an online calibration method based on the single-point active alignment method (SPAAM) for a HoloLens using a virtual tracking system. The transformation matrices between the world coordinates and the virtual holographic coordinates are calculated using a linear model and decomposed by the singular value decomposition (SVD) algorithm. The surgical workspace is analyzed, and there are nine sampling points for the offline calibration: eight at the vertices and one at the center of a cube of the workspace. 20 groups of data at each sampling point are collected. Five alignments should be implanted in the online calibration. The transformation matrices are compensated for the offset of every use of a HoloLens. To assess the accuracy of the calibration method, in accordance with the RANSAC algorithm, eight groups of additional data at random non-sampling locations are collected. Three different users that are familiar with the calibration procedures performed the calibration. The average distance error of our calibration was below 6 mm, and the rotation error was up to 5◦. It is a user-friendly, simple method for the online calibration of OST-HMDs.},
author = {Guo, Na and Wang, Tianmiao and Yang, Biao and Hu, Lei and Liu, Hongsheng and Wang, Yuhan},
doi = {10.1109/ACCESS.2019.2930701},
file = {:C\:/Users/tomis/Downloads/An_Online_Calibration_Method_for_Microsoft_HoloLens.pdf:pdf},
issn = {21693536},
journal = {IEEE Access},
keywords = {Augmented reality,HMD calibration,Optical see-through},
pages = {101795--101803},
title = {{An online calibration method for Microsoft Hololens}},
volume = {7},
year = {2019}
}

@article{Guo2012,
abstract = {In this paper, we propose a novel volume illustration technique inspired by interference microscopy, which has been successfully used in biological, medical and material science over decades. Our approach simulates the optical phenomenon in interference microscopy that accounts light interference over transparent specimens, in order to generate contrast enhanced and illustrative volume visualization results. Specifically, we propose PCVR (Phase-Contrast Volume Rendering) and DICVR (Differential Interference Contrast Volume Rendering) corresponding to Phase-Contrast microscopy and Differential Interference Contrast (DIC) microscopy respectively. Without complex transfer function design, our proposed method can enhance the image contrast and structure details according to the subtle change of Optical Path Differences (OPD), and illustrate the thickness change and occluded structures with interferometry metaphors. In addition, we also develop a user interface to enable slicing specimen sections in volume data. Focus+ context lens are also included in the system for convenient data navigation and exploration. As the proposed methods are based upon widely applied microscopy techniques, they are intuitive for domain experts to explore and analyze the volume data with the proposed methods. The feedbacks from domain users suggest our proposed techniques are useful volume visualization approaches complimentary to the traditional ones. {\textcopyright} 2012 IEEE.},
author = {Guo, Hanqi and Yuan, Xiaoru and Liu, Jie and Shan, Guihua and Chi, Xuebin and Sun, Fei},
doi = {10.1109/PacificVis.2012.6183589},
file = {:C\:/Users/tomis/Downloads/Interference_microscopy_volume_illustration_for_biomedical_data.pdf:pdf},
isbn = {9781467308649},
journal = {IEEE Pacific Visualization Symposium 2012, PacificVis 2012 - Proceedings},
keywords = {Volume illustration,biomedical visualization,feature enhancement,interference microscopy,volume rendering},
mendeley-groups = {VolumeRendering/Illustrations},
pages = {177--184},
publisher = {IEEE},
title = {{Interference microscopy volume illustration for biomedical data}},
volume = {Ill},
year = {2012}
}

@article{MacDougall2016,
abstract = {We present a unique platform for molecular visualization and design that uses novel subatomic feature detection software in tandem with 3D hyperwall visualization technology. We demonstrate the fleshing-out of pharmacophores in drug molecules, as well as reactive sites in catalysts, focusing on subatomic features. Topological analysis with picometer resolution, in conjunction with interactive volume-rendering of the Laplacian of the electronic charge density, leads to new insight into docking and catalysis. Visual data-mining is done efficiently and in parallel using a 4 × 4 3D hyperwall (a tiled array of 3D monitors driven independently by slave GPUs but displaying high-resolution, synchronized and functionally-related images). The visual texture of images for a wide variety of molecular systems are intuitive to experienced chemists but also appealing to neophytes, making the platform simultaneously useful as a tool for advanced research as well as for pedagogical and STEM education outreach purposes.},
author = {MacDougall, Preston J. and Henze, Christopher E. and Volkov, Anatoliy},
doi = {10.1016/j.jmgm.2016.09.002},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/MacDougall, Henze, Volkov - 2016 - Volume-rendering on a 3D hyperwall A molecular visualization platform for research, education and out.pdf:pdf},
issn = {18734243},
journal = {Journal of Molecular Graphics and Modelling},
keywords = {Biomolecules,Catalysis,Chemistry,Drug design,STEM outreach,Visualization,Volume-rendering},
mendeley-groups = {VolumeRendering/Molecular},
pages = {1--6},
pmid = {27639086},
publisher = {Elsevier Inc.},
title = {{Volume-rendering on a 3D hyperwall: A molecular visualization platform for research, education and outreach}},
url = {http://dx.doi.org/10.1016/j.jmgm.2016.09.002},
volume = {70},
year = {2016}
}

@article{Feng20152548,
annote = {Cited by: 0},
author = {Feng, Xiao-Meng and Wu, Ling-Da and Yu, Rong-Huan and Yang, Chao},
doi = {10.11999/JEIT150303},
journal = {Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology},
mendeley-groups = {VolumeRendering},
number = {11},
pages = {2548 -- 2554},
title = {{Enhanced depth perception grid-projection algorithm for direct volume rendering}},
volume = {37},
year = {2015}
}

@book{Aoi2020,
abstract = {The development of imaging technologies such as computed tomography and magnetic resonance imaging has made it easier to obtain three-dimensional data of the internal human body. Visualizing these data helps us to understand the complicated internal structure of the human body. Transparent stereoscopic visualization using depth information is a good way to visualize internal body structure. However, the position and depth information often become unclear when three-dimensional data are rendered transparently. In this study, we aimed to understand the structural understanding and correct depth perception in transparent stereoscopic visualization, and examined how depth perception changes by overlaying multiple iso-surfaces on transparently rendered image. The experimental results showed that multiple iso-surfaces improved the accuracy of perceived depth. It was effective when the opacity of the inner iso-surface was high and the distance between the inner iso-surface and the outer iso-surface was large.},
author = {Aoi, Daimon and Hasegawa, Kyoko and Li, Liang and Sakano, Yuichi and Tanaka, Satoshi},
booktitle = {Smart Innovation, Systems and Technologies},
doi = {10.1007/978-981-15-5852-8_6},
file = {:D\:/Thomas/Downloads/Aoi2020_Chapter_ImprovingDepthPerceptionUsingM.pdf:pdf},
isbn = {9789811558511},
issn = {21903026},
keywords = {Depth perception,Medical volumetric data,Multiple iso-surfaces,Transparent stereoscopic visualization},
mendeley-groups = {VolumeRendering},
pages = {57--66},
publisher = {Springer Singapore},
title = {{Improving depth perception using multiple iso-surfaces for transparent stereoscopic visualization of medical volume data}},
url = {http://dx.doi.org/10.1007/978-981-15-5852-8_6},
volume = {192},
year = {2020}
}

@article{Jones2008,
author = {Jones, Adam and Swan, J. Edward and Singh, Gurjot and Kolstad, Eric},
doi = {10.1109/VR.2008.4480794},
file = {:C\:/Users/tomis/Downloads/2008_Jones-etal_AR-Depth-Perception_IEEE-VR.pdf:pdf},
isbn = {9781424419715},
journal = {Proceedings - IEEE Virtual Reality},
mendeley-groups = {DepthPerceptionPapers},
number = {August},
pages = {267--268},
title = {{The effects of virtual reality, augmented reality, and motion parallax on egocentric depth perception}},
year = {2008}
}

@article{Singh2012a,
author = {Singh, Gurjot and Swan, J. Edward and Jones, J. Adam and Ellis, Stephen R.},
doi = {10.1109/VR.2012.6180933},
isbn = {9781467312462},
journal = {Proceedings - IEEE Virtual Reality},
keywords = {augmented reality,depth perception,optical see-through display,x-ray vision},
pages = {165--166},
publisher = {IEEE},
title = {{Depth judgments by reaching and matching in near-field augmented reality}},
year = {2012}
}

@article{Edwards2004,
abstract = {Over the last 10 years only a small number of systems that provide stereo augmented reality for surgical guidance have been proposed and it has been rare for such devices to be evaluated in-theatre. Over the same period we have developed a system for microscope-assisted guided interventions (MAGI). This provides stereo augmented reality navigation for ENT and neurosurgery. The aim is to enable the surgeon to see virtual structures beneath the operative surface as though the tissue were transparent. During development the system MAGI has undergone regular evaluation in the operating room. This experience has provided valuable feedback for system development as well as insight into the clinical effectiveness of AR. As a result of early difficulties encountered, a parallel project was set up in a laboratory setting to establish the accuracy of depth perception in stereo AR. An interesting anomaly was found when the virtual object is placed 0-40mm below the real surface. Despite this such errors are small (∼3mm) and the intraoperative evaluation has established that AR surgical guidance can be clinically effective. Of the 17 cases described here confidence was increased in 3 cases and in a further 2 operations patient outcome was judged to have been improved. Additionally, intuitive and easy craniotomy navigation was achieved even in two cases where registration errors were quite high. These results suggest that stereo AR should have a role in the future of surgical navigation. {\textcopyright} Springer-Verlag 2004.},
author = {Edwards, Philip J. and Johnson, Laura G. and Hawkes, David J. and Fenlon, Michael R. and Strong, Anthony J. and Gleeson, Michael J.},
doi = {10.1007/978-3-540-28626-4_45},
file = {:C\:/Users/tomis/Downloads/978-3-540-28626-4_45.pdf:pdf},
isbn = {3540228772},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Augmented reality,Image-guided surgery,Stereo depth perception},
mendeley-groups = {DepthPerceptionPapers},
pages = {369--376},
title = {{Clinical experience and perception in stereo augmented reality surgical navigation}},
volume = {3150},
year = {2004}
}

@article{Cheng2022,
abstract = {Virtual reality head-mounted displays (VR-HMDs) are crucial to Metaverse which appears to be one of the most popular terms to have been adopted over the internet recently. It provides basic infrastructure and entrance to cater for the next evolution of social interaction, and it has already been widely used in many fields. The VR-HMDs with traditional aspherical or Fresnel optics are not suitable for long-term usage because of the image quality, system size, and weight. In this study, we designed and developed a large exit pupil diameter (EPD), compact, and lightweight VR-HMD with catadioptric optics. The mathematical formula for designing the catadioptric VR optics is derived. The reason why this kind of immersive VR optics could achieve a compact size and large EPD simultaneously is answered. Various catadioptric forms are systematically proposed and compared. The design can achieve a diagonal field of view (FOV) of 96° at -1 diopter, with an EPD of 10 mm at 11 mm eye relief (ERF). The overall length (OAL) of the system was less than 20 mm. A prototype of a compact catadioptric VR-HMD system was successfully developed.},
author = {Cheng, Dewen and Hou, Qichao and Li, Yang and Zhang, Tian and Li, Danyang and Huang, Yilun and Liu, Yue and Wang, Qiwei and Hou, Weihong and Yang, Tong and Feng, Zexin and Wang, Yongtian},
doi = {10.1364/oe.452747},
file = {:C\:/Users/tomis/Downloads/oe-30-5-6584.pdf:pdf},
issn = {10944087},
journal = {Optics Express},
number = {5},
pages = {6584},
pmid = {35299440},
title = {{Optical design and pupil swim analysis of a compact, large EPD and immersive VR head mounted display}},
volume = {30},
year = {2022}
}

@article{Steffen2022,
abstract = {The aim of this exploratory study is to analyse whether three-dimensional cinematic rendering image reconstructions offer advantages over conventional volume rendering in the visualisation of cone beam computed tomography (CBCT) and computed tomography (CT) images of the facial skeleton. This is of interest, as some information gets lost during the rendering process. This especially applies to structures in the background of the image and some surface information which can be lost. The commonly applied two-dimensional representation of CBCT or CT images in three different axes requires experience for interpretation. Cinematic rendering is a new three-dimensional post processing reconstruction technique, creating photo realistic visualisations, thus possibly enabling an easier interpretation of the images. In this study, ten investigators assessed ten separate patient cases of the orofacial skeleton. For each case, a conventional volume rendering image reconstruction and a cinematic rendering reconstruction of the same area was created. A specially designed questionnaire assessed both objective and subjective criteria of image perception. Objective criteria were assessed by predefined questions on the visual perception of anatomical image characteristics, showing the two reconstruction types of each case randomly to the investigators in two sessions. Subjective criteria were assessed via a visual analogue scale, showing both reconstructions simultaneously in a third session. The results show that cinematic rendering offers advantages especially in the evaluation of depth perception and three-dimensionality. Volume rendering shows advantages in surface sharpness. Cinematic Rendering was subjectively rated higher for almost all reconstructions. The cinematic rendering process however may cause loss of information and blurring of surfaces compared to volume rendering. With respect to the subjective impression, cinematic rendering scored better than volume rendering. The visualisation is perceived as being very close to reality.},
author = {Steffen, Tobias and Winklhofer, Sebastian and Starz, Felicitas and Wiedemeier, Daniel and Ahmadli, Uzeyir and Stadlinger, Bernd},
doi = {10.1016/j.aanat.2022.151905},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steffen et al. - 2022 - Three-dimensional perception of cinematic rendering versus conventional volume rendering using CT and CBCT data.pdf:pdf},
issn = {16180402},
journal = {Annals of Anatomy},
keywords = {CBCT,CT,Cinematic rendering,Facial skeleton,Imaging,Three-dimensional},
mendeley-groups = {DepthPerceptionMedical},
pages = {151905},
pmid = {35150863},
publisher = {Elsevier},
title = {{Three-dimensional perception of cinematic rendering versus conventional volume rendering using CT and CBCT data of the facial skeleton}},
url = {https://doi.org/10.1016/j.aanat.2022.151905},
volume = {241},
year = {2022}
}

@article{stadlinger2021cinematic,
  title={Cinematic Rendering in der Digitalen Volumentomografie: Fotorealistische 3D-Rekonstruktion dentaler und maxillofazialer Pathologien},
  author={Stadlinger, Bernd and Essig, Harald and Schumann, Paul and van Waes, Hubertus and Valdec, Silvio and Winklhofer, Sebastian},
  journal={SWISS DENTAL JOURNAL SSO--Science and Clinical Topics},
  volume={131},
  number={2},
  pages={133--139},
  year={2021}
}

@article{Stadlinger2019,
author = {Stadlinger, Bernd and Valdec, Silvio and Wacht, Lorenz and Essig, Harald and Winklhofer, Sebastian},
year = {2019},
month = {07},
pages = {20190249},
title = {3D-Cinematic rendering for dental and maxillofacial imaging},
volume = {49},
journal = {Dentomaxillofacial Radiology},
doi = {10.1259/dmfr.20190249}
}

@article{Siegel2000,
abstract = {We address human factors and technology issues for the design of stereoscopic display systems that are natural and comfortable to view. Our title 'just enough reality' hints at the contrast between the popularly perceived requirements for strict 'virtual reality' and the expert's pragmatic acceptance of 'sufficient reality' to satisfy the human interface requirements of real-world applications. We first review how numerous perceptions and illusions of depth can be exploited to synergistically complement binocular stereopsis. Then we report the results of our experimental studies of stereoscopy with very small interocular separations and correspondingly small on-screen disparities, which we call 'microstereopsis.' We outline the implications of microstereopsis for the design of future stereoscopic camera and display systems, especially the possibility of achieving zoneless autostereoscopic displays. We describe a possible class of implementations based on a nonlambertian filter element, and a particular implementation that would use an electronically switched louver filter to realize it.},
author = {Siegel, Mel and Nagata, Shojiro},
doi = {10.1109/76.836283},
file = {:D\:/Thomas/download.pdf:pdf},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
mendeley-groups = {NonPhotorealisticRendering},
number = {3},
pages = {387--396},
title = {{Just enough reality: comfortable 3-D viewing via microstereopsis}},
volume = {10},
year = {2000}
}

@article{Hertzmann2000,
abstract = {We present a new set of algorithms for line-art rendering of smooth surfaces. We introduce an efficient, deterministic algorithm for finding silhouettes based on geometric duality, and an algorithm for segmenting the silhouette curves into smooth parts with constant visibility. These methods can be used to find all silhouettes in real time in software. We present an automatic method for generating hatch marks in order to convey surface shape. We demonstrate these algorithms with a drawing style inspired by A Topological Picturebook by G. Francis.},
author = {Hertzmann, Aaron and Zorin, Denis},
doi = {10.1145/344779.345074},
file = {:C\:/Users/adminuser/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hertzmann, Zorin - 2000 - Illustrating smooth surfaces.pdf:pdf},
isbn = {1581132085},
journal = {SIGGRAPH 2000 - Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques},
keywords = {Non-photorealistic rendering,direction fields,hatching,pen-and-ink illustration,silhouettes},
mendeley-groups = {NonPhotorealisticRendering},
number = {Section 5},
pages = {517--526},
title = {{Illustrating smooth surfaces}},
year = {2000}
}

@article{MeruviaPastor2002,
author = {{Meruvia Pastor}, Oscar E and Strothotte, Thomas},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meruvia Pastor, Strothotte - 2002 - Frame-Coherent Stippling.pdf:pdf},
journal = {Proceedings of Eurographics 2002, Short Presentations},
mendeley-groups = {NonPhotorealisticRendering/stippling},
pages = {145--152},
title = {{Frame-Coherent Stippling}},
year = {2002}
}

@article{Fischer2023,
author = {Fischer, Marc and Rosenberg, Jarrett and Leuze, Christoph and Hargreaves, Brian and Daniel, Bruce},
doi = {10.1109/TVCG.2023.3320239},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischer et al. - 2023 - The Impact of Occlusion on Depth Perception at Arm ' s Length.pdf:pdf},
journal = {IEEE Transactions on Visualization and Computer Graphics},
mendeley-groups = {X-RayVision},
pages = {1--9},
publisher = {IEEE},
title = {{The Impact of Occlusion on Depth Perception at Arm ' s Length}},
volume = {PP},
year = {2023}
}

@article{Liao2023,
abstract = {An important application of augmented reality (AR) is the design of interfaces that reveal parts of the real world to which the user does not have line of sight. The design space for such interfaces is vast, with many options for integrating the visualization of the occluded parts of the scene into the user's main view. This paper compares four AR interfaces for disocclusion: X-ray, Cutaway, Picture-in-picture, and Multiperspective. The interfaces are compared in a within-subjects study (N = 33) over four tasks: counting dynamic spheres, pointing to the direction of an occluded person, finding the closest object to a given object, and finding pairs of matching numbers. The results show that Cutaway leads to poor performance in tasks where the user needs to see both the occluder and the occludee; that Picture-in-picture and Multiperspective have a visualization comprehensiveness advantage over Cutaway and X-ray, but a disadvantage in terms of directional guidance; that X-ray has a task completion time disadvantage due to the visualization complexity; and that participants gave Cutaway and Picture-in-picture high, and Multiperspective and X-ray low usability scores.},
author = {Liao, Shuqi and Zhou, Yuqi and Popescu, Voicu},
doi = {10.1109/VR55154.2023.00068},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liao, Zhou, Popescu - 2023 - AR Interfaces for Disocclusion - A Comparative Study.pdf:pdf},
isbn = {9798350348156},
journal = {Proceedings - 2023 IEEE Conference Virtual Reality and 3D User Interfaces, VR 2023},
keywords = {Computing methodologies - Computer graphics - Grap,Human-centered computing - Human computer interact},
mendeley-groups = {X-RayVision},
pages = {530--540},
publisher = {IEEE},
title = {{AR Interfaces for Disocclusion - A Comparative Study}},
year = {2023}
}

@article{Wang2022,
abstract = {Augmented Reality (AR) see-through vision is an interesting research topic since it enables users to see through a wall and see the occluded objects. Most existing research focuses on the visual effects of see-through vision, while the interaction method is less studied. However, we argue that using common interaction modalities, e.g., midair click and speech, may not be the optimal way to control see-through vision. This is because when we want to see through something, it is physically related to our gaze depth/vergence and thus should be naturally controlled by the eyes. Following this idea, this paper proposes a novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since gaze depth is needed, we build a gaze tracking module with two infrared cameras and the corresponding algorithm and assemble it into the Microsoft HoloLens 2 to achieve gaze depth estimation. We then propose two different GVC modes for see-through vision to fit different scenarios. Extensive experimental results demonstrate that our gaze depth estimation is efficient and accurate. By comparing with conventional interaction modalities, our GVC techniques are also shown to be superior in terms of efficiency and more preferred by users. Finally, we present four example applications of gaze-vergence-controlled see-through vision.},
archivePrefix = {arXiv},
arxivId = {2207.02645},
author = {Wang, Zhimin and Zhao, Yuxin and Lu, Feng},
doi = {10.1109/TVCG.2022.3203110},
eprint = {2207.02645},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Zhao, Lu - 2022 - Gaze-Vergence-Controlled See-Through Vision in Augmented Reality.pdf:pdf},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Augmented Reality,Gaze Depth Estimation,Gaze Vergence Control,See-through Vision},
mendeley-groups = {X-RayVision},
number = {11},
pages = {3843--3853},
pmid = {36049007},
title = {{Gaze-Vergence-Controlled See-Through Vision in Augmented Reality}},
volume = {28},
year = {2022}
}

@inproceedings{1238308,
author = {Ren and Malik},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238308},
pages = {10--17 vol.1},
title = {{Learning a classification model for segmentation}},
year = {2003}
}

@article{Asif2023,
abstract = {The Internet of Things (IoT) and the metaverse are two rapidly evolving technologies that have the potential to shape the future of our digital world. IoT refers to the network of physical devices, vehicles, buildings, and other objects that are connected to the internet and capable of collecting and sharing data. The metaverse, on the other hand, is a virtual world where users can interact with each other and digital objects in real time. In this research paper, we aim to explore the intersection of the IoT and metaverse and the opportunities and challenges that arise from their convergence. We will examine how IoT devices can be integrated into the metaverse to create new and immersive experiences for users. We will also analyse the potential use cases and applications of this technology in various industries such as healthcare, education, and entertainment. Additionally, we will discuss the privacy, security, and ethical concerns that arise from the use of IoT devices in the metaverse. A survey is conducted through a combination of a literature review and a case study analysis. This review will provide insights into the potential impact of IoT and metaverse on society and inform the development of future technologies in this field.},
author = {Asif, Rameez and Hassan, Syed Raheel},
doi = {10.3390/iot4030018},
file = {:C\:/Users/tomis/OneDrive/Documents/IoT-04-00018.pdf:pdf},
issn = {2624831X},
journal = {IoT},
keywords = {artificial intelligence,blockchain,internet of things,metaverse,privacy,security},
mendeley-groups = {MetaVerse},
number = {3},
pages = {412--429},
title = {{Exploring the Confluence of IoT and Metaverse: Future Opportunities and Challenges}},
volume = {4},
year = {2023}
}


@article{Dey2011,
abstract = {During the last decade, pedestrian navigation applications on mobile phones have become commonplace; most of them provide a birds-eye view of the environment. Recently, mobile Augmented Reality (AR) browsers have become popular, providing a complementary, egocentric view of where points of interest are located in the environment. As points of interest are often occluded by realworld objects, we previously developed a mobile AR X-ray system, which enables users to look through occluders. We present an evaluation that compares it with two standard pedestrian navigation applications (North-up and View-up map). Participants had to walk a 900 meter route with three checkpoints along the path. Our main findings are based on the analysis of recorded videos. We could show that the number of context switches is significantly lowest in the AR X-ray condition. We believe that this finding provides useful design constraints for any developer of mobile navigation applications.},
author = {Dey, Arindam and Jarvis, Graeme and Sandor, Christian and Wibowo, Ariawan and Ville-Veikko, Mattila},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey, Arindam and Jarvis, Graeme and Sandor, Christian and Wibowo, Ariawan and Ville-Veikko - 2011 - An Evaluation of Augmented Reality X.pdf:pdf},
journal = {In Proceedings of International Conference on Artificial Reality and Telexistence},
keywords = {aug-,augmented reality,evaluation,map,mented reality x-ray,mobile phone,navigation,visualization},
mendeley-groups = {X-RayVision},
pages = {28--32},
title = {{An Evaluation of Augmented Reality X-Ray Vision for Outdoor Navigation}},
year = {2011}
}

@article{Svakhine2003,
abstract = {Volume illustration is a developing trend in volume visualization, focused on conveying volume information effectively by enhancing interesting features of the volume and omitting insignificant data. However, the calculations involved have limited the illustration process to non-interactive rendering. We have developed a new interactive volume illustration system (IVIS) that harnesses the power of programmable graphics processors, and includes a novel approach for feature halo enhancement. This interactive illustration system is a powerful tool for exploration and analysis of volumetric datasets.},
author = {Svakhine, N. A. and Ebert, D. S.},
doi = {10.1109/PCCGA.2003.1238276},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Svakhine, Ebert - 2003 - Interactive volume illustration and feature halos.pdf:pdf},
isbn = {0769520286},
issn = {15504085},
journal = {Proceedings - Pacific Conference on Computer Graphics and Applications},
keywords = {Acceleration,Biomedical imaging,Computer graphics,Data analysis,Data visualization,Hardware,Isosurfaces,Program processors,Rendering (computer graphics),Shape},
mendeley-groups = {NonPhotorealisticRendering},
pages = {347--354},
title = {{Interactive volume illustration and feature halos}},
volume = {2003-Janua},
year = {2003}
}

@article{Fan1996,
abstract = {A new stereoacuity test, the double two rod test, uses a two-alternative forced-choice (2AFC) paradigm. The subject's task is to state which pair of rods, the left or the right, contains depth information. The test gives repeatable results; comparison between the depth threshold measured binocularly and monocularly suggests that monocular cues, if present, provide only poor depth information, compared to the depth information arising from binocular disparity. As part of an investigation of stereoacuity and ageing, we tested stereoacuity as a function of viewing time and the number of comparisons made between the two pairs of rods. Stereoacuity was significantly reduced for viewing times less than 3 s while longer viewing times did not improve stereoacuity. The number of comparisons made between the two pairs of rods had no effect on stereoacuity.},
author = {Fan, W C and Brown, Brian and Yap, M K},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Brown, Yap - 1996 - A new stereotest the double two rod test.pdf:pdf},
issn = {0275-5408},
journal = {Ophthalmic \& physiological optics : the journal of the British College of Ophthalmic Opticians (Optometrists)},
keywords = {Depth perception},
mendeley-groups = {2AFC},
mendeley-tags = {Depth perception},
month = {may},
number = {3},
pages = {196--202},
pmid = {8977882},
title = {{A new stereotest: the double two rod test.}},
url = {https://www.ptonline.com/articles/how-to-get-better-mfi-results http://www.ncbi.nlm.nih.gov/pubmed/8977882},
volume = {16},
year = {1996}
}

@article{Zannoli2016,
abstract = {The depth ordering of two surfaces, one occluding the other, can in principle be determined from the correlation between the occlusion border's blur and the blur of the two surfaces. If the border is blurred, the blurrier surface is nearer; if the border is sharp, the sharper surface is nearer. Previous research has found that observers do not use this informative cue. We reexamined this finding. Using a multiplane display, we confirmed the previous finding: Our observers did not accurately judge depth order when the blur was rendered and the stimulus presented on one plane. We then presented the same simulated scenes on multiple planes, each at a different focal distance, so the blur was created by the optics of the eye. Performance was now much better, which shows that depth order can be reliably determined from blur information but only when the optical effects are similar to those in natural viewing. We asked what the critical differences were in the single- and multiplane cases. We found that chromatic aberration provides useful information but accommodative microfluctuations do not. In addition, we examined how image formation is affected by occlusions and observed some interesting phenomena that allow the eye to see around and through occluding objects and may allow observers to estimate depth in da Vinci stereopsis, where one eye's view is blocked. Finally, we evaluated how accurately different rendering and displaying techniques reproduce the retinal images that occur in real occlusions. We discuss implications for computer graphics.},
author = {Zannoli, Marina and Love, Gordon D. and Narain, Rahul and Banks, Martin S.},
doi = {10.1167/16.6.17},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zannoli et al. - 2016 - Blur and the perception of depth at occlusions.pdf:pdf},
issn = {15347362},
journal = {Journal of Vision},
keywords = {Accommodation,Blur,Chromatic aberrations,Depth perception,Micro-fluctuations of accommodation,Occlusions},
mendeley-groups = {Medical Papers For Justication of Study 2/DepthPerceptionPapers,Bluring},
number = {6},
pages = {1--25},
pmid = {27115522},
title = {{Blur and the perception of depth at occlusions}},
volume = {16},
year = {2016}
}

@article{Jing2021,
abstract = {Gaze is one of the predominant communication cues and can provide valuable implicit information such as intention or focus when performing collaborative tasks. However, little research has been done on how virtual gaze cues combining spatial and temporal characteristics impact real-life physical tasks during face to face collaboration. In this study, we explore the effect of showing joint gaze interaction in an Augmented Reality (AR) interface by evaluating three bi-directional collaborative (BDC) gaze visualisations with three levels of gaze behaviours. Using three independent tasks, we found that all bi-directional collaborative BDC visualisations are rated significantly better at representing joint attention and user intention compared to a non-collaborative (NC) condition, and hence are considered more engaging. The Laser Eye condition, spatially embodied with gaze direction, is perceived significantly more effective as it encourages mutual gaze awareness with a relatively low mental effort in a less constrained workspace. In addition, by offering additional virtual representation that compensates for verbal descriptions and hand pointing, BDC gaze visualisations can encourage more conscious use of gaze cues coupled with deictic references during co-located symmetric collaboration. We provide a summary of the lessons learned, limitations of the study, and directions for future research.},
author = {Jing, Allison and May, Kieran and Lee, Gun and Billinghurst, Mark},
doi = {10.3389/frvir.2021.697367},
file = {:C\:/Users/tomis/Downloads/frvir-02-697367.pdf:pdf},
issn = {26734192},
journal = {Frontiers in Virtual Reality},
keywords = {CSCW,augmented reality collaboration,design and evaluation methods,gaze visualisation,human-computer interaction},
number = {June},
pages = {1--17},
title = {{Eye See What You See: Exploring How Bi-Directional Augmented Reality Gaze Visualisation Influences Co-Located Symmetric Collaboration}},
volume = {2},
year = {2021}
}

@book{Frech1960,
author = {Thomas Ewing French and Thomas Ewing French},
title = {{A manual of engineering drawing for students \& draftsmen}},
year = {1960},
publisher = {Cornell University}
}

@article{Weeks2021,
abstract = {Rationale and Objectives: Three-dimensional (3D) visualization has been shown to benefit new generations of medical students and physicians-in-training in a variety of contexts. However, there is limited research directly comparing student performance after using 3D tools to those using two-dimensional (2D) screens. Materials and Methods: A CT was performed on a donated cadaver and a 3D CT hologram was created. A total of 30 first-year medical students were randomly assigned into two groups to review head and neck anatomy in a teaching session that incorporated CT. The first group used an augmented reality headset, while the second group used a laptop screen. The students were administered a five-question anatomy test before and after the session. Two-tailed t-tests were used for statistical comparison of pretest and posttest performance within and between groups. A feedback survey was distributed for qualitative data. Results: Pretest vs. posttest comparison of average percentage of questions answered correctly demonstrated both groups showing significant in-group improvement (p < 0.05), from 59% to 95% in the augmented reality group, and from 57% to 80% in the screen group. Between-group analysis indicated that posttest performance was significantly better in the augmented reality group (p = 0.022, effect size = 0.73). Conclusion: Immersive 3D visualization has the potential to improve short-term anatomic recall in the head and neck compared to traditional 2D screen-based review, as well as engage millennial learners to learn better in anatomy laboratory. Our findings may reflect additional benefit gained from the stereoscopic depth cues present in augmented reality-based visualization.},
author = {Weeks, Joanna K. and Pakpoor, Jina and Park, Brian J. and Robinson, Nicole J. and Rubinstein, Neal A. and Prouty, Stephen M. and Nachiappan, Arun C.},
doi = {10.1016/j.acra.2020.07.008},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weeks et al. - 2021 - Harnessing Augmented Reality and CT to Teach First-Year Medical Students Head and Neck Anatomy.pdf:pdf},
issn = {18784046},
journal = {Academic Radiology},
keywords = {3D visualization,Augmented reality,Medical education,Mixed reality,Near-peer,Technology in education},
mendeley-groups = {Medical Papers For Justication of Study 2/Education},
number = {6},
pages = {871--876},
pmid = {32828663},
publisher = {Elsevier Inc.},
title = {{Harnessing Augmented Reality and CT to Teach First-Year Medical Students Head and Neck Anatomy}},
url = {https://doi.org/10.1016/j.acra.2020.07.008},
volume = {28},
year = {2021}
}

@article{Edwards2021,
abstract = {Imaging has revolutionized surgery over the last 50 years. Diagnostic imaging is a key tool for deciding to perform surgery during disease management; intraoperative imaging is one of the primary drivers for minimally invasive surgery (MIS), and postoperative imaging enables effective follow-up and patient monitoring. However, notably, there is still relatively little interchange of information or imaging modality fusion between these different clinical pathway stages. This book chapter provides a critique of existing augmented reality (AR) methods or application studies described in the literature using relevant examples. The aim is not to provide a comprehensive review, but rather to give an indication of the clinical areas in which AR has been proposed, to begin to explain the lack of clinical systems and to provide some clear guidelines to those intending pursue research in this area.},
author = {Edwards, P. J. “ Eddie” and Chand, Manish and Birlo, Manuel and Stoyanov, Danail},
doi = {10.1007/978-3-030-49100-0_10},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Edwards et al. - 2021 - The Challenge of Augmented Reality in Surgery.pdf:pdf},
isbn = {9783030491000},
journal = {Digital Surgery},
mendeley-groups = {MedicalDataOverlay},
pages = {121--135},
title = {{The Challenge of Augmented Reality in Surgery}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-49100-0_10},
year = {2021}
}

@article{French2022,
author = {French, Ranran L and Deangelis, Gregory C},
doi = {10.1038/s41598-022-23219-4},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/French, Deangelis - 2022 - Scene ‑ relative object motion biases depth percepts.pdf:pdf},
isbn = {4159802223219},
issn = {2045-2322},
journal = {Scientific Reports},
mendeley-groups = {Medical Papers For Justication of Study 2/DepthPerceptionPapers},
pages = {1--17},
publisher = {Nature Publishing Group UK},
title = {{Scene‑relative object motion biases depth percepts}},
url = {https://doi.org/10.1038/s41598-022-23219-4},
year = {2022}
}

@article{Interrante1995,
abstract = {There are many applications that can benefit from the simultaneous display of multiple layers of data. The objective in these cases is to render the layered surfaces in a such way that the outer structures can be seen and seen through at the same time. This paper focuses on the particular application of radiation therapy treatment planning, in which physicians need to understand the three-dimensional distribution of radiation dose in the context of patient anatomy. We describe a promising technique for communicating the shape and position of the transparent skin surface while at the same time minimally occluding underlying isointensity dose surfaces and anatomical objects: adding a sparse, opaque texture comprised of a small set of carefully-chosen lines. We explain the perceptual motivation for explicitly drawing ridge and valley curves on a transparent surface, describe straightforward mathematical techniques for detecting and rendering these lines, and propose a small number of reasonably effective methods for selectively emphasizing the most perceptually relevant lines in the display.},
author = {Interrante, Victoria and Fuchs, Henry and Pizer, Stephen},
doi = {10.1109/visual.1995.480795},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Interrante, Fuchs, Pizer - 1995 - Enhancing transparent skin surfaces with ridge and valley lines.pdf:pdf},
journal = {Proceedings of the IEEE Visualization Conference},
mendeley-groups = {ArtisticShadingEffects},
pages = {52--59},
title = {{Enhancing transparent skin surfaces with ridge and valley lines}},
year = {1995}
}

@incollection{Kaufman2005,
title = {Overview of Volume Rendering},
editor = {Charles D. Hansen and Chris R. Johnson},
booktitle = {Visualization Handbook},
publisher = {Butterworth-Heinemann},
address = {Burlington},
pages = {127-174},
year = {2005},
isbn = {978-0-12-387582-2},
doi = {https://doi.org/10.1016/B978-012387582-2/50009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780123875822500095},
author = {ARIE KAUFMAN and KLAUS MUELLER}
}

@article{TUKORA2020,
abstract = {Numerous volume rendering techniques are available to display 3D datasets on desktop computers and virtual reality devices. Recently the spreading of mobile and standalone virtual reality headsets has brought the need for volume visualization on these platforms too. However, the volume rendering techniques that show good performance in desktop environment underachieve on these devices, due to the special hardware conditions and visualization requirements. To speed up the volumetric rendering to an accessible level a hybrid technique is introduced, a mix of the ray casting and 3D texture mapping methods. This technique increases 2-4 times the frame rate of displaying volumetric data on mobile and standalone virtual reality headsets as compared to the original methods. The new technique was created primarily to display medical images but it is not limited only to this type of volumetric data.},
author = {TUKORA, Bal{\'{a}}zs},
doi = {10.1556/606.2020.15.2.1},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/TUKORA - 2020 - Effective volume rendering on mobile and standalone vr headsets by means of a hybrid method.pdf:pdf},
issn = {17883911},
journal = {Pollack Periodica},
keywords = {3D texture mapping,Direct volume rendering,Mobile and standalone virtual reality headsets,Ray casting,Virtual reality},
mendeley-groups = {VoxelAlgorithms},
number = {2},
pages = {3--12},
title = {{Effective volume rendering on mobile and standalone vr headsets by means of a hybrid method}},
volume = {15},
year = {2020}
}

@article{Ni2011,
abstract = {In this paper, we explore the use of a projection-based handheld device to facilitate in-clinic doctor-patient communication. We present the user-centered design process used to understand the workflow of medical professionals and to identify challenges they currently face in communicating information to patients. Based on the lessons learned, we developed AnatOnMe, a prototype projection-based handheld system for enhancing information exchange in the current practice of one medical sub-specialty, physical therapy. We then present the results of a controlled experiment to understand the desirability and learning tradeoffs of using AnatOnMe to teach medical concepts on three potential projection surfaces - wall, model, and patient body. Finally, we present results of two expert reviews of the system. Copyright 2011 ACM.},
author = {Ni, Tao and Karlson, Amy K. and Wigdor, Daniel},
doi = {10.1145/1978942.1979437},
file = {:C\:/Users/tomis/Downloads/1978942.1979437.pdf:pdf},
isbn = {9781450302289},
journal = {Conference on Human Factors in Computing Systems - Proceedings},
keywords = {Doctor-patient communication,Handheld device,Healthcare,Physical therapy,Pico-projector},
pages = {3333--3342},
title = {{AnatOnMe: Facilitating doctor-patient communication using a projection-based handheld device}},
year = {2011}
}

@article{Abildgaard2010,
abstract = {Purpose An autostereoscopic display with image quality comparable to ordinary 2D displays has recently been developed. The purpose of our study was to evaluate whether the visualization of static 3D models from intracranial time-offlight (TOF) MR angiography (MRA) was improved by this display. Methods Maximum Intensity Projection (MIP) and Volume Rendering (VR) 3D models of intracranial arteries were created from ten TOF MRA datasets. Thirty-one clinically relevant intracranial arterial segments were marked in the TOF source images.Atotal of 217 markings were used. Themarkings were displayed in the 3D models as overlying red dots. Three neuroradiologists viewed the static 3D models on the autostereoscopic display, with the display operating either in autostereoscopic mode or in 2D mode. The task of the neuroradiologists was to correctly identify the marked artery. A paired comparison was made between arterial identification in autostereoscopic and 2D display mode. Results In 314 MIP 3Dmodels, 233 arterial markings (74%) were correctly identified with the display operating in autostereoscopic mode versus 179 (57%) in 2D mode. Odds ratio for correct identification with autostereoscopic mode versus 2D mode was 2.17 (95% confidence interval 1.55-3.04, P > 0.001). In 337 VR 3D models, 256 markings (76%) were correctly identified using autostereoscopic mode and 229 (68%) using 2D mode (odds ratio 1.49, 95% confidence interval 1.06-2.09, P = 0.021). Conclusion The visualization of intracranial arteries in static 3D models from TOF MRA can be improved by the use of an autostereoscopic display. {\textcopyright} CARS 2010.},
author = {Abildgaard, Andreas and Witwit, Alaa Kasid and Karlsen, J{\o}rn Skaarud and Jacobsen, Eva Astrid and Tenn{\o}e, Bj{\o}rn and Ringstad, Geir and Due-T{\o}nnessen, Paulina},
doi = {10.1007/s11548-010-0509-5},
file = {:C\:/Users/tomis/Downloads/s11548-010-0509-5.pdf:pdf},
issn = {18616429},
journal = {International Journal of Computer Assisted Radiology and Surgery},
keywords = {3D visualization,Autostereoscopic display,Autostereoscopy,Magnetic resonance angiography,Stereoscopy},
number = {5},
pages = {549--554},
title = {{An autostereoscopic 3D display can improve visualization of 3D models from intracranial MR angiography}},
volume = {5},
year = {2010}
}

@article{Joshi2013,
abstract = {Neurosurgical planning and image guided neurosurgery require the visualization of multimodal data obtained from various functional and structural image modalities, such as Magnetic Resonance Imaging (MRI), Computed Tomography (CT), functional MRI, Single photon emission computed tomography (SPECT) and so on. In the case of epilepsy neurosurgery for example, these images are used to identify brain regions to guide intracranial electrode implantation and resection. Generally, such data is visualized using 2D slices and in some cases using a 3D volume rendering along with the functional imaging results. Visualizing the activation region effectively by still preserving sufficient surrounding brain regions for context is exceedingly important to neurologists and surgeons. We present novel interaction techniques for visualization of multimodal data to facilitate improved exploration and planning for neuro-surgery. We extended the line widget from VTK to allow surgeons to control the shape of the region of the brain that they can visually crop away during exploration and surgery. We allow simple spherical, cubical, ellipsoidal and cylindrical (probe aligned cuts) for exploration purposes. In addition we integrate the cropping tool with the image- guided navigation system used for epilepsy neurosurgery. We are currently investigating the use of these new tools in surgical planning and based on further feedback from our neurosurgeons we will integrate them into the setup used for image-guided neurosurgery.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Joshi, Alark and Dustin, Scheinost and Vives, Kenneth and Spencer, Dennis and Staib, Lawrence and Xenophon, Papademtris},
doi = {10.1109/TVCG.2008.150.Novel},
eprint = {NIHMS150003},
file = {:C\:/Users/tomis/Downloads/nihms-87556.pdf:pdf},
isbn = {6176321972},
issn = {15378276},
journal = {IEEE Transactions on Visual Computing Graphics},
keywords = {User interaction,irregular cropping 1},
mendeley-groups = {3Dvs2DMedicalVis},
number = {1},
pages = {1--7},
pmid = {1000000221},
title = {{Novel Interaction Techniques for Neurosurgical Planning and Stereotactic Navigation}},
volume = {23},
year = {2013}
}

@article{Cocosco1997BrainWebOI,
  title={BrainWeb: Online Interface to a 3D MRI Simulated Brain Database},
  author={Chris A. Cocosco and Vasken Kollokian and Remi K.-S. Kwan and Alan C. Evans},
  journal={NeuroImage},
  year={1997},
  url={https://api.semanticscholar.org/CorpusID:14864257}
}

	
@article{dns,
    title = {Direct Numerical Simulation of Turbulent Channel Flow up to ${R}e_\tau \approx 5200$},
    author = {Lee, Myoungkyu and Moser, Robert D.},
    journal = {Journal of Fluid Mechanics},
    volume = {774},
    pages = {395--415},
    year = {2015},
    month = {jul},
    doi = {10.1017/jfm.2015.268},
}

	
@inproceedings{neghip,
    title = {{VolVis}: A Diversified System for Volume Research and Development},
    author = {Avila, R. and He, Taosong and Hong, Lichan and Kaufman, A. and Pfister, H. and Silva, C. and Sobierajski, L. and Wang, S.},
    booktitle = {Proceedings Visualization '94},
    pages = {31--38},
    year = {1994},
    month = {oct},
    doi = {10.1109/VISUAL.1994.346340},
}

@article{jicf_q,
    title = {A Direct Numerical Simulation Study of Turbulence and Flame Structure in Transverse Jets Analysed in Jet-Trajectory Based Coordinates},
    author = {Grout, R. W. and Gruber, A. and Kolla, H. and Bremer, P.-T. and Bennett, J. C. and Gyulassy, A. and Chen, J. H.},
    journal = {Journal of Fluid Mechanics},
    volume = {706},
    pages = {351--383},
    year = {2012},
    doi = {10.1017/jfm.2012.257},
}

@article{Tomic2023,
abstract = {Purpose: Steadily increasing use of computational/virtual phantoms in medical physics has motivated expanding development of new simulation methods and data representations for modelling human anatomy. This has emphasized the need for increased realism, user control, and availability. In breast cancer research, virtual phantoms have gained an important role in evaluating and optimizing imaging systems. For this paper, we have developed an algorithm to model breast abnormalities based on fractal Perlin noise. We demonstrate and characterize the extension of this approach to simulate breast lesions of various sizes, shapes, and complexity. Materials and method: Recently, we developed an algorithm for simulating the 3D arrangement of breast anatomy based on Perlin noise. In this paper, we have expanded the method to also model soft tissue breast lesions. We simulated lesions within the size range of clinically representative breast lesions (masses, 5–20 mm in size). Simulated lesions were blended into simulated breast tissue backgrounds and visualized as virtual digital mammography images. The lesions were evaluated by observers following the BI-RADS assessment criteria. Results: Observers categorized the lesions as round, oval or irregular, with circumscribed, microlobulated, indistinct or obscured margins. The majority of the simulated lesions were considered by the observers to have a realism score of moderate to well. The simulation method provides almost real-time lesion generation (average time and standard deviation: 1.4 ± 1.0 s). Conclusion: We presented a novel algorithm for computer simulation of breast lesions using Perlin noise. The algorithm enables efficient simulation of lesions, with different sizes and appearances.},
author = {Tomic, Hanna and Costa, Arthur C. and Bjerk{\'{e}}n, Anna and Vieira, Marcelo A.C. and Zackrisson, Sophia and Tingberg, Anders and Timberg, Pontus and Dustler, Magnus and Bakic, Predrag R.},
doi = {10.1016/j.ejmp.2023.102681},
file = {:C\:/Users/tomis/Downloads/ForJas/PIIS1120179723001588.pdf:pdf},
issn = {1724191X},
journal = {Physica Medica},
keywords = {Computational phantoms,Lesion simulation,Mammography,Virtual clinical trials},
number = {July},
pmid = {37748358},
title = {{Simulation of breast lesions based upon fractal Perlin noise}},
volume = {114},
year = {2023}
}

@article{Lawson2024,
abstract = {Fibrosis, a pathological increase in extracellular matrix proteins, is a significant health issue that hinders the function of many organs in the body, in some cases fatally. In the heart, fibrosis impacts on electrical propagation in a complex and poorly predictable fashion, potentially serving as a substrate for dangerous arrhythmias. Individual risk depends on the spatial manifestation of fibrotic tissue, and learning the spatial arrangement on the fine scale in order to predict these impacts still relies upon invasive ex vivo procedures. As a result, the effects of spatial variability on the symptomatic impact of cardiac fibrosis remain poorly understood. In this work, we address the issue of availability of such imaging data via a computational methodology for generating new realisations of cardiac fibrosis microstructure. Using the Perlin noise technique from computer graphics, together with an automated calibration process that requires only a single training image, we demonstrate successful capture of collagen texturing in four types of fibrosis microstructure observed in histological sections. We then use this generator to quantitatively analyse the conductive properties of these different types of cardiac fibrosis, as well as produce three-dimensional realisations of histologically-observed patterning. Owing to the generator's flexibility and automated calibration process, we also anticipate that it might be useful in producing additional realisations of other physiological structures.},
author = {Lawson, Brodie A.J. and Drovandi, Christopher and Burrage, Pamela and Bueno-Orovio, Alfonso and dos Santos, Rodrigo Weber and Rodriguez, Blanca and Mengersen, Kerrie and Burrage, Kevin},
doi = {10.1016/j.media.2024.103240},
file = {:C\:/Users/tomis/Downloads/ForJas/1-s2.0-S1361841524001658-main.pdf:pdf},
issn = {13618423},
journal = {Medical Image Analysis},
keywords = {Cardiac fibrosis,Generative modelling,Image generation,Perlin noise},
number = {October 2023},
pages = {103240},
pmid = {39208559},
publisher = {Elsevier B.V.},
title = {{Perlin noise generation of physiologically realistic cardiac fibrosis}},
url = {https://doi.org/10.1016/j.media.2024.103240},
volume = {98},
year = {2024}
}

@article{Alex2020,
abstract = {Colour selection is an important task in digital art and 3D modelling applications. Most colour pickers are based on continuous colour spaces or a representative sampling of them, such as the Munsell colour palette. Continuous colour space-based pickers enable users to select from all colours by displaying full saturation hues with options to lower saturation and modify value. The two-step process of colour selection from continuous pickers requires understanding of 3D colour space, e.g., where to find "brown"or "sand". In this research we investigate how continuous versus discrete pickers affect colour selection in virtual art. We compared an HSV picker with a discrete picker in a study with 40 participants aged 16-60. We found that the colour picker impacted the kinds of colours used in artworks, with significant differences in colour distribution characteristics. We discuss implications of colour selection tools for virtual reality art-making.},
author = {Alex, Marylyn and Lottridge, Danielle and Lee, Jisu and Marks, Stefan and W{\"{u}}ensche, Burkhard},
doi = {10.1145/3441000.3441054},
file = {:C\:/Users/tomis/Downloads/3441000.3441054.pdf:pdf},
isbn = {9781450389754},
journal = {ACM International Conference Proceeding Series},
keywords = {Colour palette,Colour picker,Colour selection,Colour space,Creativity tools,Virtual reality,Virtual reality art},
mendeley-groups = {ForThesis},
number = {Figure 2},
pages = {158--169},
title = {{Discrete versus Continuous Colour Pickers Impact Colour Selection in Virtual Reality Art-Making}},
year = {2020}
}

@article{Nguyen2013,
abstract = {Image-based modeling is becoming increasingly popular as a means to create realistic 3D digital models of real-world objects. Applications range from games and e-commerce to virtual worlds and 3D printing. Most research in computer vision has concentrated on the precise reconstruction of geometry. However, in order to improve realism and enable use in professional production pipelines digital models need a high-resolution texture map. In this paper we present a novel system for creating detailed texture maps from a set of input images and estimated 3D geometry. The solution uses a mesh segmentation and charting approach in order to create a low-distortion mesh parameterization suitable for objects of arbitrary genus. Texture maps for each mesh segment are created by back-projecting the best-fitting input images onto each surface segment, and smoothly fusing them together using graph-cut techniques. We investigate the effect of different input parameters, and present results obtained for reconstructing a variety of different 3D objects from input images acquired using an unconstrained and uncalibrated camera.},
author = {Nguyen, Hoang Minh and W{\"{u}}nsche, Burkhard and Delmas, Patrice and Zhang, Eugene and Lutteroth, Christof and {Van Der Mark}, Wannes},
file = {:C\:/Users/tomis/Downloads/Nguyen.pdf:pdf},
isbn = {9788086943749},
journal = {21st International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision, WSCG 2013 - Full Papers Proceedings},
keywords = {Image-based modeling,Mesh parameterization,Texture mapping,Texture reconstruction},
mendeley-groups = {ForThesis},
pages = {39--48},
title = {{High-definition texture reconstruction for 3D image-based modeling}},
year = {2013}
}

@article{Pereira2019,
abstract = {Digital three-dimensional (3-D) information concerning the location and condition of subsurface urban infrastructure is emerging as a potential new paradigm for aiding in the assessment, construction, emergency response, management, and planning of these vital assets. Subsurface infrastructure encompasses utilities (water, stormwater, wastewater, gas, electricity, telecommunications, steam, etc.), geotechnical formations, and the built underground (including tunnels, subways, garages and subsurface buildings). Traditional approaches for collecting location information include merging as-built drawings, historical records, and dead reckoning; and combining with information gathered by above-ground geophysical instruments, such as ground penetrating radars, magnetometers and acoustic sensors. This paper presents results of efforts aimed at using photogrammetric and augmented reality (AR) techniques to aid collecting, processing, and presenting 3-D location information.},
author = {Pereira, M. and Orfeo, D. and Ezequelle, W. and Burns, D. and Xia, T. and Huston, D. R.},
doi = {10.1680/icsic.64669.169},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pereira et al. - 2019 - Photogrammetry and augmented reality for underground infrastructure sensing, mapping and assessment.pdf:pdf},
isbn = {9780727764669},
journal = {International Conference on Smart Infrastructure and Construction 2019, ICSIC 2019: Driving Data-Informed Decision-Making},
mendeley-groups = {Calibration},
pages = {169--175},
title = {{Photogrammetry and augmented reality for underground infrastructure sensing, mapping and assessment}},
volume = {2019},
year = {2019}
}

@article{Wunsche2003,
abstract = {Medical data sets now comprise a diverse range of measurements such as tissue densities, sensitivity to magnetization, blood flow velocity, and material strain. The size and complexity of medical data sets makes it increasingly difficult to understand, compare, analyze and communicate the data. Visualization is an attempt to simplify these tasks according to the motto "An image says more than a thousand words". Representing complex material properties, such as strain, as a single image improves the perception of features and pattern in the data, enables the recognition of relationship between different measures and facilitates the navigation through and interaction with complex and disparate sets of data.This paper introduces a toolkit developed for exploring complex biomedical data sets. The contributions of this paper are threefold: we suggest a modular design which facilitates the comparison and exploration of multiple data sets and visualization. We introduce a novel field data structure which allows interactive creation of new fields and we present boolean filters as a universal visualization tool. Copyright {\textcopyright} 2003 by the Association for Computing Machinery, Inc.},
author = {W{\"{u}}nsche, Burkhard C.},
doi = {10.1145/604471.604505},
file = {:C\:/Users/tomis/Downloads/A-toolkit-for-visualizing-biomedical-data-sets.pdf:pdf},
isbn = {1581135785},
journal = {Proceedings of the 1st International Conference on Computer Graphics and Interactive Techniques in Australasia and South East Asia, GRAPHITE '03},
keywords = {Biomedicine,Tensor fields,User interfaces,Visualization},
mendeley-groups = {ForThesis},
number = {June},
title = {{A toolkit for visualizing biomedical data sets}},
year = {2003}
}

@article{Heide2013,
author = {Heide, Felix and Wetzstein, Gordon and Raskar, Ramesh and Heidrich, Wolfgang},
title = {Adaptive image synthesis for compressive displays},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2461912.2461925},
doi = {10.1145/2461912.2461925},
abstract = {Recent years have seen proposals for exciting new computational display technologies that are compressive in the sense that they generate high resolution images or light fields with relatively few display parameters. Image synthesis for these types of displays involves two major tasks: sampling and rendering high-dimensional target imagery, such as light fields or time-varying light fields, as well as optimizing the display parameters to provide a good approximation of the target content.In this paper, we introduce an adaptive optimization framework for compressive displays that generates high quality images and light fields using only a fraction of the total plenoptic samples. We demonstrate the framework for a large set of display technologies, including several types of auto-stereoscopic displays, high dynamic range displays, and high-resolution displays. We achieve significant performance gains, and in some cases are able to process data that would be infeasible with existing methods.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {132},
numpages = {12},
keywords = {computational displays, image synthesis}
}


@article{Schenke2005,
abstract = {PURPOSE: To assess the influence of reconstruction algorithms for positron emission tomography (PET) based volume quantification. The specifically detected activity in the threshold defined volume was investigated for different reconstruction algorithms as a function of volume size and signal to background ratio (SBR), especially for volumes smaller than 1ml. Special attention was given to the Siemens specific iterative reconstruction algorithm TrueX. METHODS: Measurements were performed with a modified in-house produced IEC body phantom on a Siemens Biograph 64 True Point PET/CT scanner (Siemens, Medical Systems) for six different SBRs (2.1, 3.8, 4.9, 6.7, 8.9, 9.4 and without active background (BG)). The phantom consisted of a water-filled cavity with built-in plastic spheres (0.27, 0.52, 1.15, 2.57, 5.58 and 11.49ml). The following reconstruction algorithms available on the Siemens Syngo workstation were evaluated: Iterative OSEM (OSEM) (4 iterations, 21 subsets), iterative TrueX (TrueX) (4 iterations, 21 subsets) and filtered backprojection (FBP). For the threshold based volume segmentation the software Rover (ABX, Dresden) was used. RESULTS: For spheres larger than 2.5ml a constant threshold (standard deviation (SD) 10%) level was found for a given SBR and reconstruction algorithm and therefore a mean threshold for the largest three spheres was calculated. This threshold could be approximated by a function inversely proportional to the SBR. The threshold decreased with increasing SBR for all sphere sizes. For the OSEM algorithm the threshold for small spheres with 0.27, 0.52 and 1.15ml varied between 17% and 44% (depending on sphere size). The threshold for the TrueX algorithm was substantially lower (up to 17%) than for the OSEM algorithm for all sphere sizes. The maximum activity in a specific volume yielded the true activity for the OSEM algorithm when using a SBR independent correction factor C, which depended on sphere size. For the largest three volumes a constant factor C=1.100.03 was found. For smaller volumes, C increased exponentially due to the partial volume effect. For the TrueX algorithm the maximum activity overestimated the true activity. CONCLUSION: The threshold values for PET based target volume segmentation increased with increasing sphere size for all tested algorithms. True activity values of spheres in the phantom could be extracted using experimentally determined correction factors C. The TrueX algorithm has to be used carefully for quantitative comparison (e.g. follow-up) and multicenter studies.},
author = {Schenke, Stefan and Burkhard, C W and Denzler, Joachim},
file = {:C\:/Users/tomis/Downloads/document.pdf:pdf},
journal = {Proceedings of IVCNZ 2005},
keywords = {gpgpu,gpu programming,volume segmentation},
mendeley-groups = {ForThesis},
number = {0959-4388 LA - eng PT - Journal Article PT - Review PT - Review, Tutorial},
pages = {59--65},
title = {{GPU-Based Volume Segmentation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.112.3023&rep=rep1&type=pdf},
volume = {11},
year = {2005}
}

@article{Li2012,
author = {Li, Ivan K Y. and Peek, Edward M. and Wunshe, Burkhard C. and Lutteroth, Christof},
file = {:C\:/Users/tomis/Downloads/AUIC2012_LiEtAl.pdf:pdf},
journal = {Proceedings of the Thirteenth Australasian User Interface Conference},
keywords = {3d,anaglyphic stereo,display,head coupled perspective,head tracking,stereoscopic 3d},
mendeley-groups = {ForThesis},
pages = {59--68},
title = {{Enhancing 3D Applicaitons Using Stereoscipic 3D and Motion Parallax}},
volume = {340},
year = {2012}
}

@article{Peek2014,
abstract = {In virtual reality applications, frame rate and latency are critical performance metrics for maintaining a comfortable user experience and avoiding simulator-sickness. Various methods may be used to improve frame rate and latency, however they often come at the cost of image quality or other performance metrics. One particularly beneficial method is synthesising additional and/or lower-latency frames using image warping. However, desktop graphics subsystems lack the required level of parallelism to effectively implement a complete image warping solution on computers with a single GPU. Fortunately many computers that are treated as single-GPU systems are, in fact, multi-GPU due to the presence of a low-performance secondary GPU integrated with the CPU package. In this paper we describe an image warping system which exploits the hardware parallelism offered by integrated GPUs to avoid the aforementioned graphics subsystem limitations. This system improves both perceived frame rate and latency: in contrast to alternative systems which only improve one or the other. We also evaluate the performance of performing image warping on integrated GPUs. Our system is able to perform a warp in as little as 4.2ms at 1920 × 1080 resolution on an Intel HD Graphics 2000 IGP at the cost of a 1.5 ms increase in application render time. This demonstrates that the overhead of our system is manageable for current VR applications even on several year old computer hardware.},
author = {Peek, Edward and W{\"{u}}nsche, Burkhard and Lutteroth, Christof},
doi = {10.1145/2683405.2683445},
file = {:C\:/Users/tomis/Downloads/PeekWuenscheLutteroth-GPUImageWarping2014.pdf:pdf},
isbn = {9781450331845},
journal = {ACM International Conference Proceeding Series},
keywords = {Frame rate,Headmounted display,Image warping,Integrated graphics processing unit,Latency},
mendeley-groups = {ForThesis},
pages = {172--177},
title = {{Using integrated GPUs to perform image warping for HMDs}},
volume = {19-21-November-2014},
year = {2014}
}

@article{Liu2010,
abstract = {The effectiveness of direct volume rendered images depends on finding transfer functions which emphasize structures in the underlying data. In order to support this process, we present a spreadsheet-like constructive visual component-based interface, which also allows novice users to efficiently find meaningful transfer functions. The interface uses a programming-by-example style approach and exploits the domain knowledge of the user without requiring visualization knowledge. Therefore, our application automatically analysis histograms with the Douglas-Peucker algorithm in order to identify potential structures in the data set. Sample visualizations of the resulting structures are presented to the user who can refine and combine them to more complex visualizations. Preliminary tests confirm that the interface is easy to use, and enables non-expert users to identify structures which they could not reveal with traditional transfer function editors.},
author = {Liu, Bingchen and W{\"{u}}nsche, Burkhard and Ropinski, Timo},
doi = {10.5220/0002844102540259},
file = {:C\:/Users/tomis/Downloads/FULLTEXT01.pdf:pdf},
isbn = {9789896740269},
journal = {GRAPP 2010 - Proceedings of the International Conference on Computer Graphics Theory and Applications},
keywords = {Direct volume rendering,Histograms,Transfer functions,Visual interfaces,Visualizations},
mendeley-groups = {ForThesis},
pages = {254--259},
title = {{Visualization by example: A constructive visual component-based interface for direct volume rendering}},
year = {2010}
}

@article{Dicken2005,
abstract = {Prototypical software applications for the quantification and visualization of thoracic CT data with the aim of aiding thoracic surgeons in risk assessment and planning of complex resections in patients with lung cancer or lung metastasis were developed. A feasibility study with data from six surgical sites demonstrates strong potential to help surgeons to improve the selection of patients and resection strategies. {\textcopyright} 2005.},
author = {Dicken, V. and Kuhnigk, J. M. and Bornemann, L. and Zidowitz, S. and Krass, S. and Peitgen, H. O.},
doi = {10.1016/j.ics.2005.03.203},
file = {:C\:/Users/tomis/Downloads/1-s2.0-S0531513105004619-main.pdf:pdf},
issn = {05315131},
journal = {International Congress Series},
keywords = {CT,Lung cancer,Lung function,Lung lobes,Metastasis,Surgery,Thorax,Visualization},
pages = {783--787},
title = {{Novel CT data analysis and visualization techniques for risk assessment and planning of thoracic surgery in oncology patients}},
volume = {1281},
year = {2005}
}

@article{Rieder2009,
abstract = {Image guided radiofrequency ablation (RFA) is becoming a standard procedure as a minimally invasive method for tumor treatment in the clinical routine. The visualization of pathological tissue and potential risk structures like vessels or important organs gives essential support in image guided pre-interventional RFA planning. In this work our aim is to present novel visualization techniques for interactive RFA planning to support the physician with spatial information of pathological structures as well as the finding of trajectories without harming vitally important tissue. Furthermore, we illustrate three-dimensional applicator models of different manufactures combined with corresponding ablation areas in homogenous tissue, as specified by the manufacturers, to enhance the estimated amount of cell destruction caused by ablation. The visualization techniques are embedded in a workflow oriented application, designed for the use in the clinical routine. To allow a high-quality volume rendering we integrated a visualization method using the fuzzy c-means algorithm. This method automatically defines a transfer function for volume visualization of vessels without the need of a segmentation mask. However, insufficient visualization results of the displayed vessels caused by low data quality can be improved using local vessel segmentation in the vicinity of the lesion. We also provide an interactive segmentation technique of liver tumors for the volumetric measurement and for the visualization of pathological tissue combined with anatomical structures. In order to support coagulation estimation with respect to the heat-sink effect of the cooling blood flow which decreases thermal ablation, a numerical simulation of the heat distribution is provided.},
author = {Rieder, Christian and Schwier, Michael and Weihusen, Andreas and Zidowitz, Stephan and Peitgen, Heinz-Otto},
doi = {10.1117/12.813729},
file = {:C\:/Users/tomis/Downloads/726134.pdf:pdf},
isbn = {9780819475121},
issn = {16057422},
journal = {Medical Imaging 2009: Visualization, Image-Guided Procedures, and Modeling},
keywords = {1,1 particularly,description of purpose,developed as a minimal,has taken,image-guided ablation therapies using,image-guided therapy,in the past few,invasive alternative for the,radiofrequency-ablation,rfa,the radiofrequency ablation,therapy planning,thermal energy has been,treatment of liver tumors,visualization,years},
pages = {726134},
title = {{Visualization of risk structures for interactive planning of image guided radiofrequency ablation of liver tumors}},
volume = {7261},
year = {2009}
}


@article{Kaufman1999,
abstract = {This paper is a survey ofvolume graphics. It includes an introduction to volumetric data and to volume modeling techniques, such as voxelization, texture mapping, amorphous phenomena, block operations, constructive solid modeling, and volume sculpting. Acomparison between surface graphics and volume graphics is given, along with a consideration of volume graphics advantages and weaknesses. The paper concludes with a discussion on special-purpose volume rendering hardware.},
author = {Kaufman, Arie E},
file = {:C\:/Users/tomis/Downloads/KaufmanVolumeGraphics.pdf:pdf},
journal = {Siggraph},
number = {Section 3},
pages = {24--47},
title = {{Introduction to volume graphics}},
volume = {99},
year = {1999}
}


@article{Kania2014,
abstract = {Inaccurate electrode placement and differences in inter-individual human anatomies can lead to misinterpretation of ECG examination. The aim of the study was to investigate the effect of precordial electrodes displacement on morphology of the ECG signal in a group of 60 patients with diagnosed cardiac disease. Shapes of ECG signals recorded from precordial leads were compared with signals interpolated at the points located at a distance up to 5 cm from lead location. Shape differences of the QRS and ST-T-U complexes were quantified using the distribution function method, correlation coefficient, root-mean-square error (RMSE), and normalized RMSE. The relative variability (RV) index was calculated to quantify inter-individual variability. ECG morphology changes were prominent in all shape parameters beyond 2 cm distance to precordial leads. Lead V2 was the most sensitive to displacement errors, followed by leads V3, V1, and V4, for which the direction of electrodes displacement plays a key role. No visible changes in ECG morphology were observed in leads V5 and V 6, only scaling effect of signal amplitude. The RV ranged from 0.639 to 0.989. Distortions in ECG tracings increase with the distance from precordial lead, which are specific to chosen electrode, direction of displacement, and for ECG segment selected for calculations. {\textcopyright} 2013 The Author(s).},
author = {Kania, Micha{\l} and Rix, Herv{\'{e}} and Fereniec, Ma{\l}gorzata and Zavala-Fernandez, Heriberto and Janusek, Dariusz and Mroczka, Tomasz and Stix, G{\"{u}}nter and Maniewski, Roman},
doi = {10.1007/s11517-013-1115-9},
file = {:C\:/Users/tomis/Downloads/11517_2013_Article_1115.pdf:pdf},
issn = {01400118},
journal = {Medical and Biological Engineering and Computing},
keywords = {Body surface potential mapping,Cardiac monitoring,Electrocardiogram,Precordial electrode displacement,Shape analysis},
number = {2},
pages = {109--119},
pmid = {24142562},
title = {{The effect of precordial lead displacement on ECG morphology}},
volume = {52},
year = {2014}
}


@article{Hadjiantoni2021,
abstract = {Background: Anatomical misplacement of the Electrocardiogram (ECG) electrode(s) is common, with significant impact on clinical diagnosis. Reasons are multi-faceted, with this review examining the consequential effects of misplacement to ECG morphology, diagnosis, prognosis, patient outcomes, and potential impact to patient care pathway. Objectives: This review examined the significance of misplacement, its' commonality and ensuing effect on patient safety, accurate ECG acquisition and diagnosis, with evaluation of reasons for such misplacement. Methodology: Review of available literature was conducted using electronic databases. In-line with the Preferred Reporting Items for Systematic reviews and Meta-analysis protocols (PRISMA) 2015 checklist, this review was conducted with search criteria, search terms, eligibility for inclusion/ exclusion criteria, extraction and data analysis predetermined by the authors. Keywords were arranged according to grouping of terms surrounding ECG, anatomical placement, and diagnosis. The search strategy was conducted during September/October 2019. Scoping searches were conducted alongside reference lists of included studies hand searched (Snowballing) for further relevant studies. The Critical Appraisal Skills Programme (CASP) was used to methodically appraise papers (CASP, 2019). Screening of titles and abstracts of identified citations was performed by a single reviewer. Eligible articles then full text screened independently by two reviewers. Disagreements were discussed and resolved by a third reviewer. In instances of unclear reporting, authors were contacted to provide further information and clarity. Assessment of relevant literature and critical appraisal of primary research, pertaining to the clinical diagnosis and effects of anatomical misplacement of ECG electrodes, formulate the thematic discussion drawn by this review. Results: This review identified a plethora of causes, ranging from: operator error; lack of anatomical awareness; inaccurate assessment of anatomical landmarks; obesity; differences in anatomy/gender; levels of undress and lack of appreciation of consequences of misplacement, both modifiable and non-modifiable attributable to electrode misplacement. Clinical diagnosis can be altered owing to erroneous placement of electrodes. ECG morphology is altered due to incorrect anatomical misplacement, culminating substandard practice, a missed diagnosis or misdiagnosis and potential harm. Conclusion: Correct anatomical placement of ECG electrodes is essential to diagnosis in the clinical setting. Peer-led educational intervention with mandatory training is essential to improve practice.},
author = {Hadjiantoni, Alexandros and Oak, Katy and Mengi, Siddhartha and Konya, Judit and Ungvari, Tamas},
doi = {10.26502/fccm.92920192},
file = {:C\:/Users/tomis/Downloads/is-the-correct-anatomical-placement-of-the-electrocardiogram-ecg-electrodes.pdf:pdf},
journal = {Cardiology and Cardiovascular Medicine},
number = {02},
pages = {182--200},
title = {{Is the Correct Anatomical Placement of the Electrocardiogram (ECG) Electrodes Essential to Diagnosis in the Clinical Setting: A Systematic Review}},
volume = {05},
year = {2021}
}

@article{Kanodia2005,
abstract = {Isosurface extraction is a standard method for visualizing scalar volume data that can be used to render a specific material boundaries inherent in multi-material data sets. Multiple transparent isosurfaces can thus be used to visualize multiple material boundaries, but still fail to capture any data in between the boundary layers. We describe how isosurfaces can be "enriched" with surrounding material information. By visualizing surrounding material, both material boundary information and gradient - or change in density - information of the scalar field are represented. Visualizing multiple transparent material-enriched isosurfaces leads to a fairly effective volumetric impression. Thus, our approach approximates results obtained from direct volume rendering. The visualization of multiple transparent isosurfaces requires a back-to-front rendering of the typically triangulated surface components. The order of the surfaces' triangles is imposed by the location of the convex cells they are extracted from, which supports fast rendering of multiple isosurface. Copyright UNION Agency - Science Press.},
author = {Kanodia, Ravindra L. and Linsen, Lars and Hamann, Bernd},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kanodia, Linsen, Hamann - 2005 - Multiple transparent material-enriched isosurfaces.pdf:pdf},
isbn = {8090310079},
journal = {13th International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision 2005, WSCG'2005 - In Co-operation with EUROGRAPHICS, Full Papers},
keywords = {Material-enriched isosurfaces,Multiple transparent isosurfaces,Volume rendering},
mendeley-groups = {VoxelAlgorithms/VolumeRendering},
number = {January},
pages = {23--30},
title = {{Multiple transparent material-enriched isosurfaces}},
year = {2005}
}

@article{Liu2017,
abstract = {Feature-based time-varying volume visualization is combined with illustrative visualization to tell the story of how mesoscale ocean eddies form in the Gulf Stream and transport heat and nutrients across the ocean basin. The internal structure of these three-dimensional eddies and the kinematics with which they move are critical to a full understanding of ocean eddies. In this work, we apply a feature-based method to track instances of ocean eddies through the time steps of a high-resolution multi-decadal regional ocean model and generate a series of eddy paths which reflect the life cycle of individual eddy instances. Based on the computed metadata, several important geometric and physical properties of eddy are computed. Illustrative visualization techniques, including visual effectiveness enhancement, focus+context, and smart visibility, are combined with the extracted volume features to explore eddy characteristics at different levels. An evaluation by domain experts indicates that combining our feature-based techniques with illustrative visualization techniques provides an insight into the role eddies play in ocean circulation. The domain experts expressed a preference for our methods over existing tools.},
author = {Liu, L. and Silver, D. and Bemis, K. and Kang, D. and Curchitser, E.},
doi = {10.1111/cgf.13201},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2017 - Illustrative Visualization of Mesoscale Ocean Eddies.pdf:pdf},
issn = {14678659},
journal = {Computer Graphics Forum},
keywords = {Categories and Subject Descriptors (according to A,I.3.8 [Computer Graphics]: Applications—Oceanograp},
mendeley-groups = {VoxelAlgorithms/VolumeRendering/Gologoly},
number = {3},
pages = {447--458},
title = {{Illustrative Visualization of Mesoscale Ocean Eddies}},
volume = {36},
year = {2017}
}

@article{Baus2014,
abstract = {This paper reviews the move from virtual reality exposure-based therapy to augmented reality exposure-based therapy (ARET). Unlike virtual reality (VR), which entails a complete virtual environment (VE), augmented reality (AR) limits itself to producing certain virtual elements to then merge them into the view of the physical world. Although, the general public may only have become aware of AR in the last few years, AR type applications have been around since beginning of the twentieth century. Since, then, technological developments have enabled an ever increasing level of seamless integration of virtual and physical elements into one view. Like VR, AR allows the exposure to stimuli which, due to various reasons, may not be suitable for real-life scenarios. As such, AR has proven itself to be a medium through which individuals suffering from specific phobia can be exposed safely to the object(s) of their fear, without the costs associated with programing complete VEs. Thus, ARET can offer an efficacious alternative to some less advantageous exposure-based therapies. Above and beyond presenting what has been accomplished in ARET, this paper covers some less well-known aspects of the history of AR, raises some ARET related issues, and proposes potential avenues to be followed. These include the type of measures to be used to qualify the user's experience in an augmented reality environment, the exclusion of certain AR-type functionalities from the definition of AR, as well as the potential use of ARET to treat non-small animal phobias, such as social phobia. {\textcopyright} 2014 Baus and Bouchard.},
author = {Baus, Oliver and Bouchard, St{\'{e}}phane},
doi = {10.3389/fnhum.2014.00112},
file = {:C\:/Users/tomis/Downloads/ThesisSubmission/CompletedStatmentsOfAuthorship/Moving_from_Virtual_Reality_Exposure-Based_Therapy.pdf:pdf},
issn = {16625161},
journal = {Frontiers in Human Neuroscience},
keywords = {Augmented reality,Exposure therapy,Phobia,Synthetic environments,Virtual reality},
number = {MAR},
pages = {1--15},
title = {{Moving from virtual reality exposure-based therapy to augmented reality exposure-based therapy: A review}},
volume = {8},
year = {2014}
}

@article{Abbey2021,
abstract = {Purpose: Three-dimensional “volumetric” imaging methods are now a common component of medical imaging across many imaging modalities. Relatively little is known about how human observers localize targets masked by noise and clutter as they scroll through a 3D image and how it compares to a similar task confined to a single 2D slice.
Approach: Gaussian random textures were used to represent noisy volumetric medical images. Subjects were able to freely inspect the images, including scrolling through 3D images as part of their search process. A total of eight experimental conditions were evaluated (2D versus 3D images, large versus small targets, power-law versus white noise). We analyze performance in these experiments using task efficiency and the classification image technique.
Results: In 3D tasks, median response times were roughly nine times longer than 2D, with larger relative differences for incorrect trials. The efficiency data show a dissociation in which subjects perform with higher statistical efficiency in 2D tasks for large targets and higher efficiency in 3D tasks with small targets. The classification images suggest that a critical mechanism behind this dissociation is an inability to integrate across multiple slices to form a 3D localization response. The central slices of 3D classification images are remarkably similar to the corresponding 2D classification images.
Conclusions: 2D and 3D tasks show similar weighting patterns between 2D images and the central slice of 3D images. There is relatively little weighting across slices in the 3D tasks, leading to lower task efficiency with respect to the ideal observer.},
author = {Abbey, Craig K. and Lago, Miguel A. and Eckstein, Miguel P.},
doi = {10.1117/1.jmi.8.4.041206},
file = {:C\:/Users/tomis/Downloads/ThesisSubmission/CompletedStatmentsOfAuthorship/041206_1.pdf:pdf},
issn = {23294310},
journal = {Journal of Medical Imaging},
keywords = {2,2020,2021,22,accepted for publication feb,classification images,localization tasks,observer templates,paper 20328ssr received dec,published online,volumetric imaging},
mendeley-groups = {3DVisulisationsVs2D},
number = {04},
pages = {1--17},
title = {{Comparative observer effects in 2D and 3D localization tasks}},
volume = {8},
year = {2021}
}

@article{Krauze2023,
abstract = {Precise perception of three-dimensional (3D) images is crucial for a rewarding experience when using novel displays. However, the capability of the human visual system to perceive binocular disparities varies across the visual field meaning that depth perception might be affected by the two-dimensional (2D) layout of items on the screen. Nevertheless, potential difficulties in perceiving 3D images during free viewing have received only a little attention so far, limiting opportunities to enhance visual effectiveness of information presentation. The aim of this study was to elucidate how the 2D layout of items in 3D images impacts visual search and distribution of maintaining attention based on the analysis of the viewer's gaze. Participants were searching for a target which was projected one plane closer to the viewer compared to distractors on a multi-plane display. The 2D layout of items was manipulated by changing the item distance from the center of the display plane from 2° to 8°. As a result, the targets were identified correctly when the items were displayed close to the center of the display plane, however, the number of errors grew with an increase in distance. Moreover, correct responses were given more often when subjects paid more attention to targets compared to other items on the screen. However, a more balanced distribution of attention over time across all items was characteristic of the incorrectly completed trials. Thus, our results suggest that items should be displayed close to each other in a 2D layout to facilitate precise perception of 3D images and considering distribution of attention maintenance based on eye-tracking might be useful in the objective assessment of user experience for novel displays},
author = {Krauze, Linda and Delesa-Velina, Mara and Pladere, Tatjana and Krumina, Gunta},
doi = {10.16910/JEMR.16.1.4},
file = {:C\:/Users/tomis/Downloads/ThesisSubmission/CompletedStatmentsOfAuthorship/jemr-16-01-d.pdf:pdf},
issn = {19958692},
journal = {Journal of Eye Movement Research},
keywords = {2D layout,3D image,area of interest,binocular disparity,depth perception,gaze distribution,item distance},
mendeley-groups = {3DVisulisationsVs2D},
number = {1},
pages = {1--12},
title = {{Why 2D layout in 3D images matters: evidence from visual search and eyetracking}},
volume = {16},
year = {2023}
}

@article{Ahlberg2007,
abstract = {BackgroundVirtual reality (VR) training has been shown previously to improve intraoperative performance during part of a laparoscopic cholecystectomy. The aim of this study was to assess the effect of proficiency-based VR training on the outcome of the first 10 entire cholecystectomies performed by novices.},
annote = {doi: 10.1016/j.amjsurg.2006.06.050},
author = {Ahlberg, Gunnar and Enochsson, Lars and Gallagher, Anthony G and Hedman, Leif and Hogman, Christian and {McClusky  III}, David A and Ramel, Stig and Smith, C Daniel and Arvidsson, Dag},
doi = {10.1016/j.amjsurg.2006.06.050},
issn = {0002-9610},
journal = {The American Journal of Surgery},
month = {jun},
number = {6},
pages = {797--804},
publisher = {Elsevier},
title = {{Proficiency-based virtual reality training significantly reduces the error rate for residents during their first 10 laparoscopic cholecystectomies}},
url = {https://doi.org/10.1016/j.amjsurg.2006.06.050},
volume = {193},
year = {2007}
}

@article{Zhang2016a,
author = {Zhang, Gang and Zhou, Xian-jun and Zhu, Cheng-zhan and Dong, Qian and Su, Lin},
doi = {10.1016/j.suronc.2016.05.023},
journal = {Surgical Oncology},
title = {{Usefulness of Three-dimensional(3D) simulation software in hepatectomy for pediatric hepatoblastoma}},
volume = {25},
year = {2016}
}

@article{Vetter2002,
abstract = {A substantial component of an image-guided surgery system (IGSS) is the kind of three-dimensional (3D) presentation to the surgeon because the visual depth perception of the complex anatomy is of significant relevance for orientation. Therefore, we examined for this contribution four different visualization techniques, which were evaluated by eight surgeons. The IGSS developed by our group supports the intraoperative orientation of the surgeon by presenting a visualization of the spatially tracked surgical instruments with respect to vitally important intrahepatic vessels, the tumor, and preoperatively calculated resection planes. In the preliminary trial presented here, we examined the human ability to perceive an intraoperative virtual scene and to solve given navigation tasks. The focus of the experiments was to measure the ability of eight surgeons to orientate themselves intrahepatically and to transfer the perceived virtual spatial relations to movements in real space. Withn auto-stereoscopic visualization making use of a prism-based display the navigation can be performed faster and more accurate than with the other visualization techniques.},
author = {Vetter, Marcus and Hassenpflug, Peter and Thorn, Matthias and Cardenas, Carlos and Grenacher, Lars and Richter, Goetz M. and Lamade, Wolfram and Herfarth, Christian and Meinzer, Hans-Peter},
doi = {10.1117/12.466920},
file = {:C\:/Users/tomis/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vetter et al. - 2002 - Superiority of autostereoscopic visualization for image-guided navigation in liver surgery.pdf:pdf},
journal = {Medical Imaging 2002: Visualization, Image-Guided Procedures, and Display},
mendeley-groups = {VoxelAlgorithms/VolumeRendering},
number = {May 2002},
pages = {196--203},
title = {{Superiority of autostereoscopic visualization for image-guided navigation in liver surgery}},
volume = {4681},
year = {2002}
}

@article{Asadi2024,
abstract = {Advancements in mixed reality (MR) have led to innovative approaches in image-guided surgery (IGS). In this paper, we provide a comprehensive analysis of the current state of MR in image-guided procedures across various surgical domains. Using the Data Visualization View (DVV) Taxonomy, we analyze the progress made since a 2013 literature review paper on MR IGS systems. In addition to examining the current surgical domains using MR systems, we explore trends in types of MR hardware used, type of data visualized, visualizations of virtual elements, and interaction methods in use. Our analysis also covers the metrics used to evaluate these systems in the operating room (OR), both qualitative and quantitative assessments, and clinical studies that have demonstrated the potential of MR technologies to enhance surgical workflows and outcomes. We also address current challenges and future directions that would further establish the use of MR in IGS.},
author = {Asadi, Zahra and Asadi, Mehrdad and Kazemipour, Negar and L{\'{e}}ger, {\'{E}}tienne and Kersten-Oertel, Marta},
doi = {10.1080/24699322.2024.2355897},
file = {:C\:/Users/tomis/Downloads/A decade of progress  bringing mixed reality image-guided surgery systems in the operating room.pdf:pdf},
issn = {24699322},
journal = {Computer Assisted Surgery},
keywords = {DVV Taxonomy,Image-guided surgery,augmented reality,mixed reality},
mendeley-groups = {VoxelAlgorithms/VolumeRendering},
number = {1},
pages = {--},
pmid = {38794834},
publisher = {Taylor \& Francis},
title = {{A decade of progress: bringing mixed reality image-guided surgery systems in the operating room}},
url = {https://doi.org/10.1080/24699322.2024.2355897},
volume = {29},
year = {2024}
}

@article{Dunn2018,
author = {Dunn, David and Dong, Qian and Fuchs, Henry and Chakravarthula, Praneeth},
doi = {10.1117/12.2314664},
file = {:C\:/Users/tomis/Downloads/Dunn-etAl_2018_DOID_MitigatingVergenceAccommodationConflictForNearEyeDisplaysViaDeformableBeamsplitters.pdf:pdf},
isbn = {9781510618787},
issn = {1996756X},
keywords = {augmented reality,deformable beamsplitters,vergence-accommodation conflict,wide field of view},
pages = {104},
title = {{Mitigating vergence-accommodation conflict for near-eye displays via deformable beamsplitters}},
year = {2018}
}

@article{Douglas–Peucker1973,
author = {DOUGLAS, DAVID H and PEUCKER, THOMAS K},
title = {ALGORITHMS FOR THE REDUCTION OF THE NUMBER OF POINTS REQUIRED TO REPRESENT A DIGITIZED LINE OR ITS CARICATURE},
journal = {Cartographica},
volume = {10},
number = {2},
pages = {112-122},
year = {1973},
doi = {10.3138/FM57-6770-U75U-7727},
URL = {https://doi.org/10.3138/FM57-6770-U75U-7727},
eprint = {https://doi.org/10.3138/FM57-6770-U75U-7727},
abstract = { All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization. }}

@article{Zollmann2012,
abstract = {Most civil engineering tasks require accessing, surveying and modifying geospatial data in the field and referencing this virtual, geospatial information to the real world situation. Augmented Reality (AR) can be a useful tool to create, edit and update geospatial data representing real world artifacts by interacting with the 3D graphical representation of the geospatial data augmented in the user's view. One of the main challenges of interactive AR visualizations of data from professional geographic information systems (GIS) is the establishment of a close linkage of comprehensible AR visualization and the geographic database that allows interactive modifications. In this paper, we address this challenge by introducing a flexible data management between GIS databases and AR visualizations that maintain data consistency between both data levels and consequently enables an interactive data roundtrip. The integration of our approach into a mobile AR platform enables us to perform first evaluations with expert end-users from utility companies. {\textcopyright} 2012 Springer-Verlag.},
author = {Zollmann, Stefanie and Schall, Gerhard and Junghanns, Sebastian and Reitmayr, Gerhard},
doi = {10.1007/978-3-642-33179-4_64},
file = {:C\:/Users/tomis/Downloads/978-3-642-33179-4_64.pdf:pdf},
isbn = {9783642331787},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {675--685},
title = {{Comprehensible and interactive visualizations of GIS data in augmented reality}},
volume = {7431 LNCS},
year = {2012}
}

@article{Gao2025,
abstract = {Extended reality (XR) devices must render high-quality 3D graphics at low latency to deliver truly immersive experiences. However, XR devices are severely power- and resource-constrained, limiting the quality of on-device (local) rendering. Offloading rendering to a powerful remote machine can enhance graphics quality, but network latency can degrade the overall experience. To mask latency, XR systems reproject the rendered frame to compensate for user motion since the rendered pose. Traditional reprojection, known as TimeWarp, uses a lightweight mechanism to compensate for latency in rotational motion, but not translational motion. Compensating for translational motion is more expensive, but is increasingly important at higher latencies.We present XRgo, the first open-source XR runtime that supports application-transparent offloading of rendering to a remote server with client-side six-degrees-of-freedom (6-DoF) reprojection. XRgo uses OpenWarp, the first open-source and OpenXR-compatible lightweight 6-DoF mesh-based reprojection technique. Compared to on-device rendering, XRgo consistently renders at the target frame rate while consuming less power. User studies show that on average, OpenWarp provides a superior XR experience than TimeWarp. Further, OpenWarp only consumes < 2% more power than TimeWarp. Our findings reveal that the quality of experience depends on both network average latency and variability, highlighting the need for end-to-end evaluations under live network conditions.},
author = {Gao, Steven and Liu, Jeffrey and Jiang, Qinjun and Sinclair, Finn and Sentosa, William and Godfrey, Brighten and Adve, Sarita},
doi = {10.1145/3712676.3714444},
file = {:C\:/Users/tomis/Downloads/3712676.3714444.pdf:pdf},
isbn = {9798400714672},
journal = {MMSys 2025 - Proceedings of the 16th ACM Multimedia Systems Conference},
keywords = {asynchronous reprojection,extended reality,low power,offloading,remote rendering,wireless networks},
pages = {124--135},
title = {{XRgo: Design and Evaluation of Rendering Offload for Low-Power Extended Reality Devices}},
year = {2025}
}

@article{Mejias2025,
abstract = {The increasing complexity of Extended Reality (XR) applications demands substantial processing power and high bandwidth communications, often unavailable on lightweight devices. Remote rendering consists of offloading processing tasks to a remote node with a powerful GPU, delivering the rendered content to the end device. The delivery is usually performed through popular streaming protocols such as Web Real-Time Communications (WebRTC), offering a data channel for interactions, or Dynamic Adaptive Streaming over HTTP (DASH), better suitable for scalability. Moreover, new streaming protocols based on QUIC are emerging as potential replacements for WebRTC and DASH and offer benefits like connection migration, stream multiplexing and multipath delivery. This work describes the integration of the two most popular multimedia frameworks, GStreamer and FFmpeg, with a rendering engine acting as a Remote Renderer, and analyzes their performance when offering different protocols for delivering the rendered content to the end device over WIFI or 5G. This solution constitutes a beyond state-of-the-art testbed to conduct cutting-edge research in the XR field.},
archivePrefix = {arXiv},
arxivId = {2507.00623},
author = {Mej{\'{i}}as, Daniel and Yeregui, Inhar and Viola, Roberto and Fern{\'{a}}ndez, Miguel and Montagud, Mario},
eprint = {2507.00623},
file = {:C\:/Users/tomis/Downloads/2507.00623v1.pdf:pdf},
title = {{Remote Rendering for Virtual Reality: performance comparison of multimedia frameworks and protocols}},
url = {http://arxiv.org/abs/2507.00623},
year = {2025}
}

@book{Petri2018,
abstract = {Virtual Reality (VR) has become common practice in the field of sports, but autonomous virtual environment (VE) systems, especially in fast reacting sports, are rare. The current study demonstrates the development of an autonomous character (AC) in karate kumite, which performs attacks against a freely moving, real athlete. The development of the AC consists of four steps: selection of relevant karate techniques, development of a decision system, creation of an animated model of the AC, and the evaluation. A Cave Automatic Virtual Environment (CAVE) and a Head Mounted Display (HMD) were chosen for the VE. The evaluation of the AC in the VEs was conducted by expert interviews (n = 6). The results reveal a feeling of comfort for all athletes in VR which underpins a high degree of realism in the VEs. Moreover, the HMDs are seen as more suitable than CAVEs for presenting a karate specific environment. Based on these results the developed AC seems applicable for anticipation research and training in karate kumite. The discussion includes further possible improvements for the AC as well as future directions for further investigations and training programs using the AC. Moreover, the procedure of the AC's creation can be transferred to other sports.},
author = {Petri, Katharina and Witte, Kerstin and Bandow, Nicole and Emmermacher, Peter and Masik, Steffen and Dannenberg, Marco and Salb, Simon and Zhang, Liang and Brunnett, Guido},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-67846-7_13},
file = {:C\:/Users/tomis/Downloads/978-3-319-67846-7.pdf:pdf},
isbn = {9783319678450},
issn = {21945357},
keywords = {Autonomous character,Karate kumite,Virtual reality},
number = {Iacss},
pages = {124--135},
title = {{Development of an Autonomous Character in Karate Kumite}},
volume = {663},
year = {2018}
}

@article{Wilson2002,
abstract = {The emerging viewpoint of embodied cognition holds that cognitive processes are deeply rooted in the body's interactions with the world. This position actually houses a number of distinct claims, some of which are more controversial than others. This paper distinguishes and evaluates the following six claims: (1) cognition is situated; (2) cognition is time-pressured; (3) we off-load cognitive work onto the environment; (4) the environment is part of the cognitive system; (5) cognition is for action; (6) offline cognition is body based. Of these, the first three and the fifth appear to be at least partially true, and their usefulness is best evaluated in terms of the range of their applicability. The fourth claim, I argue, is deeply problematic. The sixth claim has received the least attention in the literature on embodied cognition, but it may in fact be the best documented and most powerful of the six claims.},
author = {Wilson, Margaret},
doi = {10.3758/bf03196322},
file = {:C\:/Users/tomis/OneDrive/Documents/3DSBackup/BF03196322.pdf:pdf},
journal = {Psychometric Bulletin \& Review},
number = {4},
pages = {625--636},
pmid = {12613670},
title = {{Six views of embodied cognition.}},
volume = {9},
year = {2002}
}


@article{Brown1971,
abstract = {A photo test field consisted of a series of plumb lines whose images, because of their lack of straightness, permitted an analytical determination of lens distortion which, in addition to the usual concept, varies also with object distance.},
author = {Brown, Duane C},
file = {:C\:/Users/tomis/OneDrive/Documents/1971_aug_855-866.pdf:pdf},
journal = {Photogramm. Eng},
keywords = {August 1971,No. 8,PHOTOGRAMMETRIC ENGINEERING AND REMOTE SENSING,Vol. 37,pp. 855-866.},
mendeley-groups = {RadialDistortion},
number = {37},
pages = {855--866},
title = {{Close-Range Camera Calibration}},
volume = {8},
year = {1971}
}

@article{Liu2025,
abstract = {Regarding the distortion correction problem of large field of view wide-angle cameras commonly used in railway visual inspection systems, this paper proposes a novel online calibration method for non-specially made cooperative calibration objects. Based on the radial distortion divisor model, first, the spatial coordinates of natural spatial landmark points are constructed according to the known track gauge value between two parallel rails and the spacing value between sleepers. By using the image coordinate relationships corresponding to these spatial coordinates, the coordinates of the distortion center point are solved according to the radial distortion fundamental matrix. Then, a constraint equation is constructed based on the collinear constraint of vanishing points in railway images, and the Levenberg–Marquardt algorithm is used to found the radial distortion coefficients. Moreover, the distortion coefficients and the coordinates of the distortion center are re-optimized according to the least squares method (LSM) between points and the fitted straight line. Finally, based on the above, the distortion correction is carried out for the distorted railway images captured by the camera. The experimental results show that the above method can efficiently and accurately perform online distortion correction for large field of view wide-angle cameras used in railway inspection without the participation of specially made cooperative calibration objects. The whole method is simple and easy to implement, with high correction accuracy, and is suitable for the rapid distortion correction of camera images in railway online visual inspection.},
author = {Liu, Quanxin and Sun, Xiang and Peng, Yuanyuan},
doi = {10.3390/photonics12080767},
file = {:C\:/Users/tomis/OneDrive/Documents/photonics-12-00767-v2.pdf:pdf},
issn = {23046732},
journal = {Photonics},
keywords = {distortion correction,non-cooperative calibration,railway track detection,wide-angle camera},
mendeley-groups = {RadialDistortion},
number = {8},
pages = {767},
title = {{A Distortion Image Correction Method for Wide-Angle Cameras Based on Track Visual Detection}},
volume = {12},
year = {2025}
}

@article{Kang2001,
abstract = {In this paper, we address the problem of recovering the camera radial distortion coefficients from one image. The approach that we propose uses a special kind of snakes called radial distortion snakes. Radial distortion snakes behave like conventional deformable contours, except that their behavior are globally connected via a consistent model of image radial distortion. Experiments show that radial distortion snakes are more robust and accurate than conventional snakes and manual point selection.},
author = {Kang, Sing Bing},
file = {:C\:/Users/tomis/OneDrive/Documents/Radial_Distortion_Snakes.pdf:pdf},
issn = {09168532},
journal = {IEICE Transactions on Information and Systems},
keywords = {Calibration,Radial distortion,Snakes},
mendeley-groups = {RadialDistortion},
number = {12},
pages = {1603--1611},
title = {{Radial distortion snakes}},
volume = {E84-D},
year = {2001}
}


@article{Li2012a, 
abstract = {This book is written for behavioral scientists who want to consider adding R to their existing set of statistical tools, or want to switch to R as their main computation tool. The authors aim primarily to help practitioners of behavioral research make the transition to R. The focus is to provide practical advice on some of the widely-used statistical methods in behavioral research, using a set of notes and annotated examples. The book will also help beginners learn more about statistics and behavioral research. These are statistical techniques used by psychologists who do research on human subjects, but of course they are also relevant to researchers in others fields that do similar kinds of research. The authors emphasize practical data analytic skills so that they can be quickly incorporated into readers' own research.},
author = {Li, Yuelin and Baron, Jonathan},
doi = {10.1007/978-1-4614-1238-0},
file = {:C\:/Users/tomis/OneDrive/Documents/978-1-4614-1238-0_10.pdf:pdf},
isbn = {9781461412380},
journal = {Behavioral Research Data Analysis with R},
mendeley-groups = {Statistics},
number = {1973},
pages = {1--245},
title = {{Behavioral research data analysis with R}},
year = {2012}
}


@article{Meteyard2020,
abstract = {The use of Linear Mixed-effects Models (LMMs) is set to dominate statistical analyses in psychological science and may become the default approach to analyzing quantitative data. The rapid growth in adoption of LMMs has been matched by a proliferation of differences in practice. Unless this diversity is recognized, and checked, the field shall reap enormous difficulties in the future when attempts are made to consolidate or synthesize research findings. Here we examine this diversity using two methods – a survey of researchers (n = 163) and a quasi-systematic review of papers using LMMs (n = 400). The survey reveals substantive concerns among psychologists using or planning to use LMMs and an absence of agreed standards. The review of papers complements the survey, showing variation in how the models are built, how effects are evaluated and, most worryingly, how models are reported. Using these data as our departure point, we present a set of best practice guidance, focusing on the reporting of LMMs. It is the authors' intention that the paper supports a step-change in the reporting of LMMs across the psychological sciences, preventing a trajectory in which findings reported today cannot be transparently understood and used tomorrow.},
author = {Meteyard, Lotte and Davies, Robert A.I.},
doi = {10.1016/j.jml.2020.104092},
file = {:C\:/Users/tomis/OneDrive/Documents/1-s2.0-S0749596X20300061-main.pdf:pdf},
issn = {0749596X},
journal = {Journal of Memory and Language},
keywords = {Hierarchical models,Linear mixed effects models,Multilevel models},
mendeley-groups = {Statistics},
number = {January},
pages = {104092},
publisher = {Elsevier},
title = {{Best practice guidance for linear mixed-effects models in psychological science}},
url = {https://doi.org/10.1016/j.jml.2020.104092},
volume = {112},
year = {2020}
}









